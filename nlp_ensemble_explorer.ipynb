{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  Copyright (c) 2019 Regents of the University of Minnesota.\n",
    " \n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    " \n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gevent\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pymysql\n",
    "import time \n",
    "import functools as ft\n",
    "import glob, os   \n",
    "import operator as op\n",
    "import shelve\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from pathlib import Path\n",
    "from itertools import combinations, product, permutations\n",
    "from sqlalchemy.engine import create_engine\n",
    "from datetime import datetime\n",
    "from ast import literal_eval\n",
    "from scipy import stats  \n",
    "from scipy.stats.mstats import gmean\n",
    "from pythonds.basic.stack import Stack\n",
    "from pythonds.trees.binaryTree import BinaryTree\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "from typing import List, Set, Tuple "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The cell below contains the configurable parameters to ensure that our ensemble explorer runs properaly on your machine. \n",
    "Please read carfully through steps (1-11) before running the rest of the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-1: CHOOSE YOUR CORPUS\n",
    "# TODO: get working with list of corpora\n",
    "#corpora = ['mipacq','i2b2','fairview'] #options for concept extraction include 'fairview', 'mipacq' OR 'i2b2'\n",
    "\n",
    "corpus = 'fairview'\n",
    "#corpora = ['i2b2','fairview']\n",
    "\n",
    "# STEP-2: CHOOSE YOUR DATA DIRECTORY; this is where output data will be saved on your machine\n",
    "data_directory = '/Users/gms/development/nlp/nlpie/data/ensembling-u01/output/' \n",
    "\n",
    "# STEP-3: CHOOSE WHICH SYSTEMS YOU'D LIKE TO EVALUATE AGAINST THE CORPUS REFERENCE SET\n",
    "systems = ['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls']\n",
    "#systems = ['biomedicus', 'clamp']\n",
    "#systems = ['biomedicus', 'clamp']\n",
    "#systems = ['clamp', 'quick_umls', 'biomedicus']\n",
    "\n",
    "# STEP-4: CHOOSE TYPE OF RUN\n",
    "rtype = 4      # OPTIONS INCLUDE: 1->Single systems; 2->Ensemble; 3->Tests; 4 -> majority vote \n",
    "               # The Ensemble can include the max system set ['ctakes','biomedicus','clamp','metamap','quick_umls']\n",
    "    \n",
    "# STEP-5: CHOOSE WHAT TYPE OF ANALYSIS YOU'D LIKE TO RUN ON THE CORPUS\n",
    "analysis_type = 'entity' #options include 'entity' OR 'full'\n",
    "\n",
    "# STEP-(6A): ENTER DETAILS FOR ACCESSING MANUAL ANNOTATION DATA\n",
    "database_type = 'mysql+pymysql' # We use mysql+pymql as default\n",
    "database_username = 'gms'\n",
    "database_password = 'nej123' \n",
    "database_url = 'localhost' # HINT: use localhost if you're running database on your local machine\n",
    "database_name = 'concepts' # Enter database name\n",
    "\n",
    "def ref_data(corpus):\n",
    "    return corpus + '_all' # Enter the table within the database where your reference data is stored\n",
    "\n",
    "table_name = ref_data(corpus)\n",
    "\n",
    "# STEP-(6B): ENTER DETAILS FOR ACCESSING SYSTEM ANNOTATION DATA\n",
    "\n",
    "def sys_data(corpus):\n",
    "    return 'analytical_'+corpus+'.csv' # OPTIONS include 'analytical_cui_mipacq_concepts.csv' OR 'analytical_cui_i2b2_concepts.csv' \n",
    "\n",
    "system_annotation = sys_data(corpus)\n",
    "\n",
    "# STEP-7: WE'LL CREATE A 'SYSTEM OUTPUT' DIRECTORY FOR YOU INSIDE THE DIRECTORY YOU SPECIFIED IN (STEP 2)\n",
    "single_sys_dir = Path(data_directory + \"single_system_out\")\n",
    "single_sys_dir.mkdir(parents=True, exist_ok=True)\n",
    "dir_out = Path(data_directory + 'single_system_out/')\n",
    "\n",
    "# STEP-8: CREATE A DB CONNECTION POOL\n",
    "engine_request = str(database_type)+'://'+database_username+':'+database_password+\"@\"+database_url+'/'+database_name\n",
    "engine = create_engine(engine_request, pool_pre_ping=True, pool_size=20, max_overflow=30)\n",
    "\n",
    "# STEP-(9A): FILTER BY SEMTYPE\n",
    "filter_semtype = True\n",
    "\n",
    "# STEP-(9B): IF STEP-(9A) == True -> GET REFERENCE SEMTYPES\n",
    "\n",
    "### Fairview -> ['drug', 'finding', 'anatomy', 'procedure']\n",
    "### i2b2 -> ['test, treatment', 'problem']\n",
    "### MiPACQ -> ['procedures', 'disorders, sign_symptom', 'anatomy', 'chemicals_and_drugs']\n",
    "\n",
    "# semtypes = ['Anatomy']\n",
    "def ref_semtypes(filter_semtype, corpus):\n",
    "    if filter_semtype:\n",
    "        if corpus == 'fairview':\n",
    "            semtypes = ['Drug', 'Finding', 'Anatomy', 'Procedure']\n",
    "        elif corpus == 'i2b2':\n",
    "            semtypes = ['test,treatment', 'problem']\n",
    "        elif corpus == 'mipacq':\n",
    "            semtypes = ['Procedures', 'Disorders,Sign_Symptom', 'Anatomy', 'Chemicals_and_drugs']\n",
    "        \n",
    "        return semtypes\n",
    "\n",
    "semtypes = ref_semtypes(filter_semtype, corpus)\n",
    "\n",
    "# STEP-10: Set data directory/table for source documents for vectorization\n",
    "src_table = 'sofa'\n",
    "\n",
    "# STEP-11: Specificy match type from {'exact', 'overlap'}\n",
    "run_type = 'overlap'\n",
    "\n",
    "# STEP-12: Specify expression type for run (TODO: run all at once; make less kludgey)\n",
    "expression_type = 'nested' # type of merge expression: nested ((A&B)|C), pair ((A&B)|(C&D)), nested_with_singleton ((A&B)|((C&D)|E)) \n",
    "# -> NB: len(systems) for pair must be >= 4, and for nested_with_singleton == 5\n",
    "\n",
    "# STEP-13: Specify type of ensemble: merge or vote\n",
    "ensemble_type = 'vote'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "****** TODO \n",
    "-> add 'semtype' to output file name -> done\n",
    "-> filter for case when a system has no equivalent semtypes for a given reference -> done: still need to validate that all semtypes in corpus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if filter_semtype:\n",
    "#    print(semtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config class for analysis\n",
    "class AnalysisConfig():\n",
    "    \"\"\"\n",
    "    Configuration object:\n",
    "    systems to use\n",
    "    notes by corpus\n",
    "    paths by output, gold and system location\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self = self    \n",
    "        self.systems = systems\n",
    "        self.data_dir = data_directory\n",
    "    \n",
    "    def corpus_config(self): \n",
    "        usys_data = system_annotation\n",
    "        ref_data = database_name+'.'+table_name\n",
    "        return usys_data, ref_data\n",
    "\n",
    "analysisConf =  AnalysisConfig()\n",
    "#usys, ref = analysisConf.corpus_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticTypes(object):\n",
    "    '''\n",
    "    Filter semantic types based on: https://metamap.nlm.nih.gov/SemanticTypesAndGroups.shtml\n",
    "    :params: semtypes list from corpus, system to query\n",
    "    :return: list of equivalent system semtypes \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, semtypes, corpus):\n",
    "        self = self\n",
    "        \n",
    "        sql = \"SELECT st.tui, abbreviation, clamp_name, ctakes_name FROM concepts.semantic_groups sg join semantic_types st on sg.tui = st.tui where \" + corpus + \"_name in ({})\"\\\n",
    "           .format(', '.join(['%s' for _ in semtypes]))  \n",
    "        \n",
    "        stypes = pd.read_sql(sql, params=[semtypes], con=engine) \n",
    "       \n",
    "        if len(stypes['tui'].tolist()) > 0:\n",
    "            self.biomedicus_types = set(stypes['tui'].tolist())\n",
    "            self.qumls_types = set(stypes['tui'].tolist())\n",
    "        else:\n",
    "            self.biomedicus_types = None\n",
    "            self.qumls_types = None\n",
    "        \n",
    "        if stypes['clamp_name'].dropna(inplace=True) or len(stypes['clamp_name']) == 0:\n",
    "            self.clamp_types = None\n",
    "        else:\n",
    "            self.clamp_types = set(stypes['clamp_name'].tolist()[0].split(','))\n",
    "            \n",
    "        if len(stypes['ctakes_name'].tolist()) > 0:\n",
    "            self.ctakes_types = set(stypes['ctakes_name'].tolist()[0].split(','))\n",
    "        else:\n",
    "            self.ctakes_types = None\n",
    "            \n",
    "        if len(stypes['abbreviation'].tolist()) > 0:\n",
    "            self.metamap_types = set(stypes['abbreviation'].tolist())\n",
    "        else:\n",
    "            self.metamap_types = None\n",
    "            \n",
    "        self.reference_types =  set(semtypes)\n",
    "    \n",
    "    def get_system_type(self, system):  \n",
    "        \n",
    "        if system == 'biomedicus':\n",
    "            semtypes = self.biomedicus_types\n",
    "        elif system == 'ctakes':\n",
    "            semtypes = self.ctakes_types\n",
    "        elif system == 'clamp':\n",
    "            semtypes = self.clamp_types\n",
    "        elif system == 'metamap':\n",
    "            semtypes = self.metamap_types\n",
    "        elif system == 'quick_umls':\n",
    "            semtypes = self.qumls_types\n",
    "        elif system == 'reference':\n",
    "            semtypes = self.reference_types\n",
    "            \n",
    "        return semtypes\n",
    "    \n",
    "# print(SemanticTypes(['Drug'], corpus).get_system_type('biomedicus'))\n",
    "# print(SemanticTypes(['Drug'], corpus).get_system_type('quick_umls'))\n",
    "# print(SemanticTypes(['Anatomy'], corpus).get_system_type('clamp'))\n",
    "#print(SemanticTypes(['test,treatment'], 'i2b2').get_system_type('clamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semtypes = ['test,treatment']\n",
    "# semtypes = ['problem']\n",
    "# corpus = 'i2b2'\n",
    "# sys = 'clamp'\n",
    "\n",
    "# is semantic type in particular system\n",
    "def system_semtype_check(sys, semtype, corpus):\n",
    "    st = SemanticTypes([semtype], corpus).get_system_type(sys)\n",
    "    if st:\n",
    "        return sys\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# print(system_semtype_check(sys, semtypes, corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation class for systems\n",
    "class AnnotationSystems():\n",
    "    \"\"\"   \n",
    "    System annotations of interest for UMLS concept extraction\n",
    "    NB: ctakes combines all \"mentions\" annotation types\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\" \n",
    "        annotation base types\n",
    "        \"\"\"   \n",
    "        \n",
    "        self.biomedicus_types = [\"biomedicus.v2.UmlsConcept\"]\n",
    "        self.clamp_types = [\"edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA\"]\n",
    "        self.ctakes_types = [\"ctakes_mentions\"]\n",
    "        self.metamap_types = [\"org.metamap.uima.ts.Candidate\"]\n",
    "        self.qumls_types = [\"concept_jaccard_score_False\"]\n",
    "       \n",
    "    def get_system_type(self, system):\n",
    "        \n",
    "        \"\"\"\n",
    "        return system types\n",
    "        \"\"\"\n",
    "        \n",
    "        if system == \"biomedicus\":\n",
    "            view = \"Analysis\"\n",
    "        else:\n",
    "            view = \"_InitialView\"\n",
    "\n",
    "        if system == 'biomedicus':\n",
    "            types = self.biomedicus_types\n",
    "\n",
    "        elif system == 'clamp':\n",
    "            types = self.clamp_types\n",
    "\n",
    "        elif system == 'ctakes':\n",
    "            types = self.ctakes_types\n",
    "\n",
    "        elif system == 'metamap':\n",
    "            types = self.metamap_types\n",
    "        \n",
    "        elif system == \"quick_umls\":\n",
    "            types = self.qumls_types\n",
    "            \n",
    "        return types, view\n",
    "    \n",
    "annSys = AnnotationSystems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np # access to Numpy from Python layer\n",
    "import math\n",
    "\n",
    "class Metrics(object):\n",
    "    \"\"\"\n",
    "    metrics class:\n",
    "    returns an instance with confusion matrix metrics\n",
    "    \"\"\"\n",
    "    def __init__(self, system_only, gold_only, gold_system_match, system_n, neither = 0): # neither: no sys or manual annotation\n",
    "\n",
    "        self = self    \n",
    "        self.system_only = system_only\n",
    "        self.gold_only = gold_only\n",
    "        self.gold_system_match = gold_system_match\n",
    "        self.system_n = system_n\n",
    "        self.neither = neither\n",
    "        \n",
    "    def get_confusion_metrics(self, corpus = None, test = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        compute confusion matrix measures, as per  \n",
    "        https://stats.stackexchange.com/questions/51296/how-do-you-calculate-precision-and-recall-for-multiclass-classification-using-co\n",
    "        \"\"\"\n",
    "        cdef:\n",
    "            int TP, FP, FN\n",
    "            double TM\n",
    "\n",
    "        TP = self.gold_system_match\n",
    "        FP = self.system_only\n",
    "        FN = self.gold_only\n",
    "        \n",
    "        TM = TP/math.sqrt(self.system_n) # TigMetric\n",
    "       \n",
    "        if not test:\n",
    "            \n",
    "            if corpus == 'casi':\n",
    "                recall = TP/(TP + FN)\n",
    "                precision = TP/(TP + FP)\n",
    "                F = 2*(precision*recall)/(precision + recall)\n",
    "            else:\n",
    "                if self.neither == 0:\n",
    "                    confusion = [[0, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "                else:\n",
    "                    confusion = [[self.neither, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "                c = np.asarray(confusion)\n",
    "                recall = np.diag(c) / np.sum(c, axis = 1)\n",
    "                precision = np.diag(c) / np.sum(c, axis = 0)\n",
    "                F = 2*(precision*recall)/(precision + recall)\n",
    "        else:\n",
    "            precision = TP/(TP+FP)\n",
    "            recall = TP/(TP+FN)\n",
    "            F = 2*(precision*recall)/(precision + recall)\n",
    "        \n",
    "        # Tignanelli Metric\n",
    "        if FN == 0:\n",
    "            TP_FN_R = TP\n",
    "        elif FN > 0:\n",
    "            TP_FN_R = TP/FN\n",
    " \n",
    "        return F, recall, precision, TP, FP, FN, TP_FN_R, TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out(name: str, analysis_type: str, c: object):\n",
    "   \n",
    "    \"\"\"\n",
    "    write matching and reference-only sets to file for use in merging combinations:\n",
    "    exact match only\n",
    "    \"\"\"\n",
    "    \n",
    "    # write output to file\n",
    "    dir_out = analysisConf.data_dir + 'single_system_out/'\n",
    "    with open(dir_out + name + '_' + analysis_type + '_' + c.corpus + '_matches.txt', 'w') as f:\n",
    "        for item in list(c.matches):\n",
    "            f.write(\"%s\\n\" % str(item))\n",
    "\n",
    "    # write to file\n",
    "    with open(dir_out + name + '_' + analysis_type + '_' + c.corpus + '_ref_only.txt', 'w') as f:\n",
    "        for item in list(c.false_negatives):\n",
    "            f.write(\"%s\\n\" % str(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_set(df, analysis_type = 'entity', df_type = 'sys', corpus = None):\n",
    "    \n",
    "    # get values for creation of series of type tuple\n",
    "    if 'entity' in analysis_type: \n",
    "        if corpus == 'casi':\n",
    "            arg = df.case, df.overlap\n",
    "        else:    \n",
    "            if df_type == 'sys':\n",
    "                arg = df.begin, df.end, df.note_id\n",
    "            else:\n",
    "                arg = df.start, df.end, df.file\n",
    "            \n",
    "    elif 'cui' in analysis_type:\n",
    "        if df_type == 'sys':\n",
    "            arg = df.cui, df.note_id\n",
    "        else:\n",
    "            arg = df.value, df.file\n",
    "    elif 'full' in analysis_type:\n",
    "        if df_type == 'sys':\n",
    "            arg = df.begin, df.end, df.cui, df.note_id\n",
    "        else:\n",
    "            arg = df.start, df.end, df.value, df.file\n",
    "    \n",
    "    return set(list(zip(*arg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython \n",
    "\n",
    "from __main__ import write_out, df_to_set, engine\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "def get_cooccurences(ref, sys, analysis_type: str, corpus: str, single_sys = True, name = None):\n",
    "    \"\"\"\n",
    "    get coocurences between system and reference; exact match; TODO: add relaxed -> done in single system evals during ensemble run\n",
    "    \"\"\"\n",
    "    # cooccurences\n",
    "    class Coocurences(object):\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.ref_system_match = 0\n",
    "            self.ref_only = 0\n",
    "            self.system_only = 0\n",
    "            self.system_n = 0\n",
    "            self.ref_n = 0\n",
    "            self.matches = set()\n",
    "            self.false_negatives = set()\n",
    "            self.corpus = corpus\n",
    "\n",
    "    c = Coocurences()\n",
    "    \n",
    "    if c.corpus != 'casi':\n",
    "        if 'entity' in analysis_type and single_sys: \n",
    "            cols_to_keep = ['begin', 'end', 'note_id']\n",
    "            sys = sys[cols_to_keep].drop_duplicates()\n",
    "            ref = ref[['start', 'end', 'file']].drop_duplicates()\n",
    "            sys.name = name\n",
    "        elif 'cui' in analysis_type and single_sys: \n",
    "            cols_to_keep = ['cui', 'note_id']\n",
    "            sys = sys[cols_to_keep].drop_duplicates()\n",
    "            # do not overestimate FP\n",
    "            sys = sys[~sys['cui'].isnull()] \n",
    "            ref = ref[['value', 'file']].drop_duplicates()\n",
    "            ref = ref[~ref['value'].isnull()]\n",
    "            sys.name = name\n",
    "        elif 'full' in analysis_type and single_sys: \n",
    "            cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "            sys = sys[cols_to_keep].drop_duplicates()\n",
    "            sys = sys[~sys['cui'].isnull()]\n",
    "            ref = ref[['start', 'end', 'value', 'file']].drop_duplicates()\n",
    "            ref = ref[~ref['value'].isnull()]\n",
    "            sys.name = name\n",
    "\n",
    "        # matches via inner join\n",
    "        matches = pd.merge(sys, ref, how = 'inner', left_on=['begin','end','note_id'], right_on = ['start','end','file']) \n",
    "        # reference-only via left outer join\n",
    "        fn = pd.merge(ref, sys, how = 'left', left_on=['start','end','file'], right_on = ['begin','end','note_id']) \n",
    "\n",
    "        fn = fn[fn['begin'].isnull()] # get as outer join with no match\n",
    "\n",
    "        if 'entity' in analysis_type:\n",
    "            cols_to_keep = ['start', 'end', 'file']\n",
    "        else:\n",
    "            cols_to_keep = ['start', 'end', 'value', 'file']\n",
    "\n",
    "        matches = matches[cols_to_keep]\n",
    "        fn = fn[cols_to_keep]\n",
    "\n",
    "        # use for metrics \n",
    "        c.matches = c.matches.union(df_to_set(matches, analysis_type, 'ref'))\n",
    "        c.false_negatives = c.false_negatives.union(df_to_set(fn, analysis_type, 'ref'))\n",
    "        c.ref_system_match = len(c.matches)\n",
    "        c.system_only = len(sys) - len(c.matches)\n",
    "        c.system_n = len(sys)\n",
    "        c.ref_n = len(ref)\n",
    "        c.ref_only = len(c.false_negatives)\n",
    "        \n",
    "    else:\n",
    "        sql = \"select `case` from test.amia_2019_analytical_v where overlap = 1 and `system` = %(sys.name)s\"  \n",
    "        \n",
    "        matches = pd.read_sql(sql, params={\"sys.name\":sys.name}, con=engine)\n",
    "        \n",
    "        sql = \"select `case` from test.amia_2019_analytical_v where (overlap = 0 or overlap is null) and `system` = %(sys.name)s\"  \n",
    "        \n",
    "        fn = pd.read_sql(sql, params={\"sys.name\":sys.name}, con=engine)\n",
    "        \n",
    "        c.matches = df_to_set(matches, 'entity', 'sys', 'casi')\n",
    "        c.fn = df_to_set(fn, 'entity', 'sys', 'casi')\n",
    "        c.ref_system_match = len(c.matches)\n",
    "        c.system_only = len(sys) - len(c.matches)\n",
    "        c.system_n = len(matches) + len(fn)\n",
    "        c.ref_n = len(matches) + len(fn)\n",
    "        c.ref_only = len(fn)\n",
    "        \n",
    "    # sanity check\n",
    "    if len(ref) - c.ref_system_match < 0:\n",
    "        print('Error: ref_system_match > len(ref)!')\n",
    "    if len(ref) != c.ref_system_match + c.ref_only:\n",
    "        print('Error: ref count mismatch!')\n",
    "   \n",
    "    # save TP/FN\n",
    "    if single_sys and corpus != 'casi':\n",
    "        print(analysis_type)\n",
    "        write_out(sys.name, analysis_type, c)\n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_vector(doc: str, ann: List[int], labels: List[str]) -> np.array:\n",
    "\n",
    "    v = np.zeros(doc)\n",
    "    labels = list(labels)\n",
    "    \n",
    "    for (i, lab) in enumerate(labels):\n",
    "        i += 1  # 0 is reserved for no label\n",
    "        idxs = [np.arange(a.begin, a.end) for a in ann if a.label == lab]\n",
    "        idxs = [j for mask in idxs for j in mask]\n",
    "        v[idxs] = i\n",
    "\n",
    "    return v\n",
    "\n",
    "# test confusion matrix elements for vectorized annotation set; includes TN\n",
    "# https://kawahara.ca/how-to-compute-truefalse-positives-and-truefalse-negatives-in-python-for-binary-classification-problems/\n",
    "def confused(sys1, ann1):\n",
    "    TP = np.sum(np.logical_and(ann1 == 1, sys1 == 1))\n",
    "\n",
    "    # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "    TN = np.sum(np.logical_and(ann1 == 0, sys1 == 0))\n",
    "\n",
    "    # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "    FP = np.sum(np.logical_and(ann1 == 0, sys1 == 1))\n",
    "\n",
    "    # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "    FN = np.sum(np.logical_and(ann1 == 1, sys1 == 0))\n",
    "    \n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def vectorized_coocurences(r: object, analysis_type: str, corpus: str, filter_semtype, semtype = None) -> np.int64:\n",
    "    docs = get_docs(corpus)\n",
    "    if filter_semtype:\n",
    "        ann = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "    else: \n",
    "        ann = get_ref_ann(analysis_type, corpus, filter_semtype)\n",
    "        \n",
    "    sys = get_sys_ann(r)\n",
    "    cvals = []\n",
    "    labels = [\"concept\"]\n",
    "\n",
    "    for n in range(len(docs)):\n",
    "        a1 = list(ann.loc[ann[\"case\"] == docs[n][0]].itertuples(index=False))\n",
    "        s1 = list(sys.loc[sys[\"case\"] == docs[n][0]].itertuples(index=False))\n",
    "        ann1 = label_vector(docs[n][1], a1, labels)\n",
    "        sys1 = label_vector(docs[n][1], s1, labels)\n",
    "\n",
    "        TP, TN, FP, FN = confused(sys1, ann1)\n",
    "        cvals.append([TP, TN, FP, FN])\n",
    "\n",
    "    return np.sum(cvals, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm_dict(ref_only: int, system_only: int, ref_system_match: int, system_n: int, ref_n: int) -> dict:\n",
    "    \"\"\"\n",
    "    Generate dictionary of confusion matrix params and measures\n",
    "    :params: ref_only, system_only, reference_system_match -> sets\n",
    "    matches, system_n, reference_n -> counts\n",
    "    :return: dictionary object\n",
    "    \"\"\"\n",
    "\n",
    "    if ref_only + ref_system_match != ref_n:\n",
    "        print('ERROR!')\n",
    "\n",
    "    # get evaluation metrics\n",
    "    F, recall, precision, TP, FP, FN, TP_FN_R, TM  = Metrics(system_only, ref_only, ref_system_match, system_n).get_confusion_metrics()\n",
    "\n",
    "    d = {\n",
    "         'F': F[1], \n",
    "         'precision': precision[1], \n",
    "         'recall': recall[1], \n",
    "         'TP': TP, \n",
    "         'FN': FN, \n",
    "         'FP': FP, \n",
    "         'TP/FN': TP_FN_R,\n",
    "         'n_gold': ref_n, \n",
    "         'n_sys': system_n, \n",
    "         'TM': TM\n",
    "    }\n",
    "    \n",
    "    if system_n - FP != TP:\n",
    "        print('inconsistent system n!')\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ft.lru_cache(maxsize=None)\n",
    "def get_metric_data(analysis_type: str, corpus: str):\n",
    "   \n",
    "    usys_file, ref_table = AnalysisConfig().corpus_config()\n",
    "    systems = AnalysisConfig().systems\n",
    "    \n",
    "    sys_ann = pd.read_csv(analysisConf.data_dir + usys_file, dtype={'note_id': str})\n",
    "    \n",
    "    sql = \"SELECT * FROM \" + ref_table  \n",
    "    \n",
    "    ref_ann = pd.read_sql(sql, con=engine)\n",
    "    sys_ann = sys_ann.drop_duplicates()\n",
    "    \n",
    "    return ref_ann, sys_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "def geometric_mean(metrics):\n",
    "    \"\"\"\n",
    "    1. Get rank average of F1, TP/FN, TM\n",
    "        http://www.datasciencemadesimple.com/rank-dataframe-python-pandas-min-max-dense-rank-group/\n",
    "        https://stackoverflow.com/questions/46686315/in-pandas-how-to-create-a-new-column-with-a-rank-according-to-the-mean-values-o?rq=1\n",
    "    2. Take geomean of 2.\n",
    "        https://stackoverflow.com/questions/42436577/geometric-mean-applied-on-row\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.DataFrame() \n",
    "\n",
    "    metrics['F1 rank']=metrics['F'].rank(ascending=0,method='average')\n",
    "    metrics['TP/FN rank']=metrics['TP/FN'].rank(ascending=0,method='average')\n",
    "    metrics['TM rank']=metrics['TM'].rank(ascending=0,method='average')\n",
    "    metrics['Gmean'] = gmean(metrics.iloc[:,-3:],axis=1)\n",
    "\n",
    "    return metrics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metrics(analysis_type: str, corpus: str, filter_semtype, single_sys = None):\n",
    "    start = time.time()\n",
    "\n",
    "    systems = AnalysisConfig().systems\n",
    "    metrics = pd.DataFrame()\n",
    "\n",
    "    ref_ann, sys_ann = get_metric_data(analysis_type, corpus)\n",
    "    \n",
    "    for sys in systems:\n",
    "        \n",
    "        if filter_semtype:\n",
    "            st = SemanticTypes(semtypes, corpus).get_system_type(sys)\n",
    "            ref_ann = ref_ann[ref_ann['semtype'].isin(SemanticTypes(semtypes, corpus).get_system_type('reference'))]\n",
    "            \n",
    "        types, _ = AnnotationSystems().get_system_type(sys) # system types for iterable\n",
    "        for t in types:\n",
    "            print(t)\n",
    "\n",
    "            if filter_semtype:\n",
    "                system_annotations = sys_ann[sys_ann['semtypes'].isin(st)].copy()\n",
    "            else:\n",
    "                system_annotations = sys_ann.copy()\n",
    "\n",
    "            system = system_annotations[system_annotations['type'] == str(t)]\n",
    "\n",
    "            if sys == 'quick_umls':\n",
    "                system = system[system.score.astype(float) >= .8]\n",
    "\n",
    "            if sys == 'metamap':\n",
    "                system = system[system.score.abs().astype(int) >= 800]\n",
    "\n",
    "            system = system.drop_duplicates()\n",
    "            system.name = sys\n",
    "\n",
    "            c = get_cooccurences(ref_ann, system, analysis_type, corpus, True, system.name) # get matches, FN, etc.\n",
    "\n",
    "        if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "            if corpus == 'casi':\n",
    "                if sys == 'biomedicus':\n",
    "                    t = 'biomedicus.v2.Acronym'\n",
    "            \n",
    "            # get dictionary of confusion matrix metrics\n",
    "            d = cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n)\n",
    "            d['system'] = sys\n",
    "            d['type'] = t\n",
    "                \n",
    "            data = pd.DataFrame(d,  index=[0])\n",
    "            metrics = pd.concat([metrics, data], ignore_index=True)\n",
    "            metrics.drop_duplicates(keep='last', inplace=True)\n",
    "        else:\n",
    "            print(\"NO EXACT MATCHES FOR\", t)\n",
    "        elapsed = (time.time() - start)\n",
    "        print(\"elapsed:\", sys, elapsed)\n",
    "     \n",
    "    elapsed = (time.time() - start)\n",
    "    print(geometric_mean(metrics))\n",
    "    \n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    \n",
    "    if single_sys is None:\n",
    "        file_name = 'metrics_'\n",
    "    \n",
    "    metrics.to_csv(analysisConf.data_dir + corpus + '_' + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "    \n",
    "    print(\"total elapsed time:\", elapsed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ft.lru_cache(maxsize=None)\n",
    "def get_ref_n(analysis_type: str, corpus: str, filter_semtype) -> int:\n",
    "    \n",
    "    ref_ann, _ = get_metric_data(analysis_type, corpus)\n",
    "    \n",
    "    if filter_semtype:\n",
    "        ref_ann = ref_ann[ref_ann['semtype'].isin(SemanticTypes(semtypes, corpus).get_system_type('reference'))]\n",
    "            \n",
    "    if corpus == 'casi':\n",
    "        return len(ref_ann)\n",
    "        \n",
    "    else:\n",
    "        # do not overestimate fn\n",
    "        if 'entity' in analysis_type:\n",
    "            ref_ann = ref_ann[['start', 'end', 'file']].drop_duplicates()\n",
    "        elif 'cui' in analysis_type:\n",
    "            ref_ann = ref_ann[['value', 'file']].drop_duplicates()\n",
    "        elif 'full' in analysis_type:\n",
    "            ref_ann = ref_ann[['start', 'end', 'value', 'file']].drop_duplicates()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        ref_n = len(ref_ann.drop_duplicates())\n",
    "\n",
    "        return ref_n\n",
    "    \n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_sys_data(system: str, analysis_type: str, corpus: str, filter_semtype, semtype = None) -> pd.DataFrame:\n",
    "   \n",
    "    _, data = get_metric_data(analysis_type, corpus)\n",
    "    \n",
    "    out = data[data['system'] == system].copy()\n",
    "    \n",
    "    if filter_semtype:\n",
    "        st = SemanticTypes([semtype], corpus).get_system_type(system)\n",
    "    \n",
    "    if corpus == 'casi':\n",
    "        cols_to_keep = ['case', 'overlap'] \n",
    "        out = out[cols_to_keep].drop_duplicates()\n",
    "        return out\n",
    "        \n",
    "    else:\n",
    "        if filter_semtype:\n",
    "            out = out[out['semtypes'].isin(st)].copy()\n",
    "            \n",
    "        else:\n",
    "            out = out[out['system']== system].copy()\n",
    "        \n",
    "        if system == 'quick_umls':\n",
    "            out = out[(out.score.astype(float) >= 0.8) & (out[\"type\"] == 'concept_jaccard_score_False')]\n",
    "            # fix for leading spacxe on semantic type field\n",
    "            out = out.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x) \n",
    "        \n",
    "        if system == 'metamap':\n",
    "            out = out[out.score.abs().astype(int) >= 800]\n",
    "            \n",
    "        if 'entity' in analysis_type:\n",
    "            cols_to_keep = ['begin', 'end', 'note_id']\n",
    "        elif 'cui' in analysis_type:\n",
    "            cols_to_keep = ['cui', 'note_id']\n",
    "        elif 'full' in analysis_type:\n",
    "            cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "\n",
    "        #out = out[cols_to_keep]\n",
    "\n",
    "        return out.drop_duplicates()\n",
    "\n",
    "# read in system/reference matches from file\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_system_matches(system: str, analysis_type: str, corpus: str):\n",
    "   \n",
    "    if corpus == 'casi':\n",
    "        \n",
    "        sql = \"select `case`, overlap from test.amia_2019_cases where overlap = 1 and `system` = %(system)s\"  \n",
    "        data_matches = df_to_set(pd.read_sql(sql, params={\"system\":system}, con=engine), 'entity', 'sys', 'casi')\n",
    "        \n",
    "        sql = \"select `case`, overlap from test.amia_2019_cases where (overlap = 0 or overlap is null) and `system` = %(system)s\"  \n",
    "        data_fn = df_to_set(pd.read_sql(sql, params={\"system\":system}, con=engine), 'entity', 'sys', 'casi')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        dir_test = analysisConf.data_dir + 'single_system_out/'\n",
    "\n",
    "        file = dir_test + system + '_' + analysis_type + '_' + corpus + '_matches.txt'\n",
    "        data_matches = set(literal_eval(line.strip()) for line in open(file))\n",
    "\n",
    "        file = dir_test + system + '_' + analysis_type + '_' + corpus + '_ref_only.txt'\n",
    "        data_fn = set(literal_eval(line.strip()) for line in open(file)) \n",
    "\n",
    "    return data_matches, data_fn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GENERATE merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetTotals(object):\n",
    "    \"\"\" \n",
    "    returns an instance with merged match set numbers using either union or intersection of elements in set \n",
    "    \"\"\"\n",
    "    def __init__(self, ref_n, sys_n, match_set):\n",
    "\n",
    "        self = self    \n",
    "        self.ref_ann = ref_n\n",
    "        self.sys_n = sys_n\n",
    "        self.match_set = match_set\n",
    "\n",
    "    def get_ref_sys(self):\n",
    "\n",
    "        ref_only = self.ref_ann - len(self.match_set)\n",
    "        sys_only = self.sys_n - len(self.match_set)\n",
    "\n",
    "        return ref_only, sys_only, len(self.match_set), self.match_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ft.lru_cache(maxsize=None)\n",
    "def process_sentence(pt, sentence, analysis_type, corpus, filter_semtype, semtype = None):\n",
    "    \"\"\"\n",
    "    Recursively evaluate parse tree, \n",
    "    with check for existence before build\n",
    "       :param sentence: to process\n",
    "       :return class of merged annotations, boolean operated system df \n",
    "    \"\"\"\n",
    "    \n",
    "    class Results(object):\n",
    "        def __init__(self):\n",
    "            self.results = set()\n",
    "            self.system_merges = pd.DataFrame()\n",
    "            \n",
    "    r = Results()\n",
    "    \n",
    "    if 'entity' in analysis_type and corpus != 'casi': \n",
    "        cols_to_keep = ['begin', 'end', 'note_id'] # entity only\n",
    "    elif 'full' in analysis_type: \n",
    "        cols_to_keep = ['cui', 'begin', 'end', 'note_id'] # entity only\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['cui', 'note_id'] # entity only\n",
    "    elif corpus == 'casi':\n",
    "        cols_to_keep = ['case', 'overlap']\n",
    "    \n",
    "    def evaluate(parseTree):\n",
    "        oper = {'&': op.and_, '|': op.or_}\n",
    "        \n",
    "        if parseTree:\n",
    "            leftC = gevent.spawn(evaluate, parseTree.getLeftChild())\n",
    "            rightC = gevent.spawn(evaluate, parseTree.getRightChild())\n",
    "            \n",
    "            if leftC.get() and rightC.get():\n",
    "                query = set()\n",
    "                system_query = pd.DataFrame()\n",
    "                fn = oper[parseTree.getRootVal()]\n",
    "                \n",
    "                if isinstance(leftC.get(), str):\n",
    "                    # get system as leaf node \n",
    "                    left, _ = get_system_matches(leftC.get(), analysis_type, corpus)\n",
    "                    if filter_semtype:\n",
    "                        left_sys = get_sys_data(leftC.get(), analysis_type, corpus, filter_semtype, semtype)\n",
    "                    else:\n",
    "                        left_sys = get_sys_data(leftC.get(), analysis_type, corpus, filter_semtype)\n",
    "                \n",
    "                elif isinstance(leftC.get(), tuple):\n",
    "                    left = leftC.get()[0]\n",
    "                    l_sys = leftC.get()[1]\n",
    "                \n",
    "                if isinstance(rightC.get(), str):\n",
    "                    # get system as leaf node\n",
    "                    right, _ = get_system_matches(rightC.get(), analysis_type, corpus)\n",
    "                    if filter_semtype:\n",
    "                        right_sys = get_sys_data(rightC.get(), analysis_type, corpus, filter_semtype, semtype)\n",
    "                    else:\n",
    "                        right_sys = get_sys_data(rightC.get(), analysis_type, corpus, filter_semtype)\n",
    "                    \n",
    "                elif isinstance(rightC.get(), tuple):\n",
    "                    right = rightC.get()[0]\n",
    "                    r_sys = rightC.get()[1]\n",
    "                    \n",
    "                # create match set based on boolean operation\n",
    "                match_set = fn(left, right)\n",
    "               \n",
    "                if fn == op.or_:\n",
    "                    r.results = r.results.union(match_set)\n",
    "\n",
    "                    if isinstance(leftC.get(), str) and isinstance(rightC.get(), str):\n",
    "                        frames = [left_sys, right_sys]\n",
    "                        df = pd.concat(frames,  ignore_index=True)\n",
    "                        df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                    elif isinstance(leftC.get(), str) and isinstance(rightC.get(), tuple):\n",
    "                        frames = [left_sys, r_sys]\n",
    "                        df = pd.concat(frames,  ignore_index=True)\n",
    "                        df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                    elif isinstance(leftC.get(), tuple) and isinstance(rightC.get(), str):\n",
    "                        frames = [l_sys, right_sys]\n",
    "                        df = pd.concat(frames,  ignore_index=True)\n",
    "                        df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                    elif isinstance(leftC.get(), tuple) and isinstance(rightC.get(), tuple):\n",
    "                        frames = [l_sys, r_sys]\n",
    "                        df = pd.concat(frames,  ignore_index=True)\n",
    "                        df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                if fn == op.and_:\n",
    "                    if len(r.results) == 0:\n",
    "                        r.results = match_set\n",
    "                    r.results = r.results.intersection(match_set)\n",
    "\n",
    "                    if isinstance(leftC.get(), str) and isinstance(rightC.get(), str):\n",
    "                        df = left_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                        df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                    elif isinstance(leftC.get(), str) and isinstance(rightC.get(), tuple):\n",
    "                        df = left_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                        df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                    elif isinstance(leftC.get(), tuple) and isinstance(rightC.get(), str):\n",
    "                        df = l_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                        df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                    elif isinstance(leftC.get(), tuple) and isinstance(rightC.get(), tuple):\n",
    "                        df = l_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                        df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "                \n",
    "                # get matched results\n",
    "                query.update(r.results)\n",
    "                \n",
    "                # get combined system results\n",
    "                r.system_merges = df\n",
    "                \n",
    "                if len(df) > 0:\n",
    "                    system_query = system_query.append(df)\n",
    "                else:\n",
    "                    print('wtf!')\n",
    "                    \n",
    "                return query, system_query\n",
    "            else:\n",
    "                return parseTree.getRootVal()\n",
    "    \n",
    "    if sentence.n_or > 0 or sentence.n_and > 0:\n",
    "        evaluate(pt)  \n",
    "    \n",
    "    # trivial case\n",
    "    elif sentence.n_or == 0 and sentence.n_and == 0:\n",
    "        r.results, _ = get_system_matches(sentence.sentence, analysis_type, corpus)\n",
    "        if filter_semtype:\n",
    "            r.system_merges = get_sys_data(sentence.sentence, analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "            r.system_merges = get_sys_data(sentence.sentence, analysis_type, corpus, filter_semtype)\n",
    "            \n",
    "        #print('trivial:', sentence.sentence, len(r.results), len(r.system_merges))\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Incoming Boolean sentences are parsed into a binary tree.\n",
    "\n",
    "Test expressions to parse:\n",
    "\n",
    "sentence = '((((A&B)|C)|D)&E)'\n",
    "\n",
    "sentence = '(E&(D|(C|(A&B))))'\n",
    "\n",
    "sentence = '(((A|(B&C))|(D&(E&F)))|(H&I))'\n",
    "\n",
    "\"\"\"\n",
    "# build parse tree from passed sentence using grammatical rules of Boolean logic\n",
    "def buildParseTree(fpexp):\n",
    "    \"\"\"\n",
    "       Iteratively build parse tree from passed sentence using grammatical rules of Boolean logic\n",
    "       :param fpexp: sentence to parse\n",
    "       :return eTree: parse tree representation\n",
    "       Incoming Boolean sentences are parsed into a binary tree.\n",
    "       Test expressions to parse:\n",
    "       sentence = '(A&B)'\n",
    "       sentence = '(A|B)'\n",
    "       sentence = '((A|B)&C)'\n",
    "       \n",
    "    \"\"\"\n",
    "    fplist = fpexp.split()\n",
    "    pStack = Stack()\n",
    "    eTree = BinaryTree('')\n",
    "    pStack.push(eTree)\n",
    "    currentTree = eTree\n",
    "\n",
    "    for i in fplist:\n",
    "\n",
    "        if i == '(':\n",
    "            currentTree.insertLeft('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getLeftChild()\n",
    "        elif i not in ['&', '|', ')']:\n",
    "            currentTree.setRootVal(i)\n",
    "            parent = pStack.pop()\n",
    "            currentTree = parent\n",
    "        elif i in ['&', '|']:\n",
    "            currentTree.setRootVal(i)\n",
    "            currentTree.insertRight('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getRightChild()\n",
    "        elif i == ')':\n",
    "            currentTree = pStack.pop()\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    return eTree\n",
    "\n",
    "def make_parse_tree(payload):\n",
    "    \"\"\"\n",
    "    Ensure data to create tree are in correct form\n",
    "    :param sentence: sentence to preprocess\n",
    "    :return pt, parse tree graph\n",
    "            sentence, processed sentence to build tree\n",
    "            a: order\n",
    "    \"\"\"\n",
    "    def preprocess_sentence(sentence):\n",
    "        # prepare statement for case when a boolean AND/OR is given\n",
    "        sentence = payload.replace('(', ' ( '). \\\n",
    "            replace(')', ' ) '). \\\n",
    "            replace('&', ' & '). \\\n",
    "            replace('|', ' | '). \\\n",
    "            replace('  ', ' ')\n",
    "        return sentence\n",
    "\n",
    "    sentence = preprocess_sentence(payload)\n",
    "    print(sentence)\n",
    "    \n",
    "    pt = buildParseTree(sentence)\n",
    "    #pt.postorder() \n",
    "    \n",
    "    return pt\n",
    "\n",
    "class Sentence(object):\n",
    "    '''\n",
    "    Details about boolean expression -> number operators and expression\n",
    "    '''\n",
    "    def __init__(self, sentence):\n",
    "        self = self\n",
    "        self.n_and = sentence.count('&')\n",
    "        self.n_or = sentence.count('|')\n",
    "        self.sentence = sentence\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_docs(corpus):\n",
    "    sql = 'select distinct note_id, sofa from sofas where corpus = %(corpus)s order by note_id'\n",
    "    df = pd.read_sql(sql, params={\"corpus\":corpus}, con=engine)\n",
    "    df.drop_duplicates()\n",
    "    df['len_doc'] = df['sofa'].apply(len)\n",
    "    \n",
    "    subset = df[['note_id', 'len_doc']]\n",
    "    docs = [tuple(x) for x in subset.to_numpy()]\n",
    "    \n",
    "    return docs\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_ref_ann(analysis_type, corpus, filter_semtype, semtype = None):\n",
    "    \n",
    "    if filter_semtype:\n",
    "        if ',' in semtype:\n",
    "            semtype = semtype.split(',')\n",
    "        else:\n",
    "            semtype = [semtype]\n",
    "        \n",
    "    ann, _ = get_metric_data(analysis_type, corpus)\n",
    "    ann = ann.rename(index=str, columns={\"start\": \"begin\", \"file\": \"case\"})\n",
    "    \n",
    "    if filter_semtype:\n",
    "        ann = ann[ann['semtype'].isin(semtype)]\n",
    "        \n",
    "    ann[\"label\"] = 'concept'\n",
    "    cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "    ann = ann[cols_to_keep]\n",
    "    \n",
    "    return ann\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_sys_ann(r):\n",
    "    sys = r.system_merges    \n",
    "    sys = sys.rename(index=str, columns={\"note_id\": \"case\"})\n",
    "    sys[\"label\"] = 'concept'\n",
    "    cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "    sys = sys[cols_to_keep]\n",
    "\n",
    "    return sys\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_metrics(boolean_expression: str, analysis_type: str, corpus: str, run_type: str, filter_semtype, semtype = None):\n",
    "    \"\"\"\n",
    "    Traverse binary parse tree representation of Boolean sentence\n",
    "        :params: boolean expression in form of '(<annotator_engine_name1><boolean operator><annotator_engine_name2>)'\n",
    "                 analysis_type (string value of: 'entity', 'cui', 'full') used to filter set of reference and system annotations \n",
    "        :return: dictionary with values needed for confusion matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence = Sentence(boolean_expression)   \n",
    "    pt = make_parse_tree(sentence.sentence)\n",
    "    \n",
    "    if filter_semtype:\n",
    "        r = process_sentence(pt, sentence, analysis_type, corpus, filter_semtype, semtype)\n",
    "    else:\n",
    "        r = process_sentence(pt, sentence, analysis_type, corpus, filter_semtype)\n",
    "        \n",
    "    # vectorize merges using i-o labeling\n",
    "    if run_type == 'overlap':\n",
    "        if filter_semtype:\n",
    "            TP, TN, FP, FN = vectorized_coocurences(r, analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "            TP, TN, FP, FN = vectorized_coocurences(r, analysis_type, corpus, filter_semtype)\n",
    "\n",
    "        # TODO: validate against ann1/sys1 where val = 1\n",
    "        # total by number chars\n",
    "        system_n = TP + FP\n",
    "        reference_n = TP + FN\n",
    "\n",
    "        d = cm_dict(FN, FP, TP, system_n, reference_n)\n",
    "        \n",
    "        d['TN'] = TN\n",
    "        \n",
    "        # return full metrics\n",
    "        return d\n",
    "\n",
    "    elif run_type == 'exact':\n",
    "        # total by number spans\n",
    "        system_n = len(r.system_merges)\n",
    "        reference_n = get_ref_n(analysis_type, corpus, filter_semtype)\n",
    "\n",
    "        reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, r.results).get_ref_sys()\n",
    "        # get overall TP/TF and various other counts for running confusion matrix metric analysis\n",
    "        return cm_dict(reference_only, system_only, reference_system_match, system_n, reference_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all combinations of given list of annotators:\n",
    "def partly_unordered_permutations(lst, k):\n",
    "    elems = set(lst)\n",
    "    for c in combinations(lst, k):\n",
    "        for d in permutations(elems - set(c)):\n",
    "            yield c + d\n",
    "            \n",
    "def expressions(l, n):\n",
    "    for (operations, *operands), operators in product(\n",
    "            combinations(l, n), product(('&', '|'), repeat=n - 1)):\n",
    "        for operation in zip(operators, operands):\n",
    "            operations = [operations, *operation]\n",
    "        yield operations\n",
    "\n",
    "# get list of systems with a semantic type in grouping\n",
    "def get_valid_systems(systems, semtype):\n",
    "    test = []\n",
    "    for sys in systems:\n",
    "        st = system_semtype_check(sys, semtype, corpus)\n",
    "        if st:\n",
    "            test.append(sys)\n",
    "\n",
    "    return test\n",
    "\n",
    "# permute system combinations and evaluate system merges for performance\n",
    "def run_ensemble(systems, analysis_type, corpus, filter_semtype, expression_type, semtype = None):\n",
    "    metrics = pd.DataFrame()\n",
    "\n",
    "    if expression_type == 'nested':\n",
    "        for l in partly_unordered_permutations(systems, 2):\n",
    "            print('processing merge combo:', l)\n",
    "            for i in range(1, len(l)+1):\n",
    "                test = list(expressions(l, i))\n",
    "                for t in test:\n",
    "                    if i > 1:\n",
    "                        # format Boolean sentence for parse tree \n",
    "                        t = '(' + \" \".join(str(x) for x in t).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "\n",
    "                    if filter_semtype:\n",
    "                        d = get_metrics(t, analysis_type, corpus, run_type, filter_semtype, semtype)\n",
    "                    else:\n",
    "                        d = get_metrics(t, analysis_type, corpus, run_type, filter_semtype)\n",
    "\n",
    "                    d['merge'] = t\n",
    "                    d['n_terms'] = i\n",
    "\n",
    "                    frames = [metrics, pd.DataFrame(d, index=[0])]\n",
    "                    metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "                    \n",
    "    elif expression_type == 'nested_with_singleton' and len(systems) == 5:\n",
    "        # form (((a&b)|c)&(d|e))\n",
    "        \n",
    "        nested = list(expressions(systems, 3))\n",
    "        test = list(expressions(systems, 2))\n",
    "        to_do_terms = []\n",
    "    \n",
    "        for n in nested:\n",
    "            # format Boolean sentence for parse tree \n",
    "            n = '(' + \" \".join(str(x) for x in n).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "\n",
    "            for t in test:\n",
    "                t = '(' + \" \".join(str(x) for x in t).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "\n",
    "                new_and = '(' + n +'&'+ t + ')'\n",
    "                new_or = '(' + n +'|'+ t + ')'\n",
    "\n",
    "                if new_and.count('biomedicus') != 2 and new_and.count('clamp') != 2 and new_and.count('ctakes') != 2 and new_and.count('metamap') != 2 and new_and.count('quick_umls') != 2:\n",
    "\n",
    "                    if new_and.count('&') != 4 and new_or.count('|') != 4:\n",
    "                        #print(new_and)\n",
    "                        #print(new_or)\n",
    "                        to_do_terms.append(new_or)\n",
    "                        to_do_terms.append(new_and)\n",
    "        \n",
    "        print('nested_with_singleton', len(to_do_terms))\n",
    "        for term in to_do_terms:\n",
    "            if filter_semtype:\n",
    "                d = get_metrics(term, analysis_type, corpus, run_type, filter_semtype, semtype)\n",
    "            else:\n",
    "                d = get_metrics(term, analysis_type, corpus, run_type, filter_semtype)\n",
    "                \n",
    "            n = term.count('&')\n",
    "            m = term.count('|')\n",
    "            d['merge'] = term\n",
    "            d['n_terms'] = m + n + 1\n",
    "\n",
    "            frames = [metrics, pd.DataFrame(d, index=[0])]\n",
    "            metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "                        \n",
    "    elif expression_type == 'paired':\n",
    "        m = list(expressions(systems, 2))\n",
    "        test = list(expressions(m, 2))\n",
    "\n",
    "        to_do_terms = []\n",
    "        for t in test:\n",
    "            # format Boolean sentence for parse tree \n",
    "            t = '(' + \" \".join(str(x) for x in t).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "            if t.count('biomedicus') != 2 and t.count('clamp') != 2 and t.count('ctakes') != 2 and t.count('metamap') != 2 and t.count('quick_umls') != 2:\n",
    "                if t.count('&') != 3 and t.count('|') != 3:\n",
    "                    to_do_terms.append(t)\n",
    "                    if len(systems) == 5:\n",
    "                        for i in systems:\n",
    "                            if i not in t:\n",
    "                                #print('('+t+'&'+i+')')\n",
    "                                #print('('+t+'|'+i+')')\n",
    "                                new_and = '('+t+'&'+i+')'\n",
    "                                new_or = '('+t+'|'+i+')'\n",
    "                                to_do_terms.append(new_and)\n",
    "                                to_do_terms.append(new_or)\n",
    "                            \n",
    "        print('paired', len(to_do_terms))\n",
    "        for term in to_do_terms:\n",
    "            if filter_semtype:\n",
    "                d = get_metrics(term, analysis_type, corpus, run_type, filter_semtype, semtype)\n",
    "            else:\n",
    "                d = get_metrics(term, analysis_type, corpus, run_type, filter_semtype)\n",
    "                \n",
    "            n = term.count('&')\n",
    "            m = term.count('|')\n",
    "            d['merge'] = term\n",
    "            d['n_terms'] = m + n + 1\n",
    "\n",
    "            frames = [metrics, pd.DataFrame(d, index=[0])]\n",
    "            metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "        \n",
    "    return metrics\n",
    "\n",
    "# write to file\n",
    "def generate_ensemble_metrics(metrics, analysis_type, corpus, ensemble_type, filter_semtype, semtype = None):\n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    \n",
    "    file_name = corpus + '_all_'\n",
    "   \n",
    "    # drop exact matches:\n",
    "    metrics = metrics.drop_duplicates()\n",
    "    \n",
    "    if ensemble_type == 'merge':\n",
    "        metrics = metrics.sort_values(by=['n_terms', 'merge'])\n",
    "        file_name += 'merge_'\n",
    "    elif ensemble_type == 'vote':\n",
    "        file_name += 'vote_'\n",
    "    \n",
    "    #metrics = metrics.drop_duplicates(subset=['TP', 'FN', 'FP', 'n_sys', 'precision', 'recall', 'F', 'TM', 'TP/FN', 'TM', 'n_terms'])\n",
    "\n",
    "    file = file_name + analysis_type + '_' + run_type +'_'\n",
    "    \n",
    "    if filter_semtype:\n",
    "        file += semtype\n",
    "        \n",
    "    \n",
    "    geometric_mean(metrics).to_csv(analysisConf.data_dir + file + str(timestamp) + '.csv')\n",
    "    print(geometric_mean(metrics))\n",
    "    \n",
    "# control ensemble run\n",
    "def ensemble_control(systems, analysis_type, corpus, run_type, filter_semtype, semtypes = None):\n",
    "    print(semtypes, systems)\n",
    "    if filter_semtype:\n",
    "        for semtype in semtypes:\n",
    "            test = get_valid_systems(systems, semtype)\n",
    "            print('SYSYEMS FOR SEMTYPE', semtype, 'ARE', test)\n",
    "            metrics = run_ensemble(test, analysis_type, corpus, filter_semtype, expression_type, semtype)\n",
    "            if (expression_type == 'nested_with_singleton' and len(test) == 5) or expression_type in ['nested', 'paired']:\n",
    "                generate_ensemble_metrics(metrics, analysis_type, corpus, ensemble_type, filter_semtype, semtype)\n",
    "    else:\n",
    "        metrics = run_ensemble(systems, analysis_type, corpus, filter_semtype, expression_type)\n",
    "        generate_ensemble_metrics(metrics, analysis_type, corpus, ensemble_type, filter_semtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ad hoc query\n",
    "def get_merge_data(boolean_expression: str, analysis_type: str, corpus: str, filter_semtype, semtype = None):\n",
    "    \"\"\"\n",
    "    Traverse binary parse tree representation of Boolean sentence\n",
    "        :params: boolean expression in form of '(<annotator_engine_name1><boolean operator><annotator_engine_name2>)'\n",
    "                 analysis_type (string value of: 'entity', 'cui', 'full') used to filter set of reference and system annotations \n",
    "        :return: dictionary with values needed for confusion matrix\n",
    "    \"\"\"\n",
    "    sentence = Sentence(boolean_expression)   \n",
    "\n",
    "    pt = make_parse_tree(sentence.sentence)\n",
    "\n",
    "    r = process_sentence(pt, sentence, analysis_type, corpus, filter_semtype, semtype)\n",
    "    \n",
    "    system_n = len(r.system_merges)\n",
    "    reference_n = get_ref_n(analysis_type, corpus, filter_semtype)\n",
    "\n",
    "    reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, r.results).get_ref_sys()\n",
    "\n",
    "    print(cm_dict(reference_only, system_only, reference_system_match, system_n, reference_n))\n",
    "    # get matched data from merge\n",
    "    return r.system_merges # merge_eval(reference_only, system_only, reference_system_match, system_n, reference_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# majority vote\n",
    "def vectorized_annotations(ann):\n",
    "    \n",
    "    docs = get_docs(corpus)\n",
    "    labels = [\"concept\"]\n",
    "    out= []\n",
    "    \n",
    "    for n in range(len(docs)):\n",
    "        a1 = list(ann.loc[ann[\"case\"] == docs[n][0]].itertuples(index=False))\n",
    "        a = label_vector(docs[n][1], a1, labels)\n",
    "        out.append(a)\n",
    "\n",
    "    return out\n",
    "\n",
    "def flatten_list(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def majority_ensemble(systems, analysis_type, corpus, filter_semtype, semtype = None):\n",
    "    \n",
    "    d = {}\n",
    "    cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "    sys_test = []\n",
    "    \n",
    "    for system in systems:\n",
    "        sys_ann = get_sys_data(system, analysis_type, corpus, filter_semtype, semtype)\n",
    "        df = sys_ann.copy()\n",
    "        df['label'] = 'concept'\n",
    "        df = df.rename(index=str, columns={\"note_id\": \"case\"})\n",
    "        sys = df[df['system']==system][cols_to_keep].copy()\n",
    "        test = vectorized_annotations(sys)\n",
    "        d[system] = flatten_list(test) \n",
    "        sys_test.append(d[system])\n",
    "\n",
    "    output = sum(np.array(sys_test))\n",
    "    \n",
    "    n = int(len(systems) / 2)\n",
    "    #print(n)\n",
    "    if ((len(systems) % 2) != 0):\n",
    "        vote = np.where(output > n, 1, 0)\n",
    "    else:\n",
    "        vote = np.where(output > n, 1, \n",
    "         (np.where(output == n, random.randint(0, 1), 0)))\n",
    "        \n",
    "    return vote\n",
    "\n",
    "def reference(analysis_type, corpus, filter_semtype, semtype = None):\n",
    "    ref_ann = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "    #ref_ann, _ = get_metric_data(analysis_type, corpus)\n",
    "    \n",
    "\n",
    "    df = ref_ann.copy()\n",
    "    df = df.drop_duplicates(subset=['begin','end','case'])\n",
    "    df['label'] = 'concept'\n",
    "    #cases = set(df['case'].to_list())\n",
    "    #df = df1.rename(index=str, columns={\"file\": \"case\", \"start\": \"begin\"})\n",
    "\n",
    "    cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "    ref = df[cols_to_keep].copy()\n",
    "    test = vectorized_annotations(ref)\n",
    "    ref =  np.asarray(flatten_list(test), dtype=np.int32) \n",
    "\n",
    "    return ref\n",
    "\n",
    "def majority_vote_out(ref, vote, corpus):    \n",
    "    TP, TN, FP, FN = confused(ref, vote)\n",
    "    print(TP, TN, FP, FN)\n",
    "    system_n = TP + FP\n",
    "    reference_n = TP + FN\n",
    "\n",
    "    d = cm_dict(FN, FP, TP, system_n, reference_n)\n",
    "\n",
    "    d['TN'] = TN\n",
    "    d['corpus'] = corpus\n",
    "    print(d)\n",
    "    \n",
    "    metrics = pd.DataFrame(d, index=[0])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# control ensemble run\n",
    "def majority_vote(systems, analysis_type, corpus, run_type, filter_semtype, semtypes = None):\n",
    "    print(semtypes, systems)\n",
    "    if filter_semtype:\n",
    "        for semtype in semtypes:\n",
    "            test = get_valid_systems(systems, semtype)\n",
    "            print('SYSYEMS FOR SEMTYPE', semtype, 'ARE', test)\n",
    "            ref = reference(analysis_type, corpus, filter_semtype, semtype)\n",
    "            vote = majority_ensemble(test, analysis_type, corpus, filter_semtype, semtype)\n",
    "            metrics = majority_vote_out(ref, vote, corpus)\n",
    "            metrics['systems'] = ','.join(test)\n",
    "            generate_ensemble_metrics(metrics, analysis_type, corpus, ensemble_type, filter_semtype, semtype)\n",
    "            print(metrics)\n",
    "    else:\n",
    "        # ref = reference()\n",
    "        ref = reference(analysis_type, corpus, filter_semtype)\n",
    "        vote = majority_ensemble(systems, analysis_type, corpus, filter_semtype)\n",
    "        metrics = majority_vote_out(ref, vote, corpus)\n",
    "        generate_ensemble_metrics(metrics, analysis_type, corpus, ensemble_type, filter_semtype)\n",
    "        metrics['systems'] = ','.join(test)\n",
    "        print(metrics)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls'] ('analytical_fairview.csv', 'concepts.fairview_all')\n",
      "['Drug', 'Finding', 'Anatomy', 'Procedure'] ['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls']\n",
      "SYSYEMS FOR SEMTYPE Drug ARE ['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls']\n",
      "101787 6690184 20182 124503\n",
      "{'F': 0.5845477073097896, 'precision': 0.834531725274455, 'recall': 0.4498077687922577, 'TP': 101787, 'FN': 124503, 'FP': 20182, 'TP/FN': 0.8175465651430086, 'n_gold': 226290, 'n_sys': 121969, 'TM': 291.45236441056875, 'TN': 6690184, 'corpus': 'fairview'}\n",
      "          F  precision    recall      TP      FN     FP     TP/FN  n_gold  \\\n",
      "0  0.584548   0.834532  0.449808  101787  124503  20182  0.817547  226290   \n",
      "\n",
      "    n_sys          TM       TN    corpus  \\\n",
      "0  121969  291.452364  6690184  fairview   \n",
      "\n",
      "                                      systems  F1 rank  TP/FN rank  TM rank  \\\n",
      "0  biomedicus,clamp,ctakes,metamap,quick_umls      1.0         1.0      1.0   \n",
      "\n",
      "   Gmean  \n",
      "0    1.0  \n",
      "          F  precision    recall      TP      FN     FP     TP/FN  n_gold  \\\n",
      "0  0.584548   0.834532  0.449808  101787  124503  20182  0.817547  226290   \n",
      "\n",
      "    n_sys          TM       TN    corpus  \\\n",
      "0  121969  291.452364  6690184  fairview   \n",
      "\n",
      "                                      systems  \n",
      "0  biomedicus,clamp,ctakes,metamap,quick_umls  \n",
      "SYSYEMS FOR SEMTYPE Finding ARE ['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171777 6387059 153412 224408\n",
      "{'F': 0.4762494905555232, 'precision': 0.5282374250051508, 'recall': 0.43357774776935015, 'TP': 171777, 'FN': 224408, 'FP': 153412, 'TP/FN': 0.7654673630173612, 'n_gold': 396185, 'n_sys': 325189, 'TM': 301.22921530806036, 'TN': 6387059, 'corpus': 'fairview'}\n",
      "          F  precision    recall      TP      FN      FP     TP/FN  n_gold  \\\n",
      "0  0.476249   0.528237  0.433578  171777  224408  153412  0.765467  396185   \n",
      "\n",
      "    n_sys          TM       TN    corpus  \\\n",
      "0  325189  301.229215  6387059  fairview   \n",
      "\n",
      "                                      systems  F1 rank  TP/FN rank  TM rank  \\\n",
      "0  biomedicus,clamp,ctakes,metamap,quick_umls      1.0         1.0      1.0   \n",
      "\n",
      "   Gmean  \n",
      "0    1.0  \n",
      "          F  precision    recall      TP      FN      FP     TP/FN  n_gold  \\\n",
      "0  0.476249   0.528237  0.433578  171777  224408  153412  0.765467  396185   \n",
      "\n",
      "    n_sys          TM       TN    corpus  \\\n",
      "0  325189  301.229215  6387059  fairview   \n",
      "\n",
      "                                      systems  \n",
      "0  biomedicus,clamp,ctakes,metamap,quick_umls  \n",
      "SYSYEMS FOR SEMTYPE Anatomy ARE ['biomedicus', 'ctakes', 'metamap', 'quick_umls']\n",
      "23950 6842879 23181 46646\n",
      "{'F': 0.4068735294367477, 'precision': 0.5081581124949609, 'recall': 0.33925434868831095, 'TP': 23950, 'FN': 46646, 'FP': 23181, 'TP/FN': 0.5134416670239678, 'n_gold': 70596, 'n_sys': 47131, 'TM': 110.31947604233041, 'TN': 6842879, 'corpus': 'fairview'}\n",
      "          F  precision    recall     TP     FN     FP     TP/FN  n_gold  \\\n",
      "0  0.406874   0.508158  0.339254  23950  46646  23181  0.513442   70596   \n",
      "\n",
      "   n_sys          TM       TN    corpus                               systems  \\\n",
      "0  47131  110.319476  6842879  fairview  biomedicus,ctakes,metamap,quick_umls   \n",
      "\n",
      "   F1 rank  TP/FN rank  TM rank  Gmean  \n",
      "0      1.0         1.0      1.0    1.0  \n",
      "          F  precision    recall     TP     FN     FP     TP/FN  n_gold  \\\n",
      "0  0.406874   0.508158  0.339254  23950  46646  23181  0.513442   70596   \n",
      "\n",
      "   n_sys          TM       TN    corpus                               systems  \n",
      "0  47131  110.319476  6842879  fairview  biomedicus,ctakes,metamap,quick_umls  \n",
      "SYSYEMS FOR SEMTYPE Procedure ARE ['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls']\n",
      "13121 6799911 9626 113998\n",
      "{'F': 0.1751030920956054, 'precision': 0.576823317360531, 'recall': 0.10321824432224923, 'TP': 13121, 'FN': 113998, 'FP': 9626, 'TP/FN': 0.11509851050018421, 'n_gold': 127119, 'n_sys': 22747, 'TM': 86.99711918843938, 'TN': 6799911, 'corpus': 'fairview'}\n",
      "          F  precision    recall     TP      FN    FP     TP/FN  n_gold  \\\n",
      "0  0.175103   0.576823  0.103218  13121  113998  9626  0.115099  127119   \n",
      "\n",
      "   n_sys         TM       TN    corpus  \\\n",
      "0  22747  86.997119  6799911  fairview   \n",
      "\n",
      "                                      systems  F1 rank  TP/FN rank  TM rank  \\\n",
      "0  biomedicus,clamp,ctakes,metamap,quick_umls      1.0         1.0      1.0   \n",
      "\n",
      "   Gmean  \n",
      "0    1.0  \n",
      "          F  precision    recall     TP      FN    FP     TP/FN  n_gold  \\\n",
      "0  0.175103   0.576823  0.103218  13121  113998  9626  0.115099  127119   \n",
      "\n",
      "   n_sys         TM       TN    corpus  \\\n",
      "0  22747  86.997119  6799911  fairview   \n",
      "\n",
      "                                      systems  \n",
      "0  biomedicus,clamp,ctakes,metamap,quick_umls  \n",
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         11176843 function calls (11131953 primitive calls) in 52.140 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "15227/13081   10.644    0.001   10.678    0.001 {built-in method numpy.array}\n",
       "       23    9.338    0.406    9.338    0.406 <ipython-input-286-d6bec680927f>:16(<listcomp>)\n",
       "        1    5.778    5.778   52.133   52.133 <ipython-input-286-d6bec680927f>:85(majority_vote)\n",
       "  1105867    4.065    0.000    4.065    0.000 {built-in method __new__ of type object at 0x10574f778}\n",
       "      985    3.598    0.004    3.598    0.004 {pandas._libs.ops.scalar_compare}\n",
       "        1    1.639    1.639    1.646    1.646 {method 'read' of 'pandas._libs.parsers.TextReader' objects}\n",
       "      533    1.274    0.002    1.274    0.002 {method 'copy' of 'numpy.ndarray' objects}\n",
       "        5    1.228    0.246    1.229    0.246 {built-in method gc.collect}\n",
       "        4    1.152    0.288   11.891    2.973 <ipython-input-286-d6bec680927f>:50(reference)\n",
       "      943    1.077    0.001    3.345    0.004 <ipython-input-275-3c77ee2087eb>:1(label_vector)\n",
       "  1106849    0.978    0.000    0.978    0.000 {built-in method numpy.arange}\n",
       "       23    0.958    0.042   12.709    0.553 <ipython-input-286-d6bec680927f>:2(vectorized_annotations)\n",
       "      943    0.934    0.001    0.934    0.001 <ipython-input-275-3c77ee2087eb>:9(<listcomp>)\n",
       "      567    0.894    0.002    0.894    0.002 {method 'recv_into' of '_socket.socket' objects}\n",
       "      114    0.400    0.004    0.401    0.004 {method 'factorize' of 'pandas._libs.hashtable.StringHashTable' objects}\n",
       "      943    0.334    0.000    1.305    0.001 <ipython-input-275-3c77ee2087eb>:8(<listcomp>)\n",
       "       46    0.278    0.006    0.278    0.006 managers.py:2004(<listcomp>)\n",
       "      711    0.266    0.000    0.270    0.000 {pandas._libs.lib.infer_dtype}\n",
       "       92    0.255    0.003    0.255    0.003 {method 'factorize' of 'pandas._libs.hashtable.Int64HashTable' objects}\n",
       "  1103814    0.249    0.000    4.373    0.000 __init__.py:403(_make)\n",
       "       23    0.238    0.010    0.238    0.010 {pandas._libs.hashtable.ismember_object}\n",
       "        4    0.235    0.059   34.028    8.507 <ipython-input-286-d6bec680927f>:18(majority_ensemble)\n",
       "     5492    0.217    0.000    0.217    0.000 {built-in method numpy.empty}\n",
       "       19    0.210    0.011    4.359    0.229 <ipython-input-280-eb2ea29a5c5f>:27(get_sys_data)\n",
       "     1185    0.198    0.000    0.200    0.000 {built-in method builtins.sum}\n",
       "        1    0.196    0.196    0.196    0.196 {method 'get_labels_groupby' of 'pandas._libs.hashtable.Int64HashTable' objects}\n",
       "     1037    0.195    0.000    0.195    0.000 {pandas._libs.algos.take_2d_axis1_object_object}\n",
       "       29    0.189    0.007    0.189    0.007 {pandas._libs.hashtable.duplicated_int64}\n",
       "        4    0.164    0.041    0.239    0.060 <ipython-input-275-3c77ee2087eb>:16(confused)\n",
       "     1037    0.162    0.000    0.162    0.000 {pandas._libs.algos.take_2d_axis1_int64_int64}\n",
       "    45060    0.153    0.000    0.487    0.000 connections.py:1195(_read_row_from_packet)\n",
       "       69    0.150    0.002    0.181    0.003 managers.py:1841(_stack_arrays)\n",
       "        5    0.146    0.029    0.146    0.029 {built-in method numpy.where}\n",
       "    91471    0.146    0.000    0.146    0.000 {method 'settimeout' of '_socket.socket' objects}\n",
       "     3081    0.137    0.000    0.138    0.000 base.py:3918(__contains__)\n",
       "   792814    0.132    0.000    0.171    0.000 strings.py:1519(<lambda>)\n",
       "       85    0.131    0.002    0.248    0.003 blocks.py:3131(_merge_blocks)\n",
       "      246    0.115    0.000    0.115    0.000 {built-in method numpy.concatenate}\n",
       "484601/484519    0.112    0.000    0.189    0.000 {built-in method builtins.isinstance}\n",
       "1585679/1547466    0.110    0.000    0.124    0.000 {built-in method builtins.len}\n",
       "     6253    0.102    0.000    0.102    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
       "    45592    0.092    0.000    1.302    0.000 connections.py:648(_read_packet)\n",
       "     1081    0.087    0.000    0.087    0.000 {method 'take' of 'numpy.ndarray' objects}\n",
       "    91184    0.086    0.000    1.164    0.000 connections.py:687(_read_bytes)\n",
       "   225567    0.084    0.000    0.096    0.000 protocol.py:63(read)\n",
       "   225666    0.078    0.000    0.277    0.000 protocol.py:168(read_length_coded_string)\n",
       "       89    0.078    0.001    0.078    0.001 {pandas._libs.algos.take_2d_axis0_float64_float64}\n",
       "       24    0.077    0.003    0.202    0.008 {pandas._libs.lib.map_infer_mask}\n",
       "       29    0.074    0.003    1.731    0.060 frame.py:4605(drop_duplicates)\n",
       "       29    0.069    0.002    0.299    0.010 sorting.py:20(get_group_index)\n",
       "    12851    0.061    0.000    0.083    0.000 generic.py:5069(__setattr__)\n",
       "     1089    0.060    0.000    0.060    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
       "   226117    0.056    0.000    0.103    0.000 protocol.py:150(read_length_encoded_integer)\n",
       "    944/1    0.049    0.000   52.141   52.141 {built-in method builtins.exec}\n",
       "   226117    0.047    0.000    0.047    0.000 protocol.py:117(read_uint8)\n",
       "       41    0.047    0.001    0.953    0.023 connections.py:1182(_read_rowdata_packet)\n",
       "   164082    0.045    0.000    0.062    0.000 generic.py:7(_check)\n",
       "   132135    0.045    0.000    0.091    0.000 strings.py:87(g)\n",
       "    24505    0.044    0.000    0.119    0.000 dtypes.py:68(find)\n",
       "   225282    0.042    0.000    0.042    0.000 {method 'decode' of 'bytes' objects}\n",
       "      943    0.041    0.000    0.101    0.000 __init__.py:316(namedtuple)\n",
       "     1069    0.041    0.000    0.054    0.000 indexing.py:2575(maybe_convert_indices)\n",
       "   660676    0.040    0.000    0.040    0.000 {method 'strip' of 'str' objects}\n",
       "      217    0.038    0.000    0.041    0.000 cast.py:1193(construct_1d_object_array_from_listlike)\n",
       "      681    0.037    0.000    0.037    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
       "   255914    0.034    0.000    0.042    0.000 {built-in method builtins.getattr}\n",
       "    32878    0.034    0.000    0.080    0.000 common.py:1845(_is_dtype_type)\n",
       "      246    0.033    0.000    0.933    0.004 algorithms.py:559(factorize)\n",
       "    31031    0.032    0.000    0.052    0.000 {method 'format' of 'str' objects}\n",
       "    91184    0.031    0.000    0.928    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
       "      948    0.030    0.000    0.030    0.000 {built-in method numpy.zeros}\n",
       "     2318    0.029    0.000    0.760    0.000 algorithms.py:1544(take_nd)\n",
       "        1    0.027    0.027    4.514    4.514 <ipython-input-277-ee20452e93e4>:1(get_metric_data)\n",
       "    47544    0.027    0.000    0.171    0.000 {built-in method builtins.hasattr}\n",
       "       96    0.025    0.000    0.025    0.000 {built-in method pandas._libs.missing.isnaobj}\n",
       "    32753    0.024    0.000    0.099    0.000 base.py:75(is_dtype)\n",
       "     3900    0.024    0.000    0.265    0.000 indexing.py:960(_getitem_lowerdim)\n",
       "      246    0.024    0.000    0.027    0.000 sorting.py:55(maybe_lift)\n",
       "       57    0.024    0.000    0.024    0.000 {pandas._libs.lib.array_equivalent_object}\n",
       "        1    0.022    0.022    1.923    1.923 parsers.py:403(_read)\n",
       "    11008    0.021    0.000    0.051    0.000 _dtype.py:319(_name_get)\n",
       "5667/5666    0.021    0.000    0.263    0.000 series.py:152(__init__)\n",
       "     1350    0.021    0.000    0.050    0.000 managers.py:186(_rebuild_blknos_and_blklocs)\n",
       "       25    0.021    0.001    0.112    0.004 {pandas._libs.lib.map_infer}\n",
       "     1069    0.021    0.000    1.099    0.001 managers.py:1329(take)\n",
       "       40    0.021    0.001    0.021    0.001 {method 'factorize' of 'pandas._libs.hashtable.Float64HashTable' objects}\n",
       "   273595    0.021    0.000    0.021    0.000 {method 'append' of 'list' objects}\n",
       "        5    0.020    0.004    0.044    0.009 managers.py:772(_interleave)\n",
       "    22262    0.020    0.000    0.065    0.000 common.py:1981(pandas_dtype)\n",
       "    23208    0.019    0.000    0.106    0.000 common.py:1702(is_extension_array_dtype)\n",
       "     2779    0.018    0.000    0.018    0.000 {method 'fill' of 'numpy.ndarray' objects}\n",
       "     4441    0.018    0.000    0.063    0.000 managers.py:963(iget)\n",
       "      993    0.017    0.000    3.925    0.004 ops.py:1660(wrapper)\n",
       "      171    0.017    0.000    0.017    0.000 {pandas._libs.lib.maybe_convert_objects}\n",
       "     8484    0.017    0.000    0.038    0.000 blocks.py:78(__init__)\n",
       "   119115    0.017    0.000    0.017    0.000 {built-in method builtins.issubclass}\n",
       "       41    0.016    0.000    0.016    0.000 __init__.py:131(lmap)\n",
       "      246    0.015    0.000    0.848    0.003 algorithms.py:434(_factorize_array)\n",
       "    12867    0.014    0.000    0.065    0.000 common.py:93(is_bool_indexer)\n",
       "     4860    0.014    0.000    0.105    0.000 base.py:1117(__iter__)\n",
       "     3928    0.013    0.000    0.148    0.000 frame.py:2829(_ixs)\n",
       "     8484    0.013    0.000    0.093    0.000 blocks.py:3080(make_block)\n",
       "     1548    0.013    0.000    0.923    0.001 frame.py:2893(__getitem__)\n",
       "    25974    0.013    0.000    0.040    0.000 inference.py:253(is_list_like)\n",
       "     4843    0.013    0.000    0.991    0.000 indexing.py:1485(__getitem__)\n",
       "    45101    0.012    0.000    0.020    0.000 connections.py:1137(_check_packet_is_eof)\n",
       "    45592    0.012    0.000    0.022    0.000 protocol.py:214(check_error)\n",
       "     5666    0.012    0.000    0.059    0.000 managers.py:1443(__init__)\n",
       "    22032    0.012    0.000    0.018    0.000 numerictypes.py:293(issubclass_)\n",
       "    45633    0.012    0.000    0.012    0.000 {built-in method _struct.unpack}\n",
       "    16027    0.011    0.000    0.016    0.000 generic.py:363(_get_axis_name)\n",
       "     2318    0.011    0.000    0.032    0.000 algorithms.py:1417(_get_take_nd_function)\n",
       "    11016    0.011    0.000    0.029    0.000 numerictypes.py:365(issubdtype)\n",
       "    29103    0.011    0.000    0.011    0.000 {built-in method _abc._abc_instancecheck}\n",
       "    34542    0.011    0.000    0.015    0.000 base.py:652(__len__)\n",
       "8763/8755    0.010    0.000    0.150    0.000 generic.py:5053(__getattr__)\n",
       "    45592    0.010    0.000    0.010    0.000 protocol.py:56(__init__)\n",
       "     6909    0.010    0.000    0.010    0.000 generic.py:127(__init__)\n",
       "    21912    0.010    0.000    0.026    0.000 integer.py:80(construct_from_string)\n",
       "     6506    0.010    0.000    0.022    0.000 common.py:160(is_sparse)\n",
       "     2250    0.010    0.000    0.788    0.000 blocks.py:1217(take_nd)\n",
       "      943    0.009    0.000    0.576    0.001 frame.py:849(itertuples)\n",
       "     5671    0.009    0.000    0.017    0.000 series.py:354(_set_axis)\n",
       "     2226    0.009    0.000    0.035    0.000 cast.py:255(maybe_promote)\n",
       "     8484    0.009    0.000    0.011    0.000 blocks.py:199(mgr_locs)\n",
       "    45592    0.009    0.000    0.009    0.000 protocol.py:211(is_error_packet)\n",
       "    17289    0.009    0.000    0.014    0.000 managers.py:1549(internal_values)\n",
       "     2858    0.009    0.000    0.017    0.000 dtypes.py:786(construct_from_string)\n",
       "     1069    0.009    0.000    1.199    0.001 generic.py:3323(_take)\n",
       "     1377    0.009    0.000    0.188    0.000 construction.py:537(sanitize_array)\n",
       "     5072    0.009    0.000    0.021    0.000 managers.py:139(shape)\n",
       "    18729    0.009    0.000    0.064    0.000 common.py:434(is_datetime64tz_dtype)\n",
       "  524/405    0.009    0.000    0.105    0.000 base.py:253(__new__)\n",
       "    15981    0.009    0.000    0.029    0.000 generic.py:377(_get_axis)\n",
       "     9249    0.008    0.000    0.041    0.000 common.py:403(is_datetime64_dtype)\n",
       "       29    0.008    0.000    1.461    0.050 frame.py:4639(duplicated)\n",
       "     6891    0.008    0.000    0.014    0.000 series.py:392(name)\n",
       "     3900    0.008    0.000    0.192    0.000 indexing.py:2205(_getitem_axis)\n",
       "    45183    0.008    0.000    0.008    0.000 protocol.py:190(is_eof_packet)\n",
       "     3900    0.008    0.000    0.013    0.000 indexing.py:2089(_is_scalar_access)\n",
       "    29103    0.007    0.000    0.018    0.000 abc.py:137(__instancecheck__)\n",
       "     4988    0.007    0.000    0.016    0.000 base.py:3940(__getitem__)\n",
       "     3900    0.007    0.000    0.067    0.000 indexing.py:217(_has_valid_tuple)\n",
       "     9217    0.007    0.000    0.011    0.000 {pandas._libs.lib.is_scalar}\n",
       "     7800    0.007    0.000    0.055    0.000 indexing.py:2056(_validate_key)\n",
       "     8202    0.007    0.000    0.027    0.000 common.py:131(is_object_dtype)\n",
       "    28392    0.007    0.000    0.011    0.000 common.py:119(<lambda>)\n",
       "        1    0.007    0.007   52.141   52.141 <ipython-input-287-197799a2f9a3>:2(main)\n",
       "     1604    0.007    0.000    0.041    0.000 blocks.py:3034(get_block_type)\n",
       "     2949    0.007    0.000    0.019    0.000 _dtype.py:46(__str__)\n",
       "     1328    0.007    0.000    0.016    0.000 cast.py:832(maybe_castable)\n",
       "     4543    0.007    0.000    0.018    0.000 dtypes.py:973(is_dtype)\n",
       "    11821    0.007    0.000    0.012    0.000 managers.py:1522(dtype)\n",
       "     3679    0.007    0.000    0.023    0.000 blocks.py:2626(__init__)\n",
       "     1779    0.007    0.000    0.014    0.000 base.py:504(_simple_new)\n",
       "    15594    0.006    0.000    0.024    0.000 <frozen importlib._bootstrap>:1009(_handle_fromlist)\n",
       "     7136    0.006    0.000    0.051    0.000 blocks.py:225(make_block_same_class)\n",
       "    15660    0.006    0.000    0.012    0.000 generic.py:450(ndim)\n",
       "     4441    0.006    0.000    0.062    0.000 frame.py:3349(_box_col_values)\n",
       "       32    0.006    0.000    0.006    0.000 {pandas._libs.algos.take_2d_axis0_object_object}\n",
       "     1449    0.006    0.000    0.028    0.000 cast.py:953(maybe_cast_to_datetime)\n",
       "    28392    0.006    0.000    0.006    0.000 common.py:117(classes)\n",
       "    17289    0.006    0.000    0.020    0.000 series.py:476(_values)\n",
       "     7784    0.006    0.000    0.025    0.000 indexing.py:2116(_validate_integer)\n",
       "     8202    0.006    0.000    0.029    0.000 common.py:472(is_timedelta64_dtype)\n",
       "     4715    0.006    0.000    0.361    0.000 frame.py:919(<genexpr>)\n",
       "     2503    0.006    0.000    0.006    0.000 {pandas._libs.algos.ensure_int64}\n",
       "     4247    0.006    0.000    0.013    0.000 dtypes.py:672(construct_from_string)\n",
       "     2134    0.006    0.000    0.015    0.000 numeric.py:2656(seterr)\n",
       "     1315    0.006    0.000    0.113    0.000 managers.py:97(__init__)\n",
       "     5163    0.006    0.000    0.066    0.000 common.py:702(is_datetimelike)\n",
       "      943    0.006    0.000    0.616    0.001 indexing.py:1855(_getitem_axis)\n",
       "    19789    0.006    0.000    0.007    0.000 managers.py:143(ndim)\n",
       "    30763    0.006    0.000    0.006    0.000 managers.py:1488(_block)\n",
       "    11704    0.005    0.000    0.024    0.000 indexing.py:2689(is_list_like_indexer)\n",
       "        4    0.005    0.001    5.642    1.410 <ipython-input-283-5131e7c93670>:101(get_ref_ann)\n",
       "    24284    0.005    0.000    0.005    0.000 {method 'get' of 'dict' objects}\n",
       "       46    0.005    0.000    0.595    0.013 managers.py:159(rename_axis)\n",
       "      287    0.005    0.000    0.005    0.000 {method 'sendall' of '_socket.socket' objects}\n",
       "     1333    0.005    0.000    0.022    0.000 base.py:566(_shallow_copy)\n",
       "     2134    0.005    0.000    0.005    0.000 numeric.py:2758(geterr)\n",
       "     2380    0.005    0.000    0.058    0.000 base.py:4051(equals)\n",
       "    11700    0.005    0.000    0.008    0.000 indexing.py:1487(<genexpr>)\n",
       "    11821    0.005    0.000    0.017    0.000 series.py:406(dtype)\n",
       "     1049    0.005    0.000    0.027    0.000 base.py:786(array)\n",
       "     1242    0.005    0.000    0.288    0.000 frame.py:378(__init__)\n",
       "     1154    0.005    0.000    0.015    0.000 managers.py:306(_verify_integrity)\n",
       "       41    0.005    0.000    0.005    0.000 {pandas._libs.lib.to_object_array_tuples}\n",
       "     9893    0.005    0.000    0.014    0.000 common.py:1809(_get_dtype)\n",
       "     1377    0.005    0.000    0.066    0.000 construction.py:684(_try_cast)\n",
       "       41    0.005    0.000    2.012    0.049 sql.py:317(read_sql)\n",
       "     1069    0.005    0.000    0.890    0.001 managers.py:1198(reindex_indexer)\n",
       "     5825    0.004    0.000    0.008    0.000 series.py:399(name)\n",
       "     3030    0.004    0.000    0.028    0.000 common.py:1578(is_bool_dtype)\n",
       "     2858    0.004    0.000    0.007    0.000 dtypes.py:929(construct_from_string)\n",
       "     3944    0.004    0.000    0.004    0.000 dtypes.py:452(construct_from_string)\n",
       "     1049    0.004    0.000    0.010    0.000 numpy_.py:35(__init__)\n",
       "     3030    0.004    0.000    0.004    0.000 {method 'match' of 're.Pattern' objects}\n",
       "     6255    0.004    0.000    0.004    0.000 {method 'view' of 'numpy.ndarray' objects}\n",
       "     8730    0.004    0.000    0.032    0.000 base.py:5318(ensure_index)\n",
       "     1069    0.004    0.000    0.121    0.000 base.py:784(take)\n",
       "    11994    0.004    0.000    0.019    0.000 inference.py:304(is_array_like)\n",
       "     7181    0.004    0.000    0.030    0.000 common.py:572(is_categorical_dtype)\n",
       "     3900    0.004    0.000    0.013    0.000 indexing.py:229(_is_nested_tuple_indexer)\n",
       "     1041    0.004    0.000    0.773    0.001 managers.py:1233(<listcomp>)\n",
       "    10394    0.004    0.000    0.005    0.000 common.py:316(apply_if_callable)\n",
       "    17379    0.004    0.000    0.004    0.000 blocks.py:308(dtype)\n",
       "       41    0.004    0.000    0.004    0.000 result.py:1192(<listcomp>)\n",
       "     5984    0.004    0.000    0.031    0.000 {method 'any' of 'numpy.ndarray' objects}\n",
       "    15216    0.004    0.000    0.012    0.000 managers.py:141(<genexpr>)\n",
       "      111    0.004    0.000    0.004    0.000 {built-in method posix.stat}\n",
       "9463/7329    0.004    0.000    2.315    0.000 numeric.py:469(asarray)\n",
       "     2949    0.004    0.000    0.035    0.000 blocks.py:312(ftype)\n",
       "    16308    0.004    0.000    0.004    0.000 {method 'startswith' of 'str' objects}\n",
       "     4441    0.004    0.000    0.010    0.000 generic.py:3070(_set_as_cached)\n",
       "     7800    0.004    0.000    0.010    0.000 indexing.py:2695(is_label_like)\n",
       "     8484    0.004    0.000    0.004    0.000 blocks.py:89(_check_ndim)\n",
       "      943    0.003    0.000    0.447    0.000 indexing.py:1511(_getbool_axis)\n",
       "    16582    0.003    0.000    0.003    0.000 blocks.py:195(mgr_locs)\n",
       "     3900    0.003    0.000    0.340    0.000 indexing.py:2141(_getitem_tuple)\n",
       "       38    0.003    0.000    0.004    0.000 numeric.py:2551(array_equal)\n",
       "     1343    0.003    0.000    0.041    0.000 managers.py:599(_consolidate_check)\n",
       "       31    0.003    0.000    0.003    0.000 {pandas._libs.algos.take_2d_axis0_int64_int64}\n",
       "     5108    0.003    0.000    0.009    0.000 base.py:3608(values)\n",
       "       41    0.003    0.000    0.093    0.002 frame.py:1430(from_records)\n",
       "     4441    0.003    0.000    0.003    0.000 blocks.py:332(iget)\n",
       "     2973    0.003    0.000    0.010    0.000 dtypes.py:827(is_dtype)\n",
       "    11700    0.003    0.000    0.004    0.000 indexing.py:230(<genexpr>)\n",
       "     1080    0.003    0.000    0.056    0.000 indexing.py:2475(check_bool_indexer)\n",
       "     6871    0.003    0.000    0.004    0.000 inference.py:438(is_hashable)\n",
       "     2505    0.003    0.000    0.007    0.000 common.py:1545(is_float_dtype)\n",
       "     4970    0.003    0.000    0.021    0.000 common.py:536(is_interval_dtype)\n",
       "     3892    0.003    0.000    0.150    0.000 indexing.py:143(_get_loc)\n",
       "     4105    0.003    0.000    0.007    0.000 {built-in method builtins.any}\n",
       "     1010    0.003    0.000    0.003    0.000 {built-in method pandas._libs.lib.is_datetime_array}\n",
       "     1835    0.003    0.000    0.030    0.000 generic.py:3056(_get_item_cache)\n",
       "       29    0.002    0.000    0.002    0.000 {built-in method _operator.inv}\n",
       "     5984    0.002    0.000    0.028    0.000 _methods.py:42(_any)\n",
       "     1045    0.002    0.000    0.036    0.000 series.py:669(__array__)\n",
       "     3470    0.002    0.000    0.006    0.000 {pandas._libs.lib.values_from_object}\n",
       "    17409    0.002    0.000    0.002    0.000 blocks.py:165(internal_values)\n",
       "     1128    0.002    0.000    0.034    0.000 missing.py:360(array_equivalent)\n",
       "     2134    0.002    0.000    0.002    0.000 {built-in method numpy.seterrobj}\n",
       "     3908    0.002    0.000    0.003    0.000 common.py:279(is_null_slice)\n",
       "     4937    0.002    0.000    0.003    0.000 common.py:144(cast_scalar_indexer)\n",
       "      993    0.002    0.000    3.613    0.004 ops.py:1615(na_op)\n",
       "     1787    0.002    0.000    0.034    0.000 missing.py:105(_isna_new)\n",
       "     1049    0.002    0.000    0.012    0.000 numpy_.py:127(__init__)\n",
       "     5671    0.002    0.000    0.002    0.000 series.py:382(_set_subtyp)\n",
       "       41    0.002    0.000    0.106    0.003 sql.py:136(_wrap_result)\n",
       "     1192    0.002    0.000    0.002    0.000 {method 'get_loc' of 'pandas._libs.index.IndexEngine' objects}\n",
       "     2454    0.002    0.000    0.005    0.000 common.py:746(is_dtype_equal)\n",
       "     4098    0.002    0.000    0.006    0.000 managers.py:291(__len__)\n",
       "      985    0.002    0.000    3.601    0.004 ops.py:1591(_comp_method_OBJECT_ARRAY)\n",
       "    10016    0.002    0.000    0.002    0.000 {built-in method builtins.hash}\n",
       "     2739    0.002    0.000    0.004    0.000 sparse.py:196(construct_from_string)\n",
       "     1033    0.002    0.000    0.005    0.000 ops.py:43(get_op_result_name)\n",
       "     2066    0.002    0.000    0.005    0.000 common.py:868(is_integer_dtype)\n",
       "    13378    0.002    0.000    0.002    0.000 {pandas._libs.lib.is_integer}\n",
       "     3099    0.002    0.000    0.015    0.000 common.py:262(is_categorical)\n",
       "     3900    0.002    0.000    0.002    0.000 frame.py:474(axes)\n",
       "      182    0.002    0.000    1.304    0.007 managers.py:318(apply)\n",
       "     2380    0.002    0.000    0.002    0.000 base.py:613(is_)\n",
       "     2973    0.002    0.000    0.012    0.000 common.py:503(is_period_dtype)\n",
       "       75    0.002    0.000    0.017    0.000 {pandas._libs.lib.clean_index_list}\n",
       "     1343    0.002    0.000    0.037    0.000 managers.py:600(<listcomp>)\n",
       "     1123    0.002    0.000    0.005    0.000 managers.py:1556(get_values)\n",
       "     1535    0.002    0.000    0.003    0.000 base.py:547(_get_attributes_dict)\n",
       "     1010    0.002    0.000    0.006    0.000 base.py:1736(is_all_dates)\n",
       "     1067    0.002    0.000    0.008    0.000 numeric.py:3063(__exit__)\n",
       "     1832    0.002    0.000    0.003    0.000 __init__.py:221(iteritems)\n",
       "     1315    0.002    0.000    0.003    0.000 managers.py:98(<listcomp>)\n",
       "     6568    0.002    0.000    0.002    0.000 managers.py:206(items)\n",
       "      103    0.002    0.000    0.026    0.000 managers.py:1019(set)\n",
       "      943    0.002    0.000    0.002    0.000 {built-in method builtins.repr}\n",
       "     1114    0.002    0.000    0.275    0.000 generic.py:5122(_protect_consolidate)\n",
       "     1231    0.002    0.000    0.002    0.000 generic.py:349(_get_axis_number)\n",
       "      534    0.002    0.000    0.003    0.000 blocks.py:3100(_extend_blocks)\n",
       "      513    0.002    0.000    0.014    0.000 managers.py:934(get)\n",
       "      562    0.002    0.000    0.014    0.000 algorithms.py:38(_ensure_data)\n",
       "        1    0.002    0.002    0.002    0.002 parsers.py:1830(__init__)\n",
       "     1495    0.002    0.000    0.017    0.000 common.py:1643(is_extension_type)\n",
       "     1684    0.002    0.000    0.003    0.000 {method 'join' of 'str' objects}\n",
       "     1361    0.002    0.000    0.002    0.000 generic.py:5036(__finalize__)\n",
       "      155    0.002    0.000    0.003    0.000 base.py:1658(is_unique)\n",
       "     1022    0.002    0.000    0.002    0.000 generic.py:3175(_set_is_copy)\n",
       "     4486    0.002    0.000    0.002    0.000 common.py:127(<lambda>)\n",
       "     1192    0.002    0.000    0.363    0.000 {method 'extend' of 'list' objects}\n",
       "       58    0.002    0.000    0.193    0.003 managers.py:1696(form_blocks)\n",
       "      205    0.001    0.000    0.003    0.000 protocol.py:283(__init__)\n",
       "     1067    0.001    0.000    0.002    0.000 numeric.py:3054(__init__)\n",
       "     1225    0.001    0.000    0.002    0.000 generic.py:144(_init_mgr)\n",
       "        1    0.001    0.001    0.202    0.202 sorting.py:366(compress_group_index)\n",
       "     3553    0.001    0.000    0.002    0.000 managers.py:308(<genexpr>)\n",
       "     2542    0.001    0.000    0.254    0.000 managers.py:927(_consolidate_inplace)\n",
       "     4268    0.001    0.000    0.001    0.000 {built-in method numpy.geterrobj}\n",
       "     1067    0.001    0.000    0.010    0.000 numeric.py:3058(__enter__)\n",
       "     1114    0.001    0.000    0.273    0.000 generic.py:5135(f)\n",
       "      407    0.001    0.000    0.006    0.000 cast.py:846(maybe_infer_to_datetimelike)\n",
       "      567    0.001    0.000    0.896    0.002 socket.py:575(readinto)\n",
       "     3647    0.001    0.000    0.003    0.000 managers.py:591(is_consolidated)\n",
       "     1433    0.001    0.000    0.001    0.000 {method 'replace' of 'str' objects}\n",
       "   304/79    0.001    0.000    0.015    0.000 <frozen importlib._bootstrap>:978(_find_and_load)\n",
       "     1123    0.001    0.000    0.003    0.000 generic.py:381(_get_block_manager_axis)\n",
       "     1049    0.001    0.000    0.002    0.000 common.py:1118(is_datetime64_ns_dtype)\n",
       "      951    0.001    0.000    0.138    0.000 base.py:3975(_can_hold_identifiers_and_holds_name)\n",
       "     1535    0.001    0.000    0.002    0.000 base.py:551(<dictcomp>)\n",
       "     2743    0.001    0.000    0.001    0.000 {method 'search' of 're.Pattern' objects}\n",
       "    10395    0.001    0.000    0.001    0.000 {built-in method builtins.callable}\n",
       "     1093    0.001    0.000    0.003    0.000 config.py:78(_get_single_key)\n",
       "      185    0.001    0.000    0.029    0.000 missing.py:183(_isna_ndarraylike)\n",
       "      287    0.001    0.000    0.008    0.000 connections.py:744(_execute_command)\n",
       "     4715    0.001    0.000    0.001    0.000 __init__.py:388(<genexpr>)\n",
       "     7333    0.001    0.000    0.001    0.000 {pandas._libs.lib.is_float}\n",
       "      111    0.001    0.000    0.007    0.000 <frozen importlib._bootstrap_external>:1356(find_spec)\n",
       "      434    0.001    0.000    0.043    0.000 common.py:222(asarray_tuplesafe)\n",
       "     1445    0.001    0.000    0.003    0.000 common.py:1784(_is_dtype)\n",
       "     1011    0.001    0.000    0.042    0.000 generic.py:178(_validate_dtype)\n",
       "     1253    0.001    0.000    0.003    0.000 cast.py:1218(construct_1d_ndarray_preserving_na)\n",
       "     1787    0.001    0.000    0.035    0.000 missing.py:25(isna)\n",
       "      246    0.001    0.000    0.040    0.000 algorithms.py:132(_reconstruct_data)\n",
       "       39    0.001    0.000    0.182    0.005 <ipython-input-267-cd2ed8d3f7ce>:8(__init__)\n",
       "      513    0.001    0.000    0.012    0.000 frame.py:3342(_box_item_values)\n",
       "     1114    0.001    0.000    0.276    0.000 generic.py:5132(_consolidate_inplace)\n",
       "     1829    0.001    0.000    0.001    0.000 base.py:633(_reset_identity)\n",
       "      483    0.001    0.000    0.026    0.000 frame.py:742(iteritems)\n",
       "     8487    0.001    0.000    0.001    0.000 {method '__contains__' of 'frozenset' objects}\n",
       "      445    0.001    0.000    1.280    0.003 blocks.py:749(copy)\n",
       "     1245    0.001    0.000    0.144    0.000 inference.py:121(is_iterator)\n",
       "     1123    0.001    0.000    0.006    0.000 series.py:490(get_values)\n",
       "     1377    0.001    0.000    0.005    0.000 arrays.py:7(extract_array)\n",
       "     8487    0.001    0.000    0.001    0.000 {method 'isidentifier' of 'str' objects}\n",
       "       82    0.001    0.000    0.001    0.000 connections.py:1069(__del__)\n",
       "       24    0.001    0.000    0.001    0.000 {pandas._libs.algos.rank_1d_float64}\n",
       "     1299    0.001    0.000    0.004    0.000 common.py:923(is_signed_integer_dtype)\n",
       "     2256    0.001    0.000    0.003    0.000 base.py:3663(get_values)\n",
       "     1148    0.001    0.000    0.003    0.000 base.py:2650(get_loc)\n",
       "      998    0.001    0.000    0.042    0.000 base.py:802(_assert_take_fillable)\n",
       "     1045    0.001    0.000    0.002    0.000 numpy_.py:170(__array__)\n",
       "       56    0.001    0.000    0.006    0.000 format.py:1045(get_result_as_array)\n",
       "     4486    0.001    0.000    0.001    0.000 common.py:122(classes_and_not_datetimelike)\n",
       "       36    0.001    0.000    0.024    0.001 managers.py:1241(_slice_take_blocks_ax0)\n",
       "      246    0.001    0.000    0.936    0.004 frame.py:4666(f)\n",
       "       34    0.001    0.000    0.001    0.000 blocks.py:2689(set)\n",
       "       79    0.001    0.000    0.009    0.000 <frozen importlib._bootstrap>:882(_find_spec)\n",
       "     1602    0.001    0.000    0.001    0.000 {built-in method pandas._libs.missing.checknull}\n",
       "     1049    0.001    0.000    0.002    0.000 common.py:1167(is_timedelta64_ns_dtype)\n",
       "      103    0.001    0.000    0.001    0.000 socket.py:337(send)\n",
       "     2170    0.001    0.000    0.001    0.000 config.py:561(_get_deprecated_option)\n",
       "        4    0.001    0.000    0.075    0.019 <ipython-input-284-a912178f7287>:129(generate_ensemble_metrics)\n",
       "      304    0.001    0.000    0.002    0.000 <frozen importlib._bootstrap>:157(_get_module_lock)\n",
       "      103    0.001    0.000    1.230    0.012 generic.py:3205(_check_setitem_copy)\n",
       "   528/39    0.001    0.000    0.001    0.000 arrayprint.py:716(recurser)\n",
       "       41    0.001    0.000    0.009    0.000 base.py:1343(_handle_dbapi_exception)\n",
       "     7665    0.001    0.000    0.001    0.000 {method 'add' of 'set' objects}\n",
       "     3223    0.001    0.000    0.001    0.000 {method 'items' of 'dict' objects}\n",
       "       23    0.001    0.000    0.992    0.043 generic.py:960(rename)\n",
       "      959    0.001    0.000    0.003    0.000 base.py:1681(is_object)\n",
       "      256    0.001    0.000    0.005    0.000 format.py:1384(_make_fixed_width)\n",
       "       44    0.001    0.000    0.002    0.000 index_tricks.py:316(__getitem__)\n",
       "     2439    0.001    0.000    0.001    0.000 blocks.py:304(shape)\n",
       "      409    0.001    0.000    0.001    0.000 protocol.py:180(read_struct)\n",
       "     1545    0.001    0.000    0.001    0.000 {built-in method builtins.max}\n",
       "      591    0.001    0.000    0.001    0.000 _internal.py:886(npy_ctypes_check)\n",
       "     4442    0.001    0.000    0.001    0.000 base.py:3632(_values)\n",
       "       56    0.001    0.000    0.004    0.000 format.py:1060(format_values_with)\n",
       "     1123    0.001    0.000    0.002    0.000 blocks.py:184(to_dense)\n",
       "     1114    0.001    0.000    0.256    0.000 managers.py:911(consolidate)\n",
       "      721    0.001    0.000    0.001    0.000 <frozen importlib._bootstrap>:416(parent)\n",
       "     1077    0.001    0.000    0.001    0.000 config.py:546(_get_root)\n",
       "        8    0.001    0.000    0.038    0.005 format.py:503(_to_str_columns)\n",
       "     1077    0.001    0.000    0.005    0.000 config.py:96(_get_option)\n",
       "       82    0.001    0.000    1.859    0.023 base.py:1163(_execute_context)\n",
       "   304/79    0.001    0.000    0.013    0.000 <frozen importlib._bootstrap>:948(_find_and_load_unlocked)\n",
       "      128    0.001    0.000    0.017    0.000 format.py:848(format_array)\n",
       "       44    0.001    0.000    0.019    0.000 managers.py:1134(insert)\n",
       "       98    0.001    0.000    0.836    0.009 frame.py:2952(_getitem_bool_array)\n",
       "      107    0.001    0.000    1.066    0.010 generic.py:5699(copy)\n",
       "      867    0.001    0.000    0.008    0.000 common.py:1078(is_datetime64_any_dtype)\n",
       "      153    0.001    0.000    1.309    0.009 managers.py:710(copy)\n",
       "     1619    0.001    0.000    0.001    0.000 {method 'update' of 'dict' objects}\n",
       "     2250    0.001    0.000    0.001    0.000 blocks.py:191(fill_value)\n",
       "     2793    0.001    0.000    0.001    0.000 {method 'lower' of 'str' objects}\n",
       "        8    0.001    0.000    0.001    0.000 {method 'close' of '_io.TextIOWrapper' objects}\n",
       "       41    0.001    0.000    0.007    0.000 connections.py:1213(_get_descriptions)\n",
       "      163    0.001    0.000    0.003    0.000 protocol.py:237(_parse_field_descriptor)\n",
       "     2369    0.001    0.000    0.001    0.000 {pandas._libs.algos.ensure_platform_int}\n",
       "       82    0.001    0.000    0.016    0.000 base.py:748(_checkout)\n",
       "     3963    0.001    0.000    0.001    0.000 base.py:704(ndim)\n",
       "     2024    0.001    0.000    0.001    0.000 {built-in method builtins.iter}\n",
       "     1001    0.001    0.000    0.002    0.000 common.py:980(is_unsigned_integer_dtype)\n",
       "      158    0.001    0.000    0.014    0.000 base.py:584(_shallow_copy_with_infer)\n",
       "       82    0.001    0.000    0.002    0.000 base.py:69(__init__)\n",
       "      104    0.001    0.000    0.001    0.000 numeric.py:676(require)\n",
       "      209    0.001    0.000    0.012    0.000 numeric.py:67(_shallow_copy)\n",
       "      270    0.001    0.000    0.157    0.001 algorithms.py:217(_get_data_algo)\n",
       "      205    0.001    0.000    0.025    0.000 connections.py:393(_read_ok_packet)\n",
       "     1463    0.001    0.000    0.004    0.000 numeric.py:541(asanyarray)\n",
       "      303    0.001    0.000    0.006    0.000 base.py:1096(tolist)\n",
       "      555    0.001    0.000    0.001    0.000 <frozen importlib._bootstrap_external>:56(_path_join)\n",
       "      244    0.001    0.000    0.001    0.000 base.py:643(_engine)\n",
       "      349    0.001    0.000    0.001    0.000 {pandas._libs.lib.infer_datetimelike_array}\n",
       "       64    0.001    0.000    0.020    0.000 frame.py:3565(_sanitize_column)\n",
       "      122    0.001    0.000    0.002    0.000 numeric.py:34(__new__)\n",
       "     1300    0.001    0.000    0.001    0.000 format.py:301(len)\n",
       "      306    0.001    0.000    0.006    0.000 base.py:700(view)\n",
       "      120    0.001    0.000    0.030    0.000 format.py:702(_format_col)\n",
       "       56    0.001    0.000    0.001    0.000 {method 'get_indexer' of 'pandas._libs.index.IndexEngine' objects}\n",
       "      100    0.001    0.000    0.001    0.000 numerictypes.py:578(_can_coerce_all)\n",
       "      971    0.000    0.000    0.001    0.000 generic.py:426(_info_axis)\n",
       "     1701    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
       "      287    0.000    0.000    0.007    0.000 connections.py:710(_write_bytes)\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method posix.listdir}\n",
       "      943    0.000    0.000    0.000    0.000 {built-in method sys.intern}\n",
       "     1441    0.000    0.000    0.000    0.000 base.py:676(dtype)\n",
       "       41    0.000    0.000    0.961    0.023 connections.py:1149(_read_result_packet)\n",
       "      304    0.000    0.000    0.001    0.000 <frozen importlib._bootstrap>:78(acquire)\n",
       "       41    0.000    0.000    0.001    0.000 schema.py:3753(__init__)\n",
       "       82    0.000    0.000    0.001    0.000 default.py:862(_init_statement)\n",
       "      943    0.000    0.000    0.000    0.000 {built-in method sys._getframe}\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method io.open}\n",
       "      943    0.000    0.000    0.001    0.000 indexing.py:1831(_get_partial_string_timestamp_match_key)\n",
       "      275    0.000    0.000    0.018    0.000 frame.py:4685(<genexpr>)\n",
       "     1747    0.000    0.000    0.000    0.000 {pandas._libs.algos.ensure_object}\n",
       "       53    0.000    0.000    0.004    0.000 construction.py:284(extract_index)\n",
       "      492    0.000    0.000    0.001    0.000 range.py:510(__len__)\n",
       "      248    0.000    0.000    0.001    0.000 format.py:356(_get_formatter)\n",
       "     1188    0.000    0.000    0.000    0.000 frame.py:361(_constructor)\n",
       "       64    0.000    0.000    0.001    0.000 base.py:3926(contains)\n",
       "       82    0.000    0.000    0.001    0.000 cursors.py:135(mogrify)\n",
       "       56    0.000    0.000    0.011    0.000 base.py:2715(get_indexer)\n",
       "      190    0.000    0.000    0.001    0.000 indexing.py:2450(convert_to_index_sliceable)\n",
       "     1174    0.000    0.000    0.000    0.000 series.py:338(_constructor)\n",
       "      256    0.000    0.000    0.001    0.000 blocks.py:128(_consolidate_key)\n",
       "      246    0.000    0.000    0.934    0.004 _decorators.py:146(wrapper)\n",
       "      567    0.000    0.000    0.001    0.000 {method '_checkReadable' of '_io._IOBase' objects}\n",
       "       79    0.000    0.000    0.008    0.000 <frozen importlib._bootstrap_external>:1240(_get_spec)\n",
       "       82    0.000    0.000    0.002    0.000 base.py:481(checkout)\n",
       "     1136    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}\n",
       "      304    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:103(release)\n",
       "       39    0.000    0.000    0.003    0.000 {method 'get_value' of 'pandas._libs.index.IndexEngine' objects}\n",
       "       56    0.000    0.000    0.000    0.000 {pandas._libs.algos.take_1d_int64_int64}\n",
       "       64    0.000    0.000    1.275    0.020 frame.py:3356(__setitem__)\n",
       "       46    0.000    0.000    0.345    0.008 managers.py:1988(_transform_index)\n",
       "     1154    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
       "      582    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
       "      489    0.000    0.000    0.000    0.000 arrayprint.py:693(_extendLine)\n",
       "     1077    0.000    0.000    0.005    0.000 config.py:226(__call__)\n",
       "      304    0.000    0.000    0.001    0.000 <frozen importlib._bootstrap>:58(__init__)\n",
       "       35    0.000    0.000    0.251    0.007 managers.py:1887(_consolidate)\n",
       "      305    0.000    0.000    0.002    0.000 base.py:963(_ndarray_values)\n",
       "       58    0.000    0.000    0.102    0.002 construction.py:254(_homogenize)\n",
       "       41    0.000    0.000    0.027    0.001 construction.py:429(_list_to_arrays)\n",
       "      328    0.000    0.000    0.001    0.000 managers.py:1546(external_values)\n",
       "       28    0.000    0.000    0.018    0.001 indexing.py:1271(_convert_to_indexer)\n",
       "      123    0.000    0.000    0.019    0.000 base.py:2223(do_rollback)\n",
       "       39    0.000    0.000    0.005    0.000 base.py:4342(get_value)\n",
       "      164    0.000    0.000    0.000    0.000 threading.py:335(notify)\n",
       "       41    0.000    0.000    0.020    0.000 base.py:2312(has_table)\n",
       "       39    0.000    0.000    0.014    0.000 series.py:865(__getitem__)\n",
       "       34    0.000    0.000    0.001    0.000 printing.py:15(adjoin)\n",
       "       82    0.000    0.000    0.001    0.000 queue.py:135(get)\n",
       "       82    0.000    0.000    0.001    0.000 default.py:1034(create_cursor)\n",
       "     1077    0.000    0.000    0.001    0.000 config.py:602(_warn_if_deprecated)\n",
       "       82    0.000    0.000    1.841    0.022 connections.py:720(_read_query_result)\n",
       "       28    0.000    0.000    0.015    0.001 indexing.py:1108(_get_listlike_indexer)\n",
       "       82    0.000    0.000    0.001    0.000 cursors.py:116(_escape_args)\n",
       "       82    0.000    0.000    0.001    0.000 queue.py:92(put)\n",
       "       41    0.000    0.000    0.001    0.000 result.py:215(__init__)\n",
       "      306    0.000    0.000    0.006    0.000 managers.py:729(<lambda>)\n",
       "     1462    0.000    0.000    0.000    0.000 base.py:1396(nlevels)\n",
       "      555    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:58(<listcomp>)\n",
       "       82    0.000    0.000    1.844    0.022 connections.py:508(query)\n",
       "       82    0.000    0.000    0.018    0.000 base.py:2223(_contextual_connect)\n",
       "     1093    0.000    0.000    0.001    0.000 config.py:589(_translate_key)\n",
       "      142    0.000    0.000    0.000    0.000 shape_base.py:83(atleast_2d)\n",
       "      128    0.000    0.000    0.012    0.000 format.py:927(get_result)\n",
       "     1093    0.000    0.000    0.000    0.000 config.py:528(_select_options)\n",
       "      451    0.000    0.000    0.000    0.000 protocol.py:186(is_ok_packet)\n",
       "       54    0.000    0.000    0.076    0.001 fromnumeric.py:69(_wrapreduction)\n",
       "       44    0.000    0.000    0.003    0.000 managers.py:2008(_fast_count_smallints)\n",
       "        4    0.000    0.000    0.001    0.000 generic.py:9415(abs)\n",
       "      304    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:176(cb)\n",
       "       41    0.000    0.000    0.001    0.000 exc.py:390(instance)\n",
       "       64    0.000    0.000    1.274    0.020 frame.py:3433(_set_item)\n",
       "      163    0.000    0.000    0.021    0.000 construction.py:495(convert)\n",
       "      556    0.000    0.000    0.001    0.000 printing.py:50(justify)\n",
       "       82    0.000    0.000    1.841    0.022 connections.py:1073(read)\n",
       "       82    0.000    0.000    0.014    0.000 base.py:645(_finalize_fairy)\n",
       "      304    0.000    0.000    0.002    0.000 <frozen importlib._bootstrap>:147(__enter__)\n",
       "       97    0.000    0.000    0.002    0.000 iostream.py:382(write)\n",
       "     1300    0.000    0.000    0.000    0.000 __init__.py:291(strlen)\n",
       "       41    0.000    0.000    0.001    0.000 sql.py:972(__init__)\n",
       "   225/75    0.000    0.000    0.011    0.000 {built-in method builtins.__import__}\n",
       "       24    0.000    0.000    0.000    0.000 function_base.py:1149(diff)\n",
       "      608    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n",
       "       44    0.000    0.000    0.013    0.000 base.py:4919(insert)\n",
       "       22    0.000    0.000    0.044    0.002 {built-in method builtins.print}\n",
       "       39    0.000    0.000    0.002    0.000 arrayprint.py:480(_array2string)\n",
       "       39    0.000    0.000    0.000    0.000 {method 'argsort' of 'numpy.ndarray' objects}\n",
       "      567    0.000    0.000    0.000    0.000 socket.py:614(readable)\n",
       "      123    0.000    0.000    0.018    0.000 connections.py:422(rollback)\n",
       "      426    0.000    0.000    0.000    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
       "       58    0.000    0.000    0.306    0.005 construction.py:43(arrays_to_mgr)\n",
       "        8    0.000    0.000    0.000    0.000 {method 'writelines' of '_io._IOBase' objects}\n",
       "       82    0.000    0.000    0.001    0.000 base.py:507(checkin)\n",
       "       28    0.000    0.000    0.001    0.000 indexing.py:1209(_validate_read_indexer)\n",
       "      556    0.000    0.000    0.001    0.000 format.py:304(justify)\n",
       "       24    0.000    0.000    0.011    0.000 strings.py:1854(_wrap_result)\n",
       "       58    0.000    0.000    0.198    0.003 managers.py:1663(create_block_manager_from_arrays)\n",
       "       41    0.000    0.000    0.039    0.001 base.py:2133(run_callable)\n",
       "       41    0.000    0.000    0.000    0.000 base.py:2260(is_disconnect)\n",
       "        4    0.000    0.000    0.000    0.000 <ipython-input-276-996dcf9791ab>:1(cm_dict)\n",
       "      328    0.000    0.000    0.001    0.000 series.py:434(values)\n",
       "       82    0.000    0.000    0.014    0.000 connections.py:532(ping)\n",
       "        8    0.000    0.000    0.024    0.003 {_cython_magic_b3f8a51c007f82d9691fb50399c3a68a.geometric_mean}\n",
       "       82    0.000    0.000    0.012    0.000 base.py:845(_reset)\n",
       "       41    0.000    0.000    0.043    0.001 construction.py:382(to_arrays)\n",
       "      200    0.000    0.000    0.002    0.000 common.py:1431(needs_i8_conversion)\n",
       "       23    0.000    0.000    0.243    0.011 series.py:3947(isin)\n",
       "       39    0.000    0.000    0.003    0.000 arrayprint.py:518(array2string)\n",
       "       82    0.000    0.000    1.860    0.023 base.py:1138(_execute_text)\n",
       "       41    0.000    0.000    0.011    0.000 sql.py:115(_parse_date_columns)\n",
       "      103    0.000    0.000    0.001    0.000 iostream.py:195(schedule)\n",
       "       56    0.000    0.000    0.000    0.000 format.py:1019(base_formatter)\n",
       "      246    0.000    0.000    0.000    0.000 protocol.py:86(advance)\n",
       "      205    0.000    0.000    0.002    0.000 base.py:646(<lambda>)\n",
       "       29    0.000    0.000    0.003    0.000 base.py:2445(difference)\n",
       "      287    0.000    0.000    0.000    0.000 {built-in method _struct.pack}\n",
       "      136    0.000    0.000    0.001    0.000 printing.py:156(pprint_thing)\n",
       "      409    0.000    0.000    0.000    0.000 {method 'unpack_from' of 'Struct' objects}\n",
       "       82    0.000    0.000    1.844    0.022 cursors.py:324(_query)\n",
       "       41    0.000    0.000    1.966    0.048 sql.py:1055(read_query)\n",
       "       64    0.000    0.000    0.023    0.000 generic.py:3171(_set_item)\n",
       "      152    0.000    0.000    0.001    0.000 format.py:1421(_cond)\n",
       "      334    0.000    0.000    0.001    0.000 frame.py:937(__len__)\n",
       "      136    0.000    0.000    0.000    0.000 printing.py:185(as_escaped_unicode)\n",
       "      180    0.000    0.000    0.000    0.000 generic.py:3149(_clear_item_cache)\n",
       "      245    0.000    0.000    0.000    0.000 weakref.py:395(__getitem__)\n",
       "       42    0.000    0.000    0.001    0.000 range.py:69(__new__)\n",
       "       82    0.000    0.000    0.000    0.000 connections.py:1053(__init__)\n",
       "       39    0.000    0.000    0.020    0.001 series.py:4221(dropna)\n",
       "      111    0.000    0.000    0.003    0.000 missing.py:259(notna)\n",
       "       49    0.000    0.000    0.004    0.000 range.py:272(_shallow_copy)\n",
       "        4    0.000    0.000    0.002    0.001 csvs.py:290(_save_chunk)\n",
       "      300    0.000    0.000    0.002    0.000 common.py:605(is_string_dtype)\n",
       "      153    0.000    0.000    0.006    0.000 managers.py:730(<listcomp>)\n",
       "       41    0.000    0.000    0.000    0.000 result.py:588(_key_fallback)\n",
       "       41    0.000    0.000    0.039    0.001 sql.py:1197(has_table)\n",
       "     1049    0.000    0.000    0.000    0.000 common.py:1195(<lambda>)\n",
       "    28/24    0.000    0.000    0.339    0.014 strings.py:62(_map)\n",
       "      204    0.000    0.000    0.000    0.000 result.py:560(_merge_cols_by_none)\n",
       "       23    0.000    0.000    0.241    0.010 algorithms.py:370(isin)\n",
       "       54    0.000    0.000    0.000    0.000 managers.py:147(set_axis)\n",
       "      163    0.000    0.000    0.000    0.000 protocol.py:253(description)\n",
       "       82    0.000    0.000    0.014    0.000 base.py:869(close)\n",
       "      288    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}\n",
       "      256    0.000    0.000    0.001    0.000 managers.py:1844(_asarray_compat)\n",
       "       82    0.000    0.000    0.000    0.000 base.py:77(__init__)\n",
       "      136    0.000    0.000    0.002    0.000 format.py:338(_get_adjustment)\n",
       "      205    0.000    0.000    0.001    0.000 common.py:1198(is_datetime_or_timedelta_dtype)\n",
       "       98    0.000    0.000    0.000    0.000 generic.py:1814(__hash__)\n",
       "      204    0.000    0.000    0.000    0.000 result.py:461(_colnames_from_description)\n",
       "       50    0.000    0.000    0.000    0.000 range.py:136(_simple_new)\n",
       "      300    0.000    0.000    0.001    0.000 common.py:634(condition)\n",
       "       39    0.000    0.000    0.007    0.000 managers.py:1506(get_slice)\n",
       "       29    0.000    0.000    0.007    0.000 generic.py:1439(__neg__)\n",
       "     1159    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}\n",
       "      104    0.000    0.000    0.000    0.000 numeric.py:748(<setcomp>)\n",
       "      326    0.000    0.000    0.000    0.000 protocol.py:264(get_column_length)\n",
       "       35    0.000    0.000    0.000    0.000 blocks.py:3145(<listcomp>)\n",
       "       41    0.000    0.000    0.007    0.000 base.py:729(_rollback_impl)\n",
       "       41    0.000    0.000    0.000    0.000 pymysql.py:64(is_disconnect)\n",
       "       82    0.000    0.000    1.845    0.023 cursors.py:151(execute)\n",
       "       40    0.000    0.000    0.351    0.009 <ipython-input-280-eb2ea29a5c5f>:52(<lambda>)\n",
       "       83    0.000    0.000    0.001    0.000 langhelpers.py:852(__get__)\n",
       "       39    0.000    0.000    0.000    0.000 arrayprint.py:411(_get_format_function)\n",
       "       17    0.000    0.000    0.279    0.016 construction.py:170(init_dict)\n",
       "       41    0.000    0.000    0.000    0.000 exc.py:462(__init__)\n",
       "       88    0.000    0.000    0.001    0.000 function_base.py:4641(append)\n",
       "       16    0.000    0.000    0.002    0.000 format.py:931(_format_strings)\n",
       "      246    0.000    0.000    0.000    0.000 attr.py:267(__bool__)\n",
       "       82    0.000    0.000    0.001    0.000 impl.py:111(_do_get)\n",
       "      246    0.000    0.000    0.001    0.000 algorithms.py:163(_ensure_arraylike)\n",
       "       82    0.000    0.000    0.001    0.000 base.py:295(__get__)\n",
       "      304    0.000    0.000    0.001    0.000 <frozen importlib._bootstrap>:151(__exit__)\n",
       "      567    0.000    0.000    0.000    0.000 {method '_checkClosed' of '_io._IOBase' objects}\n",
       "       39    0.000    0.000    0.007    0.000 series.py:975(_get_values)\n",
       "       82    0.000    0.000    1.860    0.023 base.py:922(execute)\n",
       "      304    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:143(__init__)\n",
       "       41    0.000    0.000    0.002    0.000 sql.py:508(pandasSQL_builder)\n",
       "       41    0.000    0.000    0.000    0.000 err.py:100(raise_mysql_exception)\n",
       "      924    0.000    0.000    0.000    0.000 {built-in method _imp.acquire_lock}\n",
       "       30    0.000    0.000    0.000    0.000 sorting.py:47(_int64_cut_off)\n",
       "       23    0.000    0.000    0.992    0.043 frame.py:3942(rename)\n",
       "      139    0.000    0.000    0.000    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
       "       24    0.000    0.000    0.004    0.000 generic.py:8324(ranker)\n",
       "        4    0.000    0.000    0.000    0.000 {pandas._libs.writers.write_csv_rows}\n",
       "      555    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:222(_verbose_message)\n",
       "       21    0.000    0.000    0.002    0.000 blocks.py:536(_astype)\n",
       "        8    0.000    0.000    0.002    0.000 format.py:642(_join_multiline)\n",
       "      180    0.000    0.000    0.000    0.000 {method 'clear' of 'dict' objects}\n",
       "      110    0.000    0.000    0.000    0.000 cast.py:341(infer_dtype_from_scalar)\n",
       "        4    0.000    0.000    0.001    0.000 cast.py:597(astype_nansafe)\n",
       "       44    0.000    0.000    0.009    0.000 base.py:3830(_coerce_scalar_to_index)\n",
       "       56    0.000    0.000    0.000    0.000 format.py:1140(<listcomp>)\n",
       "  322/321    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
       "       67    0.000    0.000    0.000    0.000 base.py:2849(_convert_scalar_indexer)\n",
       "      512    0.000    0.000    0.000    0.000 format.py:1392(<genexpr>)\n",
       "      924    0.000    0.000    0.000    0.000 {built-in method _imp.release_lock}\n",
       "       28    0.000    0.000    0.004    0.000 base.py:3089(reindex)\n",
       "      128    0.000    0.000    0.000    0.000 format.py:912(__init__)\n",
       "       33    0.000    0.000    0.004    0.000 series.py:730(__array_wrap__)\n",
       "        4    0.000    0.000    0.120    0.030 {pandas._libs.reduction.reduce}\n",
       "       82    0.000    0.000    0.000    0.000 base.py:123(_join)\n",
       "       41    0.000    0.000    0.000    0.000 base.py:183(execution_options)\n",
       "      163    0.000    0.000    0.003    0.000 protocol.py:233(__init__)\n",
       "       82    0.000    0.000    0.000    0.000 base.py:580(get_connection)\n",
       "      179    0.000    0.000    0.000    0.000 inference.py:470(is_sequence)\n",
       "       82    0.000    0.000    0.000    0.000 _collections.py:140(__new__)\n",
       "      205    0.000    0.000    0.000    0.000 protocol.py:297(__getattr__)\n",
       "       82    0.000    0.000    0.016    0.000 base.py:345(connect)\n",
       "       82    0.000    0.000    0.000    0.000 {built-in method sqlalchemy.cutils._distill_params}\n",
       "       41    0.000    0.000    0.001    0.000 base.py:5408(default_index)\n",
       "      360    0.000    0.000    0.000    0.000 printing.py:59(<listcomp>)\n",
       "       39    0.000    0.000    0.004    0.000 generic.py:3840(_update_inplace)\n",
       "       82    0.000    0.000    0.000    0.000 log.py:56(_should_log_debug)\n",
       "       39    0.000    0.000    0.003    0.000 generic.py:3115(_maybe_update_cacher)\n",
       "       41    0.000    0.000    0.001    0.000 <string>:1(__init__)\n",
       "       82    0.000    0.000    0.014    0.000 mysqldb.py:120(do_ping)\n",
       "       39    0.000    0.000    0.000    0.000 arrayprint.py:358(_get_formatdict)\n",
       "      103    0.000    0.000    0.000    0.000 threading.py:1080(is_alive)\n",
       "       50    0.000    0.000    0.001    0.000 numerictypes.py:602(find_common_type)\n",
       "       39    0.000    0.000    0.008    0.000 series.py:913(_get_with)\n",
       "       23    0.000    0.000    9.338    0.406 <ipython-input-286-d6bec680927f>:15(flatten_list)\n",
       "      880    0.000    0.000    0.000    0.000 numeric.py:113(is_all_dates)\n",
       "       50    0.000    0.000    0.165    0.003 managers.py:1796(_simple_blockify)\n",
       "        8    0.000    0.000    0.001    0.000 format.py:651(<listcomp>)\n",
       "       28    0.000    0.000    0.000    0.000 indexing.py:256(_convert_scalar_indexer)\n",
       "       41    0.000    0.000    0.000    0.000 sql.py:45(_is_sqlalchemy_connectable)\n",
       "       41    0.000    0.000    1.848    0.045 base.py:2149(execute)\n",
       "        4    0.000    0.000    0.417    0.104 apply.py:228(apply_standard)\n",
       "       36    0.000    0.000    0.001    0.000 {built-in method builtins.sorted}\n",
       "       21    0.000    0.000    0.003    0.000 generic.py:5581(astype)\n",
       "        8    0.000    0.000    0.004    0.001 format.py:739(_get_formatted_column_labels)\n",
       "      256    0.000    0.000    0.000    0.000 format.py:1401(just)\n",
       "      256    0.000    0.000    0.001    0.000 managers.py:1893(<lambda>)\n",
       "       39    0.000    0.000    0.016    0.000 missing.py:522(remove_na_arraylike)\n",
       "       82    0.000    0.000    0.000    0.000 cursors.py:40(__init__)\n",
       "      303    0.000    0.000    0.000    0.000 {method 'tolist' of 'numpy.ndarray' objects}\n",
       "       41    0.000    0.000    0.001    0.000 result.py:740(_init_metadata)\n",
       "        4    0.000    0.000    0.235    0.059 apply.py:262(apply_series_generator)\n",
       "      256    0.000    0.000    0.000    0.000 format.py:1407(<listcomp>)\n",
       "       22    0.000    0.000    0.000    0.000 common.py:120(_stringify_path)\n",
       "       41    0.000    0.000    0.001    0.000 result.py:441(<listcomp>)\n",
       "       82    0.000    0.000    0.014    0.000 base.py:831(_checkin)\n",
       "       56    0.000    0.000    0.000    0.000 format.py:1078(<listcomp>)\n",
       "       41    0.000    0.000    0.001    0.000 deprecations.py:117(warned)\n",
       "      163    0.000    0.000    0.000    0.000 type_api.py:483(_cached_result_processor)\n",
       "       41    0.000    0.000    0.000    0.000 default.py:947(should_autocommit)\n",
       "       19    0.000    0.000    0.020    0.001 managers.py:1810(_multi_blockify)\n",
       "      115    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1203(_path_importer_cache)\n",
       "       39    0.000    0.000    0.000    0.000 arrayprint.py:69(_make_options_dict)\n",
       "       39    0.000    0.000    0.002    0.000 arrayprint.py:463(wrapper)\n",
       "      164    0.000    0.000    0.000    0.000 cursors.py:89(_nextset)\n",
       "       82    0.000    0.000    0.000    0.000 base.py:116(_for_class)\n",
       "       24    0.000    0.000    0.075    0.003 fromnumeric.py:1966(sum)\n",
       "       36    0.000    0.000    0.000    0.000 managers.py:2015(_preprocess_slice_or_indexer)\n",
       "       16    0.000    0.000    0.002    0.000 base.py:998(_format_with_header)\n",
       "       41    0.000    0.000    0.000    0.000 protocol.py:308(__init__)\n",
       "       79    0.000    0.000    0.008    0.000 <frozen importlib._bootstrap_external>:1272(find_spec)\n",
       "       39    0.000    0.000    0.000    0.000 numeric.py:203(_convert_scalar_indexer)\n",
       "       79    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:873(_find_spec_legacy)\n",
       "       39    0.000    0.000    0.003    0.000 generic.py:3093(_maybe_cache_changed)\n",
       "      164    0.000    0.000    0.000    0.000 cursors.py:106(nextset)\n",
       "        8    0.000    0.000    0.000    0.000 _methods.py:58(_mean)\n",
       "       41    0.000    0.000    0.000    0.000 schema.py:4027(_bind_to)\n",
       "       82    0.000    0.000    0.000    0.000 default.py:1002(_use_server_side_cursor)\n",
       "      492    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}\n",
       "      647    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}\n",
       "       24    0.000    0.000    0.000    0.000 {pandas._libs.algos.take_2d_axis1_float64_float64}\n",
       "       82    0.000    0.000    0.000    0.000 log.py:59(_should_log_info)\n",
       "        8    0.000    0.000    0.000    0.000 format.py:1553(_binify)\n",
       "      174    0.000    0.000    0.001    0.000 generic.py:1895(<genexpr>)\n",
       "      316    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:855(__enter__)\n",
       "      316    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:859(__exit__)\n",
       "       88    0.000    0.000    0.000    0.000 fromnumeric.py:1583(ravel)\n",
       "       39    0.000    0.000    0.000    0.000 {built-in method builtins.locals}\n",
       "       79    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:369(__init__)\n",
       "       39    0.000    0.000    0.005    0.000 range.py:520(__getitem__)\n",
       "      120    0.000    0.000    0.000    0.000 managers.py:1552(formatting_values)\n",
       "      164    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}\n",
       "       48    0.000    0.000    0.001    0.000 cast.py:1151(construct_1d_arraylike_from_scalar)\n",
       "       41    0.000    0.000    0.000    0.000 exc.py:328(__init__)\n",
       "       82    0.000    0.000    0.000    0.000 {built-in method _struct.unpack_from}\n",
       "       86    0.000    0.000    0.000    0.000 {pandas._libs.internals.get_blkno_placements}\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method posix.getcwd}\n",
       "   225/75    0.000    0.000    0.011    0.000 <frozen importlib._bootstrap>:211(_call_with_frames_removed)\n",
       "       35    0.000    0.000    0.114    0.003 shape_base.py:229(vstack)\n",
       "      152    0.000    0.000    0.000    0.000 format.py:1422(<listcomp>)\n",
       "       41    0.000    0.000    0.022    0.001 construction.py:484(_convert_object_array)\n",
       "       56    0.000    0.000    0.000    0.000 base.py:4459(_maybe_promote)\n",
       "       82    0.000    0.000    0.014    0.000 base.py:987(close)\n",
       "       41    0.000    0.000    0.002    0.000 result.py:714(__init__)\n",
       "      109    0.000    0.000    0.002    0.000 base.py:1729(inferred_type)\n",
       "       82    0.000    0.000    1.846    0.023 default.py:551(do_execute)\n",
       "      163    0.000    0.000    0.000    0.000 default.py:1051(get_result_processor)\n",
       "       39    0.000    0.000    0.003    0.000 arrayprint.py:1499(_array_str_implementation)\n",
       "       39    0.000    0.000    0.000    0.000 blocks.py:266(_slice)\n",
       "      461    0.000    0.000    0.000    0.000 numerictypes.py:587(<listcomp>)\n",
       "      111    0.000    0.000    0.004    0.000 <frozen importlib._bootstrap_external>:74(_path_stat)\n",
       "       41    0.000    0.000    0.021    0.001 construction.py:501(<listcomp>)\n",
       "       82    0.000    0.000    0.000    0.000 base.py:261(__init__)\n",
       "       41    0.000    0.000    0.012    0.000 result.py:1195(fetchall)\n",
       "        4    0.000    0.000    0.000    0.000 {method 'writerow' of '_csv.writer' objects}\n",
       "      205    0.000    0.000    0.000    0.000 protocol.py:77(read_all)\n",
       "      164    0.000    0.000    0.000    0.000 __init__.py:1619(isEnabledFor)\n",
       "        4    0.000    0.000    0.000    0.000 arraysetops.py:484(in1d)\n",
       "       82    0.000    0.000    0.000    0.000 connections.py:481(cursor)\n",
       "      369    0.000    0.000    0.000    0.000 cursors.py:71(_get_db)\n",
       "      101    0.000    0.000    0.000    0.000 {method 'transpose' of 'numpy.ndarray' objects}\n",
       "       58    0.000    0.000    0.001    0.000 generic.py:1848(empty)\n",
       "      136    0.000    0.000    0.001    0.000 format.py:298(__init__)\n",
       "       24    0.000    0.000    0.000    0.000 strings.py:1805(_validate)\n",
       "       41    0.000    0.000    0.000    0.000 _collections.py:151(union)\n",
       "       82    0.000    0.000    0.000    0.000 base.py:949(cursor)\n",
       "       42    0.000    0.000    0.000    0.000 pymysql.py:76(_extract_error_code)\n",
       "      122    0.000    0.000    0.000    0.000 base.py:3806(_coerce_to_ndarray)\n",
       "        8    0.000    0.000    0.042    0.005 frame.py:614(__unicode__)\n",
       "       84    0.000    0.000    0.000    0.000 range.py:89(ensure_int)\n",
       "       82    0.000    0.000    0.001    0.000 base.py:366(_return_conn)\n",
       "      360    0.000    0.000    0.000    0.000 {method 'rjust' of 'str' objects}\n",
       "       56    0.000    0.000    0.007    0.000 format.py:1128(_format_strings)\n",
       "       41    0.000    0.000    0.007    0.000 base.py:865(_autorollback)\n",
       "       41    0.000    0.000    0.000    0.000 base.py:2057(<listcomp>)\n",
       "       41    0.000    0.000    0.000    0.000 cursors.py:341(_do_get_result)\n",
       "       41    0.000    0.000    0.000    0.000 exc.py:24(__init__)\n",
       "       82    0.000    0.000    0.000    0.000 base.py:1327(_safe_close_cursor)\n",
       "       34    0.000    0.000    0.001    0.000 blocks.py:2736(should_store)\n",
       "       41    0.000    0.000    0.002    0.000 default.py:1092(get_result_proxy)\n",
       "        8    0.000    0.000    0.001    0.000 generic.py:5948(fillna)\n",
       "      182    0.000    0.000    0.001    0.000 series.py:591(__len__)\n",
       "       41    0.000    0.000    0.008    0.000 result.py:869(_soft_close)\n",
       "      264    0.000    0.000    0.000    0.000 format.py:1418(_is_number)\n",
       "       82    0.000    0.000    0.001    0.000 impl.py:102(_do_return_conn)\n",
       "       30    0.000    0.000    0.001    0.000 fromnumeric.py:2664(prod)\n",
       "       24    0.000    0.000    0.000    0.000 generic.py:297(_construct_axes_from_arguments)\n",
       "       62    0.000    0.000    0.000    0.000 managers.py:174(_is_single_block)\n",
       "       34    0.000    0.000    0.001    0.000 format.py:307(adjoin)\n",
       "       82    0.000    0.000    0.016    0.000 base.py:2259(_wrap_pool_connect)\n",
       "       41    0.000    0.000    0.001    0.000 result.py:334(_merge_cursor_description)\n",
       "       56    0.000    0.000    0.000    0.000 format.py:1138(_format_strings)\n",
       "        4    0.000    0.000    0.248    0.062 <ipython-input-286-d6bec680927f>:68(majority_vote_out)\n",
       "       41    0.000    0.000    0.000    0.000 compat.py:378(raise_from_cause)\n",
       "       41    0.000    0.000    0.000    0.000 compat.py:123(reraise)\n",
       "      120    0.000    0.000    0.000    0.000 common.py:1472(is_numeric_dtype)\n",
       "        8    0.000    0.000    0.004    0.000 generic.py:3155(_slice)\n",
       "      328    0.000    0.000    0.000    0.000 blocks.py:161(external_values)\n",
       "       41    0.000    0.000    1.848    0.045 sql.py:988(execute)\n",
       "       24    0.000    0.000    0.001    0.000 strings.py:1795(__init__)\n",
       "        8    0.000    0.000    0.041    0.005 format.py:582(to_string)\n",
       "       40    0.000    0.000    0.000    0.000 connections.py:448(escape)\n",
       "       82    0.000    0.000    0.000    0.000 cursors.py:51(close)\n",
       "       56    0.000    0.000    0.001    0.000 base.py:1669(is_boolean)\n",
       "       56    0.000    0.000    0.000    0.000 format.py:992(__init__)\n",
       "      304    0.000    0.000    0.000    0.000 format.py:1423(<genexpr>)\n",
       "       56    0.000    0.000    0.001    0.000 format.py:1412(_trim_zeros)\n",
       "        5    0.000    0.000    0.002    0.000 series.py:3466(apply)\n",
       "       40    0.000    0.000    0.000    0.000 connections.py:469(escape_string)\n",
       "       41    0.000    0.000    0.020    0.000 base.py:1591(run_callable)\n",
       "       46    0.000    0.000    0.000    0.000 common.py:457(_get_rename_function)\n",
       "       82    0.000    0.000    0.000    0.000 base.py:715(__init__)\n",
       "      196    0.000    0.000    0.000    0.000 printing.py:55(<listcomp>)\n",
       "        8    0.000    0.000    0.000    0.000 format.py:769(<listcomp>)\n",
       "       41    0.000    0.000    0.000    0.000 sql.py:74(_convert_params)\n",
       "       97    0.000    0.000    0.000    0.000 iostream.py:307(_is_master_process)\n",
       "       23    0.000    0.000    0.000    0.000 _validators.py:230(validate_axis_style_args)\n",
       "       50    0.000    0.000    0.000    0.000 protocol.py:122(read_uint16)\n",
       "       41    0.000    0.000    0.000    0.000 compiler.py:3464(quote_identifier)\n",
       "      240    0.000    0.000    0.000    0.000 format.py:535(<genexpr>)\n",
       "       41    0.000    0.000    0.000    0.000 compiler.py:3425(_escape_identifier)\n",
       "       79    0.000    0.000    0.000    0.000 six.py:184(find_module)\n",
       "       96    0.000    0.000    0.000    0.000 common.py:1513(is_string_like_dtype)\n",
       "       31    0.000    0.000    0.017    0.001 cast.py:1123(cast_scalar_to_array)\n",
       "       41    0.000    0.000    0.000    0.000 base.py:479(_still_open_and_connection_is_valid)\n",
       "        4    0.000    0.000    0.001    0.000 csvs.py:30(__init__)\n",
       "       37    0.000    0.000    0.001    0.000 common.py:1320(is_datetimelike_v_numeric)\n",
       "       32    0.000    0.000    0.000    0.000 base.py:2254(union)\n",
       "       64    0.000    0.000    0.000    0.000 frame.py:3416(_ensure_valid_index)\n",
       "       41    0.000    0.000    0.004    0.000 result.py:1178(process_rows)\n",
       "       40    0.000    0.000    0.001    0.000 apply.py:325(<genexpr>)\n",
       "       20    0.000    0.000    0.092    0.005 <ipython-input-268-7842a37f643d>:7(system_semtype_check)\n",
       "       12    0.000    0.000    0.000    0.000 {pandas._libs.algos.take_1d_object_object}\n",
       "       20    0.000    0.000    0.001    0.000 blocks.py:1982(to_native_types)\n",
       "       41    0.000    0.000    0.000    0.000 base.py:169(_clone)\n",
       "       54    0.000    0.000    0.000    0.000 fromnumeric.py:70(<dictcomp>)\n",
       "        8    0.000    0.000    0.001    0.000 base.py:1010(<listcomp>)\n",
       "        8    0.000    0.000    0.000    0.000 format.py:381(__init__)\n",
       "       82    0.000    0.000    0.000    0.000 base.py:119(_for_instance)\n",
       "      342    0.000    0.000    0.000    0.000 {method 'ljust' of 'str' objects}\n",
       "       39    0.000    0.000    0.000    0.000 arrayprint.py:365(<lambda>)\n",
       "       82    0.000    0.000    0.000    0.000 queue.py:195(_full)\n",
       "       40    0.000    0.000    0.000    0.000 {method 'translate' of 'str' objects}\n",
       "       16    0.000    0.000    0.000    0.000 blocks.py:335(set)\n",
       "      111    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:36(_relax_case)\n",
       "       23    0.000    0.000    0.992    0.043 _decorators.py:195(wrapper)\n",
       "       84    0.000    0.000    0.000    0.000 _validators.py:221(validate_bool_kwarg)\n",
       "       28    0.000    0.000    0.010    0.000 base.py:4447(get_indexer_for)\n",
       "       12    0.000    0.000    0.002    0.000 generic.py:5457(dtypes)\n",
       "      120    0.000    0.000    0.000    0.000 series.py:483(_formatting_values)\n",
       "       35    0.000    0.000    0.000    0.000 fromnumeric.py:54(_wrapfunc)\n",
       "       39    0.000    0.000    0.001    0.000 arrayprint.py:707(_formatArray)\n",
       "      123    0.000    0.000    0.000    0.000 base.py:958(__getattr__)\n",
       "       79    0.000    0.000    0.000    0.000 {built-in method _imp.is_frozen}\n",
       "        4    0.000    0.000    0.000    0.000 base.py:3752(_try_convert_to_int_index)\n",
       "       82    0.000    0.000    0.000    0.000 queue.py:203(_get)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'close' of 'pandas._libs.parsers.TextReader' objects}\n",
       "      103    0.000    0.000    0.000    0.000 threading.py:1038(_wait_for_tstate_lock)\n",
       "      489    0.000    0.000    0.000    0.000 arrayprint.py:1154(__call__)\n",
       "       28    0.000    0.000    0.001    0.000 base.py:2965(_convert_listlike_indexer)\n",
       "       41    0.000    0.000    0.000    0.000 cursors.py:299(fetchall)\n",
       "       51    0.000    0.000    0.001    0.000 {method 'all' of 'numpy.ndarray' objects}\n",
       "       39    0.000    0.000    0.000    0.000 blocks.py:3183(_safe_reshape)\n",
       "       13    0.000    0.000    0.044    0.003 managers.py:736(as_array)\n",
       "        8    0.000    0.000    0.001    0.000 format.py:801(_get_formatted_index)\n",
       "       32    0.000    0.000    0.000    0.000 protocol.py:127(read_uint24)\n",
       "       41    0.000    0.000    0.000    0.000 base.py:2054(_quote_free_identifiers)\n",
       "       35    0.000    0.000    0.000    0.000 fromnumeric.py:942(argsort)\n",
       "       57    0.000    0.000    0.000    0.000 missing.py:530(clean_reindex_fill_method)\n",
       "       82    0.000    0.000    0.000    0.000 attr.py:255(__call__)\n",
       "       82    0.000    0.000    0.000    0.000 base.py:364(invalidated)\n",
       "       41    0.000    0.000    0.002    0.000 default.py:1108(_setup_crud_result_proxy)\n",
       "        8    0.000    0.000    0.001    0.000 stats.py:256(gmean)\n",
       "       41    0.000    0.000    0.000    0.000 <string>:1(<lambda>)\n",
       "       44    0.000    0.000    0.000    0.000 apipkg.py:133(__makeattr)\n",
       "       45    0.000    0.000    0.003    0.000 range.py:176(_data)\n",
       "       21    0.000    0.000    0.002    0.000 managers.py:530(astype)\n",
       "       86    0.000    0.000    0.000    0.000 managers.py:706(nblocks)\n",
       "       96    0.000    0.000    0.000    0.000 format.py:1427(<listcomp>)\n",
       "       56    0.000    0.000    0.000    0.000 format.py:1430(<listcomp>)\n",
       "       39    0.000    0.000    0.004    0.000 series.py:388(_update_inplace)\n",
       "       41    0.000    0.000    0.000    0.000 langhelpers.py:59(__enter__)\n",
       "        8    0.000    0.000    0.041    0.005 frame.py:678(to_string)\n",
       "       25    0.000    0.000    0.000    0.000 frame.py:3585(reindexer)\n",
       "       24    0.000    0.000    0.005    0.000 generic.py:8282(rank)\n",
       "       82    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}\n",
       "       34    0.000    0.000    0.000    0.000 printing.py:36(<listcomp>)\n",
       "       41    0.000    0.000    0.000    0.000 range.py:180(_int64index)\n",
       "       82    0.000    0.000    0.000    0.000 cursors.py:332(_clear_result)\n",
       "       23    0.000    0.000    0.238    0.010 algorithms.py:407(<lambda>)\n",
       "       24    0.000    0.000    0.002    0.000 algorithms.py:835(rank)\n",
       "      240    0.000    0.000    0.000    0.000 format.py:541(<genexpr>)\n",
       "       82    0.000    0.000    0.000    0.000 queue.py:199(_put)\n",
       "       23    0.000    0.000    0.000    0.000 __init__.py:125(lrange)\n",
       "       56    0.000    0.000    0.000    0.000 format.py:1139(<lambda>)\n",
       "       41    0.000    0.000    0.000    0.000 base.py:1174(should_autocommit_text)\n",
       "        4    0.000    0.000    0.007    0.002 csvs.py:130(save)\n",
       "       82    0.000    0.000    0.000    0.000 {built-in method sys.exc_info}\n",
       "       25    0.000    0.000    0.000    0.000 inference.py:381(is_dict_like)\n",
       "      210    0.000    0.000    0.000    0.000 common.py:464(f)\n",
       "       72    0.000    0.000    0.000    0.000 base.py:143(__setattr__)\n",
       "       24    0.000    0.000    0.350    0.015 strings.py:2723(strip)\n",
       "       24    0.000    0.000    0.000    0.000 generic.py:277(_construct_axes_dict)\n",
       "        8    0.000    0.000    0.003    0.000 managers.py:684(get_slice)\n",
       "       78    0.000    0.000    0.000    0.000 cursors.py:122(<genexpr>)\n",
       "       50    0.000    0.000    0.000    0.000 numerictypes.py:654(<listcomp>)\n",
       "       61    0.000    0.000    0.000    0.000 base.py:2590(_assert_can_do_setop)\n",
       "       15    0.000    0.000    0.000    0.000 parse.py:409(urlsplit)\n",
       "       44    0.000    0.000    0.000    0.000 {built-in method numpy.bincount}\n",
       "       35    0.000    0.000    0.000    0.000 shape_base.py:220(_warn_for_nonsequence)\n",
       "       40    0.000    0.000    0.000    0.000 connections.py:462(literal)\n",
       "       24    0.000    0.000    0.001    0.000 accessor.py:167(__get__)\n",
       "       24    0.000    0.000    0.339    0.014 strings.py:57(_na_map)\n",
       "        5    0.000    0.000    0.001    0.000 series.py:3600(_reduce)\n",
       "      128    0.000    0.000    0.001    0.000 {method 'max' of 'numpy.ndarray' objects}\n",
       "       83    0.000    0.000    0.000    0.000 base.py:4683(_maybe_cast_indexer)\n",
       "      103    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
       "       52    0.000    0.000    0.000    0.000 base.py:658(__array__)\n",
       "       28    0.000    0.000    0.001    0.000 base.py:1672(is_integer)\n",
       "        4    0.000    0.000    0.007    0.002 generic.py:2882(to_csv)\n",
       "       28    0.000    0.000    0.000    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
       "      103    0.000    0.000    0.000    0.000 iostream.py:93(_event_pipe)\n",
       "       96    0.000    0.000    0.000    0.000 common.py:1542(<lambda>)\n",
       "        4    0.000    0.000    0.002    0.000 ops.py:1817(wrapper)\n",
       "       34    0.000    0.000    0.022    0.001 blocks.py:175(get_values)\n",
       "       56    0.000    0.000    0.000    0.000 format.py:1004(_value_formatter)\n",
       "       82    0.000    0.000    0.000    0.000 queue.py:191(_empty)\n",
       "       41    0.000    0.000    0.000    0.000 result.py:266(<listcomp>)\n",
       "       35    0.000    0.000    0.000    0.000 shape_base.py:283(<listcomp>)\n",
       "      114    0.000    0.000    0.000    0.000 base.py:475(<genexpr>)\n",
       "      120    0.000    0.000    0.000    0.000 blocks.py:171(formatting_values)\n",
       "       12    0.000    0.000    0.001    0.000 managers.py:225(get_dtypes)\n",
       "      182    0.000    0.000    0.000    0.000 managers.py:376(<dictcomp>)\n",
       "        8    0.000    0.000    0.000    0.000 console.py:47(get_console_size)\n",
       "        4    0.000    0.000    0.417    0.104 apply.py:109(get_result)\n",
       "        4    0.000    0.000    0.092    0.023 <ipython-input-284-a912178f7287>:16(get_valid_systems)\n",
       "        5    0.000    0.000    0.000    0.000 cast.py:1072(find_common_type)\n",
       "       39    0.000    0.000    0.000    0.000 series.py:348(_can_hold_na)\n",
       "       79    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:792(find_spec)\n",
       "      128    0.000    0.000    0.000    0.000 _methods.py:26(_amax)\n",
       "       69    0.000    0.000    0.000    0.000 managers.py:1850(_shape_compat)\n",
       "        8    0.000    0.000    0.000    0.000 format.py:431(_chk_truncate)\n",
       "       40    0.000    0.000    0.000    0.000 converters.py:68(_escape_unicode)\n",
       "       41    0.000    0.000    0.000    0.000 cursors.py:355(_show_warnings)\n",
       "      205    0.000    0.000    0.000    0.000 base.py:155(_root)\n",
       "       39    0.000    0.000    0.000    0.000 <ipython-input-267-cd2ed8d3f7ce>:40(get_system_type)\n",
       "        4    0.000    0.000    0.003    0.001 common.py:314(_get_handle)\n",
       "       39    0.000    0.000    0.000    0.000 default.py:894(<listcomp>)\n",
       "        4    0.000    0.000    0.417    0.104 frame.py:6310(apply)\n",
       "       21    0.000    0.000    0.002    0.000 blocks.py:532(astype)\n",
       "        4    0.000    0.000    0.000    0.000 csvs.py:191(_save_header)\n",
       "       15    0.000    0.000    0.000    0.000 parse.py:361(urlparse)\n",
       "       39    0.000    0.000    0.000    0.000 arrayprint.py:1149(__init__)\n",
       "        8    0.000    0.000    0.000    0.000 console.py:149(in_ipython_frontend)\n",
       "      192    0.000    0.000    0.000    0.000 format.py:1424(<genexpr>)\n",
       "       82    0.000    0.000    0.000    0.000 base.py:355(closed)\n",
       "       39    0.000    0.000    0.000    0.000 arrayprint.py:74(<dictcomp>)\n",
       "       39    0.000    0.000    0.000    0.000 base.py:697(shape)\n",
       "       13    0.000    0.000    0.044    0.003 generic.py:5250(values)\n",
       "        8    0.000    0.000    0.001    0.000 blocks.py:730(to_native_types)\n",
       "       41    0.000    0.000    0.000    0.000 result.py:1161(_fetchall_impl)\n",
       "      121    0.000    0.000    0.000    0.000 {method 'discard' of 'set' objects}\n",
       "       97    0.000    0.000    0.000    0.000 iostream.py:320(_schedule_flush)\n",
       "      185    0.000    0.000    0.000    0.000 {pandas._libs.lib.is_bool}\n",
       "       39    0.000    0.000    0.000    0.000 base.py:100(_reset_cache)\n",
       "       29    0.000    0.000    0.000    0.000 base.py:978(empty)\n",
       "        5    0.000    0.000    0.000    0.000 nanops.py:203(_get_values)\n",
       "       41    0.000    0.000    0.000    0.000 sql.py:490(_engine_builder)\n",
       "       41    0.000    0.000    0.000    0.000 elements.py:4322(_string_or_unprintable)\n",
       "       51    0.000    0.000    0.001    0.000 _methods.py:45(_all)\n",
       "       94    0.000    0.000    0.000    0.000 printing.py:62(_join_unicode)\n",
       "        8    0.000    0.000    0.001    0.000 ops.py:1815(<lambda>)\n",
       "       23    0.000    0.000    0.000    0.000 common.py:199(count_not_none)\n",
       "       16    0.000    0.000    0.000    0.000 common.py:212(dict_keys_to_ordered_list)\n",
       "       24    0.000    0.000    0.339    0.014 strings.py:1504(str_strip)\n",
       "        4    0.000    0.000    0.000    0.000 api.py:128(_union_indexes)\n",
       "      112    0.000    0.000    0.000    0.000 format.py:1107(<genexpr>)\n",
       "        1    0.000    0.000    1.646    1.646 parsers.py:1993(read)\n",
       "       41    0.000    0.000    0.000    0.000 langhelpers.py:62(__exit__)\n",
       "       42    0.000    0.000    0.000    0.000 default.py:943(no_parameters)\n",
       "      164    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}\n",
       "       79    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:719(find_spec)\n",
       "       35    0.000    0.000    0.000    0.000 common.py:94(_expand_user)\n",
       "        1    0.000    0.000    0.000    0.000 <ipython-input-283-5131e7c93670>:97(<listcomp>)\n",
       "      104    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
       "      104    0.000    0.000    0.000    0.000 {pandas._libs.algos.ensure_float64}\n",
       "        8    0.000    0.000    0.000    0.000 base.py:1050(_format_native_types)\n",
       "        8    0.000    0.000    0.000    0.000 base.py:5393(_trim_front)\n",
       "        8    0.000    0.000    0.001    0.000 series.py:3831(fillna)\n",
       "       41    0.000    0.000    0.007    0.000 base.py:180(__exit__)\n",
       "      185    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
       "        2    0.000    0.000    0.001    0.000 <frozen importlib._bootstrap_external>:1404(_fill_cache)\n",
       "       33    0.000    0.000    0.000    0.000 base.py:759(size)\n",
       "       16    0.000    0.000    0.002    0.000 base.py:983(format)\n",
       "        8    0.000    0.000    0.004    0.000 indexing.py:2170(_get_slice_axis)\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method _operator.and_}\n",
       "       28    0.000    0.000    0.000    0.000 base.py:2999(_convert_arr_indexer)\n",
       "      107    0.000    0.000    0.000    0.000 managers.py:1513(index)\n",
       "        5    0.000    0.000    0.000    0.000 common.py:179(get_filepath_or_buffer)\n",
       "        8    0.000    0.000    0.000    0.000 {method 'getvalue' of '_io.StringIO' objects}\n",
       "       16    0.000    0.000    0.000    0.000 format.py:945(_format)\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method now}\n",
       "        8    0.000    0.000    0.000    0.000 twodim_base.py:216(diag)\n",
       "       16    0.000    0.000    0.000    0.000 config.py:170(get_default_val)\n",
       "       16    0.000    0.000    0.000    0.000 blocks.py:2011(should_store)\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method _csv.writer}\n",
       "       41    0.000    0.000    0.000    0.000 protocol.py:94(rewind)\n",
       "      123    0.000    0.000    0.000    0.000 base.py:370(connection)\n",
       "        1    0.000    0.000    0.421    0.421 <ipython-input-283-5131e7c93670>:89(get_docs)\n",
       "       24    0.000    0.000    0.000    0.000 generic.py:334(<dictcomp>)\n",
       "        8    0.000    0.000    0.000    0.000 indexing.py:264(_convert_slice_indexer)\n",
       "       41    0.000    0.000    0.000    0.000 langhelpers.py:56(__init__)\n",
       "        4    0.000    0.000    0.003    0.001 csvs.py:272(_save)\n",
       "       30    0.000    0.000    0.000    0.000 parse.py:109(_coerce_args)\n",
       "       28    0.000    0.000    0.000    0.000 _methods.py:34(_sum)\n",
       "       19    0.000    0.000    0.000    0.000 posixpath.py:232(expanduser)\n",
       "       40    0.000    0.000    0.000    0.000 ops.py:66(_maybe_match_name)\n",
       "       12    0.000    0.000    0.000    0.000 managers.py:226(<listcomp>)\n",
       "       39    0.000    0.000    0.000    0.000 managers.py:1568(_can_hold_na)\n",
       "      164    0.000    0.000    0.000    0.000 {method '_is_owned' of '_thread.RLock' objects}\n",
       "       88    0.000    0.000    0.000    0.000 fromnumeric.py:2847(ndim)\n",
       "       50    0.000    0.000    0.000    0.000 inference.py:406(<genexpr>)\n",
       "        8    0.000    0.000    0.000    0.000 construction.py:509(sanitize_index)\n",
       "       41    0.000    0.000    0.000    0.000 sql.py:85(_process_parse_dates_argument)\n",
       "       41    0.000    0.000    0.000    0.000 base.py:875(is_valid)\n",
       "       41    0.000    0.000    0.000    0.000 default.py:477(set_connection_execution_options)\n",
       "        4    0.000    0.000    0.000    0.000 apply.py:16(frame_apply)\n",
       "       97    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
       "       82    0.000    0.000    0.000    0.000 {method 'popleft' of 'collections.deque' objects}\n",
       "       35    0.000    0.000    0.000    0.000 shape_base.py:209(_arrays_for_stack_dispatcher)\n",
       "       29    0.000    0.000    0.000    0.000 common.py:1752(is_complex_dtype)\n",
       "        9    0.000    0.000    0.000    0.000 cast.py:1260(maybe_cast_to_integer_array)\n",
       "       69    0.000    0.000    0.000    0.000 common.py:201(<genexpr>)\n",
       "        8    0.000    0.000    0.001    0.000 base.py:1024(to_native_types)\n",
       "       28    0.000    0.000    0.000    0.000 base.py:3035(_convert_list_indexer)\n",
       "        5    0.000    0.000    0.001    0.000 generic.py:11018(logical_func)\n",
       "       28    0.000    0.000    0.000    0.000 indexing.py:2676(is_nested_tuple)\n",
       "        1    0.000    0.000    1.899    1.899 parsers.py:1137(read)\n",
       "        8    0.000    0.000    0.000    0.000 _validators.py:325(validate_fillna_kwargs)\n",
       "        8    0.000    0.000    0.000    0.000 base.py:2897(_convert_slice_indexer)\n",
       "       24    0.000    0.000    0.000    0.000 generic.py:279(<dictcomp>)\n",
       "        8    0.000    0.000    0.000    0.000 blocks.py:364(fillna)\n",
       "       67    0.000    0.000    0.000    0.000 managers.py:1814(<lambda>)\n",
       "       16    0.000    0.000    0.000    0.000 format.py:781(has_index_names)\n",
       "        9    0.000    0.000    0.000    0.000 common.py:260(_infer_compression)\n",
       "       41    0.000    0.000    0.000    0.000 cursors.py:76(_check_executed)\n",
       "       41    0.000    0.000    0.000    0.000 base.py:709(in_transaction)\n",
       "       41    0.000    0.000    0.000    0.000 result.py:760(keys)\n",
       "        4    0.000    0.000    0.000    0.000 apply.py:36(__init__)\n",
       "        4    0.000    0.000    0.014    0.004 apply.py:302(wrap_results)\n",
       "        4    0.000    0.000    0.014    0.004 apply.py:336(wrap_results_for_axis)\n",
       "       42    0.000    0.000    0.000    0.000 common.py:175(_all_none)\n",
       "        8    0.000    0.000    0.042    0.005 base.py:48(__str__)\n",
       "       32    0.000    0.000    0.000    0.000 csvs.py:112(<genexpr>)\n",
       "        4    0.000    0.000    0.000    0.000 apply.py:323(series_generator)\n",
       "        8    0.000    0.000    0.000    0.000 fromnumeric.py:1395(diagonal)\n",
       "       32    0.000    0.000    0.000    0.000 base.py:2238(_get_reconciled_name_object)\n",
       "       16    0.000    0.000    0.000    0.000 construction.py:210(<listcomp>)\n",
       "       24    0.000    0.000    0.000    0.000 format.py:1434(_has_names)\n",
       "        1    0.000    0.000    1.923    1.923 parsers.py:536(parser_f)\n",
       "       43    0.000    0.000    0.000    0.000 cursors.py:127(<dictcomp>)\n",
       "       41    0.000    0.000    0.000    0.000 result.py:263(<listcomp>)\n",
       "       39    0.000    0.000    0.000    0.000 <ipython-input-267-cd2ed8d3f7ce>:12(<listcomp>)\n",
       "       57    0.000    0.000    0.000    0.000 missing.py:71(clean_fill_method)\n",
       "       28    0.000    0.000    0.000    0.000 base.py:5381(_ensure_has_len)\n",
       "       21    0.000    0.000    0.000    0.000 blocks.py:146(is_categorical_astype)\n",
       "        8    0.000    0.000    0.000    0.000 managers.py:524(fillna)\n",
       "        8    0.000    0.000    0.000    0.000 format.py:816(<listcomp>)\n",
       "       16    0.000    0.000    0.000    0.000 format.py:943(<lambda>)\n",
       "       82    0.000    0.000    0.000    0.000 _collections.py:145(__init__)\n",
       "       41    0.000    0.000    0.000    0.000 result.py:864(_cursor_description)\n",
       "       21    0.000    0.000    0.000    0.000 inspect.py:72(isclass)\n",
       "        1    0.000    0.000    0.000    0.000 random.py:224(_randbelow)\n",
       "       24    0.000    0.000    0.000    0.000 base.py:138(_freeze)\n",
       "        4    0.000    0.000    0.000    0.000 api.py:205(_sanitize_and_check)\n",
       "       16    0.000    0.000    0.000    0.000 format.py:789(show_row_idx_names)\n",
       "        4    0.000    0.000    0.044    0.011 apply.py:97(values)\n",
       "       34    0.000    0.000    0.000    0.000 {method 'insert' of 'list' objects}\n",
       "        4    0.000    0.000    0.000    0.000 re.py:271(_compile)\n",
       "        4    0.000    0.000    0.000    0.000 numeric.py:175(ones)\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method numpy.copyto}\n",
       "       61    0.000    0.000    0.000    0.000 base.py:2249(_validate_sort_keyword)\n",
       "        8    0.000    0.000    0.000    0.000 frame.py:606(_info_repr)\n",
       "       58    0.000    0.000    0.000    0.000 managers.py:1578(_consolidate_inplace)\n",
       "       41    0.000    0.000    0.000    0.000 protocol.py:208(is_load_local_packet)\n",
       "       41    0.000    0.000    0.000    0.000 elements.py:4139(__new__)\n",
       "       12    0.000    0.000    0.000    0.000 apply.py:89(columns)\n",
       "        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1433(<setcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 warnings.py:35(_formatwarnmsg_impl)\n",
       "        8    0.000    0.000    0.000    0.000 {method 'diagonal' of 'numpy.ndarray' objects}\n",
       "        4    0.000    0.000    0.000    0.000 arraysetops.py:745(setdiff1d)\n",
       "        8    0.000    0.000    0.000    0.000 console.py:90(in_interactive_session)\n",
       "        5    0.000    0.000    0.000    0.000 nanops.py:337(nanany)\n",
       "        8    0.000    0.000    0.000    0.000 generic.py:3110(_is_view)\n",
       "       42    0.000    0.000    0.000    0.000 range.py:165(_validate_dtype)\n",
       "        5    0.000    0.000    0.000    0.000 common.py:76(_is_url)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:900(_get_options_with_defaults)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:956(_clean_options)\n",
       "       41    0.000    0.000    0.000    0.000 default.py:1089(handle_dbapi_exception)\n",
       "      103    0.000    0.000    0.000    0.000 threading.py:507(is_set)\n",
       "        5    0.000    0.000    0.000    0.000 {built-in method numpy.putmask}\n",
       "       24    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.normalize_axis_index}\n",
       "        4    0.000    0.000    0.000    0.000 sorting.py:407(safe_sort)\n",
       "       32    0.000    0.000    0.000    0.000 base.py:5398(<genexpr>)\n",
       "        8    0.000    0.000    0.000    0.000 managers.py:627(is_view)\n",
       "        1    0.000    0.000    0.002    0.002 parsers.py:813(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:4113(reindex)\n",
       "       20    0.000    0.000    0.000    0.000 managers.py:1045(value_getitem)\n",
       "        5    0.000    0.000    0.000    0.000 managers.py:1868(_interleaved_dtype)\n",
       "        4    0.000    0.000    0.000    0.000 common.py:544(UnicodeWriter)\n",
       "        1    0.000    0.000    0.002    0.002 parsers.py:1120(_make_engine)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1352(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1502(_maybe_dedup_names)\n",
       "       41    0.000    0.000    0.000    0.000 base.py:177(__enter__)\n",
       "       41    0.000    0.000    0.000    0.000 {method 'with_traceback' of 'BaseException' objects}\n",
       "        8    0.000    0.000    0.000    0.000 _methods.py:48(_count_reduce_items)\n",
       "       24    0.000    0.000    0.000    0.000 base.py:4698(_validate_indexer)\n",
       "        4    0.000    0.000    0.000    0.000 generic.py:637(_set_axis)\n",
       "       35    0.000    0.000    0.000    0.000 blocks.py:3146(<listcomp>)\n",
       "       30    0.000    0.000    0.000    0.000 managers.py:1041(value_getitem)\n",
       "        8    0.000    0.000    0.000    0.000 format.py:648(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1926(close)\n",
       "        4    0.000    0.000    0.001    0.000 apply.py:101(dtypes)\n",
       "       41    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}\n",
       "        4    0.000    0.000    0.000    0.000 base.py:2595(_convert_can_do_setop)\n",
       "        5    0.000    0.000    0.000    0.000 range.py:330(equals)\n",
       "        8    0.000    0.000    0.004    0.000 indexing.py:148(_slice)\n",
       "        8    0.000    0.000    0.000    0.000 format.py:785(has_column_names)\n",
       "        8    0.000    0.000    0.000    0.000 format.py:795(show_col_idx_names)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:271(_init_dict)\n",
       "        3    0.000    0.000    0.000    0.000 parsers.py:1180(_is_potential_multi_index)\n",
       "       41    0.000    0.000    0.000    0.000 {built-in method builtins.globals}\n",
       "       15    0.000    0.000    0.000    0.000 <string>:1(__new__)\n",
       "       19    0.000    0.000    0.000    0.000 cast.py:1112(<genexpr>)\n",
       "        4    0.000    0.000    0.000    0.000 ops.py:1447(_align_method_SERIES)\n",
       "        8    0.000    0.000    0.000    0.000 generic.py:1818(__iter__)\n",
       "        8    0.000    0.000    0.000    0.000 generic.py:1904(__array__)\n",
       "        8    0.000    0.000    0.000    0.000 generic.py:5337(get_values)\n",
       "        8    0.000    0.000    0.000    0.000 range.py:184(_get_data_as_items)\n",
       "        4    0.000    0.000    0.000    0.000 apply.py:332(result_columns)\n",
       "       39    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method _imp.is_builtin}\n",
       "       19    0.000    0.000    0.000    0.000 {built-in method posix.fspath}\n",
       "        4    0.000    0.000    0.000    0.000 re.py:180(search)\n",
       "       50    0.000    0.000    0.000    0.000 numerictypes.py:655(<listcomp>)\n",
       "        8    0.000    0.000    0.000    0.000 {method 'mean' of 'numpy.ndarray' objects}\n",
       "       10    0.000    0.000    0.000    0.000 cast.py:1097(<genexpr>)\n",
       "       19    0.000    0.000    0.000    0.000 cast.py:1100(<genexpr>)\n",
       "        4    0.000    0.000    0.000    0.000 ops.py:1787(na_op)\n",
       "        5    0.000    0.000    0.000    0.000 nanops.py:270(_na_ok_dtype)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:2168(_get_unique_index)\n",
       "        4    0.000    0.000    0.000    0.000 frame.py:491(shape)\n",
       "        4    0.000    0.000    0.000    0.000 api.py:226(<setcomp>)\n",
       "       28    0.000    0.000    0.000    0.000 indexing.py:937(_convert_for_reindex)\n",
       "        8    0.000    0.000    0.000    0.000 format.py:351(should_show_dimensions)\n",
       "        4    0.000    0.000    0.000    0.000 series.py:737(__array_prepare__)\n",
       "        4    0.000    0.000    0.000    0.000 apply.py:93(index)\n",
       "        4    0.000    0.000    0.000    0.000 apply.py:105(agg_axis)\n",
       "       16    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
       "        5    0.000    0.000    0.000    0.000 {built-in method gc.get_referents}\n",
       "        1    0.000    0.000    0.000    0.000 random.py:218(randint)\n",
       "        8    0.000    0.000    0.000    0.000 __init__.py:275(u)\n",
       "        2    0.000    0.000    0.000    0.000 generic.py:4341(<genexpr>)\n",
       "        5    0.000    0.000    0.000    0.000 managers.py:1884(<listcomp>)\n",
       "        5    0.000    0.000    0.000    0.000 common.py:163(is_s3_url)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1936(_set_noconvert_columns)\n",
       "        8    0.000    0.000    0.000    0.000 {method 'pop' of 'list' objects}\n",
       "       24    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}\n",
       "        4    0.000    0.000    0.000    0.000 codecs.py:186(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 warnings.py:419(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 {method 'timestamp' of 'datetime.datetime' objects}\n",
       "        4    0.000    0.000    0.000    0.000 function_base.py:258(iterable)\n",
       "       10    0.000    0.000    0.000    0.000 cast.py:1107(<genexpr>)\n",
       "        4    0.000    0.000    0.000    0.000 frame.py:7600(_get_agg_axis)\n",
       "        8    0.000    0.000    0.000    0.000 blocks.py:136(is_view)\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method pandas._libs.internals.slice_len}\n",
       "        5    0.000    0.000    0.000    0.000 managers.py:604(is_mixed_type)\n",
       "        5    0.000    0.000    0.000    0.000 common.py:171(is_gcs_url)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:897(close)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1814(_do_date_conversions)\n",
       "        4    0.000    0.000    0.000    0.000 {method 'find' of 'str' objects}\n",
       "        1    0.000    0.000    0.000    0.000 random.py:174(randrange)\n",
       "        8    0.000    0.000    0.000    0.000 interactiveshell.py:698(get_ipython)\n",
       "       16    0.000    0.000    0.000    0.000 config.py:578(_get_registered_option)\n",
       "       10    0.000    0.000    0.000    0.000 cast.py:1105(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:4071(identical)\n",
       "        8    0.000    0.000    0.000    0.000 indexing.py:2700(need_slice)\n",
       "        9    0.000    0.000    0.000    0.000 managers.py:1572(is_consolidated)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:3735(reindex)\n",
       "        2    0.000    0.000    0.000    0.000 parsers.py:347(_validate_integer)\n",
       "        1    0.000    0.000    0.000    0.000 warnings.py:20(_showwarnmsg_impl)\n",
       "        1    0.000    0.000    0.000    0.000 warnings.py:96(_showwarnmsg)\n",
       "        1    0.000    0.000    0.000    0.000 linecache.py:15(getline)\n",
       "        1    0.000    0.000    0.000    0.000 linecache.py:37(getlines)\n",
       "       30    0.000    0.000    0.000    0.000 parse.py:98(_noop)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:227(itervalues)\n",
       "        1    0.000    0.000    0.000    0.000 inference.py:160(is_file_like)\n",
       "        8    0.000    0.000    0.000    0.000 base.py:5399(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 frame.py:1140(to_numpy)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:7083(isnull)\n",
       "        6    0.000    0.000    0.000    0.000 parsers.py:1195(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1536(_make_index)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:2066(<dictcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:3195(_process_date_conversion)\n",
       "        4    0.000    0.000    0.000    0.000 apply.py:328(result_index)\n",
       "        2    0.000    0.000    0.000    0.000 <ipython-input-266-01a4bf55ac48>:14(corpus_config)\n",
       "        3    0.000    0.000    0.000    0.000 <ipython-input-266-01a4bf55ac48>:9(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method sys.getfilesystemencoding}\n",
       "        1    0.000    0.000    0.000    0.000 warnings.py:117(_formatwarnmsg)\n",
       "        4    0.000    0.000    0.000    0.000 {method 'getrandbits' of '_random.Random' objects}\n",
       "        4    0.000    0.000    0.000    0.000 parse.py:394(_checknetloc)\n",
       "        1    0.000    0.000    0.000    0.000 <string>:1(<module>)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:457(is_platform_windows)\n",
       "        5    0.000    0.000    0.000    0.000 function.py:40(__call__)\n",
       "        4    0.000    0.000    0.000    0.000 ops.py:1832(<lambda>)\n",
       "        5    0.000    0.000    0.000    0.000 nanops.py:180(_get_fill_value)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:3867(_has_complex_internals)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:4209(isnull)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1162(_create_index)\n",
       "        3    0.000    0.000    0.000    0.000 parsers.py:1176(_is_index_col)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1274(_validate_usecols_arg)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1423(_has_complex_date_col)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:2064(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:3157(_make_date_converter)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:3278(_clean_na_values)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'bit_length' of 'int' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.ord}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        2    0.000    0.000    0.000    0.000 base.py:4077(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:112(_validate_header_arg)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:377(_validate_names)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:939(_check_file_or_buffer)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1329(_validate_parse_dates_arg)\n",
       "        2    0.000    0.000    0.000    0.000 parsers.py:1530(_maybe_make_multi_index_columns)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%time\n",
    "def main():\n",
    "    '''\n",
    "        corpora: i2b2, mipacq, fv017\n",
    "        analyses: entity only (exact span), cui by document, full (aka (entity and cui on exaact span/exact cui)\n",
    "        systems: ctakes, biomedicus, clamp, metamap, quick_umls\n",
    "        \n",
    "        TODO -> Vectorization (entity only) -> done:\n",
    "                add switch for use of TN on single system performance evaluations -> done\n",
    "                add switch for overlap matching versus exact span -> done\n",
    "             -> Other tasks besides concept extraction\n",
    "        \n",
    "    ''' \n",
    "    analysisConf =  AnalysisConfig()\n",
    "    print(analysisConf.systems, analysisConf.corpus_config())\n",
    "    \n",
    "    if (rtype == 1):\n",
    "        generate_metrics(analysis_type, corpus, filter_semtype)\n",
    "    elif (rtype == 2):\n",
    "        print('run_type:', run_type)\n",
    "        # TODO -> list of corpora!\n",
    "#         for corpus in corpora: \n",
    "#             table_name = ref_data(corpus)\n",
    "#             system_annotation = sys_data(corpus)\n",
    "#             semtypes = ref_semtypes(filter_semtype, corpus)\n",
    "#             analysisConf =  AnalysisConfig()\n",
    "#             usys, ref = analysisConf.corpus_config(system_annotation, table_name)\n",
    "#             print('Using:', usys, ref)\n",
    "        if filter_semtype:\n",
    "            ensemble_control(analysisConf.systems, analysis_type, corpus, run_type, filter_semtype, semtypes)\n",
    "        else:\n",
    "            ensemble_control(analysisConf.systems, analysis_type, corpus, run_type, filter_semtype)\n",
    "    elif (rtype == 3):\n",
    "        t = ['concept_jaccard_score_false']\n",
    "        test_systems(analysis_type, analysisConf.systems, corpus)  \n",
    "        test_count(analysis_type, corpus)\n",
    "        test_ensemble(analysis_type, corpus)\n",
    "    elif (rtype == 4):\n",
    "        majority_vote(systems, analysis_type, corpus, run_type, filter_semtype, semtypes)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    %prun main()\n",
    "#    pass\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ( ( ctakes & clamp ) | ( biomedicus & metamap ) \n",
      "{'F': 0.24421981730566422, 'precision': 0.3355626894952738, 'recall': 0.1919653105471241, 'TP': 7526, 'FN': 31679, 'FP': 14902, 'TP/FN': 0.23757063038606016, 'n_gold': 39205, 'n_sys': 22428, 'TM': 50.25380384748433}\n",
      "        begin     end     note_id\n",
      "0         452     455  0002204202\n",
      "1         858     868  0002204202\n",
      "2         968     975  0002204202\n",
      "3        1239    1252  0002204202\n",
      "4        1372    1381  0002204202\n",
      "5        1605    1613  0002204202\n",
      "6        1764    1783  0002204202\n",
      "7        1885    1905  0002204202\n",
      "8        2076    2096  0002204202\n",
      "9        2241    2252  0002204202\n",
      "10       2437    2446  0002204202\n",
      "11       2746    2753  0002204202\n",
      "12       2902    2915  0002204202\n",
      "13       3072    3078  0002204202\n",
      "14       3617    3624  0002204202\n",
      "15       3745    3753  0002204202\n",
      "16       3940    3951  0002204202\n",
      "17       4119    4131  0002204202\n",
      "18       4525    4532  0002204202\n",
      "19       4561    4572  0002204202\n",
      "20       4605    4611  0002204202\n",
      "21       6077    6088  0002204202\n",
      "22       6255    6267  0002204202\n",
      "23       6505    6517  0002204202\n",
      "24       7884    7894  0002204202\n",
      "25       7914    7921  0002204202\n",
      "26       8762    8772  0002204202\n",
      "27       8792    8799  0002204202\n",
      "28       9448    9456  0002204202\n",
      "29       9538    9547  0002204202\n",
      "...       ...     ...         ...\n",
      "25062   97288   97298  0000327956\n",
      "25064   98290   98295  0000327956\n",
      "25068  104907  104917  0000327956\n",
      "25070  107409  107414  0000327956\n",
      "25071  107711  107721  0000327956\n",
      "25073  120363  120373  0000327956\n",
      "25083  121188  121198  0000327956\n",
      "25089  122099  122110  0000327956\n",
      "25105  128960  128967  0000327956\n",
      "25106  129114  129125  0000327956\n",
      "25109  133158  133165  0000327956\n",
      "25111  133330  133337  0000327956\n",
      "25113  136287  136292  0000327956\n",
      "25114  145096  145099  0000327956\n",
      "25115  146114  146117  0000327956\n",
      "25119  146643  146650  0000327956\n",
      "25120  146825  146836  0000327956\n",
      "25121  147875  147891  0000327956\n",
      "25123  149903  149910  0000327956\n",
      "25125  150747  150758  0000327956\n",
      "25128  153890  153906  0000327956\n",
      "25130  154514  154519  0000327956\n",
      "25131  164984  164991  0000327956\n",
      "25136  173465  173472  0000327956\n",
      "25137  174796  174812  0000327956\n",
      "25139  177274  177290  0000327956\n",
      "25146  180733  180736  0000327956\n",
      "25147  181644  181647  0000327956\n",
      "25150  182102  182105  0000327956\n",
      "25153  184778  184783  0000327956\n",
      "\n",
      "[22428 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# control filter_semtype in get_sys_data, get_ref_n and generate_metrics. TODO consolidate. \n",
    " \n",
    "# # run single ad hoc statement\n",
    "statement = '((ctakes&clamp)|(biomedicus&metamap)'\n",
    "analysis_type = 'entity'\n",
    "corpus = 'fairview'\n",
    "matches = get_merge_data(statement, analysis_type, corpus, True, semtypes[0])\n",
    "print(matches)\n",
    "\n",
    "# import spacy\n",
    "# nlp = spacy.load('en')\n",
    "\n",
    "# sql = \"select distinct note_id, sofa from concepts.sofas where corpus = 'fairview'\"\n",
    "\n",
    "# docs = pd.read_sql(sql, con=engine)\n",
    "\n",
    "# d = {}\n",
    "\n",
    "# for row in docs.itertuples():\n",
    "#     d[row.note_id] = row.sofa\n",
    "    \n",
    "# print(len(d))\n",
    "\n",
    "# test = matches[matches['note_id'] == '0000200926']\n",
    "# print(len(test))\n",
    "\n",
    "# doc = nlp(d['0000200926'])\n",
    "\n",
    "# for row in test.itertuples():\n",
    "#     my_str = [token.text.strip('\\n').lower() for token in doc if token.idx >= (row.begin) and token.idx <= (row.end)]\n",
    "#     if 'diabetes' in my_str:\n",
    "#         print(my_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TESTS -> ensemble:\n",
    "# def test_match_consistency(matches, ref_only, ref_n, sys):\n",
    "#     \"\"\"test for reference only/match set consistency:\n",
    "#         params: match, system and reference only sets\"\"\"\n",
    "   \n",
    "#     print('len', len(sys), len(matches), len(matches.union(sys)), len(matches.intersection(sys)))\n",
    "#     assert len(matches.union(ref_only)) == ref_n, 'Reference annotation mismatch union'\n",
    "#     assert len(matches.intersection(sys)) == len(matches), 'System annotation mismatch intersect'\n",
    "#     assert len(matches.union(sys)) == len(sys), 'System annotation mismatch union'\n",
    "#     assert len(matches.intersection(ref_only)) == 0, 'Reference annotation mismatch intersect'\n",
    "\n",
    "# def test_systems(analysis_type, systems, corpus):\n",
    "#     sys = df_to_set(get_sys_data(systems[0], analysis_type, corpus), analysis_type)\n",
    "#     test_match_consistency(*get_system_matches(systems[0], analysis_type, corpus), get_ref_n(analysis_type), sys)\n",
    "#     print('Match consistency:', len(sys),get_ref_n(analysis_type))\n",
    "\n",
    "# def test_metrics(ref, sys_m, match_m):\n",
    "#     test = True\n",
    "#     reference_n = len(ref)\n",
    "#     system_n = len(sys_m)\n",
    "\n",
    "#     print('Test metrics:', type(reference_n), type(system_n), type(match_m))\n",
    "\n",
    "#     reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, match_m).get_ref_sys()\n",
    "#     F, recall, precision, _, _, _, _, _ = Metrics(system_only, reference_only, reference_system_match, system_n).get_confusion_metrics()\n",
    "#     F_, recall_, precision_, _, _, _, _, _ = Metrics(system_only, reference_only, reference_system_match, system_n).get_confusion_metrics(test)\n",
    "\n",
    "#     assert F[1] == F_, 'F1 issue'\n",
    "#     assert recall[1] == recall_, 'recall issue'\n",
    "#     assert precision[1] == precision_, 'precision issue'\n",
    "#     print(F[1], F_)\n",
    "#     print(recall[1], recall_)\n",
    "#     print(precision[1], precision_)\n",
    "\n",
    "# def test_count(analysis_type, corpus):\n",
    "#     # test match counts:\n",
    "#     ctakes, _ = get_system_matches('ctakes', analysis_type, corpus)\n",
    "#     clamp, _ = get_system_matches('clamp', analysis_type, corpus)\n",
    "#     b9, _ = get_system_matches('biomedicus', analysis_type, corpus)\n",
    "#     mm, _ = get_system_matches('metamap', analysis_type, corpus)\n",
    "\n",
    "#     print('count:', len(mm.intersection(b9.intersection(clamp.intersection(ctakes)))))\n",
    "    \n",
    "# def test_ensemble(analysis_type, corpus):\n",
    "    \n",
    "#     print('ensemble:')\n",
    "#     # Get mixed system_n\n",
    "#     ref_ann, data = get_metric_data(analysis_type, corpus)\n",
    "\n",
    "#     names = ['ctakes', 'biomedicus', 'metamap', 'clamp']\n",
    "#     if 'entity' in analysis_type: \n",
    "#         cols_to_keep = ['begin', 'end', 'note_id']\n",
    "#     elif 'cui' in analysis_type:\n",
    "#         cols_to_keep = ['cui', 'note_id']\n",
    "#     elif 'full' in analysis_type:\n",
    "#         cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "\n",
    "#     biomedicus = data[data[\"system\"]=='biomedicus'][cols_to_keep].copy()\n",
    "#     ctakes = data[data[\"system\"]=='ctakes'][cols_to_keep].copy()\n",
    "#     clamp = data[data[\"system\"]=='clamp'][cols_to_keep].copy()\n",
    "#     metamap = data[data[\"system\"]=='metamap'][cols_to_keep].copy()\n",
    "#     quickumls = data[data[\"system\"]=='quick_umls'][cols_to_keep].copy()\n",
    "\n",
    "#     print('systems:', len(biomedicus), len(clamp), len(ctakes), len(metamap), len(quickumls))\n",
    "\n",
    "#     b9 = set()\n",
    "#     cl = set()\n",
    "#     ct = set()\n",
    "#     mm = set()\n",
    "#     qu = set()\n",
    "\n",
    "#     b9 = df_to_set(get_sys_data('biomedicus', analysis_type, corpus), analysis_type)\n",
    "#     print(len(b9))\n",
    "\n",
    "#     ct = df_to_set(get_sys_data('ctakes', analysis_type, corpus), analysis_type)\n",
    "#     print(len(ct))\n",
    "\n",
    "#     cl = df_to_set(get_sys_data('clamp', analysis_type, corpus), analysis_type)\n",
    "#     print(len(cl))\n",
    "\n",
    "#     mm = df_to_set(get_sys_data('metamap', analysis_type, corpus), analysis_type)\n",
    "#     print(len(mm))\n",
    "\n",
    "#     qu = df_to_set(get_sys_data('quick_umls', analysis_type, corpus), analysis_type)\n",
    "#     print(len(qu))\n",
    "    \n",
    "#     print('various merges:')\n",
    "#     print(len(b9), len(cl), len(ct), len(mm), len(qu))\n",
    "#     print(len(mm.intersection(b9.intersection(cl.intersection(ct)))))\n",
    "#     print(len(mm.union(b9.intersection(cl.intersection(ct)))))\n",
    "#     print(len(mm.union(b9.union(cl.intersection(ct)))))\n",
    "#     print(len(mm.union(b9.union(cl.union(ct)))))\n",
    "#     print(len(b9.intersection(ct)))\n",
    "\n",
    "#     sys_m = b9.intersection(ct.intersection(qu))\n",
    "#     print('sys_m:', len(sys_m))\n",
    "\n",
    "#     # Get match merges:\n",
    "#     ct, _ = get_system_matches('ctakes', analysis_type, corpus)\n",
    "#     cl, _ = get_system_matches('clamp', analysis_type, corpus)\n",
    "#     b9, _ = get_system_matches('biomedicus', analysis_type, corpus)\n",
    "#     mm, _ = get_system_matches('metamap', analysis_type, corpus)\n",
    "#     qu, _ = get_system_matches('quick_umls', analysis_type, corpus)\n",
    "\n",
    "#     match_m = b9.intersection(ct.intersection(qu))\n",
    "#     print('match_m:', len(match_m))\n",
    "#     # reference df to set\n",
    "#     if 'entity' in analysis_type: \n",
    "#         cols_to_keep = ['end', 'start','file']\n",
    "#     elif 'cui' in analysis_type:\n",
    "#         cols_to_keep = ['value','file']\n",
    "#     elif 'full' in analysis_type:\n",
    "#         cols_to_keep = ['end', 'start', 'value','file']\n",
    "\n",
    "#     ref = df_to_set(ref_ann[cols_to_keep], analysis_type, 'ref')\n",
    "\n",
    "#     print('ref:', len(ref))\n",
    "\n",
    "#     # test difference:\n",
    "#     print('FP:', len(sys_m - match_m), len(sys_m - ref))\n",
    "#     assert len(sys_m - match_m) == len(sys_m - ref), 'FP mismatch'\n",
    "#     print('FN:', len(ref - match_m), len(ref - sys_m))\n",
    "#     assert len(ref - match_m) == len(ref - sys_m), 'FN mismatch'\n",
    "    \n",
    "#     test_metrics(ref, sys_m, match_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
