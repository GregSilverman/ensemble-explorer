{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pymysql\n",
    "import time \n",
    "import functools as ft\n",
    "import glob   \n",
    "import operator as op\n",
    "import shelve\n",
    "from itertools import combinations, product\n",
    "from sqlalchemy.engine import create_engine\n",
    "from datetime import datetime\n",
    "from ast import literal_eval\n",
    "from scipy import stats  \n",
    "from scipy.stats.mstats import gmean\n",
    "from pythonds.basic.stack import Stack\n",
    "from pythonds.trees.binaryTree import BinaryTree, height #, clear_tree_out, post_order\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biomedicus', 'clamp', 'ctakes', 'metamap'] ('analytical_cui_mipacq_concepts.csv', 'test.mipacq_all')\n"
     ]
    }
   ],
   "source": [
    "# config class for analysis\n",
    "class AnalysisConfig(object):\n",
    "    \"\"\"\n",
    "    Configuration object:\n",
    "    notes by test, full per corpus\n",
    "    paths by output, gold and system location\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self = self    \n",
    "       \n",
    "        #self.systems = ['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls']\n",
    "        self.systems = ['biomedicus', 'clamp', 'ctakes', 'metamap']\n",
    "        #self.systems = ['metamap']\n",
    "        #self.systems = ['biomedicus']\n",
    "        #self.systems = ['quick_umls']\n",
    "        self.data_dir = '/Users/gms/development/nlp/nlpie/data/amicus-u01/output/'\n",
    "    \n",
    "    def corpus_config(self, corpus):\n",
    "        \n",
    "        if corpus == 'mipacq':\n",
    "            usys_data = 'analytical_cui_mipacq_concepts.csv'\n",
    "            ref_data = 'test.mipacq_all'\n",
    "        elif corpus == 'i2b2':\n",
    "            usys_data = 'analytical_cui_i2b2_concepts.csv'\n",
    "            ref_data = 'test.i2b2_all'\n",
    "            \n",
    "        return usys_data, ref_data\n",
    "        \n",
    "corpus = 'mipacq'\n",
    "analysisConf =  AnalysisConfig()\n",
    "print(analysisConf.systems, analysisConf.corpus_config(corpus))\n",
    "usys, ref = analysisConf.corpus_config(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation class for UIMA systems\n",
    "class AnnotationSystems(object):\n",
    "    \"\"\"   \n",
    "    CAS XMI Annotations of interest\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\" \n",
    "        annotation base types\n",
    "        \"\"\"   \n",
    "        \n",
    "        self.biomedicus_dir = \"biomedicus_out/\"\n",
    "        self.biomedicus_types = [\"biomedicus.v2.UmlsConcept\"]\n",
    "                                  #\"biomedicus.v2.Negated\"\n",
    "                                 #\"biomedicus.v2.Acronym\",\n",
    "                                 #\"biomedicus.v2.DictionaryTerm\",\n",
    "                                 #\"biomedicus.v2.Historical\"]\n",
    "        \n",
    "        self.clamp_dir = \"clamp_out/\"\n",
    "        self.clamp_types = [\"edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA\"]\n",
    "                             #\"org.apache.ctakes.typesystem.type.syntax.ConllDependencyNode\",\n",
    "                             #\"edu.uth.clamp.nlp.typesystem.ClampRelationUIMA\"]    \n",
    "        \n",
    "        self.ctakes_dir = \"ctakes_out/\"\n",
    "        self.ctakes_types = ['ctakes_mentions_all']#\"org.apache.ctakes.typesystem.type.textspan.Sentence\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.DiseaseDisorderMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.MedicationMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.ProcedureMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.refsem.UmlsConcept\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.SignSymptomMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.AnatomicalSiteMention\"]\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.MeasurementAnnotation\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.EventMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.EntityMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.Predicate\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.syntax.WordToken\"]\n",
    "        \n",
    "        self.metamap_dir = \"metamap_out/\"\n",
    "        self.metamap_types = [#\"org.metamap.uima.ts.Utterance\",\n",
    "                              #\"org.metamap.uima.ts.Span\",\n",
    "                              #\"org.metamap.uima.ts.Phrase\"]\n",
    "                              \"org.metamap.uima.ts.Candidate\"]\n",
    "                              #\"org.metamap.uima.ts.CuiConcept\",\n",
    "                              #\"org.metamap.uima.ts.Negation\"]\n",
    "                \n",
    "        self.quick_umls_types = [#'concept']#,\n",
    "                                #'concept_cosine_length_false',\n",
    "                                #'concept_cosine_length_true',\n",
    "                                #'concept_cosine_score_false',\n",
    "                                #'concept_cosine_score_true',\n",
    "                                #'concept_dice_length_false',\n",
    "                                #'concept_dice_length_true',\n",
    "                                #'concept_dice_score_false',\n",
    "                                #'concept_dice_score_true',\n",
    "                                #'concept_jaccard_length_false',\n",
    "                                #'concept_jaccard_length_true',\n",
    "                                'concept_jaccard_score_False']\n",
    "                                #'concept_jaccard_score_true']\n",
    "                \n",
    "        \n",
    "        '''\n",
    "\n",
    "        self.biomedicus_dir = \"biomedicus_out/\"\n",
    "        self.biomedicus_types = [#\"biomedicus.v2.UmlsConcept\"]\n",
    "                                  #\"biomedicus.v2.Negated\"\n",
    "                                 \"biomedicus.v2.Acronym\",\n",
    "                                 \"biomedicus.v2.DictionaryTerm\",\n",
    "                                 \"biomedicus.v2.Historical\"]\n",
    "        \n",
    "        \n",
    "        self.clamp_dir = \"clamp_out/\"\n",
    "        #self.clamp_types = [#\"edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA\"]\n",
    "                             #\"org.apache.ctakes.typesystem.type.syntax.ConllDependencyNode\",\n",
    "                             #\"edu.uth.clamp.nlp.typesystem.ClampRelationUIMA\"]\n",
    "        \n",
    "        \n",
    "        self.ctakes_dir = \"ctakes_out/\"\n",
    "        self.ctakes_types = [\"org.apache.ctakes.typesystem.type.textspan.Sentence\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.DiseaseDisorderMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.MedicationMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.ProcedureMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.refsem.UmlsConcept\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.SignSymptomMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.AnatomicalSiteMention\"]\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.MeasurementAnnotation\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.EventMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.EntityMention\",\n",
    "                             \"org.apache.ctakes.typesystem.type.textsem.Predicate\",\n",
    "                             \"org.apache.ctakes.typesystem.type.syntax.WordToken\"]\n",
    "        \n",
    "        self.metamap_dir = \"metamap_out/\"\n",
    "        self.metamap_types = [\"org.metamap.uima.ts.Utterance\",\n",
    "                              \"org.metamap.uima.ts.Span\",\n",
    "                              \"org.metamap.uima.ts.Phrase\"]\n",
    "                              #\"org.metamap.uima.ts.Candidate\"]\n",
    "                              #\"org.metamap.uima.ts.CuiConcept\",\n",
    "                              #\"org.metamap.uima.ts.Negation\"]\n",
    "                              \n",
    "        '''\n",
    "       \n",
    "    def get_system_type(self, system):\n",
    "        \n",
    "        \"\"\"\n",
    "        return system types\n",
    "        \"\"\"\n",
    "        \n",
    "        if system == \"biomedicus\":\n",
    "            view = \"Analysis\"\n",
    "        else:\n",
    "            view = \"_InitialView\"\n",
    "\n",
    "        if system == 'biomedicus':\n",
    "            types = self.biomedicus_types\n",
    "            output = self.biomedicus_dir\n",
    "\n",
    "        elif system == 'clamp':\n",
    "            types = self.clamp_types\n",
    "            output = self.clamp_dir\n",
    "\n",
    "        elif system == 'ctakes':\n",
    "            types = self.ctakes_types\n",
    "            output = self.ctakes_dir\n",
    "\n",
    "        elif system == 'metamap':\n",
    "            types = self.metamap_types\n",
    "            output = self.metamap_dir\n",
    "        \n",
    "        elif system == \"quick_umls\":\n",
    "            types = self.quick_umls_types\n",
    "            output = None\n",
    "            \n",
    "        return types, view, output\n",
    "    \n",
    "annSys = AnnotationSystems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qumls = pd.read_csv('/Users/gms/development/nlp/nlpie/data/amicus-u01/output/qumls_similarity.csv')\n",
    "\n",
    "# #print(len(qumls))\n",
    "# #print(len(qumls.drop_duplicates()))\n",
    "\n",
    "# cols_to_keep = ['begin', 'end', 'note_id', 'cui', 'similarity', 'overlap', 'best_match', 'system', 'type'] \n",
    "\n",
    "# qumls = qumls[cols_to_keep].drop_duplicates()\n",
    "\n",
    "# #print(len(qumls))\n",
    "\n",
    "# #print(len(qumls[['overlap']=='score']))\n",
    "# print(len(qumls[(qumls.similarity=='jaccard') & (qumls.overlap=='length') & (qumls.best_match=='true')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='jaccard') & (qumls.overlap=='length') & (qumls.best_match=='false')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='jaccard') & (qumls.overlap=='score') & (qumls.best_match=='true')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='jaccard') & (qumls.overlap=='score') & (qumls.best_match=='false')].drop_duplicates()))\n",
    "\n",
    "# print(len(qumls[(qumls.similarity=='cosine') & (qumls.overlap=='length') & (qumls.best_match=='true')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='cosine') & (qumls.overlap=='length') & (qumls.best_match=='false')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='cosine') & (qumls.overlap=='score') & (qumls.best_match=='true')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='cosine') & (qumls.overlap=='score') & (qumls.best_match=='false')].drop_duplicates()))\n",
    "\n",
    "# print(len(qumls[(qumls.similarity=='dice') & (qumls.overlap=='length') & (qumls.best_match=='true')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='dice') & (qumls.overlap=='length') & (qumls.best_match=='false')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='dice') & (qumls.overlap=='score') & (qumls.best_match=='true')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='dice') & (qumls.overlap=='score') & (qumls.best_match=='false')].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0595040941-0', '0778429553-0', '1014681675', '2889522952-2', '3080383448-5', '3300000926-3', '3360037185-3', '3580973392', '3627629462-3', '4323116051-4', '477704053-4', '528317073', '531702602', '534061073', '54832076', '5643725437-6', '5944412090-5', '6613169476-6', '7261075903-7', '7504944368-7', '7999462393-7', '8131081430', '8171084310', '8193787896', '8295055184-8', '8823185307-8']\n"
     ]
    }
   ],
   "source": [
    "def get_notes(analysis_type, corpus):\n",
    "    \n",
    "    if 'test' in analysis_type:\n",
    "        # test set of notes\n",
    "        if corpus == 'mipacq':\n",
    "            notes = ['522412787',\n",
    "             '617637585',\n",
    "             '3307880735-8',\n",
    "             '9080688558',\n",
    "             '618370565',\n",
    "             '573718188',\n",
    "             '534584',\n",
    "             '60891',\n",
    "             '62620',\n",
    "             '616172834']\n",
    "            \n",
    "        elif corpus == 'i2b2':\n",
    "            print('TODO')\n",
    "        \n",
    "        print('TEST NOTES!')\n",
    "        #,\n",
    "#          '4130154172-4',\n",
    "#          '3580478614',\n",
    "#          '5024581165-5',\n",
    "#          '4486835700-9',\n",
    "#          '534828617',\n",
    "#          '8154986253',\n",
    "#          '533855209',\n",
    "#          '60118',\n",
    "#          '3537704982-3',\n",
    "#          '617637585',\n",
    "#          '60118',\n",
    "#          '9045889026',\n",
    "#          '8819868493-8',\n",
    "#          '533698',\n",
    "#          '535978760']\n",
    "     \n",
    "    else:\n",
    "        \n",
    "        if corpus == 'mipacq':\n",
    "        # these did not meet the minimal criteria for parsing\n",
    "            notes = [\"0595040941-0\",\n",
    "                    \"0778429553-0\",\n",
    "                    \"1014681675\",\n",
    "                    \"2889522952-2\",\n",
    "                    \"3080383448-5\",\n",
    "                    \"3300000926-3\",\n",
    "                    \"3360037185-3\",\n",
    "                    \"3580973392\",\n",
    "                    \"3627629462-3\",\n",
    "                    \"4323116051-4\",\n",
    "                    \"477704053-4\",\n",
    "                    \"528317073\",\n",
    "                    \"531702602\",\n",
    "                    \"534061073\",\n",
    "                    \"54832076\",\n",
    "                    \"5643725437-6\",\n",
    "                    \"5944412090-5\",\n",
    "                    \"6613169476-6\",\n",
    "                    \"7261075903-7\",\n",
    "                    \"7504944368-7\",\n",
    "                    \"7999462393-7\",\n",
    "                    \"8131081430\",\n",
    "                    \"8171084310\",\n",
    "                    \"8193787896\",\n",
    "                    \"8295055184-8\",\n",
    "                    \"8823185307-8\"]\n",
    "            \n",
    "        elif corpus == 'i2b2':\n",
    "            notes = ['0081', \n",
    "                     '0401']\n",
    "\n",
    "    return notes# training_notes\n",
    "print(get_notes('entity', corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np # access to Numpy from Python layer\n",
    "import math\n",
    "\n",
    "class Metrics(object):\n",
    "    \"\"\"\n",
    "    metrics class:\n",
    "    returns an instance with confusion matrix metrics\n",
    "    \"\"\"\n",
    "    def __init__(self, system_only, gold_only, gold_system_match, system_n, neither = 0):\n",
    "\n",
    "        self = self    \n",
    "        self.system_only = system_only\n",
    "        self.gold_only = gold_only\n",
    "        self.gold_system_match = gold_system_match\n",
    "        self.system_n = system_n\n",
    "        self.neither = neither\n",
    "        \n",
    "    def get_confusion_metrics(self, test = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        compute confusion matrix measures, as per  \n",
    "        https://stats.stackexchange.com/questions/51296/how-do-you-calculate-precision-and-recall-for-multiclass-classification-using-co\n",
    "        \"\"\"\n",
    "        cdef:\n",
    "            int TP, FP, FN\n",
    "            double TM\n",
    "\n",
    "        TP = self.gold_system_match\n",
    "        FP = self.system_only\n",
    "        FN = self.gold_only\n",
    "        TM = TP/math.sqrt(self.system_n) # TigMetric\n",
    "        \n",
    "        if not test:\n",
    "            if self.neither == 0:\n",
    "                confusion = [[0, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "            else:\n",
    "                confusion = [[self.neither, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "            c = np.asarray(confusion)\n",
    "            recall = np.diag(c) / np.sum(c, axis = 1)\n",
    "            precision = np.diag(c) / np.sum(c, axis = 0)\n",
    "            F = 2*(precision*recall)/(precision + recall)\n",
    "        else:\n",
    "            precision = TP/(TP+FP)\n",
    "            recall = TP/(TP+FN)\n",
    "            F = 2*(precision*recall)/(precision + recall)\n",
    "        \n",
    "        # Tignanelli Metric\n",
    "        if FN == 0:\n",
    "            TP_FN_R = TP\n",
    "        elif FN > 0:\n",
    "            TP_FN_R = TP/FN\n",
    " \n",
    "        return F, recall, precision, TP, FP, FN, TP_FN_R, TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out(name, analysis_type, c):\n",
    "   \n",
    "    \"\"\"\n",
    "    write matching and reference-only sets to file for ease in merging combinations\n",
    "    \"\"\"\n",
    "    \n",
    "    # write output to file\n",
    "    dir_out = analysisConf.data_dir + 'single_system_out/'\n",
    "    with open(dir_out + name + '_' + analysis_type + '_' + c.corpus + '_matches.txt', 'w') as f:\n",
    "        for item in list(c.matches):\n",
    "            f.write(\"%s\\n\" % str(item))\n",
    "\n",
    "    # write to file\n",
    "    with open(dir_out + name + '_' + analysis_type + '_' + c.corpus + '_ref_only.txt', 'w') as f:\n",
    "        for item in list(c.false_negatives):\n",
    "            f.write(\"%s\\n\" % str(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%cython \n",
    "\n",
    "#from __main__ import write_out\n",
    "\n",
    "#import numpy as np # access to Numpy from Python layer\n",
    "def label_vector(doc, ann, labels) -> np.array:\n",
    "\n",
    "    #print(ann, doc, labels)\n",
    "\n",
    "    v = np.zeros(doc)\n",
    "    labels = list(labels)\n",
    "    \n",
    "    for (i, lab) in enumerate(labels):\n",
    "        i += 1  # 0 is reserved for no label\n",
    "        idxs = [np.arange(a.begin, a.end) for a in ann if a.label == lab]\n",
    "            \n",
    "        idxs = [j for mask in idxs for j in mask]\n",
    "        v[idxs] = i\n",
    "\n",
    "    return v\n",
    "\n",
    "# test confusion matrix elements for vectorized annotation set; includes TN\n",
    "def confused(sys1, ann1):\n",
    "    TP = np.sum(np.logical_and(sys1 >= 1, ann1 == sys1 ))\n",
    "\n",
    "    # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "    TN = np.sum(np.logical_and(sys1 == 0, ann1 == 0))\n",
    "\n",
    "    # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "    FP = np.sum(np.logical_and(sys1 >= 1, ann1 == 0))\n",
    "\n",
    "    # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "    FN = np.sum(np.logical_and(sys1 == 0, ann1 >= 1))\n",
    "    \n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "\n",
    "def get_cooccurences(ref, sys, analysis_type, corpus, single_sys = True, name = None):\n",
    "    \"\"\"\n",
    "    get coocurences between system and reference; exact match; TODO: add relaxed\n",
    "    \"\"\"\n",
    "    # test cooccurences\n",
    "    class Coocurences(object):\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.ref_system_match = 0\n",
    "            self.ref_only = 0\n",
    "            self.system_only = 0\n",
    "            self.system_n = 0\n",
    "            self.ref_n = 0\n",
    "            self.matches = set()\n",
    "            self.false_negatives = set()\n",
    "            self.corpus = corpus\n",
    "            self.cases = set(ref[\"file\"].tolist()) # cases to label \n",
    "\n",
    "    c = Coocurences()\n",
    "    \n",
    "    # test for converting to vectorization and i-o labeling\n",
    "    def test_io():\n",
    "        test = c.cases\n",
    "        if analysis_type == 'entity':\n",
    "            docs = [(x, len(open(\"/Users/gms/development/nlp/nlpie/data/amicus-u01/i2b2/source_data/test_data/\" + x + \".txt\", 'r').read())) for x in test]\n",
    "        elif analysis_type == 'full':\n",
    "            docs = [(x, len(open(\"/Users/gms/development/nlp/nlpie/data/amicus-u01/mipacq/source_data/source/\" + x + \".source\", 'r').read())) for x in test]\n",
    "\n",
    "        ann = ref.copy()\n",
    "        ann = ann.rename(index=str, columns={\"start\": \"begin\", \"file\": \"case\"}).copy()\n",
    "        cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "        if analysis_type == 'entity':\n",
    "            labels = [\"concept\"]\n",
    "            ann[\"label\"] = 'concept'\n",
    "            ann = ann[cols_to_keep].copy()\n",
    "        elif analysis_type == 'full':  \n",
    "            ann[\"label\"] = ann[\"value\"]\n",
    "            sys[\"label\"] = sys[\"cui\"]\n",
    "            labels = set(ref['value'].tolist())\n",
    "            print('labels', len(set(labels)))\n",
    "\n",
    "        sys_ = sys.rename(index=str, columns={\"note_id\": \"case\"}).copy()\n",
    "        \n",
    "        # need for enttity-only\n",
    "        if analysis_type == 'entity':\n",
    "            sys_[\"label\"] = 'concept'\n",
    "        \n",
    "        sys_ = sys_[cols_to_keep]\n",
    "       \n",
    "        tp = []\n",
    "        tn = []\n",
    "        fp = []\n",
    "        fn = []\n",
    "        cvals = []\n",
    "        out = []\n",
    "        t = []\n",
    "        d = defaultdict(list)\n",
    "        \n",
    "        for n in range(len(docs)):\n",
    "            a1 = [i for i in ann[ann[\"case\"] == docs[n][0]].copy().itertuples(index=False)]\n",
    "            s1 = [i for i in sys_[sys_[\"case\"] == docs[n][0]].copy().itertuples(index=False)]\n",
    "\n",
    "            ann1 = label_vector(docs[n][1], a1, labels)\n",
    "            sys1 = label_vector(docs[n][1], s1, labels)\n",
    "            \n",
    "            TP, TN, FP, FN = confused(sys1, ann1)\n",
    "            cvals.append([TP, TN, FP, FN])\n",
    "            \n",
    "                 \n",
    "            d['sys'].append(list([int(i) for i in sys1]))\n",
    "            d['oracle'].append(list([int(i) for i in ann1]))\n",
    "            d['case'].append(docs[n][0])\n",
    "            \n",
    "            '''\n",
    "            print(\"tn:\", np.intersect1d(np.where(ann1 == 0)[0], np.where(sys1 == 0)[0]),  \n",
    "                  \"tp:\", np.intersect1d(np.where(ann1 == 1)[0], np.where(sys1 == 1)[0]), \n",
    "                  \"fn:\", np.intersect1d(np.where(ann1 == 1)[0], np.where(sys1 == 0)[0]), \n",
    "                  \"fp:\", np.intersect1d(np.where(ann1 == 0)[0], np.where(sys1 == 1)[0]))\n",
    "            '''\n",
    "        d['labels'] = labels\n",
    "        \n",
    "        corp = shelve.open('/Users/gms/Desktop/' + sys.name + '.dat')\n",
    "        \n",
    "        for k in d:\n",
    "            corp[k] = d[k]\n",
    "        \n",
    "        corp.close()\n",
    "       \n",
    "        return cvals\n",
    "    \n",
    "    TP, TN, FP, FN = np.sum(test_io(), axis=0)\n",
    "    F, recall, precision, TP, FP, FN, TP_FN_R, TM = Metrics(FP, FN, TP, len(sys), TN).get_confusion_metrics() #no TN\n",
    "    print('test_io():', TP, TN, FP, FN, F, recall, precision)\n",
    "    \n",
    "    # non-vectorized:\n",
    "    if 'entity' in analysis_type and single_sys: # mipacq n -> 16793\n",
    "        cols_to_keep = ['begin', 'end', 'note_id']\n",
    "        sys = sys[cols_to_keep].drop_duplicates()\n",
    "        ref = ref[['start', 'end', 'file']].drop_duplicates()\n",
    "        sys.name = name\n",
    "    elif 'cui' in analysis_type and single_sys: # mipacq n -> 10799\n",
    "        cols_to_keep = ['cui', 'note_id']\n",
    "        sys = sys[cols_to_keep].drop_duplicates()\n",
    "        # do not overestimate FP\n",
    "        sys = sys[~sys['cui'].isnull()] \n",
    "        ref = ref[['value', 'file']].drop_duplicates()\n",
    "        ref = ref[~ref['value'].isnull()]\n",
    "        sys.name = name\n",
    "    elif 'full' in analysis_type and single_sys: # mipacq n -> 17393\n",
    "        cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "        sys = sys[cols_to_keep].drop_duplicates()\n",
    "        sys = sys[~sys['cui'].isnull()]\n",
    "        ref = ref[['start', 'end', 'value', 'file']].drop_duplicates()\n",
    "        ref = ref[~ref['value'].isnull()]\n",
    "        sys.name = name\n",
    "    \n",
    "    # matches via inner join\n",
    "    matches = pd.merge(sys, ref, how = 'inner', left_on=['begin','end','note_id'], right_on = ['start','end','file']) \n",
    "    # reference-only via left outer join\n",
    "    fn = pd.merge(ref, sys, how = 'left', left_on=['start','end','file'], right_on = ['begin','end','note_id']) \n",
    "    \n",
    "    fn = fn[fn['begin'].isnull()] # get as outer join with no match\n",
    "    \n",
    "    if 'entity' in analysis_type and single_sys:\n",
    "        cols_to_keep = ['start', 'end', 'file']\n",
    "    else:\n",
    "        cols_to_keep = ['start', 'end', 'value', 'file']\n",
    "        \n",
    "        \n",
    "    matches = matches[cols_to_keep]\n",
    "    fn = fn[cols_to_keep]\n",
    "    \n",
    "    # use for metrics \n",
    "    c.matches = c.matches.union(df_to_set(matches, analysis_type, 'ref'))\n",
    "    c.false_negatives = c.false_negatives.union(df_to_set(fn, analysis_type, 'ref'))\n",
    "    c.ref_system_match = len(c.matches)\n",
    "    c.system_only = len(sys) - len(c.matches)\n",
    "    c.system_n = len(sys)\n",
    "    c.ref_n = len(ref)\n",
    "    c.ref_only = len(c.false_negatives)\n",
    "\n",
    "    # sanity check\n",
    "    if len(ref) - c.ref_system_match < 0:\n",
    "        print('Error: ref_system_match > len(ref)!')\n",
    "    if len(ref) != c.ref_system_match + c.ref_only:\n",
    "        print('Error: ref count mismatch!')\n",
    "   \n",
    "    # save TP/FN\n",
    "    if single_sys:\n",
    "        print(analysis_type)\n",
    "        write_out(sys.name, analysis_type, c)\n",
    "\n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 µs ± 9.61 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "337 µs ± 21.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "19.9 ns ± 0.645 ns per loop (mean ± std. dev. of 7 runs, 100000000 loops each)\n",
      "17.9 ns ± 0.293 ns per loop (mean ± std. dev. of 7 runs, 100000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# merging test for i-o labeled data\n",
    "\n",
    "import numpy as np\n",
    "import shelve\n",
    "         \n",
    "# load shelve\n",
    "def read_shelve():\n",
    "        corp = shelve.open('/Users/gms/Desktop/test.dat')\n",
    "        #print(corp['case'])\n",
    "        \n",
    "        return corp\n",
    "        \n",
    "test = read_shelve()\n",
    "\n",
    "# get sample for testing\n",
    "for case in test['case'][3:5]:\n",
    "    for i in range(len(test['case'][3:5])):\n",
    "        if i == 3:\n",
    "            t0 = test['oracle'][3][0:500]\n",
    "        else:\n",
    "            t1 = test['oracle'][4][0:500]\n",
    "        \n",
    "        #print('case:', case, test['sys'][i], test['oracle'][i], confused(np.array(test['sys'][i]), np.array(test['oracle'][i])))\n",
    "    #print(t0, t1)\n",
    "        \n",
    "t0 = np.array(test['oracle'][3][0:500])\n",
    "t1 = np.array(test['oracle'][5][0:500])\n",
    "\n",
    "l0 = list(t0)\n",
    "l1 = list(t1)\n",
    "#l0 = [0, [4, 1], 4, 4, 0, 0, 0, 8, 0, 0] \n",
    "#l1 = [0, [1, 4], 4, 0, 0, 0, 0, 8, 8, 8]\n",
    "\n",
    "def intersection(lst1, lst2): \n",
    "    out = list()\n",
    "    if isinstance(lst1, list) and isinstance(lst2, list):\n",
    "        out = list(set(lst1) & set(lst2))\n",
    "    elif isinstance(lst1, list) and isinstance(lst2, int):\n",
    "        out = list(set(lst1) & set([lst2]))\n",
    "    elif isinstance(lst1, int) and isinstance(lst2, list):\n",
    "        out = list(set([lst1]) & set(lst2))\n",
    "    elif isinstance(lst1, int) and isinstance(lst2, int):\n",
    "        out = list(set([lst1]) & set([lst2]))\n",
    "    if len(out) > 1:\n",
    "        return out\n",
    "    elif len(out) == 1:\n",
    "        return out[0]\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def union(lst1, lst2): \n",
    "    out = list()\n",
    "    if isinstance(lst1, list) and isinstance(lst2, list):\n",
    "        out = list(set(lst1) | set(lst2))\n",
    "    elif isinstance(lst1, list) and isinstance(lst2, int):\n",
    "        out = list(set(lst1) | set([lst2]))\n",
    "    elif isinstance(lst1, int) and isinstance(lst2, list):\n",
    "        out = list(set([lst1]) | set(lst2))\n",
    "    elif isinstance(lst1, int) and isinstance(lst2, int):\n",
    "        out = list(set([lst1]) | set([lst2]))\n",
    "    if len(out) == 1:\n",
    "        out = out[0]\n",
    "    return out\n",
    "    \n",
    "# union and intersect\n",
    "def umerges(l0, l1):\n",
    "    un = [0]*len(l0)\n",
    "    for i in range(len(l0)):\n",
    "        un[i] = union(l0[i], l1[i])\n",
    "\n",
    "    return un\n",
    "    \n",
    "%timeit un = umerges(l0, l1)\n",
    "\n",
    "def imerges(l0, l1):\n",
    "    inter = [0]*len(l0)\n",
    "    for i in range(len(l0)):\n",
    "        inter[i] = intersection(l0[i], l1[i])\n",
    "\n",
    "    return inter\n",
    "\n",
    "%timeit inter = imerges(l0, l1)\n",
    "\n",
    "union = [\n",
    "    ( [set(x) | set(y)] if isinstance(x, list) and isinstance(y, list)\n",
    "      else [set(x) | set([y])] if isinstance(x, list) and isinstance(y, int)\n",
    "      else [set([x]) | set(y)] if isinstance(x, int) and isinstance(y, list)\n",
    "      else [set([x]) | set([y])])\n",
    "        \n",
    "     for x, y in zip(l0, l1)\n",
    "]\n",
    "\n",
    "# unpack map object\n",
    "*y, = list(map(list, zip(*union)))\n",
    "%timeit y\n",
    "\n",
    "intersection = [\n",
    "    ( [set(x) & set(y)] if isinstance(x, list) and isinstance(y, list)\n",
    "      else [set(x) & set([y])] if isinstance(x, list) and isinstance(y, int)\n",
    "      else [set([x]) & set(y)] if isinstance(x, int) and isinstance(y, list)\n",
    "      else [set([x]) & set([y])])\n",
    "      for x, y in zip(l0, l1)\n",
    "    \n",
    "]\n",
    "\n",
    "*x, = list(map(list, zip(*intersection)))\n",
    "%timeit x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_data(training_notes, analysis_type, corpus):\n",
    "\n",
    "    engine = create_engine('mysql+pymysql://gms:nej123@localhost/test', pool_pre_ping=True, pool_size=20, max_overflow=30)\n",
    "    \n",
    "    usys_file, ref_table = AnalysisConfig().corpus_config(corpus)\n",
    "    systems = AnalysisConfig().systems\n",
    "    \n",
    "    sys_ann = pd.read_csv(analysisConf.data_dir + usys_file, dtype={'note_id': str})\n",
    "    \n",
    "    if 'test' not in analysis_type:\n",
    "        sql = \"SELECT * FROM \" + ref_table + \" where file not in %(training_notes)s\"  \n",
    "        sys_ann = sys_ann[~sys_ann['note_id'].isin(training_notes)]\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        sql = \"SELECT * FROM \" + ref_table + \" where file in %(training_notes)s\"  \n",
    "        sys_ann = sys_ann[sys_ann['note_id'].isin(training_notes)]\n",
    "    \n",
    "    ref_ann = pd.read_sql(sql, params={\"training_notes\":training_notes}, con=engine)\n",
    "    sys_ann = sys_ann.drop_duplicates()\n",
    "    \n",
    "    return ref_ann, sys_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "def geometric_mean(metrics):\n",
    "    \"\"\"\n",
    "    1. Get rank average of F1, TP/FN, TM\n",
    "        http://www.datasciencemadesimple.com/rank-dataframe-python-pandas-min-max-dense-rank-group/\n",
    "        https://stackoverflow.com/questions/46686315/in-pandas-how-to-create-a-new-column-with-a-rank-according-to-the-mean-values-o?rq=1\n",
    "    2. Take geomean of 2.\n",
    "        https://stackoverflow.com/questions/42436577/geometric-mean-applied-on-row\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.DataFrame() \n",
    "\n",
    "    metrics['F1 rank']=metrics['F'].rank(ascending=0,method='average')\n",
    "    metrics['TP/FN rank']=metrics['TP/FN'].rank(ascending=0,method='average')\n",
    "    metrics['TM rank']=metrics['TM'].rank(ascending=0,method='average')\n",
    "    metrics['Gmean'] = gmean(metrics.iloc[:,-3:],axis=1)\n",
    "\n",
    "    return metrics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metrics(analysis_type, corpus, single_sys = None):\n",
    "    start = time.time()\n",
    "\n",
    "    systems = AnalysisConfig().systems\n",
    "    metrics = pd.DataFrame()\n",
    "    print('corpus:', corpus)\n",
    "\n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    ref_ann, sys_ann = get_metric_data(training_notes, analysis_type, corpus)\n",
    "    \n",
    "    for sys in systems:\n",
    "            types, _, _ = AnnotationSystems().get_system_type(sys) # system types for iterable\n",
    "            for t in types:\n",
    "                print(t)\n",
    "                system = pd.DataFrame()\n",
    "                \n",
    "                system_annotations = sys_ann.copy()\n",
    "                \n",
    "                system = system_annotations[system_annotations['type'] == str(t)]\n",
    "            \n",
    "                if sys == 'quick_umls':\n",
    "                    system = system[system.similarity.astype(float) >= 0.75]\n",
    "            \n",
    "                system = system.drop_duplicates()\n",
    "                system.name = sys\n",
    "                \n",
    "                c = get_cooccurences(ref_ann, system, analysis_type, corpus, True, system.name) # get matches, FN, etc.\n",
    "                \n",
    "                print(c.ref_n, c.ref_only, c.system_n, c.system_only, c.ref_system_match)\n",
    "                \n",
    "            if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "                F, recall, precision, TP, FP, FN, TP_FN_R, TM = Metrics(c.system_only, c.ref_only, c.ref_system_match, c.system_n).get_confusion_metrics()\n",
    "                d = {'system': sys, \n",
    "                     'type': t, \n",
    "                     'F': F[1], \n",
    "                     'precision': precision[1], \n",
    "                     'recall': recall[1], \n",
    "                     'TP': TP, \n",
    "                     'FN': FN, \n",
    "                     'FP': FP, \n",
    "                     'TP/FN': TP_FN_R,\n",
    "                     'n_gold': c.ref_n, \n",
    "                     'n_sys': c.system_n, \n",
    "                     'TM': TM}\n",
    "\n",
    "                data = pd.DataFrame(d,  index=[0])\n",
    "                metrics = pd.concat([metrics, data], ignore_index=True)\n",
    "                metrics.drop_duplicates(keep='last', inplace=True)\n",
    "            else:\n",
    "                print(\"NO EXACT MATCHES FOR\", t)\n",
    "            elapsed = (time.time() - start)\n",
    "            print(\"elapsed:\", sys, elapsed)\n",
    "     \n",
    "    elapsed = (time.time() - start)\n",
    "    print(geometric_mean(metrics))\n",
    "    \n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    \n",
    "    if single_sys is None:\n",
    "        file_name = 'metrics_'\n",
    "    \n",
    "    metrics.to_csv(analysisConf.data_dir + corpus + '_' + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "    \n",
    "    print(\"total elapsed time:\", elapsed) \n",
    "\n",
    "# used to iterate through mm scores\n",
    "def generate_metrics_mm(analysis_type, corpus, single_sys = None):\n",
    "    start = time.time()\n",
    "    from pandas.api.types import is_numeric_dtype\n",
    "    #systems = [\"biomedicus\",\"ctakes\",\"metamap\",\"clamp\",\"quick_umls\"]\n",
    "    systems = AnalysisConfig().systems\n",
    "    #systems = [\"quick_umls\"]\n",
    "    metrics = pd.DataFrame()\n",
    "    print('corpus:', corpus)\n",
    "\n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    ref_ann, sys_ann = get_metric_data(training_notes, analysis_type, corpus)\n",
    "    \n",
    "    sys_ann = sys_ann[(sys_ann.score.notnull()) & (sys_ann['system'] == 'metamap')]\n",
    "    sys_ann = sys_ann[['begin', 'end', 'note_id', 'system', 'score']].drop_duplicates()\n",
    "    sys_ann.score = sys_ann.score.astype(int)\n",
    "    \n",
    "    for sys in systems:\n",
    "        types, _, _ = AnnotationSystems().get_system_type(sys) # system types for iterable\n",
    "        for t in types:\n",
    "            print(t)\n",
    "\n",
    "            for i in range(500, 1050, 50): \n",
    "\n",
    "                sys_ann = sys_ann[(sys_ann[\"score\"] >= i)].copy()\n",
    "\n",
    "                print('score:', i, len(sys_ann), sys_ann.columns) \n",
    "\n",
    "                sys_ann.name = sys + str(i)\n",
    "\n",
    "                c = get_cooccurences(ref_ann, sys_ann, analysis_type, corpus, True, sys_ann.name) # get matches, FN, etc.\n",
    "\n",
    "                print(c.ref_n, c.ref_only, c.system_n, c.system_only, c.ref_system_match)\n",
    "\n",
    "                #print(i, len(system))\n",
    "\n",
    "                if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "                    F, recall, precision, TP, FP, FN, TP_FN_R, TM = Metrics(c.system_only, c.ref_only, c.ref_system_match, c.system_n).get_confusion_metrics()\n",
    "                    d = {'system': sys + '_score_' + str(i), \n",
    "                         'type': t, \n",
    "                         'F': F[1], \n",
    "                         'precision': precision[1], \n",
    "                         'recall': recall[1], \n",
    "                         'TP': TP, \n",
    "                         'FN': FN, \n",
    "                         'FP': FP, \n",
    "                         'TP/FN': TP_FN_R,\n",
    "                         'n_gold': c.ref_n, \n",
    "                         'n_sys': c.system_n, \n",
    "                         'TM': TM}\n",
    "\n",
    "                    data = pd.DataFrame(d,  index=[0])\n",
    "                    metrics = pd.concat([metrics, data], ignore_index=True)\n",
    "                    metrics.drop_duplicates(keep='last', inplace=True)\n",
    "                else:\n",
    "                    print(\"NO EXACT MATCHES FOR\", t)\n",
    "                elapsed = (time.time() - start)\n",
    "                print(\"elapsed:\", sys, elapsed)\n",
    "     \n",
    "    elapsed = (time.time() - start)\n",
    "    print(geometric_mean(metrics))\n",
    "    \n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    # UIMA or QuickUMLS\n",
    "    if single_sys is None:\n",
    "        file_name = 'mm_metrics_'\n",
    "    metrics.to_csv(analysisConf.data_dir + corpus + '_' + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "    \n",
    "    print(\"total elapsed time:\", elapsed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in system matches from file\n",
    "\n",
    "def get_ref_n(analysis_type):\n",
    "   \n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    ref_ann, _ = get_metric_data(training_notes, analysis_type, corpus)\n",
    "    \n",
    "    # do not overestimate fn\n",
    "    if 'entity' in analysis_type:\n",
    "        ref_ann = ref_ann[['start', 'end', 'file']].drop_duplicates()\n",
    "    elif 'cui' in analysis_type:\n",
    "        ref_ann = ref_ann[['value', 'file']].drop_duplicates()\n",
    "    elif 'full' in analysis_type:\n",
    "        ref_ann = ref_ann[['start', 'end', 'value', 'file']].drop_duplicates()\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    ref_n = len(ref_ann.drop_duplicates())\n",
    "    \n",
    "    return ref_n\n",
    "\n",
    "def get_sys_data(system, analysis_type, corpus):\n",
    "    \n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    _, data = get_metric_data(training_notes, analysis_type, corpus)\n",
    "        \n",
    "    out = data[data['system']== system].copy()\n",
    "    \n",
    "    if system == 'quick_umls':\n",
    "        out = out[(out.similarity.astype(float) >= 0.75) & (out[\"type\"] == 'concept_jaccard_score_False')]\n",
    "\n",
    "    if 'entity' in analysis_type:\n",
    "        cols_to_keep = ['begin', 'end', 'note_id']\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['cui', 'note_id']\n",
    "    elif 'full' in analysis_type:\n",
    "        cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "\n",
    "    out = out[cols_to_keep]\n",
    "    \n",
    "    return out.drop_duplicates()\n",
    "\n",
    "def get_system_matches(system, analysis_type, corpus):\n",
    "    dir_test = analysisConf.data_dir + 'single_system_out/'\n",
    "\n",
    "    file = dir_test + system + '_' + analysis_type + '_' + corpus + '_matches.txt'\n",
    "    data_matches = set(literal_eval(line.strip()) for line in open(file))\n",
    "\n",
    "    file = dir_test + system + '_' + analysis_type + '_' + corpus + '_ref_only.txt'\n",
    "    data_fn = set(literal_eval(line.strip()) for line in open(file)) #{ f for f in file.readlines() }\n",
    "\n",
    "    return data_matches, data_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to generate QuickUMLS system annotations (must run from shell):\n",
    "\n",
    "import os, glob\n",
    "from client import get_quickumls_client\n",
    "from quickumls import QuickUMLS\n",
    "import pandas as pd\n",
    "\n",
    "directory_to_parse = '/Users/gms/development/nlp/nlpie/data/amicus-u01/mipacq/data_in/'\n",
    "quickumls_fp = '/Users/gms/development/nlp/engines_misc_tools/QuickUMLS/data/'\n",
    "os.chdir(directory_to_parse)\n",
    "\n",
    "#similarity = ['dice', 'cosine', 'jaccard', 'overlap']\n",
    "similarity = ['jaccard']\n",
    "overlapping_criteria = ['score', 'length']\n",
    "\n",
    "for s in similarity:\n",
    "    for o in overlapping_criteria:\n",
    "        #matcher = get_quickumls_client(similarity_name)\n",
    "        matcher = QuickUMLS(quickumls_fp=quickumls_fp, overlapping_criteria, threshold=0.7, window=5, similarity_name=s)\n",
    "        test = pd.DataFrame()\n",
    "        for fname in glob.glob(directory_to_parse + '*.txt'):\n",
    "            t = os.path.basename(fname)\n",
    "            u = t.split('.')[0]\n",
    "            with open(directory_to_parse + u + '.txt') as f:\n",
    "                f1 = f.read()\n",
    "                out = matcher.match(f1, best_match=True, ignore_syntax=False)\n",
    "                for i in out:\n",
    "                    i[0]['note_id'] = u\n",
    "                    frames = [ test, pd.DataFrame(i[0], index = [0]) ]\n",
    "                    test = pd.concat(frames, ignore_index=True)\n",
    "        test['system'] = 'quick_umls'\n",
    "        test['similarity'] = s\n",
    "        test['overlap'] = o\n",
    "        test['type'] = 'concept'\n",
    "        test['note_id'] = u\n",
    "        testt['best_match'] = 'true'\n",
    "        temp = test.rename(columns={'start': 'begin'}).copy()\n",
    "        print(temp.tail())\n",
    "\n",
    "        temp.to_csv('/Users/gms/development/nlp/nlpie/data/amicus-u01/output/qumls.csv', mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERATE merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetTotals(object):\n",
    "    \"\"\"\n",
    "    returns an instance with merged match set numbers using either union or intersection of elements in set \n",
    "    \"\"\"\n",
    "    def __init__(self, ref_n, sys_n, match_set): #_left, match_set_right):\n",
    "\n",
    "        self = self    \n",
    "        self.ref_ann = ref_n\n",
    "        self.sys_n = sys_n\n",
    "        self.match_set = match_set\n",
    "\n",
    "    def get_ref_sys(self):\n",
    "\n",
    "        ref_only = self.ref_ann - len(self.match_set)\n",
    "        sys_only = self.sys_n - len(self.match_set)\n",
    "\n",
    "        return ref_only, sys_only, len(self.match_set), self.match_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_eval(ref_only, system_only, ref_system_match, matches, system_n, ref_n):\n",
    "    \"\"\"\n",
    "    Generate confusion matrix params\n",
    "    :params: ref_only, system_only, reference_system_match -> sets\n",
    "    matches, system_n, reference_n -> counts\n",
    "    :return: dictionary object\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if ref_only + ref_system_match != ref_n:\n",
    "        print('ERROR!')\n",
    "\n",
    "    # get evaluation metrics\n",
    "    d = {}\n",
    "    \n",
    "    F, recall, precision, TP, FP, FN, TP_FN_R, TM  = Metrics(system_only, ref_only, ref_system_match, system_n).get_confusion_metrics()\n",
    "\n",
    "    d = {\n",
    "         'F': F[1], \n",
    "         'precision': precision[1], \n",
    "         'recall': recall[1], \n",
    "         'TP': TP, \n",
    "         'FN': FN, \n",
    "         'FP': FP, \n",
    "         'TP/FN': TP_FN_R,\n",
    "         'n_gold': ref_n, \n",
    "         'n_sys': system_n, \n",
    "         'TM': TM\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if system_n - FP != TP:\n",
    "        print('inconsistent system n!')\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUERY TO VALIDATE qumls system counts\n",
    "select count(*), type from (select cui, begin, end, note_id, type from\n",
    "(SELECT distinct *\n",
    " FROM test.qumls_cui\n",
    "where note_id not in (\"0595040941-0\",\n",
    "                            \"0778429553-0\",\n",
    "                            \"1014681675\",\n",
    "                            \"2889522952-2\",\n",
    "                            \"3080383448-5\",\n",
    "                            \"3300000926-3\",\n",
    "                            \"3360037185-3\",\n",
    "                            \"3580973392\",\n",
    "                            \"3627629462-3\",\n",
    "                            \"4323116051-4\",\n",
    "                            \"477704053-4\",\n",
    "                            \"528317073\",\n",
    "                            \"531702602\",\n",
    "                            \"534061073\",\n",
    "                            \"54832076\",\n",
    "                            \"5643725437-6\",\n",
    "                            \"5944412090-5\",\n",
    "                            \"6613169476-6\",\n",
    "                            \"7261075903-7\",\n",
    "                            \"7504944368-7\",\n",
    "                            \"7999462393-7\",\n",
    "                            \"8131081430\",\n",
    "                            \"8171084310\",\n",
    "                            \"8193787896\",\n",
    "                            \"8295055184-8\",\n",
    "                            \"8823185307-8\") \n",
    "                            and similarity >= 0.8 ) t\n",
    "group by cui, begin, end, note_id, type) t\n",
    "group by type;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import operator as op\n",
    "import pandas as pd\n",
    "from __main__ import get_system_matches, get_sys_data\n",
    "\n",
    "def process_sentence(pt, sentence, analysis_type, corpus):\n",
    "    \"\"\"\n",
    "    Recursively evaluate parse tree, \n",
    "    with check for existence before build\n",
    "       :param sentence: to process\n",
    "       :return class of merged annotations, boolean operated system df \n",
    "    \"\"\"\n",
    "    \n",
    "    class Results(object):\n",
    "        def __init__(self):\n",
    "            self.results = set()\n",
    "            #self.operations = []\n",
    "            self.sytem_merges = pd.DataFrame()\n",
    "            \n",
    "    r = Results()\n",
    "    \n",
    "    if 'entity' in analysis_type: \n",
    "        cols_to_keep = ['begin', 'end', 'note_id'] # entity only\n",
    "    elif 'full' in analysis_type: \n",
    "        cols_to_keep = ['cui', 'begin', 'end', 'note_id'] # entity only\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['cui', 'note_id'] # entity only\n",
    "        \n",
    "    \n",
    "    def evaluate(parseTree):\n",
    "        oper = {'&': op.and_, '|': op.or_}\n",
    "        \n",
    "        if parseTree:\n",
    "            leftC = evaluate(parseTree.getLeftChild())\n",
    "            rightC = evaluate(parseTree.getRightChild())\n",
    "            \n",
    "            if leftC and rightC:\n",
    "                query = set()\n",
    "                system_query = pd.DataFrame()\n",
    "                fn = oper[parseTree.getRootVal()]\n",
    "                \n",
    "                if isinstance(leftC, str):\n",
    "                    \n",
    "                    # get system as leaf node \n",
    "                    left, _ = get_system_matches(leftC, analysis_type, corpus)\n",
    "                    left_sys = get_sys_data(leftC, analysis_type, corpus)\n",
    "                \n",
    "                elif isinstance(leftC, tuple):\n",
    "                    left = leftC[0]\n",
    "                    l_sys = leftC[1]\n",
    "                \n",
    "                if isinstance(rightC, str):\n",
    "                    \n",
    "                    # get system as leaf node\n",
    "                    right, _ = get_system_matches(rightC, analysis_type, corpus)\n",
    "                    right_sys = get_sys_data(rightC, analysis_type, corpus)\n",
    "                    \n",
    "                elif isinstance(rightC, tuple):\n",
    "                    right = rightC[0]\n",
    "                    r_sys = rightC[1]\n",
    "                    \n",
    "                # create match set based on boolean operation\n",
    "                match_set = fn(left, right)\n",
    "                \n",
    "                if fn == op.or_:\n",
    "                    r.results = r.results.union(match_set)\n",
    "                   \n",
    "                    if isinstance(leftC, str) and isinstance(rightC, str):\n",
    "                        df = left_sys.append(right_sys)\n",
    "                        df = df[cols_to_keep].drop_duplicates()\n",
    "                        \n",
    "                    elif isinstance(leftC, str) and isinstance(rightC, tuple):\n",
    "                        df = left_sys.append(r_sys)\n",
    "                        df = df[cols_to_keep].drop_duplicates()\n",
    "                    \n",
    "                    elif isinstance(leftC, tuple) and isinstance(rightC, str):\n",
    "                        df = right_sys.append(l_sys)\n",
    "                        df = df[cols_to_keep].drop_duplicates()\n",
    "                    \n",
    "                    elif isinstance(leftC, tuple) and isinstance(rightC, tuple):\n",
    "                        df = l_sys.append(r_sys)\n",
    "                        df = df[cols_to_keep].drop_duplicates()\n",
    "        \n",
    "                if fn == op.and_:\n",
    "                    if len(r.results) == 0:\n",
    "                        r.results = match_set\n",
    "                    r.results = r.results.intersection(match_set)\n",
    "                    \n",
    "                    if isinstance(leftC, str) and isinstance(rightC, str):\n",
    "                        df = left_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                        df = df[cols_to_keep].drop_duplicates()\n",
    "                     \n",
    "                    elif isinstance(leftC, str) and isinstance(rightC, tuple):\n",
    "                        df = left_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                        df = df[cols_to_keep].drop_duplicates()\n",
    "                    \n",
    "                    elif isinstance(leftC, tuple) and isinstance(rightC, str):\n",
    "                        df = l_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                        df = df[cols_to_keep].drop_duplicates()\n",
    "                    \n",
    "                    elif isinstance(leftC, tuple) and isinstance(rightC, tuple):\n",
    "                        df = l_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                        df = df[cols_to_keep].drop_duplicates()\n",
    "                \n",
    "                # get matched results\n",
    "                query.update(r.results)\n",
    "                \n",
    "                # get combined system results\n",
    "                r.sytem_merges = df\n",
    "                \n",
    "                if len(df) > 0:\n",
    "                    system_query = system_query.append(df)\n",
    "                else:\n",
    "                    print('wtf!')\n",
    "                    \n",
    "                return query, system_query\n",
    "            else:\n",
    "                return parseTree.getRootVal()\n",
    "    \n",
    "    if sentence.n_or > 0 or sentence.n_and > 0:\n",
    "        evaluate(pt)  \n",
    "    \n",
    "    # trivial case\n",
    "    elif sentence.n_or == 0 and sentence.n_and == 0:\n",
    "        \n",
    "        r.results, _ = get_system_matches(sentence.sentence, analysis_type, corpus)\n",
    "        r.sytem_merges = get_sys_data(sentence.sentence, analysis_type, corpus)\n",
    "        print('trivial:', sentence.sentence, len(r.results), len(r.sytem_merges))\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Incoming Boolean sentences are parsed into a binary tree.\n",
    "\n",
    "Test expressions to parse:\n",
    "\n",
    "sentence = '((((A&B)|C)|D)&E)'\n",
    "\n",
    "sentence = '(E&(D|(C|(A&B))))'\n",
    "\n",
    "sentence = '(((A|(B&C))|(D&(E&F)))|(H&I))'\n",
    "\n",
    "\"\"\"\n",
    "# build parse tree from passed sentence\n",
    "# using grammatical rules of Boolean logic\n",
    "def buildParseTree(fpexp):\n",
    "    \"\"\"\n",
    "       Iteratively build parse tree from passed sentence using grammatical rules of Boolean logic\n",
    "       :param fpexp: sentence to parse\n",
    "       :return eTree: parse tree representation\n",
    "       Incoming Boolean sentences are parsed into a binary tree.\n",
    "       Test expressions to parse:\n",
    "       sentence = '(A&B)'\n",
    "       sentence = '(A|B)'\n",
    "       sentence = '((A|B)&C)'\n",
    "       \n",
    "    \"\"\"\n",
    "\n",
    "    fplist = fpexp.split()\n",
    "    pStack = Stack()\n",
    "    eTree = BinaryTree('')\n",
    "    pStack.push(eTree)\n",
    "    currentTree = eTree\n",
    "\n",
    "    for i in fplist:\n",
    "\n",
    "        if i == '(':\n",
    "            currentTree.insertLeft('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getLeftChild()\n",
    "        elif i not in ['&', '|', ')']:\n",
    "            currentTree.setRootVal(i)\n",
    "            parent = pStack.pop()\n",
    "            currentTree = parent\n",
    "        elif i in ['&', '|']:\n",
    "            currentTree.setRootVal(i)\n",
    "            currentTree.insertRight('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getRightChild()\n",
    "        elif i == ')':\n",
    "            currentTree = pStack.pop()\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    return eTree\n",
    "\n",
    "def make_parse_tree(payload):\n",
    "    \"\"\"\n",
    "    Ensure data to create tree are in standard form\n",
    "    :param sentence: sentence to preprocess\n",
    "    :return pt, parse tree graph\n",
    "            sentence, processed sentence to build tree\n",
    "            a: order\n",
    "    \"\"\"\n",
    "    def preprocess_sentence(sentence):\n",
    "        # prepare statement for case when a boolean AND/OR is given\n",
    "        sentence = payload.replace('(', ' ( '). \\\n",
    "            replace(')', ' ) '). \\\n",
    "            replace('&', ' & '). \\\n",
    "            replace('|', ' | '). \\\n",
    "            replace('  ', ' ')\n",
    "        return sentence\n",
    "\n",
    "    sentence = preprocess_sentence(payload)\n",
    "    print(sentence)\n",
    "    \n",
    "    pt = buildParseTree(sentence)\n",
    "    #pt.postorder() \n",
    "    \n",
    "    return pt\n",
    "\n",
    "class Sentence(object):\n",
    "\n",
    "    def __init__(self, sentence):\n",
    "        self = self\n",
    "        self.n_and = sentence.count('&')\n",
    "        self.n_or = sentence.count('|')\n",
    "        self.sentence = sentence\n",
    "    \n",
    "def get_metrics(boolean_expression, analysis_type, corpus):\n",
    "    \"\"\"\n",
    "    Traverse binary parse tree representation of Boolean sentence\n",
    "        :params: boolean expression in form of '(<annotator_engine_name1><boolean operator><annotator_engine_name2>)'\n",
    "                 analysis_type (string value of: 'test', 'entity', 'cui', 'full') used to filter set of reference and system annotations \n",
    "        :return: dictionary with values needed for confusion matrix\n",
    "    \"\"\"\n",
    "    sentence = Sentence(boolean_expression)   \n",
    "\n",
    "    pt = make_parse_tree(sentence.sentence)\n",
    "\n",
    "    r = process_sentence(pt, sentence, analysis_type, corpus)\n",
    "\n",
    "    system_n = len(r.sytem_merges)\n",
    "    reference_n = get_ref_n(analysis_type)\n",
    "\n",
    "    reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, r.results).get_ref_sys()\n",
    "\n",
    "    # get overall TP/TF and various other counts for running confusion matrix metric analysis\n",
    "    return merge_eval(reference_only, system_only, reference_system_match, r.results, system_n, reference_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all combinations of given list of annotators:\n",
    "def expressions(l, n):\n",
    "    for (operations, *operands), operators in product(\n",
    "            combinations(l, n), product(('&', '|'), repeat=n - 1)):\n",
    "        for operation in zip(operators, operands):\n",
    "            operations = [operations, *operation]\n",
    "        yield operations\n",
    "\n",
    "def run_ensemble(l, analysis_type, corpus):\n",
    "\n",
    "    metrics = pd.DataFrame()\n",
    "\n",
    "    for i in range(1, len(l)+1):\n",
    "        test = list(expressions(l, i))\n",
    "        for t  in test:\n",
    "            if i > 1:\n",
    "                # format Boolean sentence for parse tree \n",
    "                t = '(' + \" \".join(str(x) for x in t).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "\n",
    "            d = get_metrics(t, analysis_type, corpus)\n",
    "            d['merge'] = t\n",
    "            frames = [metrics, pd.DataFrame(d, index=[0]) ]\n",
    "            metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "    \n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    \n",
    "    file_name = corpus + '_all_merge_metrics_'\n",
    "        \n",
    "    geometric_mean(metrics).to_csv(analysisConf.data_dir + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "    print(geometric_mean(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_set(df, analysis_type = 'entity', df_type = 'sys'):\n",
    "    \n",
    "    #print(df[0:10])\n",
    "    \n",
    "    # get values for creation of series of type tuple\n",
    "    if 'entity' in analysis_type: \n",
    "        if df_type == 'sys':\n",
    "            arg = df.begin, df.end, df.note_id\n",
    "        else:\n",
    "            arg = df.start, df.end, df.file\n",
    "    elif 'cui' in analysis_type:\n",
    "        if df_type == 'sys':\n",
    "            arg = df.cui, df.note_id\n",
    "        else:\n",
    "            arg = df.value, df.file\n",
    "    elif 'full' in analysis_type:\n",
    "        if df_type == 'sys':\n",
    "            arg = df.begin, df.end, df.cui, df.note_id\n",
    "        else:\n",
    "            arg = df.start, df.end, df.value, df.file\n",
    "    \n",
    "    return set(list(zip(*arg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTS -> ensemble:\n",
    "def test_match_consistency(matches, ref_only, ref_n, sys):\n",
    "    \"\"\"test for reference only/match set consistency:\n",
    "        params: match, system and reference only sets\"\"\"\n",
    "   \n",
    "    print('len', len(sys), len(matches), len(matches.union(sys)), len(matches.intersection(sys)))\n",
    "    assert len(matches.union(ref_only)) == ref_n, 'Reference annotation mismatch union'\n",
    "    assert len(matches.intersection(sys)) == len(matches), 'System annotation mismatch intersect'\n",
    "    assert len(matches.union(sys)) == len(sys), 'System annotation mismatch union'\n",
    "    assert len(matches.intersection(ref_only)) == 0, 'Reference annotation mismatch intersect'\n",
    "\n",
    "def test_systems(analysis_type, systems, corpus):\n",
    "    sys = df_to_set(get_sys_data(systems[0], analysis_type, corpus), analysis_type)\n",
    "    test_match_consistency(*get_system_matches(systems[0], analysis_type, corpus), get_ref_n(analysis_type), sys)\n",
    "    print('Match consistency:', len(sys),get_ref_n(analysis_type))\n",
    "\n",
    "def test_metrics(ref, sys_m, match_m):\n",
    "    test = True\n",
    "    reference_n = len(ref)\n",
    "    system_n = len(sys_m)\n",
    "\n",
    "    print('Test metrics:', type(reference_n), type(system_n), type(match_m))\n",
    "\n",
    "    reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, match_m).get_ref_sys()\n",
    "    F, recall, precision, _, _, _, _, _ = Metrics(system_only, reference_only, reference_system_match, system_n).get_confusion_metrics()\n",
    "    F_, recall_, precision_, _, _, _, _, _ = Metrics(system_only, reference_only, reference_system_match, system_n).get_confusion_metrics(test)\n",
    "\n",
    "    assert F[1] == F_, 'F1 issue'\n",
    "    assert recall[1] == recall_, 'recall issue'\n",
    "    assert precision[1] == precision_, 'precision issue'\n",
    "    print(F[1], F_)\n",
    "    print(recall[1], recall_)\n",
    "    print(precision[1], precision_)\n",
    "\n",
    "def test_count(analysis_type, corpus):\n",
    "    # test match counts:\n",
    "    ctakes, _ = get_system_matches('ctakes', analysis_type, corpus)\n",
    "    clamp, _ = get_system_matches('clamp', analysis_type, corpus)\n",
    "    b9, _ = get_system_matches('biomedicus', analysis_type, corpus)\n",
    "    mm, _ = get_system_matches('metamap', analysis_type, corpus)\n",
    "\n",
    "    print('count:', len(mm.intersection(b9.intersection(clamp.intersection(ctakes)))))\n",
    "    \n",
    "def test_ensemble(analysis_type, corpus):\n",
    "    \n",
    "    print('ensemble:')\n",
    "    # Get mixed system_n\n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    ref_ann, data = get_metric_data(training_notes, analysis_type, corpus)\n",
    "\n",
    "    names = ['ctakes', 'biomedicus', 'metamap', 'clamp']\n",
    "    if 'entity' in analysis_type: \n",
    "        cols_to_keep = ['begin', 'end', 'note_id']\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['cui', 'note_id']\n",
    "    elif 'full' in analysis_type:\n",
    "        cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "\n",
    "    biomedicus = data[data[\"system\"]=='biomedicus'][cols_to_keep].copy()\n",
    "    ctakes = data[data[\"system\"]=='ctakes'][cols_to_keep].copy()\n",
    "    clamp = data[data[\"system\"]=='clamp'][cols_to_keep].copy()\n",
    "    metamap = data[data[\"system\"]=='metamap'][cols_to_keep].copy()\n",
    "    quickumls = data[data[\"system\"]=='quick_umls'][cols_to_keep].copy()\n",
    "\n",
    "    print('systems:', len(biomedicus), len(clamp), len(ctakes), len(metamap), len(quickumls))\n",
    "\n",
    "    b9 = set()\n",
    "    cl = set()\n",
    "    ct = set()\n",
    "    mm = set()\n",
    "    qu = set()\n",
    "\n",
    "    b9 = df_to_set(get_sys_data('biomedicus', analysis_type, corpus), analysis_type)\n",
    "    print(len(b9))\n",
    "\n",
    "    ct = df_to_set(get_sys_data('ctakes', analysis_type, corpus), analysis_type)\n",
    "    print(len(ct))\n",
    "\n",
    "    cl = df_to_set(get_sys_data('clamp', analysis_type, corpus), analysis_type)\n",
    "    print(len(cl))\n",
    "\n",
    "    mm = df_to_set(get_sys_data('metamap', analysis_type, corpus), analysis_type)\n",
    "    print(len(mm))\n",
    "\n",
    "    qu = df_to_set(get_sys_data('quick_umls', analysis_type, corpus), analysis_type)\n",
    "    print(len(qu))\n",
    "    \n",
    "    print('various merges:')\n",
    "    print(len(b9), len(cl), len(ct), len(mm), len(qu))\n",
    "    print(len(mm.intersection(b9.intersection(cl.intersection(ct)))))\n",
    "    print(len(mm.union(b9.intersection(cl.intersection(ct)))))\n",
    "    print(len(mm.union(b9.union(cl.intersection(ct)))))\n",
    "    print(len(mm.union(b9.union(cl.union(ct)))))\n",
    "    print(len(b9.intersection(ct)))\n",
    "\n",
    "    sys_m = b9.intersection(ct.intersection(qu))\n",
    "    print('sys_m:', len(sys_m))\n",
    "\n",
    "    # Get match merges:\n",
    "    ct, _ = get_system_matches('ctakes', analysis_type, corpus)\n",
    "    cl, _ = get_system_matches('clamp', analysis_type, corpus)\n",
    "    b9, _ = get_system_matches('biomedicus', analysis_type, corpus)\n",
    "    mm, _ = get_system_matches('metamap', analysis_type, corpus)\n",
    "    qu, _ = get_system_matches('quick_umls', analysis_type, corpus)\n",
    "\n",
    "    match_m = b9.intersection(ct.intersection(qu))\n",
    "    print('match_m:', len(match_m))\n",
    "    # reference df to set\n",
    "    if 'entity' in analysis_type: \n",
    "        cols_to_keep = ['end', 'start','file']\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['value','file']\n",
    "    elif 'full' in analysis_type:\n",
    "        cols_to_keep = ['end', 'start', 'value','file']\n",
    "\n",
    "    ref = df_to_set(ref_ann[cols_to_keep], analysis_type, 'ref')\n",
    "\n",
    "    print('ref:', len(ref))\n",
    "\n",
    "    # test difference:\n",
    "    print('FP:', len(sys_m - match_m), len(sys_m - ref))\n",
    "    assert len(sys_m - match_m) == len(sys_m - ref), 'FP mismatch'\n",
    "    print('FN:', len(ref - match_m), len(ref - sys_m))\n",
    "    assert len(ref - match_m) == len(ref - sys_m), 'FN mismatch'\n",
    "    \n",
    "    test_metrics(ref, sys_m, match_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Run: 1->Single systems; 2->Ensemble; 3->Tests; 4-> MM Test 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biomedicus', 'clamp', 'ctakes', 'metamap'] ('analytical_cui_i2b2_concepts.csv', 'test.i2b2_all')\n",
      "ctakes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trivial: ctakes 12780 52630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biomedicus\n",
      "trivial: biomedicus 13626 64917\n",
      "clamp\n",
      "trivial: clamp 23181 52884\n",
      "metamap\n",
      "trivial: metamap 14661 107927\n",
      "quick_umls\n",
      "trivial: quick_umls 14579 85676\n",
      " ( ctakes & biomedicus ) \n",
      " ( ctakes | biomedicus ) \n",
      " ( ctakes & clamp ) \n",
      " ( ctakes | clamp ) \n",
      " ( ctakes & metamap ) \n",
      " ( ctakes | metamap ) \n",
      " ( ctakes & quick_umls ) \n",
      " ( ctakes | quick_umls ) \n",
      " ( biomedicus & clamp ) \n",
      " ( biomedicus | clamp ) \n",
      " ( biomedicus & metamap ) \n",
      " ( biomedicus | metamap ) \n",
      " ( biomedicus & quick_umls ) \n",
      " ( biomedicus | quick_umls ) \n",
      " ( clamp & metamap ) \n",
      " ( clamp | metamap ) \n",
      " ( clamp & quick_umls ) \n",
      " ( clamp | quick_umls ) \n",
      " ( metamap & quick_umls ) \n",
      " ( metamap | quick_umls ) \n",
      " ( ( ctakes & biomedicus ) & clamp ) \n",
      " ( ( ctakes & biomedicus ) | clamp ) \n",
      " ( ( ctakes | biomedicus ) & clamp ) \n",
      " ( ( ctakes | biomedicus ) | clamp ) \n",
      " ( ( ctakes & biomedicus ) & metamap ) \n",
      " ( ( ctakes & biomedicus ) | metamap ) \n",
      " ( ( ctakes | biomedicus ) & metamap ) \n",
      " ( ( ctakes | biomedicus ) | metamap ) \n",
      " ( ( ctakes & biomedicus ) & quick_umls ) \n",
      " ( ( ctakes & biomedicus ) | quick_umls ) \n",
      " ( ( ctakes | biomedicus ) & quick_umls ) \n",
      " ( ( ctakes | biomedicus ) | quick_umls ) \n",
      " ( ( ctakes & clamp ) & metamap ) \n",
      " ( ( ctakes & clamp ) | metamap ) \n",
      " ( ( ctakes | clamp ) & metamap ) \n",
      " ( ( ctakes | clamp ) | metamap ) \n",
      " ( ( ctakes & clamp ) & quick_umls ) \n",
      " ( ( ctakes & clamp ) | quick_umls ) \n",
      " ( ( ctakes | clamp ) & quick_umls ) \n",
      " ( ( ctakes | clamp ) | quick_umls ) \n",
      " ( ( ctakes & metamap ) & quick_umls ) \n",
      " ( ( ctakes & metamap ) | quick_umls ) \n",
      " ( ( ctakes | metamap ) & quick_umls ) \n",
      " ( ( ctakes | metamap ) | quick_umls ) \n",
      " ( ( biomedicus & clamp ) & metamap ) \n",
      " ( ( biomedicus & clamp ) | metamap ) \n",
      " ( ( biomedicus | clamp ) & metamap ) \n",
      " ( ( biomedicus | clamp ) | metamap ) \n",
      " ( ( biomedicus & clamp ) & quick_umls ) \n",
      " ( ( biomedicus & clamp ) | quick_umls ) \n",
      " ( ( biomedicus | clamp ) & quick_umls ) \n",
      " ( ( biomedicus | clamp ) | quick_umls ) \n",
      " ( ( biomedicus & metamap ) & quick_umls ) \n",
      " ( ( biomedicus & metamap ) | quick_umls ) \n",
      " ( ( biomedicus | metamap ) & quick_umls ) \n",
      " ( ( biomedicus | metamap ) | quick_umls ) \n",
      " ( ( clamp & metamap ) & quick_umls ) \n",
      " ( ( clamp & metamap ) | quick_umls ) \n",
      " ( ( clamp | metamap ) & quick_umls ) \n",
      " ( ( clamp | metamap ) | quick_umls ) \n",
      " ( ( ( ctakes & biomedicus ) & clamp ) & metamap ) \n",
      " ( ( ( ctakes & biomedicus ) & clamp ) | metamap ) \n",
      " ( ( ( ctakes & biomedicus ) | clamp ) & metamap ) \n",
      " ( ( ( ctakes & biomedicus ) | clamp ) | metamap ) \n",
      " ( ( ( ctakes | biomedicus ) & clamp ) & metamap ) \n",
      " ( ( ( ctakes | biomedicus ) & clamp ) | metamap ) \n",
      " ( ( ( ctakes | biomedicus ) | clamp ) & metamap ) \n",
      " ( ( ( ctakes | biomedicus ) | clamp ) | metamap ) \n",
      " ( ( ( ctakes & biomedicus ) & clamp ) & quick_umls ) \n",
      " ( ( ( ctakes & biomedicus ) & clamp ) | quick_umls ) \n",
      " ( ( ( ctakes & biomedicus ) | clamp ) & quick_umls ) \n",
      " ( ( ( ctakes & biomedicus ) | clamp ) | quick_umls ) \n",
      " ( ( ( ctakes | biomedicus ) & clamp ) & quick_umls ) \n",
      " ( ( ( ctakes | biomedicus ) & clamp ) | quick_umls ) \n",
      " ( ( ( ctakes | biomedicus ) | clamp ) & quick_umls ) \n",
      " ( ( ( ctakes | biomedicus ) | clamp ) | quick_umls ) \n",
      " ( ( ( ctakes & biomedicus ) & metamap ) & quick_umls ) \n",
      " ( ( ( ctakes & biomedicus ) & metamap ) | quick_umls ) \n",
      " ( ( ( ctakes & biomedicus ) | metamap ) & quick_umls ) \n",
      " ( ( ( ctakes & biomedicus ) | metamap ) | quick_umls ) \n",
      " ( ( ( ctakes | biomedicus ) & metamap ) & quick_umls ) \n",
      " ( ( ( ctakes | biomedicus ) & metamap ) | quick_umls ) \n",
      " ( ( ( ctakes | biomedicus ) | metamap ) & quick_umls ) \n",
      " ( ( ( ctakes | biomedicus ) | metamap ) | quick_umls ) \n",
      " ( ( ( ctakes & clamp ) & metamap ) & quick_umls ) \n",
      " ( ( ( ctakes & clamp ) & metamap ) | quick_umls ) \n",
      " ( ( ( ctakes & clamp ) | metamap ) & quick_umls ) \n",
      " ( ( ( ctakes & clamp ) | metamap ) | quick_umls ) \n",
      " ( ( ( ctakes | clamp ) & metamap ) & quick_umls ) \n",
      " ( ( ( ctakes | clamp ) & metamap ) | quick_umls ) \n",
      " ( ( ( ctakes | clamp ) | metamap ) & quick_umls ) \n",
      " ( ( ( ctakes | clamp ) | metamap ) | quick_umls ) \n",
      " ( ( ( biomedicus & clamp ) & metamap ) & quick_umls ) \n",
      " ( ( ( biomedicus & clamp ) & metamap ) | quick_umls ) \n",
      " ( ( ( biomedicus & clamp ) | metamap ) & quick_umls ) \n",
      " ( ( ( biomedicus & clamp ) | metamap ) | quick_umls ) \n",
      " ( ( ( biomedicus | clamp ) & metamap ) & quick_umls ) \n",
      " ( ( ( biomedicus | clamp ) & metamap ) | quick_umls ) \n",
      " ( ( ( biomedicus | clamp ) | metamap ) & quick_umls ) \n",
      " ( ( ( biomedicus | clamp ) | metamap ) | quick_umls ) \n",
      " ( ( ( ( ctakes & biomedicus ) & clamp ) & metamap ) & quick_umls ) \n",
      " ( ( ( ( ctakes & biomedicus ) & clamp ) & metamap ) | quick_umls ) \n",
      " ( ( ( ( ctakes & biomedicus ) & clamp ) | metamap ) & quick_umls ) \n",
      " ( ( ( ( ctakes & biomedicus ) & clamp ) | metamap ) | quick_umls ) \n",
      " ( ( ( ( ctakes & biomedicus ) | clamp ) & metamap ) & quick_umls ) \n",
      " ( ( ( ( ctakes & biomedicus ) | clamp ) & metamap ) | quick_umls ) \n",
      " ( ( ( ( ctakes & biomedicus ) | clamp ) | metamap ) & quick_umls ) \n",
      " ( ( ( ( ctakes & biomedicus ) | clamp ) | metamap ) | quick_umls ) \n",
      " ( ( ( ( ctakes | biomedicus ) & clamp ) & metamap ) & quick_umls ) \n",
      " ( ( ( ( ctakes | biomedicus ) & clamp ) & metamap ) | quick_umls ) \n",
      " ( ( ( ( ctakes | biomedicus ) & clamp ) | metamap ) & quick_umls ) \n",
      " ( ( ( ( ctakes | biomedicus ) & clamp ) | metamap ) | quick_umls ) \n",
      " ( ( ( ( ctakes | biomedicus ) | clamp ) & metamap ) & quick_umls ) \n",
      " ( ( ( ( ctakes | biomedicus ) | clamp ) & metamap ) | quick_umls ) \n",
      " ( ( ( ( ctakes | biomedicus ) | clamp ) | metamap ) & quick_umls ) \n",
      " ( ( ( ( ctakes | biomedicus ) | clamp ) | metamap ) | quick_umls ) \n",
      "            F  precision    recall     TP    FN      FP          TP/FN  \\\n",
      "0    0.368178   0.242827  0.761031  12780  4013   39850   3.184650e+00   \n",
      "1    0.333521   0.209899  0.811410  13626  3167   51291   4.302494e+00   \n",
      "2    0.665385   0.438337  1.380397  23181 -6388   29703  5.544886e-313   \n",
      "3    0.235103   0.135842  0.873042  14661  2132   93266   6.876642e+00   \n",
      "4    0.284554   0.170164  0.868159  14579  2214   71097   6.584914e+00   \n",
      "5    0.396376   0.277146  0.695647  11682  5111   30469   2.285658e+00   \n",
      "6    0.319431   0.195289  0.876794  14724  2069   60672   7.116481e+00   \n",
      "7    0.680080   0.640725  0.724588  12168  4625    6823   2.630919e+00   \n",
      "8    0.460587   0.274990  1.416840  23793 -7000   62730  5.546444e-313   \n",
      "9    0.468106   0.363102  0.658548  11059  5734   19398   1.928671e+00   \n",
      "10   0.223047   0.125919  0.975526  16382   411  113718   3.985888e+01   \n",
      "11   0.370934   0.258480  0.656583  11026  5767   31631   1.911913e+00   \n",
      "12   0.290514   0.170760  0.972608  16333   460   79316   3.550652e+01   \n",
      "13   0.702525   0.644573  0.771929  12963  3830    7148   3.384595e+00   \n",
      "14   0.416551   0.244078  1.419877  23844 -7051   73846  5.550473e-313   \n",
      "15   0.420097   0.302725  0.686119  11522  5271   26539   2.185923e+00   \n",
      "16   0.221209   0.124385  0.998333  16765    28  118018   5.987500e+02   \n",
      "17   0.351444   0.234566  0.700470  11763  5030   38385   2.338569e+00   \n",
      "18   0.280489   0.163692  0.979098  16442   351   84003   4.684330e+01   \n",
      "19   0.692036   0.594752  0.827369  13894  2899    9467   4.792687e+00   \n",
      "20   0.310523   0.174231  1.426070  23948 -7155  113502  5.548285e-313   \n",
      "21   0.711853   0.649973  0.786756  13212  3581    7115   3.689472e+00   \n",
      "22   0.363604   0.207624  1.461800  24548 -7755   93685  5.547874e-313   \n",
      "23   0.391599   0.273979  0.686179  11523  5270   30535   2.186528e+00   \n",
      "24   0.210493   0.116909  1.055023  17717  -924  133828  5.548819e-313   \n",
      "25   0.666009   0.668909  0.663133  11136  5657    5512   1.968535e+00   \n",
      "26   0.498571   0.302690  1.412910  23727 -6934   54660  5.551886e-313   \n",
      "27   0.713176   0.623274  0.833383  13995  2798    8459   5.001787e+00   \n",
      "28   0.389989   0.225937  1.423808  23910 -7117   81916  5.548819e-313   \n",
      "29   0.486566   0.408894  0.600667  10087  6706   14582   1.504175e+00   \n",
      "..        ...        ...       ...    ...   ...     ...            ...   \n",
      "91   0.416138   0.285125  0.769904  12929  3864   32416   3.346014e+00   \n",
      "92   0.212195   0.117800  1.068005  17935 -1142  134315  5.543948e-313   \n",
      "93   0.513173   0.411342  0.682010  11453  5340   16390   2.144757e+00   \n",
      "94   0.309137   0.181424  1.044185  17535  -742   79117  5.550473e-313   \n",
      "95   0.351768   0.223753  0.822128  13806  2987   47896   4.622029e+00   \n",
      "96   0.252732   0.138181  1.477878  24818 -8025  154787  5.547051e-313   \n",
      "97   0.645784   0.738205  0.573930   9638  7155    3418   1.347030e+00   \n",
      "98   0.303807   0.180823  0.949801  15950   843   72258   1.892052e+01   \n",
      "99   0.419222   0.286636  0.780027  13099  3694   32600   3.546021e+00   \n",
      "100  0.213864   0.118713  1.077532  18095 -1302  134332  5.553464e-313   \n",
      "101  0.477487   0.367055  0.682963  11469  5324   19777   2.154207e+00   \n",
      "102  0.299290   0.174709  1.043113  17517  -724   82747  5.553398e-313   \n",
      "103  0.339506   0.213888  0.822664  13815  2978   50775   4.639019e+00   \n",
      "104  0.251561   0.137459  1.480498  24862 -8069  156007  5.550473e-313   \n",
      "105  0.610735   0.765942  0.507831   8528  8265    2606   1.031821e+00   \n",
      "106  0.300204   0.178858  0.933603  15678  1115   71978   1.406099e+01   \n",
      "107  0.415399   0.284980  0.765914  12862  3931   32271   3.271941e+00   \n",
      "108  0.211935   0.117675  1.065146  17887 -1094  134117  5.544765e-313   \n",
      "109  0.540484   0.448245  0.680522  11428  5365   14067   2.130103e+00   \n",
      "110  0.312897   0.184075  1.042399  17505  -712   77592  5.544397e-313   \n",
      "111  0.360683   0.231041  0.821831  13801  2992   45933   4.612634e+00   \n",
      "112  0.255619   0.139912  1.477520  24812 -8019  152528  5.547874e-313   \n",
      "113  0.660049   0.722191  0.607753  10206  6587    3926   1.549416e+00   \n",
      "114  0.308081   0.183100  0.970583  16299   494   72718   3.299393e+01   \n",
      "115  0.419941   0.286772  0.784017  13166  3627   32745   3.629997e+00   \n",
      "116  0.214120   0.118836  1.080391  18143 -1350  134530  5.546753e-313   \n",
      "117  0.456229   0.342144  0.684452  11494  5299   22100   2.169089e+00   \n",
      "118  0.295872   0.172335  1.044900  17547  -754   84272  5.550044e-313   \n",
      "119  0.331610   0.207638  0.822962  13820  2973   52738   4.648503e+00   \n",
      "120  0.248771   0.135791  1.480855  24868 -8075  158266  5.550044e-313   \n",
      "\n",
      "     n_gold   n_sys          TM  \\\n",
      "0     16793   52630   55.707564   \n",
      "1     16793   64917   53.479725   \n",
      "2     16793   52884  100.802202   \n",
      "3     16793  107927   44.627087   \n",
      "4     16793   85676   49.807890   \n",
      "5     16793   42151   56.900130   \n",
      "6     16793   75396   53.623068   \n",
      "7     16793   18991   88.296865   \n",
      "8     16793   86523   80.887874   \n",
      "9     16793   30457   63.368335   \n",
      "10    16793  130100   45.418028   \n",
      "11    16793   42657   53.385441   \n",
      "12    16793   95649   52.811165   \n",
      "13    16793   20111   91.408943   \n",
      "14    16793   97690   76.287619   \n",
      "15    16793   38061   59.059229   \n",
      "16    16793  134783   45.665267   \n",
      "17    16793   50148   52.528051   \n",
      "18    16793  100445   51.878867   \n",
      "19    16793   23361   90.903704   \n",
      "20    16793  137450   64.594699   \n",
      "21    16793   20327   92.668455   \n",
      "22    16793  118233   71.391542   \n",
      "23    16793   42058   56.187700   \n",
      "24    16793  151545   45.511314   \n",
      "25    16793   16648   86.307431   \n",
      "26    16793   78387   84.746312   \n",
      "27    16793   22454   93.395520   \n",
      "28    16793  105826   73.499331   \n",
      "29    16793   24669   64.222358   \n",
      "..      ...     ...         ...   \n",
      "91    16793   45345   60.715592   \n",
      "92    16793  152250   45.964520   \n",
      "93    16793   27843   68.637467   \n",
      "94    16793   96652   56.402759   \n",
      "95    16793   61702   55.579962   \n",
      "96    16793  179605   58.560877   \n",
      "97    16793   13056   84.349372   \n",
      "98    16793   88208   53.704008   \n",
      "99    16793   45699   61.275208   \n",
      "100   16793  152427   46.347641   \n",
      "101   16793   31246   64.882614   \n",
      "102   16793  100264   55.320643   \n",
      "103   16793   64590   54.358598   \n",
      "104   16793  180869   58.459352   \n",
      "105   16793   11134   80.820509   \n",
      "106   16793   87656   52.954130   \n",
      "107   16793   45133   60.542647   \n",
      "108   16793  152004   45.878583   \n",
      "109   16793   25495   71.571929   \n",
      "110   16793   95097   56.764747   \n",
      "111   16793   59734   56.467656   \n",
      "112   16793  177340   58.919414   \n",
      "113   16793   14132   85.852659   \n",
      "114   16793   89017   54.629155   \n",
      "115   16793   45911   61.446263   \n",
      "116   16793  152673   46.433132   \n",
      "117   16793   33594   62.710510   \n",
      "118   16793  101819   54.990601   \n",
      "119   16793   66558   53.568306   \n",
      "120   16793  183134   58.110735   \n",
      "\n",
      "                                                 merge  F1 rank  TP/FN rank  \\\n",
      "0                                               ctakes     54.0        52.0   \n",
      "1                                           biomedicus     66.0        41.0   \n",
      "2                                                clamp     12.0       118.0   \n",
      "3                                              metamap    108.0        24.0   \n",
      "4                                           quick_umls     93.0        25.0   \n",
      "5                                  (ctakes&biomedicus)     47.0        59.0   \n",
      "6                                  (ctakes|biomedicus)     71.0        23.0   \n",
      "7                                       (ctakes&clamp)      8.0        55.0   \n",
      "8                                       (ctakes|clamp)     31.0       109.0   \n",
      "9                                     (ctakes&metamap)     29.0        68.0   \n",
      "10                                    (ctakes|metamap)    110.0         7.0   \n",
      "11                                 (ctakes&quick_umls)     52.0        69.0   \n",
      "12                                 (ctakes|quick_umls)     89.0         8.0   \n",
      "13                                  (biomedicus&clamp)      3.0        48.0   \n",
      "14                                  (biomedicus|clamp)     43.0        93.0   \n",
      "15                                (biomedicus&metamap)     40.0        61.0   \n",
      "16                                (biomedicus|metamap)    111.0         1.0   \n",
      "17                             (biomedicus&quick_umls)     59.0        58.0   \n",
      "18                             (biomedicus|quick_umls)     94.0         5.0   \n",
      "19                                     (clamp&metamap)      5.0        31.0   \n",
      "20                                     (clamp|metamap)     75.0       101.0   \n",
      "21                                  (clamp&quick_umls)      2.0        43.0   \n",
      "22                                  (clamp|quick_umls)     55.0       103.0   \n",
      "23                                (metamap&quick_umls)     48.0        60.0   \n",
      "24                                (metamap|quick_umls)    118.0       100.0   \n",
      "25                         ((ctakes&biomedicus)&clamp)     11.0        67.0   \n",
      "26                         ((ctakes&biomedicus)|clamp)     25.0        92.0   \n",
      "27                         ((ctakes|biomedicus)&clamp)      1.0        30.0   \n",
      "28                         ((ctakes|biomedicus)|clamp)     50.0        99.0   \n",
      "29                       ((ctakes&biomedicus)&metamap)     26.0        78.0   \n",
      "..                                                 ...      ...         ...   \n",
      "91               (((ctakes&clamp)|metamap)&quick_umls)     44.0        50.0   \n",
      "92               (((ctakes&clamp)|metamap)|quick_umls)    115.0       121.0   \n",
      "93               (((ctakes|clamp)&metamap)&quick_umls)     24.0        64.0   \n",
      "94               (((ctakes|clamp)&metamap)|quick_umls)     76.0        94.0   \n",
      "95               (((ctakes|clamp)|metamap)&quick_umls)     58.0        34.0   \n",
      "96               (((ctakes|clamp)|metamap)|quick_umls)    104.0       105.0   \n",
      "97           (((biomedicus&clamp)&metamap)&quick_umls)     16.0        80.0   \n",
      "98           (((biomedicus&clamp)&metamap)|quick_umls)     81.0        18.0   \n",
      "99           (((biomedicus&clamp)|metamap)&quick_umls)     42.0        46.0   \n",
      "100          (((biomedicus&clamp)|metamap)|quick_umls)    114.0        87.0   \n",
      "101          (((biomedicus|clamp)&metamap)&quick_umls)     27.0        63.0   \n",
      "102          (((biomedicus|clamp)&metamap)|quick_umls)     84.0        88.0   \n",
      "103          (((biomedicus|clamp)|metamap)&quick_umls)     63.0        33.0   \n",
      "104          (((biomedicus|clamp)|metamap)|quick_umls)    106.0        95.0   \n",
      "105  ((((ctakes&biomedicus)&clamp)&metamap)&quick_u...     20.0        85.0   \n",
      "106  ((((ctakes&biomedicus)&clamp)&metamap)|quick_u...     83.0        22.0   \n",
      "107  ((((ctakes&biomedicus)&clamp)|metamap)&quick_u...     45.0        51.0   \n",
      "108  ((((ctakes&biomedicus)&clamp)|metamap)|quick_u...    116.0       119.0   \n",
      "109  ((((ctakes&biomedicus)|clamp)&metamap)&quick_u...     22.0        65.0   \n",
      "110  ((((ctakes&biomedicus)|clamp)&metamap)|quick_u...     73.0       120.0   \n",
      "111  ((((ctakes&biomedicus)|clamp)|metamap)&quick_u...     56.0        35.0   \n",
      "112  ((((ctakes&biomedicus)|clamp)|metamap)|quick_u...    102.0       104.0   \n",
      "113  ((((ctakes|biomedicus)&clamp)&metamap)&quick_u...     14.0        76.0   \n",
      "114  ((((ctakes|biomedicus)&clamp)&metamap)|quick_u...     78.0        10.0   \n",
      "115  ((((ctakes|biomedicus)&clamp)|metamap)&quick_u...     41.0        45.0   \n",
      "116  ((((ctakes|biomedicus)&clamp)|metamap)|quick_u...    113.0       107.0   \n",
      "117  ((((ctakes|biomedicus)|clamp)&metamap)&quick_u...     34.0        62.0   \n",
      "118  ((((ctakes|biomedicus)|clamp)&metamap)|quick_u...     87.0        96.0   \n",
      "119  ((((ctakes|biomedicus)|clamp)|metamap)&quick_u...     67.0        32.0   \n",
      "120  ((((ctakes|biomedicus)|clamp)|metamap)|quick_u...    107.0        97.0   \n",
      "\n",
      "     TM rank       Gmean  \n",
      "0       71.0   59.970523  \n",
      "1       86.0   60.081596  \n",
      "2        1.0   10.983619  \n",
      "3      121.0   58.216431  \n",
      "4      103.0   54.277685  \n",
      "5       66.0   60.467245  \n",
      "6       84.0   46.359952  \n",
      "7        9.0   19.858463  \n",
      "8       21.0   45.591383  \n",
      "9       40.0   48.861156  \n",
      "10     119.0   33.487741  \n",
      "11      88.0   74.498550  \n",
      "12      92.0   30.957566  \n",
      "13       4.0   11.689987  \n",
      "14      24.0   46.752520  \n",
      "15      59.0   57.349803  \n",
      "16     115.0   13.903490  \n",
      "17      97.0   73.032601  \n",
      "18     101.0   26.345064  \n",
      "19       5.0   11.249782  \n",
      "20      38.0   63.281342  \n",
      "21       3.0    9.364566  \n",
      "22      28.0   53.845486  \n",
      "23      70.0   62.681243  \n",
      "24     117.0  109.217371  \n",
      "25      12.0   25.523205  \n",
      "26      15.0   35.549090  \n",
      "27       2.0    6.170061  \n",
      "28      25.0   49.777157  \n",
      "29      39.0   50.732939  \n",
      "..       ...         ...  \n",
      "91      50.0   49.294834  \n",
      "92     110.0  115.327918  \n",
      "93      31.0   41.584442  \n",
      "94      69.0   80.018658  \n",
      "95      73.0   50.668405  \n",
      "96      61.0   82.395045  \n",
      "97      17.0   33.610507  \n",
      "98      83.0   41.963952  \n",
      "99      47.0   45.974722  \n",
      "100    109.0   99.100633  \n",
      "101     36.0   44.713362  \n",
      "102     74.0   81.057459  \n",
      "103     79.0   52.265081  \n",
      "104     62.0   79.550110  \n",
      "105     22.0   39.692707  \n",
      "106     91.0   47.923471  \n",
      "107     51.0   50.295651  \n",
      "108    112.0  115.508276  \n",
      "109     27.0   38.999498  \n",
      "110     67.0   87.640577  \n",
      "111     68.0   49.538629  \n",
      "112     60.0   81.269297  \n",
      "113     14.0   29.693060  \n",
      "114     77.0   31.125583  \n",
      "115     46.0   44.974133  \n",
      "116    108.0  108.096607  \n",
      "117     43.0   49.291137  \n",
      "118     75.0   85.088749  \n",
      "119     85.0   53.625613  \n",
      "120     63.0   80.947540  \n",
      "\n",
      "[121 rows x 15 columns]\n",
      "CPU times: user 41min 53s, sys: 5min 11s, total: 47min 4s\n",
      "Wall time: 30min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def main():\n",
    "    \n",
    "    rtype = int(input(\"Run: 1->Single systems; 2->Ensemble; 3->Tests; 4-> MM Test\"))\n",
    "   \n",
    "    '''\n",
    "        corpora: i2b2, mipacq, fv017\n",
    "        analyses: entity only (exact span), cui by document, full (aka (entity and cui on exaact span/exact cui)\n",
    "                  NB: add \"_test\" using mipacq to egnerate small test sample \n",
    "        systems: ctakes, biomedicus, clamp, metamap, quick_umls\n",
    "        \n",
    "        TODO -> Vectorization (entity only and full):\n",
    "                add switch for use of TN on single system performance evaluations \n",
    "                add switch for overlap matching versus exact span\n",
    "             -> Other tasks besides concept extraction\n",
    "             -> Use of https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html\n",
    "        \n",
    "    ''' \n",
    "    corpus = 'i2b2'\n",
    "    #corpus = 'mipacq'\n",
    "    analysis_type = 'entity'\n",
    "    #analysis_type = 'full'\n",
    "    analysisConf =  AnalysisConfig()\n",
    "    print(analysisConf.systems, analysisConf.corpus_config(corpus))\n",
    "    \n",
    "    if (rtype == 1):\n",
    "        generate_metrics(analysis_type, corpus)\n",
    "    elif (rtype == 2):\n",
    "        l = ['ctakes','biomedicus','clamp','metamap','quick_umls']\n",
    "        run_ensemble(l, analysis_type, corpus) \n",
    "    elif (rtype == 3):\n",
    "        systems = ['biomedicus']\n",
    "        t = ['concept_jaccard_score_false']\n",
    "        test_systems(analysis_type, systems, corpus)  \n",
    "        test_count(analysis_type, corpus)\n",
    "        test_ensemble(analysis_type, corpus)\n",
    "    elif (rtype == 4):\n",
    "        generate_metrics_test(analysis_type, corpus)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dkpro-cassis\n",
    "from cassis import *\n",
    "import numpy as np\n",
    "import collections\n",
    "from typing import List, Set\n",
    "\n",
    "ts_test = \"/Users/gms/development/nlp/nlpie/data/irr_mts/\"\n",
    "dir_test = \"/Users/gms/development/nlp/nlpie/data/irr_mts/\"\n",
    "#fname = \"527982345-v1.txt.xmi\"\n",
    "fname = \"92_1666/alber475.xmi\"\n",
    "case = fname.split('/')[0]\n",
    "view_name = \"_InitialView\"\n",
    "Span = collections.namedtuple('Span',['begin','end','label']) # define named tuple for span object \n",
    "  \n",
    "def get_ann(fname, dir_test, ts_test, view_name):\n",
    "    #t = \"org.apache.ctakes.typesystem.type.textsem.DiseaseDisorderMention\"\n",
    "    t = \"webanno.custom.Term\"\n",
    "    with open(ts_test + 'TypeSystem.xml', 'rb') as f:\n",
    "        typesystem = load_typesystem(f)\n",
    "    with open(dir_test + fname, 'rb') as f:\n",
    "        cas = load_cas_from_xmi(f, typesystem=typesystem)\n",
    "    view = cas.get_view(view_name)\n",
    "    #print([x for x in view.select(t)])\n",
    "    #print(view.sofa_string, len(view.sofa_string))\n",
    "    d = {}\n",
    "    ann = []\n",
    "    labels = set()\n",
    "    attribs = get_attribs(view.select(t))\n",
    "    # only parse if type exists in file\n",
    "    if view.select(t):\n",
    "        for sentence in view.select(t): \n",
    "            for i in range(len(attribs)):\n",
    "                key = attribs[i]\n",
    "                # helper method to get val for given key\n",
    "                val = sentence.__getattribute__(attribs[i])\n",
    "                d[key] = val \n",
    "               \n",
    "                if key == 'termType':\n",
    "                    #print(key, val)\n",
    "                    labels.add(val)\n",
    "                \n",
    "                if i == len(attribs) - 1:\n",
    "                    ann.append( Span(d[\"begin\"], d[\"end\"], d[\"termType\"]))\n",
    "    \n",
    "    #print(ann, labels)\n",
    "    return ann, view.sofa_string, labels\n",
    "# extract attributes from cas Annotation object\n",
    "\n",
    "def get_attribs(v):\n",
    "    attribs = []\n",
    "    for sentence in v:\n",
    "        #print(sentence)\n",
    "        for s in sentence.__dir__():\n",
    "            if '__' not in s:\n",
    "                if s not in attribs:\n",
    "                    #print(s)\n",
    "                    attribs.append(s)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    return attribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
