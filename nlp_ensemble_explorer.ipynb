{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  Copyright (c) 2019 Regents of the University of Minnesota.\n",
    " \n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    " \n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gevent\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pymysql\n",
    "import time \n",
    "import functools as ft\n",
    "import glob, os, sys   \n",
    "import operator as op\n",
    "import shelve\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from pathlib import Path\n",
    "from itertools import combinations, product, permutations\n",
    "from sqlalchemy.engine import create_engine\n",
    "from datetime import datetime\n",
    "from ast import literal_eval\n",
    "from scipy import stats  \n",
    "from scipy.stats.mstats import gmean\n",
    "from pythonds.basic.stack import Stack\n",
    "from pythonds.trees.binaryTree import BinaryTree\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "from typing import List, Set, Tuple \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The cell below contains the configurable parameters to ensure that our ensemble explorer runs properaly on your machine. \n",
    "Please read carfully through steps (1-11) before running the rest of the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-1: CHOOSE YOUR CORPUS\n",
    "# TODO: get working with list of corpora\n",
    "#corpora = ['mipacq','i2b2','fairview'] #options for concept extraction include 'fairview', 'mipacq' OR 'i2b2'\n",
    "\n",
    "# cross-system semantic union merge filter for cross system aggregations using custom system annotations file with corpus name and system name using 'ray_test': \n",
    "# need to add semantic type filrering when reading in sys_data\n",
    "#corpus = 'ray_test'\n",
    "#corpus = 'clinical_trial2'\n",
    "#corpus = 'fairview'\n",
    "#corpora = ['i2b2','fairview']\n",
    "corpus = 'mipacq'\n",
    "\n",
    "# STEP-2: CHOOSE YOUR DATA DIRECTORY; this is where output data will be saved on your machine\n",
    "data_directory = '/Users/gms/development/nlp/nlpie/data/ensembling-u01/output/' \n",
    "\n",
    "# STEP-3: CHOOSE WHICH SYSTEMS YOU'D LIKE TO EVALUATE AGAINST THE CORPUS REFERENCE SET\n",
    "systems = ['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls']\n",
    "#systems = ['biomedicus', 'clamp', 'metamap', 'quick_umls']\n",
    "#systems = ['biomedicus', 'quick_umls']\n",
    "#systems = ['biomedicus', 'ctakes', 'quick_umls']\n",
    "#systems = ['biomedicus', 'clamp', 'ctakes', 'metamap']\n",
    "#systems = ['biomedicus', 'clamp']\n",
    "#systems = ['ctakes', 'quick_umls', 'biomedicus', 'metamap']\n",
    "#systems = ['biomedicus', 'metamap']\n",
    "#systems = ['ray_test']\n",
    "#systems = ['metamap']\n",
    "\n",
    "# STEP-4: CHOOSE TYPE OF RUN\n",
    "rtype = 7      # OPTIONS INCLUDE: 1->Single systems; 2->Ensemble; 3->Tests; 4 -> majority vote \n",
    "               # The Ensemble can include the max system set ['ctakes','biomedicus','clamp','metamap','quick_umls']\n",
    "    \n",
    "# STEP-5: CHOOSE WHAT TYPE OF ANALYSIS YOU'D LIKE TO RUN ON THE CORPUS\n",
    "analysis_type = 'entity' #options include 'entity', 'cui' OR 'full'\n",
    "\n",
    "# STEP-(6A): ENTER DETAILS FOR ACCESSING MANUAL ANNOTATION DATA\n",
    "database_type = 'mysql+pymysql' # We use mysql+pymql as default\n",
    "database_username = 'gms'\n",
    "database_password = 'nej123' \n",
    "database_url = 'localhost' # HINT: use localhost if you're running database on your local machine\n",
    "#database_name = 'clinical_trial' # Enter database name\n",
    "database_name = 'concepts' # Enter database name\n",
    "\n",
    "def ref_data(corpus):\n",
    "    return corpus + '_all' # Enter the table within the database where your reference data is stored\n",
    "\n",
    "table_name = ref_data(corpus)\n",
    "\n",
    "# STEP-(6B): ENTER DETAILS FOR ACCESSING SYSTEM ANNOTATION DATA\n",
    "\n",
    "def sys_data(corpus, analysis_type):\n",
    "    if analysis_type == 'entity':\n",
    "        return 'analytical_'+corpus+'.csv' # OPTIONS include 'analytical_cui_mipacq_concepts.csv' OR 'analytical_cui_i2b2_concepts.csv' \n",
    "    elif analysis_type in ('cui', 'full'):\n",
    "        return 'analytical_'+corpus+'_cui.csv' # OPTIONS include 'analytical_cui_mipacq_concepts.csv' OR 'analytical_cui_i2b2_concepts.csv' \n",
    "        \n",
    "system_annotation = sys_data(corpus, analysis_type)\n",
    "\n",
    "# STEP-7: CREATE A DB CONNECTION POOL\n",
    "engine_request = str(database_type)+'://'+database_username+':'+database_password+\"@\"+database_url+'/'+database_name\n",
    "engine = create_engine(engine_request, pool_pre_ping=True, pool_size=20, max_overflow=30)\n",
    "\n",
    "# STEP-(8A): FILTER BY SEMTYPE\n",
    "filter_semtype = True #False\n",
    "\n",
    "# STEP-(8B): IF STEP-(8A) == True -> GET REFERENCE SEMTYPES\n",
    "\n",
    "def ref_semtypes(filter_semtype, corpus):\n",
    "    if filter_semtype:\n",
    "        if corpus == 'fairview':\n",
    "            semtypes = ['Drug', 'Finding', 'Anatomy', 'Procedure']\n",
    "        elif corpus == 'i2b2':\n",
    "            semtypes = ['test,treatment', 'problem']\n",
    "        elif corpus == 'mipacq':\n",
    "            semtypes = ['Procedures', 'Disorders,Sign_Symptom', 'Anatomy', 'Chemicals_and_drugs']\n",
    "        elif corpus in ['clinical_trial', 'clinical_trial2']:\n",
    "            semtypes = ['drug,drug::drug_name,drug::drug_dose,dietary_supplement::dietary_supplement_name,dietary_supplement::dietary_supplement_dose',\n",
    "                        'temporal_measurement,qualifier,measurement',\n",
    "                        'device',\n",
    "                        'condition,observation', \n",
    "                        'demographics::age,demographics::sex,demographics::race_ethnicity,demographics::bmi,demographics::weight',\n",
    "                        'diet',\n",
    "                        'measurement,qualifier',\n",
    "                        'procedure,observation']\n",
    "        \n",
    "        return semtypes\n",
    "\n",
    "semtypes = ref_semtypes(filter_semtype, corpus)\n",
    "\n",
    "# STEP-9: Set data directory/table for source documents for vectorization\n",
    "src_table = 'sofa'\n",
    "\n",
    "# STEP-10: Specificy match type from {'exact', 'overlap', 'cui' -> kludge for majority}\n",
    "run_type = 'overlap'\n",
    "\n",
    "# for clinical trial, measurement/temoral are single system since no overlap for intersect\n",
    "# STEP-11: Specify expression type for run (TODO: run all at once; make less kludgey)\n",
    "expression_type = 'nested' #'nested_with_singleton' # type of merge expression: nested ((A&B)|C), paired ((A&B)|(C&D)), nested_with_singleton ((A&B)|((C&D)|E)) \n",
    "# -> NB: len(systems) for pair must be >= 4, and for nested_with_singleton == 5; single-> skip merges\n",
    "\n",
    "# STEP-12: Specify type of ensemble: merge or vote\n",
    "ensemble_type = 'merge'\n",
    "\n",
    "# STEP-13: run on negation modifier (TODO: negated entity)\n",
    "modification = None #'negation' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****** TODO \n",
    "-> add majority vote to union for analysis_type = 'full'\n",
    "-> case for multiple labels on same/overlapping span/same system; disambiguate (order by score if exists and select random for ties): done!\n",
    "-> port to command line \n",
    "----------------------->\n",
    "-> still need to validate that all semtypes in corpus!\n",
    "-> handle case where intersect merges are empty/any confusion matrix values are 0; specificallly on empty df in evaluate method: done!\n",
    "-> case when system annotations empty from semtype filter; print as 0: done!\n",
    "-> trim whitespace on CSV import -> done for semtypes\n",
    "-> eliminate rtype = 1 for expression_type = 'single'\n",
    "-> cross-system semantic union merge on aggregation\n",
    "-> negation: testing\n",
    "-> other modification, such as 'present'\n",
    "-> clean up configuration process\n",
    "-> allow iteration through all corpora and semtypes\n",
    "-> optimize vecorization (remove confusion?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config class for analysis\n",
    "class AnalysisConfig():\n",
    "    \"\"\"\n",
    "    Configuration object:\n",
    "    systems to use\n",
    "    notes by corpus\n",
    "    paths by output, gold and system location\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self = self    \n",
    "        self.systems = systems\n",
    "        self.data_dir = data_directory\n",
    "    \n",
    "    def corpus_config(self): \n",
    "        usys_data = system_annotation\n",
    "        ref_data = database_name+'.'+table_name\n",
    "        return usys_data, ref_data\n",
    "\n",
    "analysisConf =  AnalysisConfig()\n",
    "#usys, ref = analysisConf.corpus_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticTypes(object):\n",
    "    '''\n",
    "    Filter semantic types based on: https://metamap.nlm.nih.gov/SemanticTypesAndGroups.shtml\n",
    "    :params: semtypes list from corpus, system to query\n",
    "    :return: list of equivalent system semtypes \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, semtypes, corpus):\n",
    "        self = self\n",
    "        \n",
    "        if corpus == 'clinical_trial2':\n",
    "            corpus = 'clinical_trial' # kludge!!\n",
    "        sql = \"SELECT st.tui, abbreviation, clamp_name, ctakes_name, biomedicus_name FROM clinical_trial.semantic_groups sg join semantic_types st on sg.tui = st.tui where \" + corpus + \"_name in ({})\"\\\n",
    "            .format(', '.join(['%s' for _ in semtypes]))  \n",
    "#        sql = \"SELECT st.tui, abbreviation, clamp_name, ctakes_name FROM concepts.semantic_groups sg join semantic_types st on sg.tui = st.tui where \" + corpus + \"_name in ({})\"\\\n",
    "#           .format(', '.join(['%s' for _ in semtypes]))  \n",
    "        \n",
    "        stypes = pd.read_sql(sql, params=[semtypes], con=engine) \n",
    "       \n",
    "        if len(stypes['tui'].tolist()) > 0:\n",
    "            self.biomedicus_types = set(stypes['tui'].tolist())\n",
    "            self.qumls_types = set(stypes['tui'].tolist())\n",
    "        \n",
    "        else:\n",
    "            self.biomedicus_types = None\n",
    "            self.qumls_types = None\n",
    "        \n",
    "        if stypes['clamp_name'].dropna(inplace=True) or len(stypes['clamp_name']) == 0:\n",
    "            self.clamp_types = None\n",
    "        else:\n",
    "            self.clamp_types = set(stypes['clamp_name'].tolist()[0].split(','))\n",
    "         \n",
    "        if stypes['ctakes_name'].dropna(inplace=True) or len(stypes['ctakes_name']) == 0:\n",
    "            self.ctakes_types = None\n",
    "        else:\n",
    "            self.ctakes_types = set(stypes['ctakes_name'].tolist()[0].split(','))\n",
    " \n",
    "# # Kludge for b9 temporal\n",
    "        if stypes['biomedicus_name'].dropna(inplace=True) or len(stypes['biomedicus_name']) > 0:\n",
    "            self.biomedicus_types.update(set(stypes['biomedicus_name'].tolist()[0].split(',')))\n",
    "        #else:\n",
    "        #    self.biomedicus_type = None\n",
    "        \n",
    "        if len(stypes['abbreviation'].tolist()) > 0:\n",
    "            self.metamap_types = set(stypes['abbreviation'].tolist())\n",
    "        else:\n",
    "            self.metamap_types = None\n",
    "        \n",
    "        self.reference_types =  set(semtypes)\n",
    "    \n",
    "    def get_system_type(self, system):  \n",
    "        \n",
    "        if system == 'biomedicus':\n",
    "            semtypes = self.biomedicus_types\n",
    "        elif system == 'ctakes':\n",
    "            semtypes = self.ctakes_types\n",
    "        elif system == 'clamp':\n",
    "            semtypes = self.clamp_types\n",
    "        elif system == 'metamap':\n",
    "            semtypes = self.metamap_types\n",
    "        elif system == 'quick_umls':\n",
    "            semtypes = self.qumls_types\n",
    "        elif system == 'reference':\n",
    "            semtypes = self.reference_types\n",
    "            \n",
    "        return semtypes\n",
    "    \n",
    "# print(SemanticTypes(['Drug'], corpus).get_system_type('biomedicus'))\n",
    "#print(SemanticTypes(['Drug'], corpus).get_system_type('quick_umls'))\n",
    "#print(SemanticTypes(['drug'], corpus).get_system_type('clamp'))\n",
    "#print(SemanticTypes(['Anatomy'], 'mipacq').get_system_type('ctakes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semtypes = ['test,treatment']\n",
    "#semtypes = 'drug,drug::drug_name,drug::drug_dose,dietary_supplement::dietary_supplement_name,dietary_supplement::dietary_supplement_dose'\n",
    "#semtypes =  'demographics::age,demographics::sex,demographics::race_ethnicity,demographics::bmi,demographics::weight'\n",
    "#corpus = 'clinical_trial'\n",
    "#sys = 'quick_umls'\n",
    "\n",
    "# is semantic type in particular system\n",
    "def system_semtype_check(sys, semtype, corpus):\n",
    "    st = SemanticTypes([semtype], corpus).get_system_type(sys)\n",
    "    if st:\n",
    "        return sys\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#print(system_semtype_check(sys, semtypes, corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation class for systems\n",
    "class AnnotationSystems():\n",
    "    \"\"\"   \n",
    "    System annotations of interest for UMLS concept extraction\n",
    "    NB: ctakes combines all \"mentions\" annotation types\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\" \n",
    "        annotation base types\n",
    "        \"\"\"   \n",
    "        \n",
    "        self.biomedicus_types = [\"biomedicus.v2.UmlsConcept\"]\n",
    "        self.clamp_types = [\"edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA\"]\n",
    "        self.ctakes_types = [\"ctakes_mentions\"]\n",
    "        self.metamap_types = [\"org.metamap.uima.ts.Candidate\"]\n",
    "        self.qumls_types = [\"concept_jaccard_score_False\"]\n",
    "       \n",
    "    def get_system_type(self, system):\n",
    "        \n",
    "        \"\"\"\n",
    "        return system types\n",
    "        \"\"\"\n",
    "        \n",
    "        if system == \"biomedicus\":\n",
    "            view = \"Analysis\"\n",
    "        else:\n",
    "            view = \"_InitialView\"\n",
    "\n",
    "        if system == 'biomedicus':\n",
    "            types = self.biomedicus_types\n",
    "\n",
    "        elif system == 'clamp':\n",
    "            types = self.clamp_types\n",
    "\n",
    "        elif system == 'ctakes':\n",
    "            types = self.ctakes_types\n",
    "\n",
    "        elif system == 'metamap':\n",
    "            types = self.metamap_types\n",
    "        \n",
    "        elif system == \"quick_umls\":\n",
    "            types = self.qumls_types\n",
    "            \n",
    "        return types, view\n",
    "    \n",
    "annSys = AnnotationSystems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%cython\n",
    "\n",
    "#import numpy as np # access to Numpy from Python layer\n",
    "#import math\n",
    "\n",
    "class Metrics(object):\n",
    "    \"\"\"\n",
    "    metrics class:\n",
    "    returns an instance with confusion matrix metrics\n",
    "    \"\"\"\n",
    "    def __init__(self, system_only, gold_only, gold_system_match, system_n, neither = 0): # neither: no sys or manual annotation\n",
    "\n",
    "        self = self    \n",
    "        self.system_only = system_only\n",
    "        self.gold_only = gold_only\n",
    "        self.gold_system_match = gold_system_match\n",
    "        self.system_n = system_n\n",
    "        self.neither = neither\n",
    "        \n",
    "    def get_confusion_metrics(self, corpus = None, test = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        compute confusion matrix measures, as per  \n",
    "        https://stats.stackexchange.com/questions/51296/how-do-you-calculate-precision-and-recall-for-multiclass-classification-using-co\n",
    "        \"\"\"\n",
    "#         cdef:\n",
    "#             int TP, FP, FN\n",
    "#             double TM\n",
    "\n",
    "        TP = self.gold_system_match\n",
    "        FP = self.system_only\n",
    "        FN = self.gold_only\n",
    "        \n",
    "        TM = TP/math.sqrt(self.system_n) # TigMetric\n",
    "       \n",
    "        if not test:\n",
    "            \n",
    "            if corpus == 'casi':\n",
    "                recall = TP/(TP + FN)\n",
    "                precision = TP/(TP + FP)\n",
    "                F = 2*(precision*recall)/(precision + recall)\n",
    "            else:\n",
    "                if self.neither == 0:\n",
    "                    confusion = [[0, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "                else:\n",
    "                    confusion = [[self.neither, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "                c = np.asarray(confusion)\n",
    "                \n",
    "                if TP != 0 or FP != 0:\n",
    "                    precision = TP/(TP+FP)\n",
    "                else:\n",
    "                    precision = 0\n",
    "                \n",
    "                if TP != 0 or FN != 0:\n",
    "                    recall = TP/(TP+FN)\n",
    "                else:\n",
    "                    recall = 0\n",
    "                \n",
    "                if precision + recall != 0:\n",
    "                    F = 2*(precision*recall)/(precision + recall)\n",
    "                else:\n",
    "                    F = 0\n",
    "    \n",
    "#                 recall = np.diag(c) / np.sum(c, axis = 1)\n",
    "#                 precision = np.diag(c) / np.sum(c, axis = 0)\n",
    "#                 #print('Yo!', np.mean(precision), np.mean(recall))\n",
    "#                 if np.mean(precision) != 0 and np.mean(recall) != 0:\n",
    "#                     F = 2*(precision*recall)/(precision + recall)\n",
    "#                 else:\n",
    "#                     F = 0\n",
    "        else:\n",
    "            precision = TP/(TP+FP)\n",
    "            recall = TP/(TP+FN)\n",
    "            F = 2*(precision*recall)/(precision + recall)\n",
    "        \n",
    "        # Tignanelli Metric\n",
    "        if FN == 0:\n",
    "            TP_FN_R = TP\n",
    "        elif FN > 0:\n",
    "            TP_FN_R = TP/FN\n",
    " \n",
    "        return F, recall, precision, TP, FP, FN, TP_FN_R, TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_set(df, analysis_type = 'entity', df_type = 'sys', corpus = None):\n",
    "    \n",
    "    # get values for creation of series of type tuple\n",
    "    if 'entity' in analysis_type: \n",
    "        if corpus == 'casi':\n",
    "            arg = df.case, df.overlap\n",
    "        else:    \n",
    "            arg = df.begin, df.end, df.case\n",
    "            \n",
    "    elif 'cui' in analysis_type:\n",
    "        arg = df.value, df.case\n",
    "    elif 'full' in analysis_type:\n",
    "        arg = df.begin, df.end, df.value, df.case\n",
    "    \n",
    "    return set(list(zip(*arg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%cython \n",
    "\n",
    "from __main__ import df_to_set, engine\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "def get_cooccurences(ref, sys, analysis_type: str, corpus: str):\n",
    "    \"\"\"\n",
    "    get cooccurences between system and reference; exact match; TODO: add relaxed -> done in single system evals during ensemble run\n",
    "    \"\"\"\n",
    "    # cooccurences\n",
    "    class Cooccurences(object):\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.ref_system_match = 0\n",
    "            self.ref_only = 0\n",
    "            self.system_only = 0\n",
    "            self.system_n = 0\n",
    "            self.ref_n = 0\n",
    "            self.matches = set()\n",
    "            self.false_negatives = set()\n",
    "            self.corpus = corpus\n",
    "\n",
    "    c = Cooccurences()\n",
    "    \n",
    "    if c.corpus != 'casi':\n",
    "        if analysis_type in ['cui', 'full']:\n",
    "            sys = sys.rename(index=str, columns={\"note_id\": \"case\", \"cui\": \"value\"})\n",
    "            # do not overestimate FP\n",
    "            sys = sys[~sys['value'].isnull()] \n",
    "            ref = ref[~ref['value'].isnull()]\n",
    "        \n",
    "        if 'entity' in analysis_type: \n",
    "            sys = sys.rename(index=str, columns={\"note_id\": \"case\"})\n",
    "            cols_to_keep = ['begin', 'end', 'case']\n",
    "        elif 'cui' in analysis_type: \n",
    "            cols_to_keep = ['value', 'case']\n",
    "        elif 'full' in analysis_type: \n",
    "            cols_to_keep = ['begin', 'end', 'value', 'case']\n",
    "        \n",
    "        sys = sys[cols_to_keep].drop_duplicates()\n",
    "        ref = ref[cols_to_keep].drop_duplicates()\n",
    "        # matches via inner join\n",
    "        tp = pd.merge(sys, ref, how = 'inner', left_on=cols_to_keep, right_on = cols_to_keep) \n",
    "        # reference-only via left outer join\n",
    "        fn = pd.merge(ref, sys, how = 'left', left_on=cols_to_keep, right_on = cols_to_keep, indicator=True) \n",
    "        fn = fn[fn[\"_merge\"] == 'left_only']\n",
    "\n",
    "        tp = tp[cols_to_keep]\n",
    "        fn = fn[cols_to_keep]\n",
    "\n",
    "        # use for metrics \n",
    "        c.matches = c.matches.union(df_to_set(tp, analysis_type, 'ref'))\n",
    "        c.false_negatives = c.false_negatives.union(df_to_set(fn, analysis_type, 'ref'))\n",
    "        c.ref_system_match = len(c.matches)\n",
    "        c.system_only = len(sys) - len(c.matches) # fp\n",
    "        c.system_n = len(sys)\n",
    "        c.ref_n = len(ref)\n",
    "        c.ref_only = len(c.false_negatives)\n",
    "        \n",
    "    else:\n",
    "        sql = \"select `case` from test.amia_2019_analytical_v where overlap = 1 and `system` = %(sys.name)s\"  \n",
    "        tp = pd.read_sql(sql, params={\"sys.name\":sys.name}, con=engine)\n",
    "        \n",
    "        sql = \"select `case` from test.amia_2019_analytical_v where (overlap = 0 or overlap is null) and `system` = %(sys.name)s\"  \n",
    "        fn = pd.read_sql(sql, params={\"sys.name\":sys.name}, con=engine)\n",
    "        \n",
    "        c.matches = df_to_set(tp, 'entity', 'sys', 'casi')\n",
    "        c.fn = df_to_set(fn, 'entity', 'sys', 'casi')\n",
    "        c.ref_system_match = len(c.matches)\n",
    "        c.system_only = len(sys) - len(c.matches)\n",
    "        c.system_n = len(tp) + len(fn)\n",
    "        c.ref_n = len(tp) + len(fn)\n",
    "        c.ref_only = len(fn)\n",
    "        \n",
    "    # sanity check\n",
    "    if len(ref) - c.ref_system_match < 0:\n",
    "        print('Error: ref_system_match > len(ref)!')\n",
    "    if len(ref) != c.ref_system_match + c.ref_only:\n",
    "        print('Error: ref count mismatch!', len(ref), c.ref_system_match, c.ref_only)\n",
    "   \n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_vector(doc: str, ann: List[int], labels: List[str]) -> np.array:\n",
    "\n",
    "    v = np.zeros(doc)\n",
    "    labels = list(labels)\n",
    "    \n",
    "    for (i, lab) in enumerate(labels):\n",
    "        i += 1  # 0 is reserved for no label\n",
    "        idxs = [np.arange(a.begin, a.end) for a in ann if a.label == lab]\n",
    "        idxs = [j for mask in idxs for j in mask]\n",
    "        v[idxs] = i\n",
    "\n",
    "    return v\n",
    "\n",
    "# test confusion matrix elements for vectorized annotation set; includes TN\n",
    "# https://kawahara.ca/how-to-compute-truefalse-positives-and-truefalse-negatives-in-python-for-binary-classification-problems/\n",
    "# def confused(sys1, ann1):\n",
    "#     TP = np.sum(np.logical_and(ann1 == 1, sys1 == 1))\n",
    "\n",
    "#     # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "#     TN = np.sum(np.logical_and(ann1 == 0, sys1 == 0))\n",
    "\n",
    "#     # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "#     FP = np.sum(np.logical_and(ann1 == 0, sys1 == 1))\n",
    "\n",
    "#     # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "#     FN = np.sum(np.logical_and(ann1 == 1, sys1 == 0))\n",
    "    \n",
    "#     return TP, TN, FP, FN\n",
    "\n",
    "def confused(sys1, ann1):\n",
    "    TP = np.sum(np.logical_and(ann1 > 0, sys1 == ann1))\n",
    "\n",
    "    # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "    TN = np.sum(np.logical_and(ann1 == 0, sys1 == ann1))\n",
    "\n",
    "    # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "    FP = np.sum(np.logical_and(sys1 > 0, sys1 != ann1))\n",
    "\n",
    "    # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "    FN = np.sum(np.logical_and(ann1 > 0, sys1 == 0))\n",
    "    \n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def vectorized_cooccurences(r: object, analysis_type: str, corpus: str, filter_semtype, semtype = None) -> np.int64:\n",
    "    docs = get_docs(corpus)\n",
    "    \n",
    "    if filter_semtype:\n",
    "        ann = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "    else: \n",
    "        ann = get_ref_ann(analysis_type, corpus, filter_semtype)\n",
    "        \n",
    "    sys = get_sys_ann(analysis_type, r)\n",
    "    \n",
    "    #cvals = []\n",
    "    if analysis_type == 'entity':\n",
    "        labels = [\"concept\"]\n",
    "    elif analysis_type in ['cui', 'full']:\n",
    "        labels = list(set(ann[\"value\"].tolist()))\n",
    "    \n",
    "    sys2 = list()\n",
    "    ann2 = list()\n",
    "    s2 = list()\n",
    "    a2 = list()\n",
    "    \n",
    "    for n in range(len(docs)):\n",
    "        \n",
    "        if analysis_type != 'cui':\n",
    "            a1 = list(ann.loc[ann[\"case\"] == docs[n][0]].itertuples(index=False))\n",
    "            s1 = list(sys.loc[sys[\"case\"] == docs[n][0]].itertuples(index=False))\n",
    "            ann1 = label_vector(docs[n][1], a1, labels)\n",
    "            sys1 = label_vector(docs[n][1], s1, labels)\n",
    "\n",
    "            #TP, TN, FP, FN = confused(sys1, ann1)\n",
    "            #cvals.append([TP, TN, FP, FN])\n",
    "            sys2.append(list(sys1))\n",
    "            ann2.append(list(ann1))\n",
    "\n",
    "        else:\n",
    "            a = ann.loc[ann[\"case\"] == docs[n][0]]['label'].tolist()\n",
    "            s = sys.loc[sys[\"case\"] == docs[n][0]]['label'].tolist()\n",
    "            x = [1 if x in a else 0 for x in labels]\n",
    "            y = [1 if x in s else 0 for x in labels]\n",
    "#             x_sparse = sparse.csr_matrix(x)\n",
    "#             y_sparse = sparse.csr_matrix(y)\n",
    "            s2.append(y)\n",
    "            a2.append(x)\n",
    "           \n",
    "            \n",
    "            #a1 = list(ann.loc[ann[\"case\"] == docs[n][0]].itertuples(index=False))\n",
    "            #s1 = list(sys.loc[sys[\"case\"] == docs[n][0]].itertuples(index=False))\n",
    "   \n",
    "    if analysis_type != 'cui':\n",
    "        a2 = [item for sublist in ann2 for item in sublist]\n",
    "        s2 = [item for sublist in sys2 for item in sublist]\n",
    "        report = classification_report(a2, s2, output_dict=True)\n",
    "        #print('classification:', report)\n",
    "        macro_precision =  report['macro avg']['precision'] \n",
    "        macro_recall = report['macro avg']['recall']    \n",
    "        macro_f1 = report['macro avg']['f1-score']\n",
    "        TN, FP, FN, TP = confusion_matrix(a2, s2).ravel()\n",
    "        \n",
    "        #return (np.sum(cvals, axis=0), (macro_precision, macro_recall, macro_f1))\n",
    "        return ((TP, TN, FP, FN), (macro_precision, macro_recall, macro_f1))\n",
    "    else:\n",
    "        x_sparse = sparse.csr_matrix(a2)\n",
    "        y_sparse = sparse.csr_matrix(s2)\n",
    "        report = classification_report(x_sparse, y_sparse, output_dict=True)\n",
    "        macro_precision =  report['macro avg']['precision'] \n",
    "        macro_recall = report['macro avg']['recall']    \n",
    "        macro_f1 = report['macro avg']['f1-score']\n",
    "        #print((macro_precision, macro_recall, macro_f1))\n",
    "        return ((0, 0, 0, 0), (macro_precision, macro_recall, macro_f1))\n",
    "                                       \n",
    "        \n",
    "def vectorized_complementarity(r: object, analysis_type: str, corpus: str, c: tuple, filter_semtype, semtype = None) -> np.int64:\n",
    "    docs = get_docs(corpus)\n",
    "    \n",
    "    if filter_semtype:\n",
    "        ann = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "    else: \n",
    "        ann = get_ref_ann(analysis_type, corpus, filter_semtype)\n",
    "    \n",
    "    if filter_semtype and SemanticTypes([semtype], corpus).get_system_type(r.sysA) and SemanticTypes([semtype], corpus).get_system_type(r.sysB) or not filter_semtype: \n",
    "        sysA = get_sys_data(r.sysA, analysis_type, corpus, filter_semtype, semtype)\n",
    "        sysA = sysA.rename(index=str, columns={\"note_id\": \"case\"})\n",
    "        sysB = get_sys_data(r.sysB, analysis_type, corpus, filter_semtype, semtype)\n",
    "        sysB = sysB.rename(index=str, columns={\"note_id\": \"case\"})\n",
    "\n",
    "        if analysis_type == 'entity':\n",
    "            sysA[\"label\"] = 'concept'\n",
    "            sysB[\"label\"] = 'concept'\n",
    "            cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "        elif analysis_type == 'full':\n",
    "            sysA[\"label\"] = sysA[\"cui\"]\n",
    "            sysB[\"label\"] = sysB[\"cui\"]\n",
    "            cols_to_keep = ['begin', 'end', 'case', 'value', 'label']\n",
    "        elif analysis_type == 'cui':\n",
    "            sysA[\"label\"] = sysA[\"cui\"]\n",
    "            sysB[\"label\"] = sysA[\"cui\"]\n",
    "            cols_to_keep = ['case', 'cui', 'label']\n",
    "\n",
    "        sysA = sysA[cols_to_keep]\n",
    "        sysB = sysB[cols_to_keep]\n",
    "\n",
    "        if analysis_type == 'entity':\n",
    "            labels = [\"concept\"]\n",
    "        elif analysis_type in ['cui', 'full']:\n",
    "            labels = list(set(ann[\"value\"].tolist()))\n",
    "\n",
    "        sys_a2 = list()\n",
    "        sys_b2 = list()\n",
    "        ann2 = list()\n",
    "        s_a2 = list()\n",
    "        s_b2 = list()\n",
    "\n",
    "        sys2 = list()\n",
    "        a2 = list()\n",
    "\n",
    "        for n in range(len(docs)):\n",
    "\n",
    "            if analysis_type != 'cui':\n",
    "                a1 = list(ann.loc[ann[\"case\"] == docs[n][0]].itertuples(index=False))\n",
    "                # get systems A and b for complementarity test\n",
    "\n",
    "                # test\n",
    "                s_a1 = list(sysA.loc[sysA[\"case\"] == docs[n][0]].itertuples(index=False))\n",
    "                s_b1 = list(sysB.loc[sysB[\"case\"] == docs[n][0]].itertuples(index=False))\n",
    "                ann1 = label_vector(docs[n][1], a1, labels)\n",
    "                sys_a1 = label_vector(docs[n][1], s_a1, labels)\n",
    "                sys_b1 = label_vector(docs[n][1], s_b1, labels)\n",
    "\n",
    "                #TP, TN, FP, FN = confused(sys1, ann1)\n",
    "                #cvals.append([TP, TN, FP, FN])\n",
    "\n",
    "                # get for Aright/Awrong and Bright/Bwrong\n",
    "                sys_a2.append(list(sys_a1))\n",
    "                sys_b2.append(list(sys_b1))\n",
    "\n",
    "    #             # combine 2-system vectors for complementarity Y_positive/Y_negaative\n",
    "    #             sys2.append(list(np.where(np.logical_or(sys_a1, sys_b1), 1, 0)))\n",
    "\n",
    "                ann2.append(list(ann1))\n",
    "\n",
    "            else:\n",
    "                a = ann.loc[ann[\"case\"] == docs[n][0]]['label'].tolist()\n",
    "                s = sys.loc[sys[\"case\"] == docs[n][0]]['label'].tolist()\n",
    "                x = [1 if x in a else 0 for x in labels]\n",
    "                y = [1 if x in s else 0 for x in labels]\n",
    "    #             x_sparse = sparse.csr_matrix(x)\n",
    "    #             y_sparse = sparse.csr_matrix(y)\n",
    "                s2.append(y)\n",
    "                a2.append(x)\n",
    "\n",
    "\n",
    "                #a1 = list(ann.loc[ann[\"case\"] == docs[n][0]].itertuples(index=False))\n",
    "                #s1 = list(sys.loc[sys[\"case\"] == docs[n][0]].itertuples(index=False))\n",
    "\n",
    "        if analysis_type != 'cui':\n",
    "            a2 = [item for sublist in ann2 for item in sublist]\n",
    "\n",
    "            # right/wrong for A and B\n",
    "            s_a2 = [item for sublist in sys_a2 for item in sublist]\n",
    "            s_b2 = [item for sublist in sys_b2 for item in sublist]\n",
    "\n",
    "            # intersection of A and B\n",
    "            s_2 = list(np.where(np.logical_and(s_a2, s_b2), 1, 0))\n",
    "\n",
    "            # TODO: wrong&positive; wrong&negative\n",
    "\n",
    "            # wrong -> both systems: numerator\n",
    "            # https://medium.com/better-programming/how-to-count-occurrences-in-a-python-list-f799072538b3\n",
    "            f_negatives = list(filter(lambda number: number == 1, np.subtract(a2,s_2)))\n",
    "            f_positives = list(filter(lambda number: number == -1, np.subtract(a2,s_2)))\n",
    "\n",
    "            FN = len(f_negatives)\n",
    "            FP = len(f_positives)\n",
    "\n",
    "            # wrong -> by system: denom\n",
    "            af_negatives = list(filter(lambda number: number == 1, np.subtract(a2,s_a2)))\n",
    "            af_positives = list(filter(lambda number: number == -1, np.subtract(a2,s_a2)))\n",
    "\n",
    "            aFN = len(af_negatives)\n",
    "            aFP = len(af_positives)\n",
    "\n",
    "            bf_negatives = list(filter(lambda number: number == 1, np.subtract(a2,s_b2)))\n",
    "            bf_positives = list(filter(lambda number: number == -1, np.subtract(a2,s_b2)))\n",
    "\n",
    "            bFN = len(bf_negatives)\n",
    "            bFP = len(bf_positives)\n",
    "\n",
    "            print('out:', c, FN, FP, aFN, aFP, bFN, bFP, semtype)\n",
    "\n",
    "    #         report_p = classification_report(a2, s2, output_dict=True)\n",
    "    #         #print('classification:', report)\n",
    "    #         macro_precision =  report_Y['macro avg']['precision'] \n",
    "    #         macro_recall = report_Y['macro avg']['recall']    \n",
    "    #         macro_f1 = report_Y['macro avg']['f1-score']\n",
    "    #         TN, FP, FN, TP = confusion_matrix(a2, s2).ravel()\n",
    "\n",
    "    #         report_r = classification_report(a2, s2, output_dict=True)\n",
    "    #         #print('classification:', report)\n",
    "    #         macro_precision =  reportA['macro avg']['precision'] \n",
    "    #         macro_recall = report_A['macro avg']['recall']    \n",
    "    #         macro_f1 = report_Y!['macro avg']['f1-score']\n",
    "    #         TN, FP, FN, TP = confusion_matrix(a2, s2).ravel()  \n",
    "\n",
    "\n",
    "            #return (np.sum(cvals, axis=0), (macro_precision, macro_recall, macro_f1))\n",
    "            #return ((TP, TN, FP, FN), (macro_precision, macro_recall, macro_f1))\n",
    "        else:\n",
    "            x_sparse = sparse.csr_matrix(a2)\n",
    "            y_sparse = sparse.csr_matrix(s2)\n",
    "            report = classification_report(x_sparse, y_sparse, output_dict=True)\n",
    "            macro_precision =  report['macro avg']['precision'] \n",
    "            macro_recall = report['macro avg']['recall']    \n",
    "            macro_f1 = report['macro avg']['f1-score']\n",
    "            #print((macro_precision, macro_recall, macro_f1))\n",
    "            return ((0, 0, 0, 0), (macro_precision, macro_recall, macro_f1))\n",
    "\n",
    "    else:\n",
    "        print(str(c) + 'has no semtype', semtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm_dict(ref_only: int, system_only: int, ref_system_match: int, system_n: int, ref_n: int) -> dict:\n",
    "    \"\"\"\n",
    "    Generate dictionary of confusion matrix params and measures\n",
    "    :params: ref_only, system_only, reference_system_match -> sets\n",
    "    matches, system_n, reference_n -> counts\n",
    "    :return: dictionary object\n",
    "    \"\"\"\n",
    "\n",
    "    if ref_only + ref_system_match != ref_n:\n",
    "        print('ERROR!')\n",
    "        \n",
    "    # get evaluation metrics\n",
    "    F, recall, precision, TP, FP, FN, TP_FN_R, TM  = Metrics(system_only, ref_only, ref_system_match, system_n).get_confusion_metrics()\n",
    "\n",
    "    d = {\n",
    "#          'F1': F[1], \n",
    "#          'precision': precision[1], \n",
    "#          'recall': recall[1], \n",
    "         'F1': F, \n",
    "         'precision': precision, \n",
    "         'recall': recall, \n",
    "         'TP': TP, \n",
    "         'FN': FN, \n",
    "         'FP': FP, \n",
    "         'TP/FN': TP_FN_R,\n",
    "         'n_gold': ref_n, \n",
    "         'n_sys': system_n, \n",
    "         'TM': TM\n",
    "    }\n",
    "    \n",
    "    if system_n - FP != TP:\n",
    "        print('inconsistent system n!')\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ft.lru_cache(maxsize=None)\n",
    "def get_metric_data(analysis_type: str, corpus: str):\n",
    "   \n",
    "    usys_file, ref_table = AnalysisConfig().corpus_config()\n",
    "    systems = AnalysisConfig().systems\n",
    "    \n",
    "    sys_ann = pd.read_csv(analysisConf.data_dir + usys_file, dtype={'note_id': str})\n",
    "    \n",
    "    sql = \"SELECT * FROM \" + ref_table #+ \" where semtype in('Anatomy', 'Chemicals_and_drugs')\" \n",
    "    \n",
    "    ref_ann = pd.read_sql(sql, con=engine)\n",
    "    sys_ann = sys_ann.drop_duplicates()\n",
    "    \n",
    "    return ref_ann, sys_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%cython\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "def geometric_mean(metrics):\n",
    "    \"\"\"\n",
    "    1. Get rank average of F1, TP/FN, TM\n",
    "        http://www.datasciencemadesimple.com/rank-dataframe-python-pandas-min-max-dense-rank-group/\n",
    "        https://stackoverflow.com/questions/46686315/in-pandas-how-to-create-a-new-column-with-a-rank-according-to-the-mean-values-o?rq=1\n",
    "    2. Take geomean of rank averages\n",
    "        https://stackoverflow.com/questions/42436577/geometric-mean-applied-on-row\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.DataFrame() \n",
    "\n",
    "    metrics['F1 rank']=metrics['F1'].rank(ascending=0,method='average')\n",
    "    metrics['TP/FN rank']=metrics['TP/FN'].rank(ascending=0,method='average')\n",
    "    metrics['TM rank']=metrics['TM'].rank(ascending=0,method='average')\n",
    "    metrics['Gmean'] = gmean(metrics.iloc[:,-3:],axis=1)\n",
    "\n",
    "    return metrics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metrics(analysis_type: str, corpus: str, filter_semtype, semtype = None):\n",
    "    start = time.time()\n",
    "\n",
    "    systems = AnalysisConfig().systems\n",
    "    metrics = pd.DataFrame()\n",
    "\n",
    "    __, sys_ann = get_metric_data(analysis_type, corpus)\n",
    "    c = None\n",
    "    \n",
    "    for sys in systems:\n",
    "       \n",
    "        if filter_semtype and semtype:\n",
    "            ref_ann = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "            ref_ann = get_ref_ann(analysis_type, corpus, filter_semtype)\n",
    "            \n",
    "        system_annotations = sys_ann[sys_ann['system'] == sys].copy()\n",
    "\n",
    "        if filter_semtype:\n",
    "            st = SemanticTypes([semtype], corpus).get_system_type(sys)\n",
    "\n",
    "            if st: \n",
    "                system_annotations = sys_ann[sys_ann['semtypes'].isin(st)].copy()\n",
    "        else:\n",
    "            system_annotations = sys_ann.copy()\n",
    "\n",
    "        if (filter_semtype and st) or filter_semtype is False:\n",
    "            system = system_annotations.copy()\n",
    "\n",
    "            if sys == 'quick_umls':\n",
    "                system = system[system.score.astype(float) >= .8]\n",
    "\n",
    "            if sys == 'metamap' and modification == None:\n",
    "                system = system.fillna(0)\n",
    "                system = system[system.score.abs().astype(int) >= 800]\n",
    "\n",
    "            system = system.drop_duplicates()\n",
    "\n",
    "            ref_ann = ref_ann.rename(index=str, columns={\"start\": \"begin\", \"file\": \"case\"})\n",
    "            c = get_cooccurences(ref_ann, system, analysis_type, corpus) # get matches, FN, etc.\n",
    "\n",
    "            \n",
    "            if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "                # get dictionary of confusion matrix metrics\n",
    "                d = cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n)\n",
    "                d['system'] = sys\n",
    "\n",
    "                data = pd.DataFrame(d,  index=[0])\n",
    "                metrics = pd.concat([metrics, data], ignore_index=True)\n",
    "                metrics.drop_duplicates(keep='last', inplace=True)\n",
    "            else:\n",
    "                print(\"NO EXACT MATCHES FOR\", sys)\n",
    "            elapsed = (time.time() - start)\n",
    "            print(\"elapsed:\", sys, elapsed)\n",
    "   \n",
    "    if c:\n",
    "        elapsed = (time.time() - start)\n",
    "        print(geometric_mean(metrics))\n",
    "\n",
    "        now = datetime.now()\n",
    "        timestamp = datetime.timestamp(now)\n",
    "\n",
    "        file_name = 'metrics_'\n",
    "\n",
    "        metrics.to_csv(analysisConf.data_dir + corpus + '_' + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "\n",
    "        print(\"total elapsed time:\", elapsed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ft.lru_cache(maxsize=None)\n",
    "def get_ref_n(analysis_type: str, corpus: str, filter_semtype: str) -> int:\n",
    "    \n",
    "    ref_ann, _ = get_metric_data(analysis_type, corpus)\n",
    "    \n",
    "    if filter_semtype:\n",
    "        ref_ann = ref_ann[ref_ann['semtype'].isin(SemanticTypes(semtypes, corpus).get_system_type('reference'))]\n",
    "            \n",
    "    if corpus == 'casi':\n",
    "        return len(ref_ann)\n",
    "        \n",
    "    else:\n",
    "        # do not overestimate fn\n",
    "        if 'entity' in analysis_type:\n",
    "            ref_ann = ref_ann[['start', 'end', 'file']].drop_duplicates()\n",
    "        elif 'cui' in analysis_type:\n",
    "            ref_ann = ref_ann[['value', 'file']].drop_duplicates()\n",
    "        elif 'full' in analysis_type:\n",
    "            ref_ann = ref_ann[['start', 'end', 'value', 'file']].drop_duplicates()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        ref_n = len(ref_ann.drop_duplicates())\n",
    "\n",
    "        return ref_n\n",
    "    \n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_sys_data(system: str, analysis_type: str, corpus: str, filter_semtype, semtype = None) -> pd.DataFrame:\n",
    "   \n",
    "    _, data = get_metric_data(analysis_type, corpus)\n",
    "    \n",
    "    out = data[data['system'] == system].copy()\n",
    "    \n",
    "    if filter_semtype:\n",
    "        st = SemanticTypes([semtype], corpus).get_system_type(system)\n",
    "        print(system, 'st', st)\n",
    "    \n",
    "    if corpus == 'casi':\n",
    "        cols_to_keep = ['case', 'overlap'] \n",
    "        out = out[cols_to_keep].drop_duplicates()\n",
    "        return out\n",
    "        \n",
    "    else:\n",
    "        if filter_semtype:\n",
    "            out = out[out['semtypes'].isin(st)].copy()\n",
    "            \n",
    "        else:\n",
    "            out = out[out['system']== system].copy()\n",
    "            \n",
    "        if modification == 'negation':\n",
    "            out = out[out['modification'] == 'negation'].copy()\n",
    "        \n",
    "        if system == 'quick_umls':\n",
    "            out = out[(out.score.astype(float) >= 0.8) & (out[\"type\"] == 'concept_jaccard_score_False')]\n",
    "            # fix for leading space on semantic type field\n",
    "            #out = out.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x) \n",
    "            out['semtypes'] = out['semtypes'].str.strip()\n",
    "        \n",
    "        if system == 'metamap' and modification == None:\n",
    "            out = out[out.score.abs().astype(int) >= 800]\n",
    "            \n",
    "        if 'entity' in analysis_type:\n",
    "            cols_to_keep = ['begin', 'end', 'note_id']\n",
    "        elif 'cui' in analysis_type:\n",
    "            cols_to_keep = ['cui', 'note_id']\n",
    "        elif 'full' in analysis_type:\n",
    "            cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "\n",
    "        out = out[cols_to_keep]\n",
    "        \n",
    "        return out.drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GENERATE merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetTotals(object):\n",
    "    \"\"\" \n",
    "    returns an instance with merged match set numbers using either union or intersection of elements in set \n",
    "    \"\"\"\n",
    "    def __init__(self, ref_n, sys_n, match_set):\n",
    "\n",
    "        self = self    \n",
    "        self.ref_ann = ref_n\n",
    "        self.sys_n = sys_n\n",
    "        self.match_set = match_set\n",
    "\n",
    "    def get_ref_sys(self):\n",
    "\n",
    "        ref_only = self.ref_ann - len(self.match_set)\n",
    "        sys_only = self.sys_n - len(self.match_set)\n",
    "\n",
    "        return ref_only, sys_only, len(self.match_set), self.match_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_vote(df):\n",
    "    df['length'] = (df.end - df.begin).abs()\n",
    "    \n",
    "    cases = set(df['note_id'].tolist())\n",
    "    \n",
    "    for case in cases:\n",
    "        i = 0\n",
    "        data = []\n",
    "        out = pd.DataFrame()\n",
    "        \n",
    "        test = df[df['note_id']==case].copy()\n",
    "        \n",
    "        for row in test.itertuples():\n",
    "\n",
    "            iix = pd.IntervalIndex.from_arrays(test.begin, test.end, closed='neither')\n",
    "            span_range = pd.Interval(row.begin, row.end)\n",
    "            fx = test[iix.overlaps(span_range)].copy()\n",
    "\n",
    "            maxLength = fx['length'].max()\n",
    "            minLength = fx['length'].min()\n",
    "            maxScore = abs(float(fx['score'].max()))\n",
    "            minScore = abs(float(fx['score'].min()))\n",
    "\n",
    "            if len(fx) > 1: \n",
    "                if i%5000 == 0:\n",
    "                    print('iteration:', i)\n",
    "\n",
    "                # if gretaer scxore or longer span exists, use as tie-breaker\n",
    "                if maxScore > minScore:\n",
    "                    fx = fx[fx['score'] == maxScore]\n",
    "                elif maxLength > minLength:\n",
    "                    fx = fx[fx['length'] == fx['length'].max()]\n",
    "\n",
    "            i += 1\n",
    "            data.append(fx)\n",
    "\n",
    "        out = pd.concat(data, axis=0)\n",
    "        print(out.head())\n",
    "   \n",
    "    # Remaining ties: randomly reindex to keep random row when dropping duplicates: https://gist.github.com/cadrev/6b91985a1660f26c2742\n",
    "    out.reset_index(inplace=True)\n",
    "    out = out.reindex(np.random.permutation(out.index))\n",
    "    out = out.drop_duplicates(['begin', 'end', 'note_id', 'length', 'cui'])\n",
    "    \n",
    "    return out  \n",
    "    \n",
    "    \n",
    "def disambiguate(arg):\n",
    "    \n",
    "    arg['length'] = (arg.end - arg.begin).abs()\n",
    "    \n",
    "    arg.sort_values(by=['note_id','begin'],inplace=True)\n",
    "    \n",
    "    df = arg[['begin', 'end', 'note_id', 'cui', 'concept', 'semtype', 'score', 'length', 'type', 'system', 'polarity']].copy()\n",
    "    df.sort_values(by=['note_id','begin'],inplace=True)\n",
    "\n",
    "    i = 0\n",
    "    data = []\n",
    "    out = pd.DataFrame()\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        # get overlapping intervals: \n",
    "        # https://stackoverflow.com/questions/58192068/is-it-possible-to-use-pandas-overlap-in-a-dataframe\n",
    "        iix = pd.IntervalIndex.from_arrays(df.begin, df.end, closed='neither')\n",
    "        span_range = pd.Interval(row.begin, row.end)\n",
    "        fx = df[iix.overlaps(span_range)].copy()\n",
    "\n",
    "        maxLength = fx['length'].max()\n",
    "        minLength = fx['length'].min()\n",
    "        maxScore = abs(float(fx['score'].max()))\n",
    "        minScore = abs(float(fx['score'].min()))\n",
    "        \n",
    "        if maxScore > minScore:\n",
    "            fx = fx[fx['score'] == maxScore]\n",
    "\n",
    "        elif maxLength > minLength:\n",
    "            fx = fx[fx['length'] == maxLength]\n",
    "            \n",
    "        data.append(fx)\n",
    "\n",
    "    out = pd.concat(data, axis=0)\n",
    "   \n",
    "    # randomly reindex to keep random row when dropping duplicates: https://gist.github.com/cadrev/6b91985a1660f26c2742\n",
    "    out.reset_index(inplace=True)\n",
    "    out = out.reindex(np.random.permutation(out.index))\n",
    "    \n",
    "    return out.drop_duplicates(subset=['begin', 'end', 'note_id'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ft.lru_cache(maxsize=None)\n",
    "def process_sentence(pt, sentence, analysis_type, corpus, filter_semtype, semtype = None):\n",
    "    \"\"\"\n",
    "    Recursively evaluate parse tree, \n",
    "    with check for existence before build\n",
    "       :param sentence: to process\n",
    "       :return class of merged annotations, boolean operated system df \n",
    "    \"\"\"\n",
    "    \n",
    "    class Results(object):\n",
    "        def __init__(self):\n",
    "            self.results = set()\n",
    "            self.system_merges = pd.DataFrame()\n",
    "            \n",
    "    r = Results()\n",
    "    \n",
    "    if 'entity' in analysis_type and corpus != 'casi': \n",
    "        cols_to_keep = ['begin', 'end', 'note_id'] # entity only\n",
    "    elif 'full' in analysis_type: \n",
    "        cols_to_keep = ['cui', 'begin', 'end', 'note_id'] # entity only\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['cui', 'note_id'] # entity only\n",
    "    elif corpus == 'casi':\n",
    "        cols_to_keep = ['case', 'overlap']\n",
    "    \n",
    "    def evaluate(parseTree):\n",
    "        oper = {'&': op.and_, '|': op.or_}\n",
    "        \n",
    "        if parseTree:\n",
    "            leftC = gevent.spawn(evaluate, parseTree.getLeftChild())\n",
    "            rightC = gevent.spawn(evaluate, parseTree.getRightChild())\n",
    "            \n",
    "            if leftC.get() is not None and rightC.get() is not None:\n",
    "                system_query = pd.DataFrame()\n",
    "                fn = oper[parseTree.getRootVal()]\n",
    "                \n",
    "                if isinstance(leftC.get(), str):\n",
    "                    # get system as leaf node \n",
    "                    if filter_semtype:\n",
    "                        left_sys = get_sys_data(leftC.get(), analysis_type, corpus, filter_semtype, semtype)\n",
    "                    else:\n",
    "                        left_sys = get_sys_data(leftC.get(), analysis_type, corpus, filter_semtype)\n",
    "                \n",
    "                elif isinstance(leftC.get(), pd.DataFrame):\n",
    "                    l_sys = leftC.get()\n",
    "                \n",
    "                if isinstance(rightC.get(), str):\n",
    "                    # get system as leaf node\n",
    "                    if filter_semtype:\n",
    "                        right_sys = get_sys_data(rightC.get(), analysis_type, corpus, filter_semtype, semtype)\n",
    "                    else:\n",
    "                        right_sys = get_sys_data(rightC.get(), analysis_type, corpus, filter_semtype)\n",
    "                    \n",
    "                elif isinstance(rightC.get(), pd.DataFrame):\n",
    "                    r_sys = rightC.get()\n",
    "                    \n",
    "                if fn == op.or_:\n",
    "\n",
    "                    if isinstance(leftC.get(), str) and isinstance(rightC.get(), str):\n",
    "                        frames = [left_sys, right_sys]\n",
    "\n",
    "                    elif isinstance(leftC.get(), str) and isinstance(rightC.get(), pd.DataFrame):\n",
    "                        frames = [left_sys, r_sys]\n",
    "\n",
    "                    elif isinstance(leftC.get(), pd.DataFrame) and isinstance(rightC.get(), str):\n",
    "                        frames = [l_sys, right_sys]\n",
    "\n",
    "                    elif isinstance(leftC.get(), pd.DataFrame) and isinstance(rightC.get(), pd.DataFrame):\n",
    "                        frames = [l_sys, r_sys]\n",
    "                    \n",
    "                    # add in vote\n",
    "                        \n",
    "                    df = pd.concat(frames,  ignore_index=True)\n",
    "                    #df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "                    \n",
    "                    if analysis_type == 'full':\n",
    "                        df = union_vote(df)\n",
    "\n",
    "                if fn == op.and_:\n",
    "                    \n",
    "                    if isinstance(leftC.get(), str) and isinstance(rightC.get(), str):\n",
    "                        if not left_sys.empty and not right_sys.empty:\n",
    "                            df = left_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "                        else:\n",
    "                            df = pd.DataFrame(columns=cols_to_keep)\n",
    "\n",
    "                    elif isinstance(leftC.get(), str) and isinstance(rightC.get(), pd.DataFrame):\n",
    "                        if not left_sys.empty and not r_sys.empty:\n",
    "                            df = left_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "                        else:\n",
    "                            df = pd.DataFrame(columns=cols_to_keep)\n",
    "\n",
    "                    elif isinstance(leftC.get(), pd.DataFrame) and isinstance(rightC.get(), str):\n",
    "                        if not l_sys.empty and not right_sys.empty:\n",
    "                            df = l_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "                        else:\n",
    "                            df = pd.DataFrame(columns=cols_to_keep)\n",
    "\n",
    "                    elif isinstance(leftC.get(), pd.DataFrame) and isinstance(rightC.get(), pd.DataFrame):\n",
    "                        if not l_sys.empty and not r_sys.empty:\n",
    "                            df = l_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "                        else:\n",
    "                            df = pd.DataFrame(columns=cols_to_keep)\n",
    "                \n",
    "                # get combined system results\n",
    "                r.system_merges = df\n",
    "                \n",
    "                if len(df) > 0:\n",
    "                    system_query = system_query.append(df)\n",
    "                else:\n",
    "                    print('wtf!')\n",
    "                    \n",
    "                return system_query\n",
    "            else:\n",
    "                return parseTree.getRootVal()\n",
    "    \n",
    "    if sentence.n_or > 0 or sentence.n_and > 0:\n",
    "        evaluate(pt)  \n",
    "    \n",
    "    # trivial case\n",
    "    elif sentence.n_or == 0 and sentence.n_and == 0:\n",
    "        \n",
    "        if filter_semtype:\n",
    "            r.system_merges = get_sys_data(sentence.sentence, analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "            r.system_merges = get_sys_data(sentence.sentence, analysis_type, corpus, filter_semtype)\n",
    "        \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Incoming Boolean sentences are parsed into a binary tree.\n",
    "\n",
    "Test expressions to parse:\n",
    "\n",
    "sentence = '((((A&B)|C)|D)&E)'\n",
    "\n",
    "sentence = '(E&(D|(C|(A&B))))'\n",
    "\n",
    "sentence = '(((A|(B&C))|(D&(E&F)))|(H&I))'\n",
    "\n",
    "\"\"\"\n",
    "# build parse tree from passed sentence using grammatical rules of Boolean logic\n",
    "def buildParseTree(fpexp):\n",
    "    \"\"\"\n",
    "       Iteratively build parse tree from passed sentence using grammatical rules of Boolean logic\n",
    "       :param fpexp: sentence to parse\n",
    "       :return eTree: parse tree representation\n",
    "       Incoming Boolean sentences are parsed into a binary tree.\n",
    "       Test expressions to parse:\n",
    "       sentence = '(A&B)'\n",
    "       sentence = '(A|B)'\n",
    "       sentence = '((A|B)&C)'\n",
    "       \n",
    "    \"\"\"\n",
    "    fplist = fpexp.split()\n",
    "    pStack = Stack()\n",
    "    eTree = BinaryTree('')\n",
    "    pStack.push(eTree)\n",
    "    currentTree = eTree\n",
    "\n",
    "    for i in fplist:\n",
    "\n",
    "        if i == '(':\n",
    "            currentTree.insertLeft('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getLeftChild()\n",
    "        elif i not in ['&', '|', ')']:\n",
    "            currentTree.setRootVal(i)\n",
    "            parent = pStack.pop()\n",
    "            currentTree = parent\n",
    "        elif i in ['&', '|']:\n",
    "            currentTree.setRootVal(i)\n",
    "            currentTree.insertRight('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getRightChild()\n",
    "        elif i == ')':\n",
    "            currentTree = pStack.pop()\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    return eTree\n",
    "\n",
    "def make_parse_tree(payload):\n",
    "    \"\"\"\n",
    "    Ensure data to create tree are in correct form\n",
    "    :param sentence: sentence to preprocess\n",
    "    :return pt, parse tree graph\n",
    "            sentence, processed sentence to build tree\n",
    "            a: order\n",
    "    \"\"\"\n",
    "    def preprocess_sentence(sentence):\n",
    "        # prepare statement for case when a boolean AND/OR is given\n",
    "        sentence = payload.replace('(', ' ( '). \\\n",
    "            replace(')', ' ) '). \\\n",
    "            replace('&', ' & '). \\\n",
    "            replace('|', ' | '). \\\n",
    "            replace('  ', ' ')\n",
    "        return sentence\n",
    "\n",
    "    sentence = preprocess_sentence(payload)\n",
    "    print('Processing sentence:', sentence)\n",
    "    \n",
    "    pt = buildParseTree(sentence)\n",
    "    #pt.postorder() \n",
    "    \n",
    "    return pt\n",
    "\n",
    "class Sentence(object):\n",
    "    '''\n",
    "    Details about boolean expression -> number operators and expression\n",
    "    '''\n",
    "    def __init__(self, sentence):\n",
    "        self = self\n",
    "        self.n_and = sentence.count('&')\n",
    "        self.n_or = sentence.count('|')\n",
    "        self.sentence = sentence\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_docs(corpus):\n",
    "    \n",
    "    # KLUDGE!!!\n",
    "    if corpus == 'ray_test':\n",
    "        corpus = 'fairview'\n",
    "        \n",
    "    sql = 'select distinct note_id, sofa from sofas where corpus = %(corpus)s order by note_id'\n",
    "    df = pd.read_sql(sql, params={\"corpus\":corpus}, con=engine)\n",
    "    df.drop_duplicates()\n",
    "    df['len_doc'] = df['sofa'].apply(len)\n",
    "    \n",
    "    subset = df[['note_id', 'len_doc']]\n",
    "    docs = [tuple(x) for x in subset.to_numpy()]\n",
    "    \n",
    "    return docs\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_ref_ann(analysis_type, corpus, filter_semtype, semtype = None):\n",
    "    \n",
    "    if filter_semtype:\n",
    "        if ',' in semtype:\n",
    "            semtype = semtype.split(',')\n",
    "        else:\n",
    "            semtype = [semtype]\n",
    "        \n",
    "    ann, _ = get_metric_data(analysis_type, corpus)\n",
    "    ann = ann.rename(index=str, columns={\"start\": \"begin\", \"file\": \"case\"})\n",
    "    \n",
    "    if filter_semtype:\n",
    "        ann = ann[ann['semtype'].isin(semtype)]\n",
    "    if analysis_type == 'entity':   \n",
    "        ann[\"label\"] = 'concept'\n",
    "    elif analysis_type in ['cui','full']:\n",
    "        ann[\"label\"] = ann[\"value\"]\n",
    "        \n",
    "    if modification == 'negation':\n",
    "        ann = ann[ann['semtype'] == 'negation']\n",
    "    \n",
    "    \n",
    "    if analysis_type == 'entity':\n",
    "        cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "    elif analysis_type == 'cui':\n",
    "        cols_to_keep = ['value', 'case', 'label']\n",
    "    elif analysis_type == 'full':\n",
    "        cols_to_keep = ['begin', 'end', 'value', 'case', 'label']\n",
    "    ann = ann[cols_to_keep]\n",
    "    \n",
    "    return ann\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_sys_ann(analysis_type, r):\n",
    "    sys = r.system_merges   \n",
    "    \n",
    "    sys = sys.rename(index=str, columns={\"note_id\": \"case\"})\n",
    "    if analysis_type == 'entity':\n",
    "        sys[\"label\"] = 'concept'\n",
    "        cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "    elif analysis_type == 'full':\n",
    "        sys[\"label\"] = sys[\"cui\"]\n",
    "        cols_to_keep = ['begin', 'end', 'case', 'value', 'label']\n",
    "    elif analysis_type == 'cui':\n",
    "        sys[\"label\"] = sys[\"cui\"]\n",
    "        cols_to_keep = ['case', 'cui', 'label']\n",
    "    \n",
    "    sys = sys[cols_to_keep]\n",
    "    return sys\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_metrics(boolean_expression: str, analysis_type: str, corpus: str, run_type: str, filter_semtype, semtype = None):\n",
    "    \"\"\"\n",
    "    Traverse binary parse tree representation of Boolean sentence\n",
    "        :params: boolean expression in form of '(<annotator_engine_name1><boolean operator><annotator_engine_name2>)'\n",
    "                 analysis_type (string value of: 'entity', 'cui', 'full') used to filter set of reference and system annotations \n",
    "        :return: dictionary with values needed for confusion matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence = Sentence(boolean_expression)   \n",
    "    pt = make_parse_tree(sentence.sentence)\n",
    "    \n",
    "    if filter_semtype:\n",
    "        r = process_sentence(pt, sentence, analysis_type, corpus, filter_semtype, semtype)\n",
    "    else:\n",
    "        r = process_sentence(pt, sentence, analysis_type, corpus, filter_semtype)\n",
    "        \n",
    "    # vectorize merges using i-o labeling\n",
    "    if run_type == 'overlap':\n",
    "        if filter_semtype:\n",
    "             ((TP, TN, FP, FN),(p,r,f1)) = vectorized_cooccurences(r, analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "            ((TP, TN, FP, FN),(p,r,f1)) = vectorized_cooccurences(r, analysis_type, corpus, filter_semtype)\n",
    "        \n",
    "        print('results:',((TP, TN, FP, FN),(p,r,f1)))\n",
    "        # TODO: validate against ann1/sys1 where val = 1\n",
    "        # total by number chars\n",
    "        system_n = TP + FP\n",
    "        reference_n = TP + FN\n",
    "\n",
    "        if analysis_type != 'cui':\n",
    "            d = cm_dict(FN, FP, TP, system_n, reference_n)\n",
    "        else:\n",
    "            d = dict()\n",
    "            d['F1'] = 0\n",
    "            d['precision'] = 0 \n",
    "            d['recall'] = 0\n",
    "            d['TP/FN'] = 0\n",
    "            d['TM'] = 0\n",
    "            \n",
    "        d['TN'] = TN\n",
    "        d['macro_p'] = p\n",
    "        d['macro_r'] = r\n",
    "        d['macro_f1'] = f1\n",
    "        \n",
    "        \n",
    "        # return full metrics\n",
    "        return d\n",
    "\n",
    "    elif run_type == 'exact':\n",
    "        # total by number spans\n",
    "        \n",
    "        if filter_semtype:\n",
    "            ann = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "        else: \n",
    "            ann = get_ref_ann(analysis_type, corpus, filter_semtype)\n",
    "        \n",
    "        c = get_cooccurences(ann, r.system_merges, analysis_type, corpus) # get matches, FN, etc.\n",
    "\n",
    "        if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "            # get dictionary of confusion matrix metrics\n",
    "            d = cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n)\n",
    "        else:\n",
    "            d = None\n",
    "            \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_valid_systems(['biomedicus'], 'Anatomy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all combinations of given list of annotators:\n",
    "def partly_unordered_permutations(lst, k):\n",
    "    elems = set(lst)\n",
    "    for c in combinations(lst, k):\n",
    "        for d in permutations(elems - set(c)):\n",
    "            yield c + d\n",
    "            \n",
    "def expressions(l, n):\n",
    "    for (operations, *operands), operators in product(\n",
    "            combinations(l, n), product(('&', '|'), repeat=n - 1)):\n",
    "        for operation in zip(operators, operands):\n",
    "            operations = [operations, *operation]\n",
    "        yield operations\n",
    "\n",
    "# get list of systems with a semantic type in grouping\n",
    "def get_valid_systems(systems, semtype):\n",
    "    test = []\n",
    "    for sys in systems:\n",
    "        st = system_semtype_check(sys, semtype, corpus)\n",
    "        if st:\n",
    "            test.append(sys)\n",
    "\n",
    "    return test\n",
    "\n",
    "# permute system combinations and evaluate system merges for performance\n",
    "def run_ensemble(systems, analysis_type, corpus, filter_semtype, expression_type, semtype = None):\n",
    "    metrics = pd.DataFrame()\n",
    "    \n",
    "    # pass single system to evaluate\n",
    "    if expression_type == 'single':\n",
    "        for system in systems:\n",
    "            if filter_semtype:\n",
    "                d = get_metrics(system, analysis_type, corpus, run_type, filter_semtype, semtype)\n",
    "            else:\n",
    "                d = get_metrics(system, analysis_type, corpus, run_type, filter_semtype)\n",
    "            d['merge'] = system\n",
    "            d['n_terms'] = 1\n",
    "\n",
    "            frames = [metrics, pd.DataFrame(d, index=[0])]\n",
    "            metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "    \n",
    "    elif expression_type == 'nested':\n",
    "        for l in partly_unordered_permutations(systems, 2):\n",
    "            print('processing merge combo:', l)\n",
    "            for i in range(1, len(l)+1):\n",
    "                test = list(expressions(l, i))\n",
    "                for t in test:\n",
    "                    if i > 1:\n",
    "                        # format Boolean sentence for parse tree \n",
    "                        t = '(' + \" \".join(str(x) for x in t).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "\n",
    "                    if filter_semtype:\n",
    "                        d = get_metrics(t, analysis_type, corpus, run_type, filter_semtype, semtype)\n",
    "                    else:\n",
    "                        d = get_metrics(t, analysis_type, corpus, run_type, filter_semtype)\n",
    "\n",
    "                    d['merge'] = t\n",
    "                    d['n_terms'] = i\n",
    "\n",
    "                    frames = [metrics, pd.DataFrame(d, index=[0])]\n",
    "                    metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "                    \n",
    "    elif expression_type == 'nested_with_singleton' and len(systems) == 5:\n",
    "        # form (((a&b)|c)&(d|e))\n",
    "        \n",
    "        nested = list(expressions(systems, 3))\n",
    "        test = list(expressions(systems, 2))\n",
    "        to_do_terms = []\n",
    "    \n",
    "        for n in nested:\n",
    "            # format Boolean sentence for parse tree \n",
    "            n = '(' + \" \".join(str(x) for x in n).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "\n",
    "            for t in test:\n",
    "                t = '(' + \" \".join(str(x) for x in t).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "\n",
    "                new_and = '(' + n +'&'+ t + ')'\n",
    "                new_or = '(' + n +'|'+ t + ')'\n",
    "\n",
    "                if new_and.count('biomedicus') != 2 and new_and.count('clamp') != 2 and new_and.count('ctakes') != 2 and new_and.count('metamap') != 2 and new_and.count('quick_umls') != 2:\n",
    "\n",
    "                    if new_and.count('&') != 4 and new_or.count('|') != 4:\n",
    "                        #print(new_and)\n",
    "                        #print(new_or)\n",
    "                        to_do_terms.append(new_or)\n",
    "                        to_do_terms.append(new_and)\n",
    "        \n",
    "        print('nested_with_singleton', len(to_do_terms))\n",
    "        for term in to_do_terms:\n",
    "            if filter_semtype:\n",
    "                d = get_metrics(term, analysis_type, corpus, run_type, filter_semtype, semtype)\n",
    "            else:\n",
    "                d = get_metrics(term, analysis_type, corpus, run_type, filter_semtype)\n",
    "                \n",
    "            n = term.count('&')\n",
    "            m = term.count('|')\n",
    "            d['merge'] = term\n",
    "            d['n_terms'] = m + n + 1\n",
    "\n",
    "            frames = [metrics, pd.DataFrame(d, index=[0])]\n",
    "            metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "                        \n",
    "    elif expression_type == 'paired':\n",
    "        m = list(expressions(systems, 2))\n",
    "        test = list(expressions(m, 2))\n",
    "\n",
    "        to_do_terms = []\n",
    "        for t in test:\n",
    "            # format Boolean sentence for parse tree \n",
    "            t = '(' + \" \".join(str(x) for x in t).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "            if t.count('biomedicus') != 2 and t.count('clamp') != 2 and t.count('ctakes') != 2 and t.count('metamap') != 2 and t.count('quick_umls') != 2:\n",
    "                if t.count('&') != 3 and t.count('|') != 3:\n",
    "                    to_do_terms.append(t)\n",
    "                    if len(systems) == 5:\n",
    "                        for i in systems:\n",
    "                            if i not in t:\n",
    "                                #print('('+t+'&'+i+')')\n",
    "                                #print('('+t+'|'+i+')')\n",
    "                                new_and = '('+t+'&'+i+')'\n",
    "                                new_or = '('+t+'|'+i+')'\n",
    "                                to_do_terms.append(new_and)\n",
    "                                to_do_terms.append(new_or)\n",
    "                            \n",
    "        print('paired', len(to_do_terms))\n",
    "        for term in to_do_terms:\n",
    "            if filter_semtype:\n",
    "                d = get_metrics(term, analysis_type, corpus, run_type, filter_semtype, semtype)\n",
    "            else:\n",
    "                d = get_metrics(term, analysis_type, corpus, run_type, filter_semtype)\n",
    "                \n",
    "            n = term.count('&')\n",
    "            m = term.count('|')\n",
    "            d['merge'] = term\n",
    "            d['n_terms'] = m + n + 1\n",
    "\n",
    "            frames = [metrics, pd.DataFrame(d, index=[0])]\n",
    "            metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "        \n",
    "    return metrics\n",
    "\n",
    "# write to file\n",
    "def generate_ensemble_metrics(metrics, analysis_type, corpus, ensemble_type, filter_semtype, semtype = None):\n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    \n",
    "    file_name = corpus + '_all_'\n",
    "   \n",
    "    # drop exact matches:\n",
    "    metrics = metrics.drop_duplicates()\n",
    "    \n",
    "    if ensemble_type == 'merge':\n",
    "        metrics = metrics.sort_values(by=['n_terms', 'merge'])\n",
    "        file_name += 'merge_'\n",
    "    elif ensemble_type == 'vote':\n",
    "        file_name += 'vote_'\n",
    "    \n",
    "    #metrics = metrics.drop_duplicates(subset=['TP', 'FN', 'FP', 'n_sys', 'precision', 'recall', 'F', 'TM', 'TP/FN', 'TM', 'n_terms'])\n",
    "\n",
    "    file = file_name + analysis_type + '_' + run_type +'_'\n",
    "    \n",
    "    if filter_semtype:\n",
    "        file += semtype\n",
    "        \n",
    "    \n",
    "    geometric_mean(metrics).to_csv(analysisConf.data_dir + file + str(timestamp) + '.csv')\n",
    "    print(geometric_mean(metrics))\n",
    "    \n",
    "# control ensemble run\n",
    "def ensemble_control(systems, analysis_type, corpus, run_type, filter_semtype, semtypes = None):\n",
    "    if filter_semtype:\n",
    "        for semtype in semtypes:\n",
    "            test = get_valid_systems(systems, semtype)\n",
    "            print('SYSTEMS FOR SEMTYPE', semtype, 'ARE', test)\n",
    "            metrics = run_ensemble(test, analysis_type, corpus, filter_semtype, expression_type, semtype)\n",
    "            if (expression_type == 'nested_with_singleton' and len(test) == 5) or expression_type in ['nested', 'paired', 'single']:\n",
    "                generate_ensemble_metrics(metrics, analysis_type, corpus, ensemble_type, filter_semtype, semtype)\n",
    "    else:\n",
    "        metrics = run_ensemble(systems, analysis_type, corpus, filter_semtype, expression_type)\n",
    "        generate_ensemble_metrics(metrics, analysis_type, corpus, ensemble_type, filter_semtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ad hoc query for performance evaluation\n",
    "def get_merge_data(boolean_expression: str, analysis_type: str, corpus: str, run_type: str, filter_semtype, semtype = None):\n",
    "    \"\"\"\n",
    "    Traverse binary parse tree representation of Boolean sentence\n",
    "        :params: boolean expression in form of '(<annotator_engine_name1><boolean operator><annotator_engine_name2>)'\n",
    "                 analysis_type (string value of: 'entity', 'cui', 'full') used to filter set of reference and system annotations \n",
    "        :return: dictionary with values needed for confusion matrix\n",
    "    \"\"\"\n",
    "    if filter_semtype:\n",
    "        ann = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "    else: \n",
    "        ann = get_ref_ann(analysis_type, corpus, filter_semtype)\n",
    "    \n",
    "    sentence = Sentence(boolean_expression)   \n",
    "\n",
    "    pt = make_parse_tree(sentence.sentence)\n",
    "\n",
    "    r = process_sentence(pt, sentence, analysis_type, corpus, filter_semtype, semtype)\n",
    "\n",
    "    if run_type == 'overlap' and rtype != 6:\n",
    "        if filter_semtype:\n",
    "             ((TP, TN, FP, FN),(p,r,f1)) = vectorized_cooccurences(r, analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "             ((TP, TN, FP, FN),(p,r,f1)) = vectorized_cooccurences(r, analysis_type, corpus, filter_semtype)\n",
    "\n",
    "        # TODO: validate against ann1/sys1 where val = 1\n",
    "        # total by number chars\n",
    "        system_n = TP + FP\n",
    "        reference_n = TP + FN\n",
    "\n",
    "        d = cm_dict(FN, FP, TP, system_n, reference_n)\n",
    "        print(d)\n",
    "        \n",
    "    elif run_type == 'exact':\n",
    "        c = get_cooccurences(ann, r.system_merges, analysis_type, corpus) # get matches, FN, etc.\n",
    "\n",
    "        if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "            # get dictionary of confusion matrix metrics\n",
    "            d = cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n)\n",
    "\n",
    "            print('cm', d)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # get matched data from merge\n",
    "    return r.system_merges # merge_eval(reference_only, system_only, reference_system_match, system_n, reference_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# majority vote \n",
    "def vectorized_annotations(ann):\n",
    "    \n",
    "    docs = get_docs(corpus)\n",
    "    labels = [\"concept\"]\n",
    "    out= []\n",
    "    \n",
    "    for n in range(len(docs)):\n",
    "        a1 = list(ann.loc[ann[\"case\"] == docs[n][0]].itertuples(index=False))\n",
    "        a = label_vector(docs[n][1], a1, labels)\n",
    "        out.append(a)\n",
    "\n",
    "    return out\n",
    "\n",
    "def flatten_list(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def get_reference_vector(analysis_type, corpus, filter_semtype, semtype = None):\n",
    "    ref_ann = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "\n",
    "    df = ref_ann.copy()\n",
    "    df = df.drop_duplicates(subset=['begin','end','case'])\n",
    "    df['label'] = 'concept'\n",
    "\n",
    "    cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "    ref = df[cols_to_keep].copy()\n",
    "    test = vectorized_annotations(ref)\n",
    "    ref =  np.asarray(flatten_list(test), dtype=np.int32) \n",
    "\n",
    "    return ref\n",
    "\n",
    "def majority_overlap_sys(systems, analysis_type, corpus, filter_semtype, semtype = None):\n",
    "    \n",
    "    d = {}\n",
    "    cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "    sys_test = []\n",
    "    \n",
    "    for system in systems:\n",
    "        sys_ann = get_sys_data(system, analysis_type, corpus, filter_semtype, semtype)\n",
    "        df = sys_ann.copy()\n",
    "        df['label'] = 'concept'\n",
    "        df = df.rename(index=str, columns={\"note_id\": \"case\"})\n",
    "        sys = df[df['system']==system][cols_to_keep].copy()\n",
    "        test = vectorized_annotations(sys)\n",
    "        d[system] = flatten_list(test) \n",
    "        sys_test.append(d[system])\n",
    "\n",
    "    output = sum(np.array(sys_test))\n",
    "    \n",
    "    n = int(len(systems) / 2)\n",
    "    #print(n)\n",
    "    if ((len(systems) % 2) != 0):\n",
    "        vote = np.where(output > n, 1, 0)\n",
    "    else:\n",
    "        vote = np.where(output > n, 1, \n",
    "         (np.where(output == n, random.randint(0, 1), 0)))\n",
    "        \n",
    "    return vote\n",
    "\n",
    "def majority_overlap_vote_out(ref, vote, corpus):    \n",
    "    TP, TN, FP, FN = confused(ref, vote)\n",
    "    print(TP, TN, FP, FN)\n",
    "    system_n = TP + FP\n",
    "    reference_n = TP + FN\n",
    "\n",
    "    d = cm_dict(FN, FP, TP, system_n, reference_n)\n",
    "\n",
    "    d['TN'] = TN\n",
    "    d['corpus'] = corpus\n",
    "    print(d)\n",
    "    \n",
    "    metrics = pd.DataFrame(d, index=[0])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# control vote run\n",
    "def majority_vote(systems, analysis_type, corpus, run_type, filter_semtype, semtypes = None):\n",
    "    print(semtypes, systems)\n",
    "    if filter_semtype:\n",
    "        for semtype in semtypes:\n",
    "            test = get_valid_systems(systems, semtype)\n",
    "            print('SYSYEMS FOR SEMTYPE', semtype, 'ARE', test)\n",
    "            \n",
    "            if run_type == 'overlap':\n",
    "                ref = get_reference_vector(analysis_type, corpus, filter_semtype, semtype)\n",
    "                vote = majority_overlap_sys(test, analysis_type, corpus, filter_semtype, semtype)\n",
    "                metrics = majority_overlap_vote_out(ref, vote, corpus)\n",
    "                #generate_ensemble_metrics(metrics, analysis_type, corpus, ensemble_type, filter_semtype, semtype)\n",
    "            elif run_type == 'exact':\n",
    "                sys = majority_exact_sys(test, analysis_type, corpus, filter_semtype, semtype)\n",
    "                d = majority_exact_vote_out(sys, filter_semtype, semtype)\n",
    "                metrics = pd.DataFrame(d, index=[0])\n",
    "            elif run_type == 'cui':\n",
    "                sys = majority_cui_sys(test, analysis_type, corpus, filter_semtype, semtype)\n",
    "                d = majority_cui_vote_out(sys, filter_semtype, semtype)\n",
    "                metrics = pd.DataFrame(d, index=[0])\n",
    "           \n",
    "            metrics['systems'] = ','.join(test)\n",
    "            generate_ensemble_metrics(metrics, analysis_type, corpus, ensemble_type, filter_semtype, semtype)\n",
    "                \n",
    "    else:\n",
    "        if run_type == 'overlap':\n",
    "            ref = get_reference_vector(analysis_type, corpus, filter_semtype)\n",
    "            vote = majority_overlap_sys(systems, analysis_type, corpus, filter_semtype)\n",
    "            metrics = majority_overlap_vote_out(ref, vote, corpus)\n",
    "            \n",
    "        elif run_type == 'exact':\n",
    "            sys = majority_exact_sys(systems, analysis_type, corpus, filter_semtype)\n",
    "            d = majority_exact_vote_out(sys, filter_semtype)\n",
    "            metrics = pd.DataFrame(d, index=[0])\n",
    "            \n",
    "        elif run_type == 'cui':\n",
    "            sys = majority_cui_sys(systems, analysis_type, corpus, filter_semtype)\n",
    "            d = majority_cui_vote_out(sys, filter_semtype)\n",
    "            metrics = pd.DataFrame(d, index=[0])\n",
    "            \n",
    "        metrics['systems'] = ','.join(systems)\n",
    "        generate_ensemble_metrics(metrics, analysis_type, corpus, ensemble_type, filter_semtype)\n",
    "    \n",
    "    print(metrics)\n",
    "    \n",
    "def majority_cui_sys(systems, analysis_type, corpus, filter_semtype, semtype = None):\n",
    "   \n",
    "    cols_to_keep = ['cui', 'note_id', 'system']\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for system in systems:\n",
    "        if filter_semtype:\n",
    "            sys = get_sys_data(system, analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "            sys = get_sys_data(system, analysis_type, corpus, filter_semtype)\n",
    "            \n",
    "        sys = sys[sys['system'] == system][cols_to_keep].drop_duplicates()\n",
    "        \n",
    "        frames = [df, sys]\n",
    "        df = pd.concat(frames)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def majority_cui_vote_out(sys, filter_semtype, semtype = None):\n",
    "    \n",
    "    sys = sys.astype(str)\n",
    "    sys['value_cui'] = list(zip(sys.cui, sys.note_id.astype(str)))\n",
    "    sys['count'] = sys.groupby(['value_cui'])['value_cui'].transform('count')\n",
    "\n",
    "    n = int(len(systems) / 2)\n",
    "    if ((len(systems) % 2) != 0):\n",
    "        sys = sys[sys['count'] > n]\n",
    "    else:\n",
    "        # https://stackoverflow.com/questions/23330654/update-a-dataframe-in-pandas-while-iterating-row-by-row\n",
    "        for i in sys.index:\n",
    "            if sys.at[i, 'count'] == n:\n",
    "                sys.at[i, 'count'] = random.choice([1,len(systems)])\n",
    "        sys = sys[sys['count'] > n]\n",
    "\n",
    "    sys = sys.drop_duplicates(subset=['value_cui', 'cui', 'note_id'])\n",
    "    ref = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "\n",
    "    c = get_cooccurences(ref, sys, analysis_type, corpus) # get matches, FN, etc.\n",
    "\n",
    "    if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "        # get dictionary of confusion matrix metrics\n",
    "        print(cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n))\n",
    "        return cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n)\n",
    "    \n",
    "\n",
    "def majority_exact_sys(systems, analysis_type, corpus, filter_semtype, semtype = None):\n",
    "   \n",
    "    cols_to_keep = ['begin', 'end', 'note_id', 'system']\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for system in systems:\n",
    "        if filter_semtype:\n",
    "            sys = get_sys_data(system, analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "            sys = get_sys_data(system, analysis_type, corpus, filter_semtype)\n",
    "            \n",
    "        sys = sys[sys['system'] == system][cols_to_keep].drop_duplicates()\n",
    "        \n",
    "        frames = [df, sys]\n",
    "        df = pd.concat(frames)\n",
    "        \n",
    "    return df\n",
    "        \n",
    "def majority_exact_vote_out(sys, filter_semtype, semtype = None):\n",
    "    sys['span'] = list(zip(sys.begin, sys.end, sys.note_id.astype(str)))\n",
    "    sys['count'] = sys.groupby(['span'])['span'].transform('count')\n",
    "\n",
    "    n = int(len(systems) / 2)\n",
    "    if ((len(systems) % 2) != 0):\n",
    "        sys = sys[sys['count'] > n]\n",
    "    else:\n",
    "        # https://stackoverflow.com/questions/23330654/update-a-dataframe-in-pandas-while-iterating-row-by-row\n",
    "        for i in sys.index:\n",
    "            if sys.at[i, 'count'] == n:\n",
    "                sys.at[i, 'count'] = random.choice([1,len(systems)])\n",
    "        sys = sys[sys['count'] > n]\n",
    "\n",
    "    sys = sys.drop_duplicates(subset=['span', 'begin', 'end', 'note_id'])\n",
    "    ref = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "\n",
    "    c = get_cooccurences(ref, sys, analysis_type, corpus) # get matches, FN, etc.\n",
    "\n",
    "    if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "        # get dictionary of confusion matrix metrics\n",
    "        print(cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n))\n",
    "        return cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n)\n",
    "    \n",
    "#ensemble_type = 'vote'        \n",
    "#filter_semtype = False\n",
    "#majority_vote(systems, analysis_type, corpus, run_type, filter_semtype, semtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls'] ('analytical_mipacq.csv', 'concepts.mipacq_all')\n",
      "biomedicus st {'T065', 'T062', 'T059', 'T063', 'T058', 'T060', 'T061'}\n",
      "clamp st {'treatment', 'test'}\n",
      "out: ('biomedicus', 'clamp') 16455 7410 12824 14451 11251 28932 Procedures\n",
      "biomedicus st {'T050', 'T047', 'T190', 'T184', 'T049', 'T048', 'T033', 'T019', 'T037', 'T191', 'T046', 'T020'}\n",
      "clamp st {'labvalue', 'problem'}\n",
      "out: ('biomedicus', 'clamp') 26137 9142 22088 35848 13320 47662 Disorders,Sign_Symptom\n",
      "('biomedicus', 'clamp')has no semtype Anatomy\n",
      "biomedicus st {'T131', 'T121', 'T125', 'T109', 'T114', 'T130', 'T122', 'T126', 'T120', 'T195', 'T123', 'T103', 'T197', 'T127', 'T129', 'T104', 'T196', 'T192', 'T116', 'T200'}\n",
      "clamp st {'drug'}\n",
      "out: ('biomedicus', 'clamp') 7024 3041 4697 6655 6155 4817 Chemicals_and_drugs\n",
      "ctakes st {'ProcedureMention'}\n",
      "out: ('biomedicus', 'ctakes') 15055 6531 12824 14451 13781 8955 Procedures\n",
      "ctakes st {'SignSymptomMention', 'DiseaseDisorderMention'}\n",
      "out: ('biomedicus', 'ctakes') 25636 17082 22088 35848 18873 28217 Disorders,Sign_Symptom\n",
      "biomedicus st {'T018', 'T025', 'T024', 'T017', 'T021', 'T023', 'T022', 'T030', 'T029', 'T031', 'T026'}\n",
      "ctakes st {'AnatomicalSiteMention'}\n",
      "out: ('biomedicus', 'ctakes') 6136 3229 5554 7971 3486 4463 Anatomy\n",
      "ctakes st {'MedicationMention'}\n",
      "out: ('biomedicus', 'ctakes') 4838 4400 4697 6655 1614 9982 Chemicals_and_drugs\n",
      "metamap st {'diap', 'lbpr', 'mbrt', 'topp', 'hlca', 'edac', 'resa'}\n",
      "out: ('biomedicus', 'metamap') 15961 8331 12824 14451 11367 23031 Procedures\n",
      "metamap st {'cgab', 'sosy', 'acab', 'mobd', 'neop', 'inpo', 'dsyn', 'patf', 'anab', 'comd', 'emod', 'fndg'}\n",
      "out: ('biomedicus', 'metamap') 30261 20617 22088 35848 22648 40486 Disorders,Sign_Symptom\n",
      "metamap st {'blor', 'bdsy', 'bsoj', 'emst', 'bpoc', 'bdsu', 'tisu', 'celc', 'anst', 'cell', 'ffas'}\n",
      "out: ('biomedicus', 'metamap') 15459 2246 5554 7971 13585 4630 Anatomy\n",
      "metamap st {'chvf', 'imft', 'orch', 'hops', 'enzy', 'vita', 'phsu', 'inch', 'chem', 'elii', 'antb', 'chvs', 'nnon', 'clnd', 'bodm', 'irda', 'bacs', 'aapp', 'horm', 'rcpt'}\n",
      "out: ('biomedicus', 'metamap') 9059 3697 4697 6655 6640 13285 Chemicals_and_drugs\n",
      "quick_umls st {'T065', 'T062', 'T059', 'T063', 'T058', 'T060', 'T061'}\n",
      "out: ('biomedicus', 'quick_umls') 13836 10594 12824 14451 8922 21488 Procedures\n",
      "quick_umls st {'T050', 'T047', 'T190', 'T184', 'T049', 'T048', 'T033', 'T019', 'T037', 'T191', 'T046', 'T020'}\n",
      "out: ('biomedicus', 'quick_umls') 23236 27705 22088 35848 15726 44469 Disorders,Sign_Symptom\n",
      "quick_umls st {'T018', 'T025', 'T024', 'T017', 'T021', 'T023', 'T022', 'T030', 'T029', 'T031', 'T026'}\n",
      "out: ('biomedicus', 'quick_umls') 7037 6771 5554 7971 5614 8615 Anatomy\n",
      "quick_umls st {'T131', 'T121', 'T125', 'T109', 'T114', 'T130', 'T122', 'T126', 'T120', 'T195', 'T123', 'T103', 'T197', 'T127', 'T129', 'T104', 'T196', 'T192', 'T116', 'T200'}\n",
      "out: ('biomedicus', 'quick_umls') 4869 5202 4697 6655 4201 7269 Chemicals_and_drugs\n",
      "out: ('clamp', 'ctakes') 17312 4820 11251 28932 13781 8955 Procedures\n",
      "out: ('clamp', 'ctakes') 23877 8209 13320 47662 18873 28217 Disorders,Sign_Symptom\n",
      "('clamp', 'ctakes')has no semtype Anatomy\n",
      "out: ('clamp', 'ctakes') 6359 3206 6155 4817 1614 9982 Chemicals_and_drugs\n",
      "out: ('clamp', 'metamap') 16356 8431 11251 28932 11367 23031 Procedures\n",
      "out: ('clamp', 'metamap') 26325 11853 13320 47662 22648 40486 Disorders,Sign_Symptom\n",
      "('clamp', 'metamap')has no semtype Anatomy\n",
      "out: ('clamp', 'metamap') 10208 2537 6155 4817 6640 13285 Chemicals_and_drugs\n",
      "out: ('clamp', 'quick_umls') 14740 8968 11251 28932 8922 21488 Procedures\n",
      "out: ('clamp', 'quick_umls') 20420 13293 13320 47662 15726 44469 Disorders,Sign_Symptom\n",
      "('clamp', 'quick_umls')has no semtype Anatomy\n",
      "out: ('clamp', 'quick_umls') 6614 3228 6155 4817 4201 7269 Chemicals_and_drugs\n",
      "out: ('ctakes', 'metamap') 16745 6019 13781 8955 11367 23031 Procedures\n",
      "out: ('ctakes', 'metamap') 28459 16361 18873 28217 22648 40486 Disorders,Sign_Symptom\n",
      "out: ('ctakes', 'metamap') 14544 1046 3486 4463 13585 4630 Anatomy\n",
      "out: ('ctakes', 'metamap') 7063 5984 1614 9982 6640 13285 Chemicals_and_drugs\n",
      "out: ('ctakes', 'quick_umls') 14904 6166 13781 8955 8922 21488 Procedures\n",
      "out: ('ctakes', 'quick_umls') 21661 17568 18873 28217 15726 44469 Disorders,Sign_Symptom\n",
      "out: ('ctakes', 'quick_umls') 6379 3059 3486 4463 5614 8615 Anatomy\n",
      "out: ('ctakes', 'quick_umls') 4365 4880 1614 9982 4201 7269 Chemicals_and_drugs\n",
      "out: ('metamap', 'quick_umls') 13338 12503 11367 23031 8922 21488 Procedures\n",
      "out: ('metamap', 'quick_umls') 25971 24600 22648 40486 15726 44469 Disorders,Sign_Symptom\n",
      "out: ('metamap', 'quick_umls') 15453 2031 13585 4630 5614 8615 Anatomy\n",
      "out: ('metamap', 'quick_umls') 8731 3863 6640 13285 4201 7269 Chemicals_and_drugs\n",
      " done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "         244341608 function calls (242792478 primitive calls) in 170.677 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "       40   61.528    1.538  165.272    4.132 <ipython-input-72-4385cfd74bdf>:116(vectorized_complementarity)\n",
       "    40739   10.846    0.000   10.846    0.000 {pandas._libs.ops.scalar_compare}\n",
       " 27847836    6.863    0.000    6.863    0.000 <ipython-input-72-4385cfd74bdf>:221(<lambda>)\n",
       " 27847836    6.848    0.000    6.848    0.000 <ipython-input-72-4385cfd74bdf>:227(<lambda>)\n",
       " 27847836    6.767    0.000    6.767    0.000 <ipython-input-72-4385cfd74bdf>:226(<lambda>)\n",
       " 27847836    6.626    0.000    6.626    0.000 <ipython-input-72-4385cfd74bdf>:220(<lambda>)\n",
       " 27847836    6.384    0.000    6.384    0.000 <ipython-input-72-4385cfd74bdf>:214(<lambda>)\n",
       " 27847836    6.322    0.000    6.322    0.000 <ipython-input-72-4385cfd74bdf>:213(<lambda>)\n",
       "        1    5.427    5.427  170.699  170.699 <ipython-input-86-dfa1f957127f>:115(ad_hoc)\n",
       "11849621/11849425    2.481    0.000    4.121    0.000 {built-in method builtins.isinstance}\n",
       "  40717/1    2.278    0.000  170.699  170.699 {built-in method builtins.exec}\n",
       "    40716    1.754    0.000    4.491    0.000 __init__.py:316(namedtuple)\n",
       "  4158878    1.141    0.000    1.640    0.000 generic.py:10(_check)\n",
       "5799544/4459217    1.007    0.000    1.452    0.000 {built-in method builtins.len}\n",
       "   491739    0.987    0.000    1.908    0.000 generic.py:5276(__setattr__)\n",
       "   162864    0.967    0.000   11.602    0.000 indexing.py:1364(_getitem_lowerdim)\n",
       "6319710/6319707    0.905    0.000    0.932    0.000 {built-in method builtins.getattr}\n",
       "204658/204657    0.690    0.000    6.180    0.000 series.py:183(__init__)\n",
       "    40716    0.687    0.000    2.154    0.000 <ipython-input-72-4385cfd74bdf>:1(label_vector)\n",
       "376700/295049    0.684    0.000    1.068    0.000 {built-in method numpy.array}\n",
       "   828980    0.662    0.000    1.745    0.000 common.py:1565(is_extension_array_dtype)\n",
       "   203852    0.650    0.000    2.034    0.000 base.py:1026(__iter__)\n",
       "   163533    0.648    0.000    2.723    0.000 managers.py:979(iget)\n",
       "    82004    0.635    0.000    4.205    0.000 algorithms.py:1565(take_nd)\n",
       "   829218    0.618    0.000    0.872    0.000 dtypes.py:75(find)\n",
       "    40716    0.617    0.000    0.617    0.000 <ipython-input-72-4385cfd74bdf>:9(<listcomp>)\n",
       "  1143741    0.615    0.000    1.221    0.000 {pandas._libs.lib.is_list_like}\n",
       "       36    0.593    0.016    0.593    0.016 <ipython-input-72-4385cfd74bdf>:204(<listcomp>)\n",
       "       36    0.587    0.016    0.587    0.016 <ipython-input-72-4385cfd74bdf>:203(<listcomp>)\n",
       "   632114    0.582    0.000    0.582    0.000 {built-in method numpy.arange}\n",
       "       36    0.580    0.016    0.580    0.016 <ipython-input-72-4385cfd74bdf>:200(<listcomp>)\n",
       "    41400    0.559    0.000    1.295    0.000 managers.py:212(_rebuild_blknos_and_blklocs)\n",
       "   205164    0.556    0.000    0.556    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
       "   287294    0.540    0.000    1.252    0.000 blocks.py:118(__init__)\n",
       "   530051    0.518    0.000    2.240    0.000 common.py:99(is_bool_indexer)\n",
       "   287294    0.513    0.000    3.366    0.000 blocks.py:3027(make_block)\n",
       "   203580    0.491    0.000   32.064    0.000 indexing.py:1754(__getitem__)\n",
       "   204657    0.476    0.000    2.145    0.000 managers.py:1467(__init__)\n",
       "   290862    0.475    0.000    1.255    0.000 _dtype.py:319(_name_get)\n",
       "   652240    0.456    0.000    0.627    0.000 generic.py:409(_get_axis_name)\n",
       "327244/327236    0.438    0.000    0.748    0.000 generic.py:5257(__getattr__)\n",
       "    40716    0.433    0.000   22.713    0.001 frame.py:955(itertuples)\n",
       "   162864    0.431    0.000    6.705    0.000 frame.py:2722(_ixs)\n",
       "  3000967    0.420    0.000    0.420    0.000 {built-in method builtins.issubclass}\n",
       "    40883    0.406    0.000    0.764    0.000 indexers.py:173(maybe_convert_indices)\n",
       "   245754    0.395    0.000    0.395    0.000 generic.py:190(__init__)\n",
       "    82004    0.389    0.000    1.108    0.000 algorithms.py:1436(_get_take_nd_function)\n",
       "    81812    0.377    0.000    1.308    0.000 cast.py:347(maybe_promote)\n",
       "   327279    0.365    0.000    0.627    0.000 series.py:428(name)\n",
       "   162864    0.363    0.000    8.422    0.000 indexing.py:2116(_getitem_axis)\n",
       "  1242400    0.360    0.000    0.490    0.000 base.py:615(__len__)\n",
       "   287294    0.352    0.000    0.390    0.000 blocks.py:251(mgr_locs)\n",
       "   652240    0.350    0.000    1.155    0.000 generic.py:422(_get_axis)\n",
       "   343667    0.346    0.000    0.874    0.000 common.py:1708(_is_dtype_type)\n",
       "  1148145    0.340    0.000    0.340    0.000 {built-in method _abc._abc_instancecheck}\n",
       "   409626    0.338    0.000    0.494    0.000 managers.py:1548(dtype)\n",
       "   581740    0.329    0.000    0.483    0.000 numerictypes.py:293(issubclass_)\n",
       "    40883    0.329    0.000   10.782    0.000 managers.py:1373(take)\n",
       "   325728    0.301    0.000    2.181    0.000 indexing.py:1976(_validate_key)\n",
       "   434068    0.281    0.000    0.818    0.000 {built-in method builtins.hasattr}\n",
       "   166293    0.276    0.000    0.276    0.000 {built-in method numpy.empty}\n",
       "      888    0.275    0.000    0.275    0.000 {method 'recv_into' of '_socket.socket' objects}\n",
       "   325728    0.270    0.000    1.070    0.000 indexing.py:2045(_validate_integer)\n",
       "   290870    0.269    0.000    0.780    0.000 numerictypes.py:365(issubdtype)\n",
       "  1148145    0.268    0.000    0.609    0.000 abc.py:137(__instancecheck__)\n",
       "   162864    0.265    0.000    0.469    0.000 indexing.py:2016(_is_scalar_access)\n",
       "    82942    0.264    0.000    0.773    0.000 blocks.py:343(ftype)\n",
       "   204658    0.257    0.000    0.522    0.000 series.py:376(_set_axis)\n",
       "   165208    0.251    0.000    0.603    0.000 managers.py:163(shape)\n",
       "    81812    0.248    0.000    5.273    0.000 blocks.py:1271(take_nd)\n",
       "   248152    0.238    0.000    1.056    0.000 common.py:222(is_object_dtype)\n",
       "    40787    0.237    0.000    0.237    0.000 {pandas._libs.algos.take_2d_axis1_int64_int64}\n",
       "   203580    0.235    0.000   15.646    0.000 frame.py:1026(<genexpr>)\n",
       "   490479    0.234    0.000    0.364    0.000 managers.py:1565(internal_values)\n",
       "   164317    0.231    0.000    0.382    0.000 base.py:3911(__getitem__)\n",
       "    40716    0.230    0.000    0.710    0.000 <ipython-input-72-4385cfd74bdf>:8(<listcomp>)\n",
       "   162864    0.227    0.000    2.408    0.000 indexing.py:695(_has_valid_tuple)\n",
       "    40883    0.226    0.000   11.730    0.000 generic.py:3300(take)\n",
       "   163533    0.224    0.000    2.541    0.000 frame.py:3073(_box_col_values)\n",
       "   592167    0.222    0.000    0.222    0.000 {built-in method __new__ of type object at 0x1087bc778}\n",
       "    42169    0.208    0.000    0.689    0.000 frame.py:2767(__getitem__)\n",
       "   488592    0.208    0.000    0.338    0.000 indexing.py:1756(<genexpr>)\n",
       "    41322    0.199    0.000    0.458    0.000 cast.py:1088(maybe_castable)\n",
       "   245920    0.197    0.000    2.093    0.000 blocks.py:275(make_block_same_class)\n",
       "   286503    0.191    0.000    0.353    0.000 series.py:432(name)\n",
       "    40716    0.190    0.000   16.606    0.000 indexing.py:1902(_getitem_axis)\n",
       "   123743    0.184    0.000    0.699    0.000 blocks.py:2591(__init__)\n",
       "    41870    0.182    0.000    1.128    0.000 blocks.py:2981(get_block_type)\n",
       "   162864    0.181    0.000    0.756    0.000 indexing.py:710(_is_nested_tuple_indexer)\n",
       "   488592    0.181    0.000    0.717    0.000 indexers.py:23(is_list_like_indexer)\n",
       "    41075    0.180    0.000    1.660    0.000 indexers.py:278(check_array_indexer)\n",
       "        1    0.178    0.178    0.180    0.180 {method 'read' of 'pandas._libs.parsers.TextReader' objects}\n",
       "    40768    0.178    0.000    0.178    0.000 {pandas._libs.algos.take_2d_axis1_object_object}\n",
       "   549049    0.175    0.000    0.422    0.000 __init__.py:403(_make)\n",
       "   205121    0.174    0.000    0.887    0.000 construction.py:337(extract_array)\n",
       "    41323    0.174    0.000    1.162    0.000 construction.py:388(sanitize_array)\n",
       "   409626    0.173    0.000    0.667    0.000 series.py:414(dtype)\n",
       "   262581    0.172    0.000    0.670    0.000 base.py:247(is_dtype)\n",
       "    82942    0.167    0.000    0.483    0.000 _dtype.py:46(__str__)\n",
       "      755    0.163    0.000    0.163    0.000 {method 'copy' of 'numpy.ndarray' objects}\n",
       "   983213    0.162    0.000    0.162    0.000 managers.py:1520(_block)\n",
       "    81834    0.161    0.000    0.273    0.000 numpy_.py:138(__init__)\n",
       "    81962    0.159    0.000    1.010    0.000 base.py:4046(equals)\n",
       "    83033    0.156    0.000    0.166    0.000 base.py:3897(__contains__)\n",
       "   860916    0.156    0.000    0.156    0.000 {method 'get' of 'dict' objects}\n",
       "   325728    0.154    0.000    0.388    0.000 indexing.py:2394(is_label_like)\n",
       "   372284    0.153    0.000    0.219    0.000 inference.py:358(is_hashable)\n",
       "    82167    0.153    0.000    0.257    0.000 generic.py:5235(__finalize__)\n",
       "   490479    0.153    0.000    0.517    0.000 series.py:480(_values)\n",
       "   529831    0.149    0.000    0.726    0.000 inference.py:220(is_array_like)\n",
       "    40747    0.147    0.000   16.953    0.000 __init__.py:518(wrapper)\n",
       "    40883    0.143    0.000   13.066    0.000 generic.py:3399(_take_with_is_copy)\n",
       "   488592    0.141    0.000    0.382    0.000 indexing.py:716(<genexpr>)\n",
       "    40717    0.139    0.000    0.139    0.000 {built-in method numpy.zeros}\n",
       "    41324    0.137    0.000    2.923    0.000 managers.py:122(__init__)\n",
       "    40883    0.136    0.000    8.488    0.000 managers.py:1224(reindex_indexer)\n",
       "    40864    0.135    0.000    0.135    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
       "    41058    0.134    0.000    0.462    0.000 managers.py:329(_verify_integrity)\n",
       "    40747    0.132    0.000   11.819    0.000 array_ops.py:202(comparison_op)\n",
       "    41096    0.131    0.000    0.321    0.000 frame.py:414(__init__)\n",
       "    40751    0.130    0.000    4.250    0.000 __init__.py:448(_construct_result)\n",
       "   162864    0.130    0.000   14.140    0.000 indexing.py:2065(_getitem_tuple)\n",
       "   408982    0.129    0.000    0.173    0.000 common.py:330(apply_if_callable)\n",
       "   163533    0.126    0.000    0.352    0.000 generic.py:3227(_set_as_cached)\n",
       "    41361    0.121    0.000    0.473    0.000 base.py:526(_shallow_copy)\n",
       "    40739    0.120    0.000   11.105    0.000 array_ops.py:42(comp_method_OBJECT_ARRAY)\n",
       "   287294    0.120    0.000    0.120    0.000 blocks.py:129(_check_ndim)\n",
       "    84054    0.119    0.000    0.437    0.000 common.py:1435(is_bool_dtype)\n",
       "    40716    0.119    0.000   15.265    0.000 indexing.py:1779(_getbool_axis)\n",
       "   495624    0.118    0.000    0.352    0.000 managers.py:165(<genexpr>)\n",
       "   163466    0.115    0.000    0.501    0.000 {built-in method builtins.any}\n",
       "   574847    0.114    0.000    0.114    0.000 blocks.py:339(dtype)\n",
       "    40883    0.112    0.000    0.757    0.000 base.py:744(take)\n",
       "   288054    0.112    0.000    0.218    0.000 base.py:5293(ensure_index)\n",
       "   163533    0.112    0.000    0.112    0.000 blocks.py:365(iget)\n",
       "331552/249901    0.111    0.000    1.026    0.000 numeric.py:469(asarray)\n",
       "   162864    0.109    0.000    6.814    0.000 indexing.py:627(_get_loc)\n",
       "    40787    0.106    0.000    5.387    0.000 managers.py:1260(<listcomp>)\n",
       "   162864    0.106    0.000    0.120    0.000 common.py:284(is_null_slice)\n",
       "      152    0.105    0.001    0.105    0.001 managers.py:1959(<listcomp>)\n",
       "   204904    0.101    0.000    0.725    0.000 {method 'any' of 'numpy.ndarray' objects}\n",
       "   287003    0.095    0.000    0.127    0.000 managers.py:167(ndim)\n",
       "    40751    0.094    0.000   17.206    0.000 common.py:49(new_method)\n",
       "   165997    0.093    0.000    0.093    0.000 {method 'view' of 'numpy.ndarray' objects}\n",
       "   249501    0.092    0.000    0.121    0.000 common.py:1672(_get_dtype)\n",
       "   534653    0.091    0.000    0.091    0.000 blocks.py:247(mgr_locs)\n",
       "   165373    0.090    0.000    0.184    0.000 base.py:3638(values)\n",
       "    81834    0.089    0.000    0.089    0.000 numpy_.py:42(__init__)\n",
       "   339042    0.089    0.000    0.132    0.000 common.py:208(<lambda>)\n",
       "    41401    0.087    0.000    0.924    0.000 managers.py:655(_consolidate_check)\n",
       "   339042    0.086    0.000    0.086    0.000 common.py:206(classes)\n",
       "   162864    0.085    0.000    0.160    0.000 generic.py:515(ndim)\n",
       "   129675    0.081    0.000    0.115    0.000 common.py:1844(pandas_dtype)\n",
       "   162864    0.079    0.000    0.079    0.000 frame.py:515(axes)\n",
       "   204658    0.079    0.000    0.079    0.000 series.py:403(_set_subtyp)\n",
       "    40797    0.078    0.000    0.078    0.000 {built-in method pandas._libs.lib.is_datetime_array}\n",
       "    41075    0.078    0.000    2.125    0.000 indexing.py:2286(check_bool_indexer)\n",
       "    19049    0.076    0.000    0.248    0.000 connections.py:1195(_read_row_from_packet)\n",
       "    40883    0.076    0.000    0.076    0.000 {method 'take' of 'numpy.ndarray' objects}\n",
       "   530493    0.075    0.000    0.075    0.000 {pandas._libs.lib.is_integer}\n",
       "   455317    0.075    0.000    0.075    0.000 {built-in method builtins.hash}\n",
       "    81834    0.075    0.000    0.348    0.000 blocks.py:219(array_values)\n",
       "    41636    0.074    0.000    0.074    0.000 {method 'settimeout' of '_socket.socket' objects}\n",
       "    42510    0.074    0.000    0.178    0.000 base.py:472(_simple_new)\n",
       "    41871    0.074    0.000    0.152    0.000 common.py:252(is_sparse)\n",
       "   412052    0.073    0.000    0.075    0.000 {pandas._libs.lib.is_scalar}\n",
       "    81812    0.072    0.000    0.273    0.000 cast.py:503(_ensure_dtype_type)\n",
       "   127179    0.071    0.000    0.404    0.000 common.py:403(is_datetime64tz_dtype)\n",
       "   204904    0.070    0.000    0.624    0.000 _methods.py:42(_any)\n",
       "    81834    0.069    0.000    0.437    0.000 series.py:515(array)\n",
       "   136308    0.069    0.000    0.173    0.000 managers.py:314(__len__)\n",
       "    40818    0.068    0.000   15.714    0.000 {method 'extend' of 'list' objects}\n",
       "    90517    0.067    0.000    0.148    0.000 <frozen importlib._bootstrap>:1009(_handle_fromlist)\n",
       "   164029    0.067    0.000    0.090    0.000 common.py:151(cast_scalar_indexer)\n",
       "    40787    0.066    0.000    0.066    0.000 {built-in method builtins.repr}\n",
       "    45196    0.065    0.000    0.172    0.000 dtypes.py:1124(is_dtype)\n",
       "   326821    0.064    0.000    0.064    0.000 {method 'startswith' of 'str' objects}\n",
       "   490479    0.064    0.000    0.064    0.000 blocks.py:213(internal_values)\n",
       "    40751    0.063    0.000    0.365    0.000 dispatch.py:21(should_extension_dispatch)\n",
       "    40916    0.062    0.000    0.111    0.000 {method 'join' of 'str' objects}\n",
       "   162864    0.061    0.000    0.061    0.000 indexing.py:93(iloc)\n",
       "    83201    0.058    0.000    0.171    0.000 {pandas._libs.lib.values_from_object}\n",
       "    40787    0.058    0.000    0.496    0.000 series.py:707(__array__)\n",
       "    82734    0.058    0.000    0.115    0.000 missing.py:132(_isna_new)\n",
       "    82876    0.057    0.000    0.057    0.000 {method 'fill' of 'numpy.ndarray' objects}\n",
       "    41002    0.056    0.000    0.114    0.000 missing.py:402(array_equivalent)\n",
       "    82373    0.055    0.000    0.131    0.000 common.py:685(is_dtype_equal)\n",
       "    43847    0.055    0.000    0.134    0.000 common.py:372(is_datetime64_dtype)\n",
       "   327272    0.055    0.000    0.055    0.000 {pandas._libs.lib.item_from_zerodim}\n",
       "    41401    0.055    0.000    0.828    0.000 managers.py:656(<listcomp>)\n",
       "    41047    0.054    0.000    0.098    0.000 numpy_.py:417(to_numpy)\n",
       "    20467    0.053    0.000    0.514    0.000 connections.py:648(_read_packet)\n",
       "     1862    0.050    0.000    0.059    0.000 {pandas._libs.lib.infer_dtype}\n",
       "    40797    0.050    0.000    0.187    0.000 base.py:1737(is_all_dates)\n",
       "    40885    0.050    0.000    0.221    0.000 generic.py:5331(_protect_consolidate)\n",
       "    81962    0.049    0.000    0.062    0.000 base.py:573(is_)\n",
       "      820    0.049    0.000    0.049    0.000 socket.py:337(send)\n",
       "   203580    0.049    0.000    0.049    0.000 __init__.py:388(<genexpr>)\n",
       "   123200    0.049    0.000    0.075    0.000 managers.py:331(<genexpr>)\n",
       "    40934    0.048    0.000    0.423    0.000 connections.py:687(_read_bytes)\n",
       "    40795    0.046    0.000    0.082    0.000 generic.py:3627(_set_is_copy)\n",
       "    95673    0.045    0.000    0.052    0.000 protocol.py:63(read)\n",
       "    43374    0.045    0.000    0.120    0.000 base.py:5393(maybe_extract_name)\n",
       "   409721    0.044    0.000    0.044    0.000 {built-in method builtins.callable}\n",
       "    41095    0.043    0.000    0.051    0.000 generic.py:219(_init_mgr)\n",
       "    41324    0.042    0.000    0.080    0.000 managers.py:128(<listcomp>)\n",
       "   366444    0.042    0.000    0.042    0.000 {method 'isidentifier' of 'str' objects}\n",
       "    97110    0.041    0.000    0.148    0.000 protocol.py:168(read_length_coded_string)\n",
       "    41323    0.041    0.000    0.515    0.000 construction.py:506(_try_cast)\n",
       "    41324    0.041    0.000    0.041    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
       "   179268    0.040    0.000    0.040    0.000 managers.py:232(items)\n",
       "    40920    0.040    0.000    0.040    0.000 {method 'replace' of 'str' objects}\n",
       "    41524    0.039    0.000    0.039    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
       "    41420    0.039    0.000    0.054    0.000 generic.py:396(_get_axis_number)\n",
       "    40724    0.038    0.000    0.274    0.000 base.py:3949(_can_hold_identifiers_and_holds_name)\n",
       "       26    0.038    0.001    0.038    0.001 {method 'factorize' of 'pandas._libs.hashtable.StringHashTable' objects}\n",
       "    40751    0.037    0.000    0.102    0.000 __init__.py:101(get_op_result_name)\n",
       "    40885    0.037    0.000    0.163    0.000 generic.py:5344(f)\n",
       "    41870    0.037    0.000    0.106    0.000 base.py:505(_get_attributes_dict)\n",
       "    41035    0.036    0.000    0.089    0.000 generic.py:426(_get_block_manager_axis)\n",
       "       23    0.036    0.002    0.036    0.002 {pandas._libs.hashtable.ismember_object}\n",
       "   246494    0.036    0.000    0.036    0.000 {pandas._libs.lib.is_float}\n",
       "    45260    0.035    0.000    0.139    0.000 dtypes.py:917(is_dtype)\n",
       "   366444    0.035    0.000    0.035    0.000 {method '__contains__' of 'frozenset' objects}\n",
       "    44855    0.034    0.000    0.091    0.000 common.py:441(is_timedelta64_dtype)\n",
       "    40838    0.034    0.000    0.129    0.000 construction.py:570(is_empty_data)\n",
       "   123092    0.034    0.000    0.036    0.000 managers.py:647(is_consolidated)\n",
       "    41058    0.034    0.000    0.109    0.000 {built-in method builtins.sum}\n",
       "    82734    0.033    0.000    0.149    0.000 missing.py:49(isna)\n",
       "    46032    0.033    0.000    0.170    0.000 common.py:542(is_categorical_dtype)\n",
       "    82441    0.033    0.000    0.033    0.000 {built-in method pandas._libs.missing.checknull}\n",
       "    82087    0.033    0.000    0.033    0.000 {pandas._libs.algos.ensure_int64}\n",
       "    40787    0.032    0.000    0.076    0.000 numpy_.py:183(__array__)\n",
       "    82208    0.032    0.000    0.093    0.000 managers.py:943(_consolidate_inplace)\n",
       "    41870    0.031    0.000    0.069    0.000 base.py:509(<dictcomp>)\n",
       "   326215    0.031    0.000    0.031    0.000 {method 'add' of 'set' objects}\n",
       "    98201    0.031    0.000    0.056    0.000 protocol.py:150(read_length_encoded_integer)\n",
       "    82004    0.030    0.000    0.111    0.000 base.py:3700(_internal_get_values)\n",
       "    40885    0.030    0.000    0.251    0.000 generic.py:5341(_consolidate_inplace)\n",
       "    40816    0.030    0.000    0.102    0.000 base.py:766(_assert_take_fillable)\n",
       "    46346    0.030    0.000    0.169    0.000 common.py:472(is_period_dtype)\n",
       "       19    0.029    0.002    0.746    0.039 <ipython-input-77-92e351042532>:27(get_sys_data)\n",
       "    46282    0.028    0.000    0.201    0.000 common.py:506(is_interval_dtype)\n",
       "    41870    0.028    0.000    0.231    0.000 common.py:339(is_categorical)\n",
       "    42557    0.028    0.000    0.070    0.000 generic.py:3581(_get_item_cache)\n",
       "    82167    0.027    0.000    0.027    0.000 generic.py:238(attrs)\n",
       "    40724    0.027    0.000    0.139    0.000 base.py:1679(is_object)\n",
       "    40885    0.026    0.000    0.088    0.000 managers.py:927(consolidate)\n",
       "       36    0.026    0.001    0.026    0.001 {built-in method numpy.where}\n",
       "    98202    0.025    0.000    0.025    0.000 protocol.py:117(read_uint8)\n",
       "      107    0.025    0.000    0.485    0.005 connections.py:1182(_read_rowdata_packet)\n",
       "      102    0.024    0.000    0.030    0.000 managers.py:1828(_stack_arrays)\n",
       "   163152    0.023    0.000    0.023    0.000 base.py:626(ndim)\n",
       "       41    0.023    0.001    0.023    0.001 {method 'factorize' of 'pandas._libs.hashtable.Int64HashTable' objects}\n",
       "1976/1051    0.023    0.000    0.182    0.000 base.py:276(__new__)\n",
       "    40812    0.023    0.000    0.023    0.000 indexing.py:231(loc)\n",
       "    42617    0.022    0.000    0.022    0.000 base.py:592(_reset_identity)\n",
       "    41722    0.022    0.000    0.506    0.000 inference.py:96(is_iterator)\n",
       "    82154    0.022    0.000    0.022    0.000 blocks.py:335(shape)\n",
       "    40934    0.021    0.000    0.301    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
       "    94766    0.021    0.000    0.021    0.000 {method 'decode' of 'bytes' objects}\n",
       "    81812    0.020    0.000    0.020    0.000 blocks.py:243(fill_value)\n",
       "   123296    0.020    0.000    0.020    0.000 base.py:3670(_values)\n",
       "        1    0.020    0.020    0.020    0.020 {method 'get_labels_groupby' of 'pandas._libs.hashtable.Int64HashTable' objects}\n",
       "    40724    0.018    0.000    0.036    0.000 generic.py:491(_info_axis)\n",
       "    42633    0.018    0.000    0.018    0.000 base.py:1182(name)\n",
       "   158822    0.018    0.000    0.018    0.000 {method 'append' of 'list' objects}\n",
       "    40716    0.017    0.000    0.057    0.000 indexing.py:1870(_get_partial_string_timestamp_match_key)\n",
       "       21    0.017    0.001    0.017    0.001 {pandas._libs.hashtable.duplicated_int64}\n",
       "    41520    0.017    0.000    0.142    0.000 numeric.py:541(asanyarray)\n",
       "    40716    0.017    0.000    0.017    0.000 {built-in method sys._getframe}\n",
       "    81963    0.016    0.000    0.016    0.000 {pandas._libs.algos.ensure_platform_int}\n",
       "    40716    0.016    0.000    0.016    0.000 {built-in method sys.intern}\n",
       "    42215    0.015    0.000    0.015    0.000 {method 'update' of 'dict' objects}\n",
       "      152    0.015    0.000    0.034    0.000 blocks.py:3085(_merge_blocks)\n",
       "      525    0.015    0.000    0.018    0.000 cast.py:1458(construct_1d_object_array_from_listlike)\n",
       "      460    0.014    0.000    0.014    0.000 {built-in method numpy.concatenate}\n",
       "    40884    0.014    0.000    0.014    0.000 function.py:42(__call__)\n",
       "    41861    0.014    0.000    0.014    0.000 base.py:638(dtype)\n",
       "      699    0.013    0.000    0.013    0.000 {method 'sendall' of '_socket.socket' objects}\n",
       "      487    0.013    0.000    0.015    0.000 {pandas._libs.lib.maybe_convert_objects}\n",
       "    41102    0.011    0.000    0.011    0.000 series.py:359(_constructor)\n",
       "    40997    0.011    0.000    0.011    0.000 frame.py:399(_constructor)\n",
       "       47    0.010    0.000    0.010    0.000 {pandas._libs.algos.take_2d_axis0_float64_float64}\n",
       "    42208    0.008    0.000    0.008    0.000 {pandas._libs.algos.ensure_object}\n",
       "    20569    0.008    0.000    0.008    0.000 {built-in method _struct.unpack}\n",
       "    41525    0.008    0.000    0.008    0.000 {method 'items' of 'dict' objects}\n",
       "    20468    0.007    0.000    0.015    0.000 protocol.py:214(check_error)\n",
       "    42098    0.007    0.000    0.007    0.000 base.py:1383(nlevels)\n",
       "    19156    0.006    0.000    0.011    0.000 connections.py:1137(_check_packet_is_eof)\n",
       "       98    0.006    0.000    0.006    0.000 construction.py:484(<listcomp>)\n",
       "    20468    0.006    0.000    0.006    0.000 protocol.py:211(is_error_packet)\n",
       "    20467    0.006    0.000    0.006    0.000 protocol.py:56(__init__)\n",
       " 3906/288    0.006    0.000    0.011    0.000 arrayprint.py:716(recurser)\n",
       "       21    0.005    0.000    0.031    0.001 sorting.py:21(get_group_index)\n",
       "      369    0.005    0.000    0.053    0.000 managers.py:1037(set)\n",
       "    19370    0.004    0.000    0.004    0.000 protocol.py:190(is_eof_packet)\n",
       "      196    0.004    0.000    0.009    0.000 {pandas._libs.lib.clean_index_list}\n",
       "        1    0.004    0.004    0.981    0.981 <ipython-input-74-ae69ae0eb560>:1(get_metric_data)\n",
       "     1717    0.004    0.000    0.027    0.000 common.py:219(asarray_tuplesafe)\n",
       "      492    0.004    0.000    0.008    0.000 protocol.py:283(__init__)\n",
       "     1065    0.004    0.000    0.022    0.000 cast.py:1209(maybe_cast_to_datetime)\n",
       "       68    0.004    0.000    0.004    0.000 sorting.py:62(maybe_lift)\n",
       "       96    0.004    0.000    0.585    0.006 <ipython-input-65-6b0f59a80c87>:8(__init__)\n",
       "      115    0.003    0.000    0.003    0.000 {pandas._libs.algos.take_2d_axis0_object_object}\n",
       "       98    0.003    0.000    1.079    0.011 sql.py:336(read_sql)\n",
       "    25599    0.003    0.000    0.005    0.000 strings.py:1691(<lambda>)\n",
       "      987    0.003    0.000    0.014    0.000 cast.py:1102(maybe_infer_to_datetimelike)\n",
       "        4    0.003    0.001    0.008    0.002 {pandas._libs.lib.map_infer_mask}\n",
       "      697    0.003    0.000    0.023    0.000 connections.py:744(_execute_command)\n",
       "        1    0.003    0.003    0.228    0.228 parsers.py:416(_read)\n",
       "       99    0.003    0.000    0.053    0.001 managers.py:1700(form_blocks)\n",
       "       21    0.003    0.000    0.195    0.009 frame.py:4772(drop_duplicates)\n",
       "     3618    0.003    0.000    0.004    0.000 arrayprint.py:693(_extendLine)\n",
       "      293    0.003    0.000    0.003    0.000 {built-in method pandas._libs.missing.isnaobj}\n",
       "       98    0.003    0.000    0.003    0.000 {pandas._libs.lib.to_object_array_tuples}\n",
       "     1667    0.003    0.000    0.003    0.000 {method 'get_loc' of 'pandas._libs.index.IndexEngine' objects}\n",
       "      888    0.003    0.000    0.279    0.000 socket.py:575(readinto)\n",
       "        4    0.003    0.001    1.054    0.264 <ipython-input-81-c9fd36f58387>:106(get_ref_ann)\n",
       "       68    0.002    0.000    0.096    0.001 algorithms.py:585(factorize)\n",
       "      313    0.002    0.000    0.005    0.000 base.py:1656(is_unique)\n",
       "     3630    0.002    0.000    0.003    0.000 base.py:194(construct_from_string)\n",
       "        1    0.002    0.002    0.002    0.002 {method 'factorize' of 'pandas._libs.hashtable.Float64HashTable' objects}\n",
       "       98    0.002    0.000    0.025    0.000 base.py:1343(_handle_dbapi_exception)\n",
       "      820    0.002    0.000    0.053    0.000 iostream.py:195(schedule)\n",
       "      782    0.002    0.000    0.057    0.000 iostream.py:382(write)\n",
       "      504    0.002    0.000    0.009    0.000 protocol.py:237(_parse_field_descriptor)\n",
       "      102    0.002    0.000    0.002    0.000 result.py:1192(<listcomp>)\n",
       "     1103    0.002    0.000    0.003    0.000 protocol.py:180(read_struct)\n",
       "       96    0.002    0.000    0.025    0.000 managers.py:1274(_slice_take_blocks_ax0)\n",
       "      275    0.002    0.000    0.193    0.001 managers.py:368(apply)\n",
       "     3952    0.002    0.000    0.020    0.000 common.py:987(is_datetime64_any_dtype)\n",
       "      288    0.002    0.000    0.022    0.000 {method 'get_value' of 'pandas._libs.index.IndexEngine' objects}\n",
       "       98    0.002    0.000    0.144    0.001 frame.py:1549(from_records)\n",
       "      107    0.002    0.000    0.019    0.000 connections.py:1213(_get_descriptions)\n",
       "      293    0.002    0.000    0.012    0.000 missing.py:225(_isna_ndarraylike)\n",
       "      288    0.002    0.000    0.034    0.000 base.py:4372(get_value)\n",
       "     1721    0.002    0.000    0.002    0.000 _internal.py:886(npy_ctypes_check)\n",
       "      200    0.002    0.000    0.789    0.004 base.py:1163(_execute_context)\n",
       "      196    0.002    0.000    0.002    0.000 {method 'get_indexer' of 'pandas._libs.index.IndexEngine' objects}\n",
       "      288    0.002    0.000    0.105    0.000 series.py:868(__getitem__)\n",
       "      196    0.002    0.000    0.062    0.000 base.py:748(_checkout)\n",
       "      731    0.002    0.000    0.003    0.000 blocks.py:3053(_extend_blocks)\n",
       "      197    0.002    0.000    0.004    0.000 base.py:69(__init__)\n",
       "      288    0.002    0.000    0.015    0.000 arrayprint.py:480(_array2string)\n",
       "      432    0.002    0.000    0.043    0.000 base.py:544(_shallow_copy_with_infer)\n",
       "      669    0.002    0.000    0.016    0.000 managers.py:950(get)\n",
       "     1873    0.002    0.000    0.004    0.000 common.py:830(is_signed_integer_dtype)\n",
       "      619    0.002    0.000    0.016    0.000 base.py:5463(_maybe_cast_data_without_dtype)\n",
       "    25599    0.001    0.000    0.001    0.000 {method 'strip' of 'str' objects}\n",
       "      979    0.001    0.000    0.001    0.000 {pandas._libs.lib.infer_datetimelike_array}\n",
       "     4625    0.001    0.000    0.002    0.000 common.py:216(<lambda>)\n",
       "      492    0.001    0.000    0.072    0.000 connections.py:393(_read_ok_packet)\n",
       "      196    0.001    0.000    0.047    0.000 base.py:2706(get_indexer)\n",
       "       98    0.001    0.000    0.176    0.002 sql.py:121(_wrap_result)\n",
       "        1    0.001    0.001    0.001    0.001 parsers.py:1864(__init__)\n",
       "      175    0.001    0.000    0.001    0.000 blocks.py:368(set)\n",
       "     2107    0.001    0.000    0.004    0.000 common.py:1401(is_float_dtype)\n",
       "      557    0.001    0.000    0.006    0.000 base.py:602(_engine)\n",
       "      152    0.001    0.000    0.208    0.001 managers.py:184(rename_axis)\n",
       "      699    0.001    0.000    0.018    0.000 connections.py:710(_write_bytes)\n",
       "       76    0.001    0.000    0.266    0.003 generic.py:932(rename)\n",
       "        1    0.001    0.001    0.001    0.001 {built-in method _socket.getaddrinfo}\n",
       "      674    0.001    0.000    0.031    0.000 frame.py:890(items)\n",
       "       60    0.001    0.000    0.058    0.001 {built-in method builtins.print}\n",
       "       96    0.001    0.000    0.003    0.000 indexing.py:1600(_validate_read_indexer)\n",
       "      288    0.001    0.000    0.020    0.000 arrayprint.py:518(array2string)\n",
       "      651    0.001    0.000    0.006    0.000 base.py:1001(tolist)\n",
       "      355    0.001    0.000    0.005    0.000 numeric.py:53(__new__)\n",
       "     1433    0.001    0.000    0.003    0.000 common.py:775(is_integer_dtype)\n",
       "      669    0.001    0.000    0.014    0.000 frame.py:3066(_box_item_values)\n",
       "      200    0.001    0.000    0.003    0.000 default.py:862(_init_statement)\n",
       "       77    0.001    0.000    0.003    0.000 index_tricks.py:316(__getitem__)\n",
       "       21    0.001    0.000    0.158    0.008 frame.py:4829(duplicated)\n",
       "     1590    0.001    0.000    0.004    0.000 base.py:2637(get_loc)\n",
       "       77    0.001    0.000    0.038    0.000 managers.py:1158(insert)\n",
       "       98    0.001    0.000    0.002    0.000 schema.py:3753(__init__)\n",
       "       95    0.001    0.000    0.001    0.000 {pandas._libs.algos.take_2d_axis0_int64_int64}\n",
       "      813    0.001    0.000    0.008    0.000 base.py:5409(_maybe_cast_with_dtype)\n",
       "       98    0.001    0.000    0.005    0.000 construction.py:331(extract_index)\n",
       "       68    0.001    0.000    0.082    0.001 algorithms.py:456(_factorize_array)\n",
       "      178    0.001    0.000    0.002    0.000 numeric.py:676(require)\n",
       "      196    0.001    0.000    0.019    0.000 base.py:481(checkout)\n",
       "       96    0.001    0.000    0.052    0.001 indexing.py:1503(_get_listlike_indexer)\n",
       "      537    0.001    0.000    0.033    0.000 numeric.py:107(_shallow_copy)\n",
       "      192    0.001    0.000    0.001    0.000 {pandas._libs.algos.take_1d_int64_int64}\n",
       "      133    0.001    0.000    0.002    0.000 numeric.py:2551(array_equal)\n",
       "      288    0.001    0.000    0.048    0.000 managers.py:1538(get_slice)\n",
       "      288    0.001    0.000    0.142    0.000 series.py:4416(dropna)\n",
       "       98    0.001    0.000    0.052    0.001 base.py:2312(has_table)\n",
       "      288    0.001    0.000    0.002    0.000 arrayprint.py:411(_get_format_function)\n",
       "       98    0.001    0.000    0.031    0.000 construction.py:488(_list_to_arrays)\n",
       "      820    0.001    0.000    0.002    0.000 threading.py:1080(is_alive)\n",
       "      575    0.001    0.000    0.168    0.000 blocks.py:694(copy)\n",
       "      288    0.001    0.000    0.001    0.000 arrayprint.py:358(_get_formatdict)\n",
       "      295    0.001    0.000    0.047    0.000 base.py:2223(do_rollback)\n",
       "     1319    0.001    0.000    0.002    0.000 common.py:887(is_unsigned_integer_dtype)\n",
       "      203    0.001    0.000    0.002    0.000 cursors.py:116(_escape_args)\n",
       "     4625    0.001    0.000    0.001    0.000 common.py:211(classes_and_not_datetimelike)\n",
       "      288    0.001    0.000    0.054    0.000 series.py:982(_get_values)\n",
       "      288    0.001    0.000    0.014    0.000 generic.py:3255(_maybe_update_cacher)\n",
       "      288    0.001    0.000    0.016    0.000 arrayprint.py:463(wrapper)\n",
       "      456    0.001    0.000    0.002    0.000 blocks.py:169(_consolidate_key)\n",
       "      266    0.001    0.000    0.200    0.001 managers.py:766(copy)\n",
       "       99    0.001    0.000    0.047    0.000 construction.py:300(_homogenize)\n",
       "      196    0.001    0.000    0.002    0.000 queue.py:135(get)\n",
       "      298    0.001    0.000    0.027    0.000 range.py:387(_shallow_copy)\n",
       "      288    0.001    0.000    0.016    0.000 missing.py:299(notna)\n",
       "      487    0.001    0.000    0.026    0.000 construction.py:586(convert)\n",
       "      102    0.001    0.000    0.004    0.000 result.py:215(__init__)\n",
       "      196    0.001    0.000    0.068    0.000 base.py:2223(_contextual_connect)\n",
       "      288    0.001    0.000    0.058    0.000 series.py:912(_get_with)\n",
       "      288    0.001    0.000    0.016    0.000 generic.py:3998(_update_inplace)\n",
       "      288    0.001    0.000    0.124    0.000 missing.py:578(remove_na_arraylike)\n",
       "      830    0.001    0.000    0.005    0.000 base.py:853(_ndarray_values)\n",
       "      492    0.001    0.000    0.001    0.000 {built-in method numpy.copyto}\n",
       "      492    0.001    0.000    0.002    0.000 numeric.py:293(full)\n",
       "      288    0.001    0.000    0.002    0.000 arrayprint.py:69(_make_options_dict)\n",
       "      205    0.001    0.000    0.749    0.004 connections.py:508(query)\n",
       "      158    0.001    0.000    0.001    0.000 numerictypes.py:578(_can_coerce_all)\n",
       "       98    0.001    0.000    0.001    0.000 base.py:2260(is_disconnect)\n",
       "      391    0.001    0.000    0.001    0.000 threading.py:335(notify)\n",
       "      469    0.001    0.000    0.001    0.000 {method 'match' of 're.Pattern' objects}\n",
       "      196    0.001    0.000    0.002    0.000 queue.py:92(put)\n",
       "      152    0.001    0.000    0.162    0.001 managers.py:1941(_transform_index)\n",
       "      196    0.001    0.000    0.041    0.000 connections.py:532(ping)\n",
       "      532    0.001    0.000    0.007    0.000 base.py:660(view)\n",
       "      620    0.001    0.000    0.002    0.000 managers.py:1568(get_values)\n",
       "     1822    0.001    0.000    0.001    0.000 range.py:675(__len__)\n",
       "       98    0.001    0.000    0.003    0.000 sql.py:1068(__init__)\n",
       "       76    0.001    0.000    0.038    0.001 managers.py:1875(_consolidate)\n",
       "      492    0.001    0.000    0.007    0.000 common.py:575(is_string_dtype)\n",
       "      196    0.001    0.000    0.003    0.000 base.py:507(checkin)\n",
       "      288    0.001    0.000    0.001    0.000 {built-in method builtins.locals}\n",
       "      888    0.001    0.000    0.001    0.000 {method '_checkReadable' of '_io._IOBase' objects}\n",
       "      288    0.001    0.000    0.002    0.000 numeric.py:261(_convert_scalar_indexer)\n",
       "      288    0.001    0.000    0.035    0.000 range.py:685(__getitem__)\n",
       "       98    0.001    0.000    0.002    0.000 exc.py:390(instance)\n",
       "      782    0.001    0.000    0.001    0.000 iostream.py:307(_is_master_process)\n",
       "       99    0.001    0.000    0.002    0.000 range.py:83(__new__)\n",
       "     1091    0.001    0.000    0.001    0.000 protocol.py:186(is_ok_packet)\n",
       "      196    0.001    0.000    0.034    0.000 base.py:645(_finalize_fairy)\n",
       "      785    0.001    0.000    0.007    0.000 common.py:1647(_is_dtype)\n",
       "       81    0.001    0.000    0.011    0.000 frame.py:3570(_sanitize_column)\n",
       "      366    0.001    0.000    0.001    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
       "      288    0.001    0.000    0.013    0.000 generic.py:3238(_maybe_cache_changed)\n",
       "      200    0.001    0.000    0.790    0.004 base.py:1138(_execute_text)\n",
       "      295    0.001    0.000    0.046    0.000 connections.py:422(rollback)\n",
       "      288    0.001    0.000    0.001    0.000 blocks.py:311(_slice)\n",
       "       98    0.001    0.000    0.030    0.000 sql.py:100(_parse_date_columns)\n",
       "      196    0.001    0.000    0.031    0.000 base.py:845(_reset)\n",
       "       98    0.001    0.000    0.039    0.000 construction.py:437(to_arrays)\n",
       "       98    0.001    0.000    0.120    0.001 base.py:2133(run_callable)\n",
       "      288    0.001    0.000    0.020    0.000 arrayprint.py:1499(_array_str_implementation)\n",
       "      590    0.001    0.000    0.001    0.000 protocol.py:86(advance)\n",
       "       77    0.001    0.000    0.004    0.000 managers.py:1963(_fast_count_smallints)\n",
       "     1103    0.001    0.000    0.001    0.000 {method 'unpack_from' of 'Struct' objects}\n",
       "      706    0.001    0.000    0.001    0.000 weakref.py:395(__getitem__)\n",
       "      766    0.001    0.000    0.001    0.000 {built-in method builtins.min}\n",
       "       98    0.001    0.000    0.952    0.010 sql.py:1167(read_query)\n",
       "       98    0.001    0.000    0.018    0.000 base.py:729(_rollback_impl)\n",
       "      205    0.001    0.000    0.003    0.000 cursors.py:135(mogrify)\n",
       "      498    0.001    0.000    0.001    0.000 range.py:155(_data)\n",
       "      710    0.001    0.000    0.001    0.000 {built-in method _struct.pack}\n",
       "      820    0.001    0.000    0.001    0.000 {method 'acquire' of '_thread.lock' objects}\n",
       "      114    0.001    0.000    0.158    0.001 generic.py:5706(copy)\n",
       "      601    0.001    0.000    0.001    0.000 result.py:461(_colnames_from_description)\n",
       "      205    0.001    0.000    0.001    0.000 connections.py:1053(__init__)\n",
       "      504    0.001    0.000    0.001    0.000 protocol.py:253(description)\n",
       "      205    0.001    0.000    0.742    0.004 connections.py:1073(read)\n",
       "       98    0.000    0.000    0.001    0.000 result.py:588(_key_fallback)\n",
       "      363    0.000    0.000    0.001    0.000 dtypes.py:867(construct_from_string)\n",
       "     1008    0.000    0.000    0.001    0.000 protocol.py:264(get_column_length)\n",
       "       98    0.000    0.000    0.001    0.000 pymysql.py:64(is_disconnect)\n",
       "     3618    0.000    0.000    0.000    0.000 arrayprint.py:1154(__call__)\n",
       "      197    0.000    0.000    0.001    0.000 base.py:77(__init__)\n",
       "      205    0.000    0.000    0.750    0.004 cursors.py:324(_query)\n",
       "      601    0.000    0.000    0.001    0.000 result.py:560(_merge_cols_by_none)\n",
       "      597    0.000    0.000    0.001    0.000 generic.py:3294(_clear_item_cache)\n",
       "      226    0.000    0.000    0.002    0.000 langhelpers.py:852(__get__)\n",
       "      205    0.000    0.000    0.754    0.004 cursors.py:151(execute)\n",
       "      820    0.000    0.000    0.001    0.000 threading.py:1038(_wait_for_tstate_lock)\n",
       "      460    0.000    0.000    0.005    0.000 common.py:608(is_excluded_dtype)\n",
       "      532    0.000    0.000    0.007    0.000 managers.py:784(copy_func)\n",
       "      200    0.000    0.000    0.790    0.004 base.py:922(execute)\n",
       "     1380    0.000    0.000    0.004    0.000 common.py:613(<genexpr>)\n",
       "      205    0.000    0.000    0.743    0.004 connections.py:720(_read_query_result)\n",
       "       98    0.000    0.000    0.001    0.000 exc.py:462(__init__)\n",
       "      233    0.000    0.000    0.001    0.000 shape_base.py:83(atleast_2d)\n",
       "       99    0.000    0.000    0.062    0.001 managers.py:1667(create_block_manager_from_arrays)\n",
       "      504    0.000    0.000    0.009    0.000 protocol.py:233(__init__)\n",
       "       71    0.000    0.000    0.148    0.002 frame.py:2824(_getitem_bool_array)\n",
       "      708    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}\n",
       "       99    0.000    0.000    0.115    0.001 construction.py:56(arrays_to_mgr)\n",
       "      196    0.000    0.000    0.035    0.000 base.py:869(close)\n",
       "      363    0.000    0.000    0.001    0.000 dtypes.py:1072(construct_from_string)\n",
       "       98    0.000    0.000    0.001    0.000 err.py:100(raise_mysql_exception)\n",
       "      172    0.000    0.000    0.047    0.000 base.py:4488(get_indexer_for)\n",
       "      107    0.000    0.000    0.001    0.000 range.py:131(_simple_new)\n",
       "      107    0.000    0.000    0.505    0.005 connections.py:1149(_read_result_packet)\n",
       "      248    0.000    0.000    0.002    0.000 indexing.py:2261(convert_to_index_sliceable)\n",
       "      196    0.000    0.000    0.001    0.000 base.py:123(_join)\n",
       "       96    0.000    0.000    0.020    0.000 base.py:3100(reindex)\n",
       "      197    0.000    0.000    0.002    0.000 base.py:295(__get__)\n",
       "      363    0.000    0.000    0.001    0.000 dtypes.py:715(construct_from_string)\n",
       "      102    0.000    0.000    0.002    0.000 result.py:441(<listcomp>)\n",
       "      102    0.000    0.000    0.004    0.000 result.py:740(_init_metadata)\n",
       "      288    0.000    0.000    0.002    0.000 blocks.py:3111(_safe_reshape)\n",
       "       98    0.000    0.000    0.000    0.000 sql.py:39(_is_sqlalchemy_connectable)\n",
       "       98    0.000    0.000    0.001    0.000 base.py:183(execution_options)\n",
       "       98    0.000    0.000    0.120    0.001 sql.py:1339(has_table)\n",
       "      288    0.000    0.000    0.001    0.000 base.py:2852(_convert_scalar_indexer)\n",
       "      589    0.000    0.000    0.000    0.000 attr.py:267(__bool__)\n",
       "      492    0.000    0.000    0.006    0.000 common.py:605(condition)\n",
       "       77    0.000    0.000    0.028    0.000 base.py:4972(insert)\n",
       "       76    0.000    0.000    0.266    0.004 frame.py:4000(rename)\n",
       "       21    0.000    0.000    0.000    0.000 {built-in method _operator.inv}\n",
       "       98    0.000    0.000    0.004    0.000 sql.py:570(pandasSQL_builder)\n",
       "      196    0.000    0.000    0.017    0.000 impl.py:111(_do_get)\n",
       "      288    0.000    0.000    0.011    0.000 arrayprint.py:707(_formatArray)\n",
       "      200    0.000    0.000    0.002    0.000 default.py:1034(create_cursor)\n",
       "      363    0.000    0.000    0.000    0.000 dtypes.py:334(construct_from_string)\n",
       "      651    0.000    0.000    0.000    0.000 {method 'tolist' of 'numpy.ndarray' objects}\n",
       "      889    0.000    0.000    0.000    0.000 socket.py:614(readable)\n",
       "      820    0.000    0.000    0.000    0.000 iostream.py:93(_event_pipe)\n",
       "      499    0.000    0.000    0.001    0.000 type_api.py:483(_cached_result_processor)\n",
       "      196    0.000    0.000    0.000    0.000 base.py:580(get_connection)\n",
       "      196    0.000    0.000    0.003    0.000 base.py:1667(is_boolean)\n",
       "      152    0.000    0.000    0.001    0.000 managers.py:171(set_axis)\n",
       "      200    0.000    0.000    0.000    0.000 {built-in method sqlalchemy.cutils._distill_params}\n",
       "      196    0.000    0.000    0.000    0.000 log.py:56(_should_log_debug)\n",
       "       76    0.000    0.000    0.000    0.000 blocks.py:3098(<listcomp>)\n",
       "      620    0.000    0.000    0.002    0.000 series.py:520(_internal_get_values)\n",
       "       81    0.000    0.000    0.054    0.001 frame.py:2922(__setitem__)\n",
       "      205    0.000    0.000    0.000    0.000 cursors.py:40(__init__)\n",
       "      462    0.000    0.000    0.001    0.000 protocol.py:122(read_uint16)\n",
       "      492    0.000    0.000    0.001    0.000 protocol.py:297(__getattr__)\n",
       "       76    0.000    0.000    0.000    0.000 {method 'argsort' of 'numpy.ndarray' objects}\n",
       "      197    0.000    0.000    0.000    0.000 _collections.py:140(__new__)\n",
       "      496    0.000    0.000    0.001    0.000 managers.py:1831(_asarray_compat)\n",
       "      102    0.000    0.000    0.001    0.000 default.py:947(should_autocommit)\n",
       "       98    0.000    0.000    0.753    0.008 base.py:2149(execute)\n",
       "       81    0.000    0.000    0.052    0.001 frame.py:2988(_set_item)\n",
       "      410    0.000    0.000    0.000    0.000 cursors.py:89(_nextset)\n",
       "      499    0.000    0.000    0.001    0.000 default.py:1051(get_result_processor)\n",
       "      110    0.000    0.000    0.002    0.000 deprecations.py:117(warned)\n",
       "      196    0.000    0.000    0.035    0.000 base.py:831(_checkin)\n",
       "       98    0.000    0.000    0.001    0.000 exc.py:328(__init__)\n",
       "       98    0.000    0.000    0.003    0.000 base.py:5387(default_index)\n",
       "      175    0.000    0.000    0.001    0.000 blocks.py:2655(should_store)\n",
       "      107    0.000    0.000    0.001    0.000 protocol.py:308(__init__)\n",
       "      182    0.000    0.000    0.003    0.000 algorithms.py:64(_ensure_data)\n",
       "      620    0.000    0.000    0.001    0.000 blocks.py:240(to_dense)\n",
       "      196    0.000    0.000    0.062    0.000 base.py:345(connect)\n",
       "      196    0.000    0.000    0.041    0.000 mysqldb.py:120(do_ping)\n",
       "       68    0.000    0.000    0.016    0.000 algorithms.py:264(_check_object_for_strings)\n",
       "      178    0.000    0.000    0.000    0.000 numeric.py:748(<setcomp>)\n",
       "      888    0.000    0.000    0.000    0.000 {method '_checkClosed' of '_io._IOBase' objects}\n",
       "      288    0.000    0.000    0.017    0.000 series.py:409(_update_inplace)\n",
       "      205    0.000    0.000    0.001    0.000 connections.py:481(cursor)\n",
       "       77    0.000    0.000    0.002    0.000 {built-in method builtins.sorted}\n",
       "      288    0.000    0.000    0.000    0.000 arrayprint.py:365(<lambda>)\n",
       "      154    0.000    0.000    0.001    0.000 function_base.py:4641(append)\n",
       "       98    0.000    0.000    0.022    0.000 result.py:1195(fetchall)\n",
       "      203    0.000    0.000    0.754    0.004 default.py:551(do_execute)\n",
       "      107    0.000    0.000    0.000    0.000 cursors.py:341(_do_get_result)\n",
       "      462    0.000    0.000    0.000    0.000 {built-in method _struct.unpack_from}\n",
       "        4    0.000    0.000    0.001    0.000 series.py:628(__array_ufunc__)\n",
       "       99    0.000    0.000    0.032    0.000 managers.py:1797(_simple_blockify)\n",
       "      200    0.000    0.000    0.000    0.000 default.py:1002(_use_server_side_cursor)\n",
       "        1    0.000    0.000    0.021    0.021 sorting.py:373(compress_group_index)\n",
       "      454    0.000    0.000    0.001    0.000 numeric.py:83(_validate_dtype)\n",
       "       98    0.000    0.000    0.000    0.000 compat.py:123(reraise)\n",
       "        1    0.000    0.000    0.000    0.000 <ipython-input-81-c9fd36f58387>:102(<listcomp>)\n",
       "       96    0.000    0.000    0.000    0.000 managers.py:1970(_preprocess_slice_or_indexer)\n",
       "      197    0.000    0.000    0.000    0.000 log.py:59(_should_log_info)\n",
       "      782    0.000    0.000    0.001    0.000 iostream.py:320(_schedule_flush)\n",
       "      266    0.000    0.000    0.008    0.000 managers.py:790(<listcomp>)\n",
       "      197    0.000    0.000    0.001    0.000 base.py:116(_for_class)\n",
       "      493    0.000    0.000    0.000    0.000 protocol.py:77(read_all)\n",
       "      394    0.000    0.000    0.000    0.000 __init__.py:1619(isEnabledFor)\n",
       "      271    0.000    0.000    0.000    0.000 {pandas._libs.internals.get_blkno_placements}\n",
       "       76    0.000    0.000    0.000    0.000 inference.py:395(is_sequence)\n",
       "      192    0.000    0.000    0.000    0.000 {built-in method numpy.can_cast}\n",
       "      927    0.000    0.000    0.000    0.000 cursors.py:71(_get_db)\n",
       "       98    0.000    0.000    0.027    0.000 construction.py:592(<listcomp>)\n",
       "       98    0.000    0.000    0.000    0.000 schema.py:4027(_bind_to)\n",
       "      196    0.000    0.000    0.001    0.000 base.py:949(cursor)\n",
       "      196    0.000    0.000    0.002    0.000 impl.py:102(_do_return_conn)\n",
       "      102    0.000    0.000    0.004    0.000 result.py:714(__init__)\n",
       "      288    0.000    0.000    0.000    0.000 arrayprint.py:74(<dictcomp>)\n",
       "      392    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}\n",
       "       98    0.000    0.000    0.000    0.000 _collections.py:151(union)\n",
       "      456    0.000    0.000    0.003    0.000 managers.py:1881(<lambda>)\n",
       "      288    0.000    0.000    0.000    0.000 arrayprint.py:1149(__init__)\n",
       "       98    0.000    0.000    0.001    0.000 compat.py:378(raise_from_cause)\n",
       "       98    0.000    0.000    0.018    0.000 base.py:865(_autorollback)\n",
       "      117    0.000    0.000    0.002    0.000 cast.py:1524(maybe_cast_to_integer_array)\n",
       "      196    0.000    0.000    0.035    0.000 base.py:987(close)\n",
       "      410    0.000    0.000    0.001    0.000 cursors.py:106(nextset)\n",
       "       97    0.000    0.000    0.000    0.000 {method 'translate' of 'str' objects}\n",
       "      500    0.000    0.000    0.000    0.000 cast.py:1483(construct_1d_ndarray_preserving_na)\n",
       "      363    0.000    0.000    0.000    0.000 dtype.py:182(construct_from_string)\n",
       "      288    0.000    0.000    0.001    0.000 base.py:619(shape)\n",
       "      167    0.000    0.000    0.000    0.000 {pandas._libs.lib.array_equivalent_object}\n",
       "       79    0.000    0.000    0.001    0.000 numerictypes.py:602(find_common_type)\n",
       "       77    0.000    0.000    0.013    0.000 base.py:3809(_coerce_scalar_to_index)\n",
       "      100    0.000    0.000    0.001    0.000 compiler.py:3464(quote_identifier)\n",
       "       97    0.000    0.000    0.001    0.000 connections.py:469(escape_string)\n",
       "       98    0.000    0.000    0.001    0.000 base.py:2054(_quote_free_identifiers)\n",
       "      196    0.000    0.000    0.063    0.000 base.py:2259(_wrap_pool_connect)\n",
       "      102    0.000    0.000    0.019    0.000 result.py:869(_soft_close)\n",
       "       98    0.000    0.000    0.000    0.000 exc.py:24(__init__)\n",
       "      102    0.000    0.000    0.005    0.000 default.py:1092(get_result_proxy)\n",
       "       98    0.000    0.000    0.001    0.000 range.py:170(_int64index)\n",
       "       76    0.000    0.000    0.267    0.004 _decorators.py:225(wrapper)\n",
       "      200    0.000    0.000    0.001    0.000 base.py:1327(_safe_close_cursor)\n",
       "      196    0.000    0.000    0.000    0.000 queue.py:195(_full)\n",
       "       98    0.000    0.000    0.052    0.001 base.py:1591(run_callable)\n",
       "       98    0.000    0.000    0.001    0.000 connections.py:448(escape)\n",
       "       98    0.000    0.000    0.000    0.000 base.py:169(_clone)\n",
       "      314    0.000    0.000    0.004    0.000 base.py:1730(inferred_type)\n",
       "       99    0.000    0.000    0.000    0.000 common.py:183(all_none)\n",
       "      391    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}\n",
       "      205    0.000    0.000    0.000    0.000 cursors.py:51(close)\n",
       "      100    0.000    0.000    0.000    0.000 compiler.py:3425(_escape_identifier)\n",
       "      102    0.000    0.000    0.003    0.000 result.py:334(_merge_cursor_description)\n",
       "      369    0.000    0.000    0.000    0.000 generic.py:3656(_check_setitem_copy)\n",
       "       81    0.000    0.000    0.041    0.001 generic.py:3623(_set_item)\n",
       "      198    0.000    0.000    0.000    0.000 common.py:179(ensure_python_int)\n",
       "      154    0.000    0.000    0.001    0.000 fromnumeric.py:1583(ravel)\n",
       "       96    0.000    0.000    0.002    0.000 base.py:2966(_convert_listlike_indexer)\n",
       "       98    0.000    0.000    0.027    0.000 construction.py:574(_convert_object_array)\n",
       "       98    0.000    0.000    0.000    0.000 sql.py:57(_convert_params)\n",
       "      293    0.000    0.000    0.001    0.000 common.py:1369(is_string_like_dtype)\n",
       "       23    0.000    0.000    0.038    0.002 algorithms.py:390(isin)\n",
       "       68    0.000    0.000    0.097    0.001 frame.py:4861(f)\n",
       "      585    0.000    0.000    0.000    0.000 managers.py:1544(index)\n",
       "      288    0.000    0.000    0.000    0.000 series.py:370(_can_hold_na)\n",
       "      197    0.000    0.000    0.000    0.000 missing.py:601(clean_reindex_fill_method)\n",
       "      197    0.000    0.000    0.001    0.000 base.py:119(_for_instance)\n",
       "      365    0.000    0.000    0.000    0.000 base.py:4722(_maybe_cast_indexer)\n",
       "      309    0.000    0.000    0.000    0.000 _validators.py:207(validate_bool_kwarg)\n",
       "      494    0.000    0.000    0.000    0.000 {method 'discard' of 'set' objects}\n",
       "       98    0.000    0.000    0.001    0.000 base.py:2057(<listcomp>)\n",
       "      288    0.000    0.000    0.001    0.000 base.py:61(_reset_cache)\n",
       "       23    0.000    0.000    0.041    0.002 series.py:4241(isin)\n",
       "     1016    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
       "      196    0.000    0.000    0.002    0.000 base.py:366(_return_conn)\n",
       "      196    0.000    0.000    0.000    0.000 base.py:715(__init__)\n",
       "      991    0.000    0.000    0.000    0.000 numeric.py:155(is_all_dates)\n",
       "      102    0.000    0.000    0.005    0.000 default.py:1108(_setup_crud_result_proxy)\n",
       "       68    0.000    0.000    0.010    0.000 algorithms.py:171(_reconstruct_data)\n",
       "      289    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
       "      102    0.000    0.000    0.002    0.000 result.py:1178(process_rows)\n",
       "  281/280    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'connect' of '_socket.socket' objects}\n",
       "       96    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}\n",
       "      196    0.000    0.000    0.000    0.000 base.py:261(__init__)\n",
       "      195    0.000    0.000    0.000    0.000 queue.py:203(_get)\n",
       "      107    0.000    0.000    0.000    0.000 base.py:1186(name)\n",
       "      196    0.000    0.000    0.000    0.000 attr.py:255(__call__)\n",
       "      196    0.000    0.000    0.001    0.000 base.py:4505(_maybe_promote)\n",
       "       98    0.000    0.000    0.754    0.008 sql.py:1085(execute)\n",
       "       98    0.000    0.000    0.000    0.000 cursors.py:299(fetchall)\n",
       "      102    0.000    0.000    0.000    0.000 result.py:266(<listcomp>)\n",
       "      364    0.000    0.000    0.001    0.000 series.py:548(__len__)\n",
       "      294    0.000    0.000    0.000    0.000 base.py:958(__getattr__)\n",
       "      196    0.000    0.000    0.000    0.000 base.py:364(invalidated)\n",
       "      102    0.000    0.000    0.001    0.000 base.py:1174(should_autocommit_text)\n",
       "      152    0.000    0.000    0.001    0.000 common.py:464(get_rename_function)\n",
       "      783    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
       "      820    0.000    0.000    0.000    0.000 threading.py:507(is_set)\n",
       "      649    0.000    0.000    0.000    0.000 numerictypes.py:587(<listcomp>)\n",
       "      138    0.000    0.000    0.001    0.000 {method 'all' of 'numpy.ndarray' objects}\n",
       "      192    0.000    0.000    0.001    0.000 cursors.py:122(<genexpr>)\n",
       "       76    0.000    0.000    0.014    0.000 shape_base.py:229(vstack)\n",
       "      308    0.000    0.000    0.000    0.000 base.py:429(<genexpr>)\n",
       "      152    0.000    0.000    0.000    0.000 cast.py:549(infer_dtype_from_scalar)\n",
       "      155    0.000    0.000    0.000    0.000 {method 'transpose' of 'numpy.ndarray' objects}\n",
       "       26    0.000    0.000    0.000    0.000 fromnumeric.py:69(_wrapreduction)\n",
       "      363    0.000    0.000    0.000    0.000 {method 'search' of 're.Pattern' objects}\n",
       "       98    0.000    0.000    0.000    0.000 <string>:1(<lambda>)\n",
       "       89    0.000    0.000    0.005    0.000 frame.py:4887(<genexpr>)\n",
       "      184    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
       "       76    0.000    0.000    0.007    0.000 cast.py:1390(cast_scalar_to_array)\n",
       "      440    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
       "       97    0.000    0.000    0.001    0.000 connections.py:462(literal)\n",
       "      288    0.000    0.000    0.000    0.000 managers.py:1572(_can_hold_na)\n",
       "      597    0.000    0.000    0.000    0.000 {method 'clear' of 'dict' objects}\n",
       "      205    0.000    0.000    0.000    0.000 cursors.py:332(_clear_result)\n",
       "      196    0.000    0.000    0.000    0.000 {built-in method sys.exc_info}\n",
       "       98    0.000    0.000    0.000    0.000 base.py:479(_still_open_and_connection_is_valid)\n",
       "       71    0.000    0.000    0.000    0.000 generic.py:1797(__hash__)\n",
       "      363    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}\n",
       "       98    0.000    0.000    0.000    0.000 langhelpers.py:59(__enter__)\n",
       "       99    0.000    0.000    0.000    0.000 pymysql.py:76(_extract_error_code)\n",
       "      442    0.000    0.000    0.000    0.000 fromnumeric.py:2847(ndim)\n",
       "      293    0.000    0.000    0.000    0.000 common.py:1398(<lambda>)\n",
       "       97    0.000    0.000    0.000    0.000 converters.py:68(_escape_unicode)\n",
       "       98    0.000    0.000    0.000    0.000 protocol.py:94(rewind)\n",
       "       76    0.000    0.000    0.000    0.000 shape_base.py:220(_warn_for_nonsequence)\n",
       "       76    0.000    0.000    0.000    0.000 shape_base.py:283(<listcomp>)\n",
       "       76    0.000    0.000    0.001    0.000 fromnumeric.py:942(argsort)\n",
       "      196    0.000    0.000    0.000    0.000 queue.py:199(_put)\n",
       "       68    0.000    0.000    0.017    0.000 algorithms.py:255(_get_data_algo)\n",
       "       21    0.000    0.000    0.002    0.000 generic.py:1911(__array_wrap__)\n",
       "       77    0.000    0.000    0.000    0.000 {built-in method numpy.bincount}\n",
       "      392    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}\n",
       "      196    0.000    0.000    0.000    0.000 base.py:355(closed)\n",
       "       96    0.000    0.000    0.001    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
       "      149    0.000    0.000    0.000    0.000 frame.py:1037(__len__)\n",
       "      100    0.000    0.000    0.000    0.000 range.py:316(dtype)\n",
       "      196    0.000    0.000    0.000    0.000 queue.py:191(_empty)\n",
       "       47    0.000    0.000    0.000    0.000 {built-in method numpy.min_scalar_type}\n",
       "       98    0.000    0.000    0.000    0.000 result.py:1161(_fetchall_impl)\n",
       "      205    0.000    0.000    0.000    0.000 connections.py:1069(__del__)\n",
       "        1    0.000    0.000    0.000    0.000 {pandas._libs.lib.map_infer}\n",
       "       96    0.000    0.000    0.000    0.000 default.py:894(<listcomp>)\n",
       "      490    0.000    0.000    0.000    0.000 base.py:155(_root)\n",
       "      107    0.000    0.000    0.000    0.000 cursors.py:355(_show_warnings)\n",
       "      126    0.000    0.000    0.000    0.000 generic.py:1899(<genexpr>)\n",
       "        9    0.000    0.000    0.001    0.000 blocks.py:554(astype)\n",
       "       96    0.000    0.000    0.001    0.000 base.py:3003(_convert_arr_indexer)\n",
       "       78    0.000    0.000    0.000    0.000 managers.py:1562(external_values)\n",
       "      301    0.000    0.000    0.000    0.000 base.py:370(connection)\n",
       "       96    0.000    0.000    0.000    0.000 <ipython-input-65-6b0f59a80c87>:51(get_system_type)\n",
       "      198    0.000    0.000    0.000    0.000 common.py:187(<genexpr>)\n",
       "      103    0.000    0.000    0.000    0.000 default.py:943(no_parameters)\n",
       "       79    0.000    0.000    0.000    0.000 numerictypes.py:654(<listcomp>)\n",
       "       21    0.000    0.000    0.003    0.000 generic.py:1440(__neg__)\n",
       "       98    0.000    0.000    0.017    0.000 base.py:180(__exit__)\n",
       "       76    0.000    0.000    0.000    0.000 fromnumeric.py:54(_wrapfunc)\n",
       "       64    0.000    0.000    0.001    0.000 common.py:1282(needs_i8_conversion)\n",
       "      288    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}\n",
       "       98    0.000    0.000    0.000    0.000 langhelpers.py:62(__exit__)\n",
       "      502    0.000    0.000    0.000    0.000 {pandas._libs.lib.is_bool}\n",
       "       98    0.000    0.000    0.000    0.000 sql.py:552(_engine_builder)\n",
       "      369    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}\n",
       "      461    0.000    0.000    0.000    0.000 base.py:609(<lambda>)\n",
       "       20    0.000    0.000    0.000    0.000 numeric.py:2656(seterr)\n",
       "      138    0.000    0.000    0.001    0.000 _methods.py:45(_all)\n",
       "       96    0.000    0.000    0.001    0.000 _methods.py:34(_sum)\n",
       "       98    0.000    0.000    0.000    0.000 managers.py:199(_is_single_block)\n",
       "      391    0.000    0.000    0.000    0.000 {method '_is_owned' of '_thread.RLock' objects}\n",
       "      197    0.000    0.000    0.000    0.000 missing.py:73(clean_fill_method)\n",
       "       21    0.000    0.000    0.001    0.000 base.py:2436(difference)\n",
       "       68    0.000    0.000    0.001    0.000 algorithms.py:247(_get_values_for_rank)\n",
       "       77    0.000    0.000    0.000    0.000 base.py:621(__array__)\n",
       "      299    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}\n",
       "      102    0.000    0.000    0.000    0.000 result.py:263(<listcomp>)\n",
       "      102    0.000    0.000    0.000    0.000 range.py:210(start)\n",
       "      236    0.000    0.000    0.000    0.000 common.py:471(f)\n",
       "       22    0.000    0.000    0.000    0.000 sorting.py:54(_int64_cut_off)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'close' of 'pandas._libs.parsers.TextReader' objects}\n",
       "      102    0.000    0.000    0.000    0.000 managers.py:1837(_shape_compat)\n",
       "       98    0.000    0.000    0.000    0.000 default.py:477(set_connection_execution_options)\n",
       "       96    0.000    0.000    0.000    0.000 base.py:5360(_ensure_has_len)\n",
       "       42    0.000    0.000    0.000    0.000 generic.py:1852(empty)\n",
       "      102    0.000    0.000    0.000    0.000 range.py:233(stop)\n",
       "       78    0.000    0.000    0.000    0.000 series.py:438(values)\n",
       "        4    0.000    0.000    0.000    0.000 cast.py:799(astype_nansafe)\n",
       "       81    0.000    0.000    0.000    0.000 frame.py:3046(_ensure_valid_index)\n",
       "       98    0.000    0.000    0.000    0.000 result.py:760(keys)\n",
       "      179    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
       "       65    0.000    0.000    0.000    0.000 common.py:1108(is_datetime_or_timedelta_dtype)\n",
       "      197    0.000    0.000    0.000    0.000 _collections.py:145(__init__)\n",
       "       98    0.000    0.000    0.000    0.000 elements.py:4322(_string_or_unprintable)\n",
       "       98    0.000    0.000    0.000    0.000 base.py:709(in_transaction)\n",
       "       20    0.000    0.000    0.000    0.000 numeric.py:2758(geterr)\n",
       "      275    0.000    0.000    0.000    0.000 managers.py:419(<dictcomp>)\n",
       "       22    0.000    0.000    0.000    0.000 fromnumeric.py:2664(prod)\n",
       "       96    0.000    0.000    0.000    0.000 <ipython-input-65-6b0f59a80c87>:14(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 socket.py:139(__init__)\n",
       "        9    0.000    0.000    0.001    0.000 generic.py:5563(astype)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:786(_request_authentication)\n",
       "       98    0.000    0.000    0.000    0.000 langhelpers.py:56(__init__)\n",
       "       20    0.000    0.000    0.000    0.000 inference.py:299(is_dict_like)\n",
       "       96    0.000    0.000    0.000    0.000 base.py:3043(_convert_list_indexer)\n",
       "      195    0.000    0.000    0.000    0.000 {method 'popleft' of 'collections.deque' objects}\n",
       "      291    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
       "      101    0.000    0.000    0.000    0.000 elements.py:4139(__new__)\n",
       "        3    0.000    0.000    0.000    0.000 {function socket.close at 0x1095c1510}\n",
       "      171    0.000    0.000    0.000    0.000 managers.py:1057(value_getitem)\n",
       "       68    0.000    0.000    0.000    0.000 algorithms.py:205(_ensure_arraylike)\n",
       "       76    0.000    0.000    0.000    0.000 shape_base.py:209(_arrays_for_stack_dispatcher)\n",
       "       98    0.000    0.000    0.000    0.000 sql.py:68(_process_parse_dates_argument)\n",
       "       98    0.000    0.000    0.000    0.000 base.py:875(is_valid)\n",
       "      102    0.000    0.000    0.000    0.000 result.py:864(_cursor_description)\n",
       "        4    0.000    0.000    0.002    0.000 strings.py:2040(__init__)\n",
       "       98    0.000    0.000    0.000    0.000 {method 'with_traceback' of 'BaseException' objects}\n",
       "     14/4    0.000    0.000    0.000    0.000 langhelpers.py:273(get_cls_kwargs)\n",
       "      107    0.000    0.000    0.000    0.000 cursors.py:127(<dictcomp>)\n",
       "      107    0.000    0.000    0.000    0.000 cursors.py:76(_check_executed)\n",
       "      107    0.000    0.000    0.000    0.000 protocol.py:208(is_load_local_packet)\n",
       "        4    0.000    0.000    0.001    0.000 strings.py:2120(_wrap_result)\n",
       "       76    0.000    0.000    0.000    0.000 blocks.py:3099(<listcomp>)\n",
       "        4    0.000    0.000    0.009    0.002 strings.py:204(_map_object)\n",
       "       21    0.000    0.000    0.000    0.000 generic.py:343(_construct_axes_dict)\n",
       "        3    0.000    0.000    0.005    0.002 managers.py:1811(_multi_blockify)\n",
       "      102    0.000    0.000    0.000    0.000 range.py:256(step)\n",
       "        2    0.000    0.000    0.000    0.000 connections.py:371(_force_close)\n",
       "     20/3    0.000    0.000    0.001    0.000 visitors.py:85(_compiler_dispatch)\n",
       "        3    0.000    0.000    0.001    0.000 compiler.py:1999(visit_select)\n",
       "        1    0.000    0.000    0.001    0.001 parsers.py:1112(_make_engine)\n",
       "        1    0.000    0.000    0.180    0.180 parsers.py:2035(read)\n",
       "       80    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
       "       16    0.000    0.000    0.000    0.000 common.py:1225(is_datetimelike_v_numeric)\n",
       "       21    0.000    0.000    0.000    0.000 generic.py:345(<dictcomp>)\n",
       "        4    0.000    0.000    0.001    0.000 generic.py:9533(abs)\n",
       "       10    0.000    0.000    0.000    0.000 elements.py:717(__getattr__)\n",
       "       40    0.000    0.000    0.000    0.000 {built-in method numpy.geterrobj}\n",
       "        4    0.000    0.000    0.000    0.000 array_ops.py:305(logical_op)\n",
       "        4    0.000    0.000    0.002    0.000 strings.py:2051(_validate)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:834(_interleave)\n",
       "       98    0.000    0.000    0.000    0.000 base.py:177(__enter__)\n",
       "        1    0.000    0.000    0.049    0.049 <ipython-input-81-c9fd36f58387>:89(get_docs)\n",
       "       26    0.000    0.000    0.000    0.000 fromnumeric.py:70(<dictcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 {pandas._libs.ops_dispatch.maybe_dispatch_ufunc_to_dunder_op}\n",
       "       78    0.000    0.000    0.000    0.000 blocks.py:202(external_values)\n",
       "        8    0.000    0.000    0.000    0.000 type_api.py:505(_dialect_info)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:1761(_label_select_column)\n",
       "        4    0.000    0.000    0.001    0.000 __init__.py:544(wrapper)\n",
       "       20    0.000    0.000    0.000    0.000 base.py:104(__setattr__)\n",
       "        1    0.000    0.000    0.002    0.002 parsers.py:792(__init__)\n",
       "        1    0.000    0.000    0.006    0.006 connections.py:183(__init__)\n",
       "        1    0.000    0.000    0.006    0.006 connections.py:564(connect)\n",
       "        3    0.000    0.000    0.001    0.000 compiler.py:500(__init__)\n",
       "       98    0.000    0.000    0.000    0.000 default.py:1089(handle_dbapi_exception)\n",
       "       20    0.000    0.000    0.000    0.000 {built-in method numpy.seterrobj}\n",
       "        8    0.000    0.000    0.000    0.000 config.py:83(_get_single_key)\n",
       "        4    0.000    0.000    0.002    0.000 accessor.py:183(__get__)\n",
       "        1    0.000    0.000    0.002    0.002 socket.py:691(create_connection)\n",
       "       21    0.000    0.000    0.000    0.000 base.py:870(empty)\n",
       "       96    0.000    0.000    0.000    0.000 indexing.py:1344(_convert_for_reindex)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:973(_get_server_information)\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method _operator.and_}\n",
       "       10    0.000    0.000    0.000    0.000 numeric.py:3054(__init__)\n",
       "       10    0.000    0.000    0.000    0.000 numeric.py:3058(__enter__)\n",
       "        9    0.000    0.000    0.001    0.000 managers.py:581(astype)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:3719(apply)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:3857(_reduce)\n",
       "        1    0.000    0.000    0.223    0.223 parsers.py:1131(read)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:780(visit_label)\n",
       "        1    0.000    0.000    0.015    0.015 base.py:622(__connect)\n",
       "        8    0.000    0.000    0.000    0.000 config.py:551(_get_root)\n",
       "        3    0.000    0.000    0.000    0.000 selectable.py:2687(__init__)\n",
       "        3    0.000    0.000    0.001    0.000 default.py:342(check_unicode)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:1278(visit_typeclause)\n",
       "        5    0.000    0.000    0.000    0.000 types.py:69(__init__)\n",
       "        8    0.000    0.000    0.000    0.000 config.py:101(_get_option)\n",
       "       16    0.000    0.000    0.000    0.000 common.py:1614(is_complex_dtype)\n",
       "        9    0.000    0.000    0.000    0.000 cursors.py:280(fetchone)\n",
       "        1    0.000    0.000    0.004    0.004 mysqldb.py:136(_check_unicode_returns)\n",
       "       98    0.000    0.000    0.000    0.000 {built-in method builtins.globals}\n",
       "       79    0.000    0.000    0.000    0.000 numerictypes.py:655(<listcomp>)\n",
       "       16    0.000    0.000    0.000    0.000 config.py:566(_get_deprecated_option)\n",
       "       21    0.000    0.000    0.000    0.000 base.py:2581(_assert_can_do_setop)\n",
       "        1    0.000    0.000  170.699  170.699 <ipython-input-86-dfa1f957127f>:2(main)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.__build_class__}\n",
       "        3    0.000    0.000    0.000    0.000 weakref.py:358(remove)\n",
       "        1    0.000    0.000    0.000    0.000 socket.py:221(makefile)\n",
       "        8    0.000    0.000    0.000    0.000 array_ops.py:326(fill_bool)\n",
       "        4    0.000    0.000    0.009    0.002 strings.py:122(_na_map)\n",
       "        3    0.000    0.000    0.000    0.000 registry.py:53(_collection_gced)\n",
       "        2    0.000    0.000    0.000    0.000 socket.py:652(close)\n",
       "        8    0.000    0.000    0.000    0.000 config.py:230(__call__)\n",
       "       43    0.000    0.000    0.000    0.000 inference.py:325(<genexpr>)\n",
       "       21    0.000    0.000    0.000    0.000 base.py:666(size)\n",
       "        5    0.000    0.000    0.000    0.000 frame.py:3590(reindexer)\n",
       "        1    0.000    0.000    0.043    0.043 construction.py:213(init_dict)\n",
       "        1    0.000    0.000    0.228    0.228 parsers.py:530(parser_f)\n",
       "        1    0.000    0.000    0.000    0.000 _auth.py:186(scramble_caching_sha2)\n",
       "        4    0.000    0.000    0.000    0.000 langhelpers.py:1136(constructor_copy)\n",
       "        4    0.000    0.000    0.000    0.000 compiler.py:3477(_requires_quotes)\n",
       "        3    0.000    0.000    0.000    0.000 parse.py:361(urlparse)\n",
       "       10    0.000    0.000    0.000    0.000 numeric.py:3063(__exit__)\n",
       "       21    0.000    0.000    0.000    0.000 base.py:2210(_validate_sort_keyword)\n",
       "        4    0.000    0.000    0.010    0.002 strings.py:2980(strip)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1345(__init__)\n",
       "        9    0.000    0.000    0.000    0.000 <string>:1(__init__)\n",
       "        9    0.000    0.000    0.000    0.000 sqltypes.py:140(__init__)\n",
       "        1    0.000    0.000    0.002    0.002 default.py:331(_check_unicode_returns)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:885(_get_options_with_defaults)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1517(_maybe_dedup_names)\n",
       "        7    0.000    0.000    0.000    0.000 langhelpers.py:253(_inspect_func_args)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:864(anon_label)\n",
       "        3    0.000    0.000    0.000    0.000 selectable.py:2959(_froms)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:2157(_setup_select_stack)\n",
       "        4    0.000    0.000    0.000    0.000 fromnumeric.py:2171(all)\n",
       "        8    0.000    0.000    0.000    0.000 config.py:607(_warn_if_deprecated)\n",
       "        4    0.000    0.000    0.009    0.002 strings.py:1676(str_strip)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:939(_clean_options)\n",
       "        6    0.000    0.000    0.000    0.000 type_api.py:440(dialect_impl)\n",
       "        4    0.000    0.000    0.000    0.000 type_api.py:1440(adapt_type)\n",
       "        1    0.000    0.000    0.005    0.005 default.py:286(initialize)\n",
       "        6    0.000    0.000    0.000    0.000 compiler.py:3529(quote)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:1316(visit_cast)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:1747(_extend_string)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2184(_get_server_version_info)\n",
       "        1    0.000    0.000    0.009    0.009 base.py:2347(initialize)\n",
       "        1    0.000    0.000    0.001    0.001 socket.py:731(getaddrinfo)\n",
       "        8    0.000    0.000    0.000    0.000 config.py:533(_select_options)\n",
       "        4    0.000    0.000    0.000    0.000 __init__.py:124(_maybe_match_name)\n",
       "        4    0.000    0.000    0.010    0.002 strings.py:1946(wrapper)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:4299(reindex)\n",
       "        4    0.000    0.000    0.000    0.000 series.py:687(construct_return)\n",
       "       21    0.000    0.000    0.000    0.000 langhelpers.py:1145(<genexpr>)\n",
       "        7    0.000    0.000    0.000    0.000 elements.py:705(comparator)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:2395(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 selectable.py:2988(_get_display_froms)\n",
       "        3    0.000    0.000    0.000    0.000 selectable.py:3641(_columns_plus_names)\n",
       "        3    0.000    0.000    0.001    0.000 compiler.py:274(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 types.py:689(_adapt_string_for_cast)\n",
       "        9    0.000    0.000    0.000    0.000 inspect.py:72(isclass)\n",
       "        1    0.000    0.000    0.000    0.000 socket.py:563(__init__)\n",
       "        1    0.000    0.000    0.006    0.006 __init__.py:88(Connect)\n",
       "        9    0.000    0.000    0.000    0.000 blocks.py:187(is_categorical_astype)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:313(_init_dict)\n",
       "        2    0.000    0.000    0.000    0.000 parsers.py:367(_validate_integer)\n",
       "        3    0.000    0.000    0.000    0.000 parsers.py:1174(_is_potential_multi_index)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:304(<dictcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 langhelpers.py:897(expire_instance)\n",
       "        3    0.000    0.000    0.000    0.000 <string>:1(select)\n",
       "        1    0.000    0.000    0.009    0.009 attr.py:279(exec_once)\n",
       "        1    0.000    0.000    0.009    0.009 strategies.py:194(first_connect)\n",
       "        3    0.000    0.000    0.001    0.000 base.py:1287(_cursor_execute)\n",
       "      9/3    0.000    0.000    0.001    0.000 compiler.py:349(process)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2172(get_isolation_level)\n",
       "        3    0.000    0.000    0.000    0.000 posixpath.py:232(expanduser)\n",
       "        5    0.000    0.000    0.000    0.000 socket.py:416(close)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'setsockopt' of '_socket.socket' objects}\n",
       "        4    0.000    0.000    0.000    0.000 array_ops.py:264(na_logical_op)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1969(close)\n",
       "        1    0.000    0.000    0.000    0.000 _auth.py:34(scramble_native_password)\n",
       "        1    0.000    0.000    0.000    0.000 langhelpers.py:925(__getattr__)\n",
       "        1    0.000    0.000    0.000    0.000 elements.py:3066(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:4299(apply_map)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:1542(_truncated_identifier)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:2186(_compose_select_body)\n",
       "        4    0.000    0.000    0.000    0.000 default.py:409(type_descriptor)\n",
       "        4    0.000    0.000    0.000    0.000 result.py:1279(first)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:1954(visit_CHAR)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method _hashlib.new}\n",
       "        6    0.000    0.000    0.000    0.000 {method 'digest' of '_hashlib.HASH' objects}\n",
       "        1    0.000    0.000    0.000    0.000 cast.py:1342(find_common_type)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:259(infer_compression)\n",
       "        8    0.000    0.000    0.000    0.000 series.py:678(<genexpr>)\n",
       "        2    0.000    0.000    0.000    0.000 charset.py:43(by_name)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:305(<dictcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 _auth.py:48(_my_crypt)\n",
       "        6    0.000    0.000    0.000    0.000 base.py:38(_from_objects)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:2340(literal_column)\n",
       "       10    0.000    0.000    0.000    0.000 type_api.py:1430(to_instance)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:834(visit_column)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:1564(_process_anon)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:2906(__getitem__)\n",
       "        2    0.000    0.000    0.000    0.000 types.py:510(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 types.py:674(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 weakref.py:408(__setitem__)\n",
       "        3    0.000    0.000    0.000    0.000 socket.py:412(_real_close)\n",
       "        4    0.000    0.000    0.000    0.000 __init__.py:427(_align_method_SERIES)\n",
       "        1    0.000    0.000    0.000    0.000 nanops.py:234(_get_values)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:11341(logical_func)\n",
       "        4    0.000    0.000    0.000    0.000 series.py:658(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1979(_set_noconvert_columns)\n",
       "        2    0.000    0.000    0.000    0.000 connections.py:637(write_packet)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:890(_process_auth)\n",
       "        1    0.000    0.000    0.009    0.009 langhelpers.py:1440(go)\n",
       "        1    0.000    0.000    0.000    0.000 elements.py:54(collate)\n",
       "        3    0.000    0.000    0.001    0.000 elements.py:399(compile)\n",
       "        4    0.000    0.000    0.000    0.000 type_api.py:524(adapt)\n",
       "        2    0.000    0.000    0.000    0.000 sqltypes.py:411(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:94(__getattr__)\n",
       "        1    0.000    0.000    0.006    0.006 strategies.py:106(connect)\n",
       "        1    0.000    0.000    0.000    0.000 compiler.py:1318(_generate_generic_binary)\n",
       "       17    0.000    0.000    0.000    0.000 base.py:1753(attr)\n",
       "        1    0.000    0.000    0.002    0.002 base.py:2794(_detect_sql_mode)\n",
       "        1    0.000    0.000    0.000    0.000 idna.py:147(encode)\n",
       "       14    0.000    0.000    0.000    0.000 {method 'get' of 'mappingproxy' objects}\n",
       "        4    0.000    0.000    0.000    0.000 {method 'difference' of 'set' objects}\n",
       "        7    0.000    0.000    0.000    0.000 {method 'update' of 'set' objects}\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method gc.get_referents}\n",
       "        1    0.000    0.000    0.000    0.000 weakref.py:356(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 enum.py:284(__call__)\n",
       "        3    0.000    0.000    0.000    0.000 parse.py:409(urlsplit)\n",
       "        4    0.000    0.000    0.000    0.000 socket.py:97(_intenum_converter)\n",
       "        8    0.000    0.000    0.000    0.000 config.py:594(_translate_key)\n",
       "        1    0.000    0.000    0.000    0.000 construction.py:590(create_series_with_explicit_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:4076(identical)\n",
       "        1    0.000    0.000    0.000    0.000 frame.py:1249(to_numpy)\n",
       "        5    0.000    0.000    0.000    0.000 range.py:444(equals)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:40(is_url)\n",
       "        3    0.000    0.000    0.000    0.000 common.py:58(_expand_user)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:144(get_filepath_or_buffer)\n",
       "        4    0.000    0.000    0.000    0.000 series.py:661(<listcomp>)\n",
       "        1    0.000    0.000    0.004    0.004 connections.py:401(_send_autocommit_mode)\n",
       "        3    0.000    0.000    0.000    0.000 _collections.py:361(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 _collections.py:759(<listcomp>)\n",
       "        3    0.000    0.000    0.001    0.000 elements.py:464(_compiler)\n",
       "        2    0.000    0.000    0.000    0.000 elements.py:681(self_group)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:2455(_from_objects)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:3831(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:3950(_from_objects)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:4184(__new__)\n",
       "        3    0.000    0.000    0.000    0.000 selectable.py:2120(__init__)\n",
       "        1    0.000    0.000    0.009    0.009 attr.py:291(__call__)\n",
       "        1    0.000    0.000    0.000    0.000 default.py:270(_type_memos)\n",
       "        1    0.000    0.000    0.000    0.000 compiler.py:1261(visit_binary)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:2082(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 result.py:1146(_fetchone_impl)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:2296(_compat_first)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2309(_get_default_schema_name)\n",
       "        1    0.000    0.000    0.001    0.001 base.py:2750(_detect_casing)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2809(_detect_ansiquotes)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.__import__}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'split' of 're.Pattern' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {function SocketIO.close at 0x1095c1e18}\n",
       "        8    0.000    0.000    0.000    0.000 weakref.py:435(__contains__)\n",
       "        4    0.000    0.000    0.000    0.000 enum.py:526(__new__)\n",
       "        3    0.000    0.000    0.000    0.000 hashlib.py:139(__hash_new)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method _hashlib.openssl_sha256}\n",
       "        6    0.000    0.000    0.000    0.000 parse.py:109(_coerce_args)\n",
       "        1    0.000    0.000    0.000    0.000 converters.py:12(escape_item)\n",
       "        1    0.000    0.000    0.000    0.000 nanops.py:396(nanany)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:99(_freeze)\n",
       "        2    0.000    0.000    0.000    0.000 generic.py:4530(<genexpr>)\n",
       "        6    0.000    0.000    0.000    0.000 blocks.py:225(get_values)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:798(as_array)\n",
       "        6    0.000    0.000    0.000    0.000 managers.py:1815(<lambda>)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:88(stringify_path)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:4027(reindex)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1551(_make_index)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1842(_do_date_conversions)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:74(_makefile)\n",
       "        1    0.000    0.000    0.004    0.004 connections.py:383(autocommit)\n",
       "        3    0.000    0.000    0.000    0.000 util.py:11(int2byte)\n",
       "        3    0.000    0.000    0.000    0.000 _collections.py:755(unique_list)\n",
       "        6    0.000    0.000    0.000    0.000 base.py:39(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:3944(_set_table)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:4345(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 attr.py:276(_memoized_attr__exec_once_mutex)\n",
       "        1    0.000    0.000    0.003    0.003 base.py:914(scalar)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:410(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:890(escape_literal_column)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:3598(format_label)\n",
       "        1    0.000    0.000    0.015    0.015 base.py:425(__init__)\n",
       "        1    0.000    0.000    0.001    0.001 default.py:379(<setcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 result.py:1306(scalar)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:2902(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 pymysql.py:51(supports_server_side_cursors)\n",
       "        2    0.000    0.000    0.000    0.000 <ipython-input-64-01a4bf55ac48>:14(corpus_config)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'find' of 'bytes' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {method 'union' of 'set' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n",
       "        1    0.000    0.000    0.000    0.000 re.py:271(_compile)\n",
       "       21    0.000    0.000    0.000    0.000 typing.py:890(cast)\n",
       "        3    0.000    0.000    0.000    0.000 cast.py:1381(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 nanops.py:329(_na_ok_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:356(_construct_axes_from_arguments)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:7264(isna)\n",
       "        4    0.000    0.000    0.000    0.000 managers.py:1062(value_getitem)\n",
       "        9    0.000    0.000    0.000    0.000 managers.py:1582(_consolidate_inplace)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:1872(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:1855(_interleaved_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:882(close)\n",
       "        3    0.000    0.000    0.000    0.000 parsers.py:1170(_is_index_col)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1321(_validate_parse_dates_arg)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:2110(<dictcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:3258(_process_date_conversion)\n",
       "        2    0.000    0.000    0.000    0.000 connections.py:96(pack_int24)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:946(_get_auth_plugin_handler)\n",
       "        3    0.000    0.000    0.000    0.000 _collections.py:730(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:1271(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 elements.py:3098(_from_objects)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:4340(_select_iterables)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:4559(_interpret_as_column_or_from)\n",
       "        3    0.000    0.000    0.000    0.000 type_api.py:307(_has_column_expression)\n",
       "        3    0.000    0.000    0.000    0.000 <string>:1(cast)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:419(type)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:708(default_from)\n",
       "        1    0.000    0.000    0.000    0.000 compiler.py:872(visit_collation)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:1758(_add_to_result_map)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:2080(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 compiler.py:3579(format_collation)\n",
       "        1    0.000    0.000    0.000    0.000 impl.py:142(_inc_overflow)\n",
       "        4    0.000    0.000    0.000    0.000 result.py:901(close)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:1778(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:2381(_is_mariadb)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:2407(_supports_cast)\n",
       "        1    0.000    0.000    0.000    0.000 mysqldb.py:214(_detect_charset)\n",
       "        3    0.000    0.000    0.000    0.000 <ipython-input-64-01a4bf55ac48>:9(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 {method 'pop' of 'list' objects}\n",
       "        4    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'issuperset' of 'set' objects}\n",
       "        5    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method sys.getfilesystemencoding}\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method posix.fspath}\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:416(parent)\n",
       "        1    0.000    0.000    0.000    0.000 re.py:232(compile)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:1361(debug)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'update' of '_hashlib.HASH' objects}\n",
       "        2    0.000    0.000    0.000    0.000 socket.py:406(_decref_socketios)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        1    0.000    0.000    0.000    0.000 converters.py:47(escape_bool)\n",
       "        1    0.000    0.000    0.000    0.000 inference.py:130(is_file_like)\n",
       "        2    0.000    0.000    0.000    0.000 cast.py:1367(<genexpr>)\n",
       "        3    0.000    0.000    0.000    0.000 cast.py:1370(<genexpr>)\n",
       "        2    0.000    0.000    0.000    0.000 cast.py:1374(<genexpr>)\n",
       "        2    0.000    0.000    0.000    0.000 cast.py:1376(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:275(maybe_iterable_to_list)\n",
       "        1    0.000    0.000    0.000    0.000 nanops.py:166(_get_fill_value)\n",
       "        1    0.000    0.000    0.000    0.000 nanops.py:189(_maybe_get_mask)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:4091(<genexpr>)\n",
       "        2    0.000    0.000    0.000    0.000 generic.py:255(_validate_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:381(<dictcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:5412(values)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:660(is_mixed_type)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:78(validate_header_arg)\n",
       "        8    0.000    0.000    0.000    0.000 series.py:659(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:4400(isna)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:923(_check_file_or_buffer)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1156(_create_index)\n",
       "        6    0.000    0.000    0.000    0.000 parsers.py:1191(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:2108(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:3213(_make_date_converter)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:3346(_clean_na_values)\n",
       "        1    0.000    0.000    0.000    0.000 charset.py:40(by_id)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:101(lenenc_int)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:389(get_autocommit)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:964(character_set_name)\n",
       "        1    0.000    0.000    0.000    0.000 protocol.py:60(get_all_data)\n",
       "        1    0.000    0.000    0.000    0.000 protocol.py:196(is_auth_switch_request)\n",
       "        3    0.000    0.000    0.000    0.000 _collections.py:396(__iter__)\n",
       "        6    0.000    0.000    0.000    0.000 elements.py:3941(_get_table)\n",
       "        1    0.000    0.000    0.000    0.000 elements.py:4062(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 elements.py:4544(_literal_as_binds)\n",
       "        7    0.000    0.000    0.000    0.000 type_api.py:60(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 operators.py:1399(is_boolean)\n",
       "        4    0.000    0.000    0.000    0.000 type_api.py:521(_gen_dialect_impl)\n",
       "        3    0.000    0.000    0.000    0.000 selectable.py:3002(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 attr.py:340(for_modify)\n",
       "        1    0.000    0.000    0.006    0.006 default.py:452(connect)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:399(process)\n",
       "        1    0.000    0.000    0.000    0.000 compiler.py:1168(_get_operator_dispatch)\n",
       "        1    0.000    0.000    0.015    0.015 base.py:296(_create_connection)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:1350(get_select_precolumns)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2367(_warn_for_known_db_issues)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2385(_is_mysql)\n",
       "        1    0.000    0.000    0.000    0.000 <ipython-input-86-dfa1f957127f>:108(Results)\n",
       "        1    0.000    0.000    0.000    0.000 <ipython-input-86-dfa1f957127f>:109(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'split' of 'bytes' objects}\n",
       "        4    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'isdigit' of 'str' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.ord}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
       "        1    0.000    0.000    0.000    0.000 <string>:1(<module>)\n",
       "        6    0.000    0.000    0.000    0.000 parse.py:98(_noop)\n",
       "        3    0.000    0.000    0.000    0.000 <string>:1(__new__)\n",
       "        2    0.000    0.000    0.000    0.000 {pandas._libs.algos.ensure_float64}\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:1576(is_consolidated)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:120(is_s3_url)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:127(is_gcs_url)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:396(_validate_names)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1271(_validate_usecols_arg)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1432(_has_complex_date_col)\n",
       "        2    0.000    0.000    0.000    0.000 parsers.py:1545(_maybe_make_multi_index_columns)\n",
       "        1    0.000    0.000    0.000    0.000 charset.py:18(encoding)\n",
       "        1    0.000    0.000    0.000    0.000 util.py:4(byte2int)\n",
       "        1    0.000    0.000    0.000    0.000 protocol.py:142(read_string)\n",
       "        3    0.000    0.000    0.000    0.000 _collections.py:733(__missing__)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:751(_select_iterable)\n",
       "        1    0.000    0.000    0.000    0.000 operators.py:1376(is_comparison)\n",
       "        5    0.000    0.000    0.000    0.000 type_api.py:269(result_processor)\n",
       "        3    0.000    0.000    0.000    0.000 selectable.py:3663(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:352(__str__)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%time\n",
    "def main():\n",
    "    '''\n",
    "        corpora: i2b2, mipacq, fv017\n",
    "        analyses: entity only (exact span), cui by document, full (aka (entity and cui on exaact span/exact cui)\n",
    "        systems: ctakes, biomedicus, clamp, metamap, quick_umls\n",
    "        \n",
    "        TODO -> Vectorization (entity only) -> done:\n",
    "                add switch for use of TN on single system performance evaluations -> done\n",
    "                add switch for overlap matching versus exact span -> done\n",
    "             -> Other tasks besides concept extraction\n",
    "        \n",
    "    ''' \n",
    "    analysisConf =  AnalysisConfig()\n",
    "    print(analysisConf.systems, analysisConf.corpus_config())\n",
    "    \n",
    "    if (rtype == 1):\n",
    "        print(semtypes, systems)\n",
    "        if filter_semtype:\n",
    "            for semtype in semtypes:\n",
    "                test = get_valid_systems(systems, semtype)\n",
    "                print('SYSYEMS FOR SEMTYPE', semtype, 'ARE', test)\n",
    "                generate_metrics(analysis_type, corpus, filter_semtype, semtype)\n",
    "            \n",
    "        else:\n",
    "            generate_metrics(analysis_type, corpus, filter_semtype)\n",
    "        \n",
    "    elif (rtype == 2):\n",
    "        print('run_type:', run_type)\n",
    "        if filter_semtype:\n",
    "            print(semtypes)\n",
    "            ensemble_control(analysisConf.systems, analysis_type, corpus, run_type, filter_semtype, semtypes)\n",
    "        else:\n",
    "            ensemble_control(analysisConf.systems, analysis_type, corpus, run_type, filter_semtype)\n",
    "    elif (rtype == 3):\n",
    "        t = ['concept_jaccard_score_false']\n",
    "        test_systems(analysis_type, analysisConf.systems, corpus)  \n",
    "        test_count(analysis_type, corpus)\n",
    "        test_ensemble(analysis_type, corpus)\n",
    "    elif (rtype == 4):\n",
    "        if filter_semtype:\n",
    "            majority_vote(systems, analysis_type, corpus, run_type, filter_semtype, semtypes)\n",
    "        else:\n",
    "            majority_vote(systems, analysis_type, corpus, run_type, filter_semtype)\n",
    "    elif (rtype == 5):\n",
    "        \n",
    "        # control filter_semtype in get_sys_data, get_ref_n and generate_metrics. TODO consolidate. \n",
    "        # # run single ad hoc statement\n",
    "        statement = '((ctakes&biomedicus)|metamap)'\n",
    "\n",
    "        def ad_hoc(analysis_type, corpus, statement):\n",
    "            sys = get_merge_data(statement, analysis_type, corpus, run_type, filter_semtype)\n",
    "            sys = sys.rename(index=str, columns={\"note_id\": \"case\"})\n",
    "            sys['label'] = 'concept'\n",
    "\n",
    "            ref = get_reference_vector(analysis_type, corpus, filter_semtype)\n",
    "            sys = vectorized_annotations(sys)\n",
    "            sys = np.asarray(flatten_list(list(sys)), dtype=np.int32)\n",
    "\n",
    "            return ref, sys\n",
    "\n",
    "        ref, sys = ad_hoc(analysis_type, corpus, statement)\n",
    "        \n",
    "        # query by term:\n",
    "        \n",
    "        # import spacy\n",
    "        # nlp = spacy.load('en')\n",
    "\n",
    "        # sql = \"select distinct note_id, sofa from concepts.sofas where corpus = 'fairview'\"\n",
    "\n",
    "        # docs = pd.read_sql(sql, con=engine)\n",
    "\n",
    "        # d = {}\n",
    "\n",
    "        # for row in docs.itertuples():\n",
    "        #     d[row.note_id] = row.sofa\n",
    "\n",
    "        # print(len(d))\n",
    "\n",
    "        # test = matches[matches['note_id'] == '0000200926']\n",
    "        # print(len(test))\n",
    "\n",
    "        # doc = nlp(d['0000200926'])\n",
    "\n",
    "        # for row in test.itertuples():\n",
    "        #     my_str = [token.text.strip('\\n').lower() for token in doc if token.idx >= (row.begin) and token.idx <= (row.end)]\n",
    "        #     if 'diabetes' in my_str:\n",
    "        #         print(my_str)\n",
    "\n",
    "    elif (rtype == 6): # 5 w/o evaluation\n",
    "        \n",
    "        statement = '(ctakes|biomedicus)'\n",
    "\n",
    "        def ad_hoc(analysis_type, corpus, statement):\n",
    "            sys = get_merge_data(statement, analysis_type, corpus, run_type, filter_semtype)\n",
    "            sys = sys.rename(index=str, columns={\"note_id\": \"case\"})\n",
    "\n",
    "            return sys\n",
    "\n",
    "        sys = ad_hoc(analysis_type, corpus, statement).sort_values(by=['case', 'begin'])\n",
    "        print(sys.head())\n",
    "        \n",
    "        print(statement + '\\n', sys.head(100))\n",
    "        sys.to_csv('test.csv')\n",
    "    \n",
    "    elif (rtype == 7): # complementarity ala http://www.lrec-conf.org/proceedings/lrec2016/pdf/105_Paper.pdf\n",
    "       \n",
    "        class Results(object):\n",
    "            def __init__(self):\n",
    "                self.sysA = ''\n",
    "                self.sysB = ''\n",
    "            \n",
    "        r = Results()\n",
    "        \n",
    "        def ad_hoc(analysis_type, corpus, systems):\n",
    "            #sys = get_merge_data(statement, analysis_type, corpus, run_type, filter_semtype)\n",
    "            \n",
    "            for c in list(combinations(systems, 2)):\n",
    "                \n",
    "                r.sysA = c[0]\n",
    "                r.sysB = c[1]\n",
    "\n",
    "                if filter_semtype:\n",
    "                    for semtype in semtypes:\n",
    "                        vectorized_complementarity(r, analysis_type, corpus, c, filter_semtype, semtype)\n",
    "                else:\n",
    "                    vectorized_complementarity(r, analysis_type, corpus, c, filter_semtype)\n",
    "\n",
    "            #return sys\n",
    "\n",
    "        ad_hoc(analysis_type, corpus, systems)\n",
    "        #print(sys.head())\n",
    "        \n",
    "        #print(statement + '\\n', sys.head(100))\n",
    "        #sys.to_csv('test.csv')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    %prun main()\n",
    "    print('done!')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tests to implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TESTS -> ensemble:\n",
    "# def test_match_consistency(matches, ref_only, ref_n, sys):\n",
    "#     \"\"\"test for reference only/match set consistency:\n",
    "#         params: match, system and reference only sets\"\"\"\n",
    "   \n",
    "#     print('len', len(sys), len(matches), len(matches.union(sys)), len(matches.intersection(sys)))\n",
    "#     assert len(matches.union(ref_only)) == ref_n, 'Reference annotation mismatch union'\n",
    "#     assert len(matches.intersection(sys)) == len(matches), 'System annotation mismatch intersect'\n",
    "#     assert len(matches.union(sys)) == len(sys), 'System annotation mismatch union'\n",
    "#     assert len(matches.intersection(ref_only)) == 0, 'Reference annotation mismatch intersect'\n",
    "\n",
    "# def test_systems(analysis_type, systems, corpus):\n",
    "#     sys = df_to_set(get_sys_data(systems[0], analysis_type, corpus), analysis_type)\n",
    "#     test_match_consistency(*get_system_matches(systems[0], analysis_type, corpus), get_ref_n(analysis_type), sys)\n",
    "#     print('Match consistency:', len(sys),get_ref_n(analysis_type))\n",
    "\n",
    "# def test_metrics(ref, sys_m, match_m):\n",
    "#     test = True\n",
    "#     reference_n = len(ref)\n",
    "#     system_n = len(sys_m)\n",
    "\n",
    "#     print('Test metrics:', type(reference_n), type(system_n), type(match_m))\n",
    "\n",
    "#     reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, match_m).get_ref_sys()\n",
    "#     F, recall, precision, _, _, _, _, _ = Metrics(system_only, reference_only, reference_system_match, system_n).get_confusion_metrics()\n",
    "#     F_, recall_, precision_, _, _, _, _, _ = Metrics(system_only, reference_only, reference_system_match, system_n).get_confusion_metrics(test)\n",
    "\n",
    "#     assert F[1] == F_, 'F1 issue'\n",
    "#     assert recall[1] == recall_, 'recall issue'\n",
    "#     assert precision[1] == precision_, 'precision issue'\n",
    "#     print(F[1], F_)\n",
    "#     print(recall[1], recall_)\n",
    "#     print(precision[1], precision_)\n",
    "\n",
    "# def test_count(analysis_type, corpus):\n",
    "#     # test match counts:\n",
    "#     ctakes, _ = get_system_matches('ctakes', analysis_type, corpus)\n",
    "#     clamp, _ = get_system_matches('clamp', analysis_type, corpus)\n",
    "#     b9, _ = get_system_matches('biomedicus', analysis_type, corpus)\n",
    "#     mm, _ = get_system_matches('metamap', analysis_type, corpus)\n",
    "\n",
    "#     print('count:', len(mm.intersection(b9.intersection(clamp.intersection(ctakes)))))\n",
    "    \n",
    "# def test_ensemble(analysis_type, corpus):\n",
    "    \n",
    "#     print('ensemble:')\n",
    "#     # Get mixed system_n\n",
    "#     ref_ann, data = get_metric_data(analysis_type, corpus)\n",
    "\n",
    "#     names = ['ctakes', 'biomedicus', 'metamap', 'clamp']\n",
    "#     if 'entity' in analysis_type: \n",
    "#         cols_to_keep = ['begin', 'end', 'note_id']\n",
    "#     elif 'cui' in analysis_type:\n",
    "#         cols_to_keep = ['cui', 'note_id']\n",
    "#     elif 'full' in analysis_type:\n",
    "#         cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "\n",
    "#     biomedicus = data[data[\"system\"]=='biomedicus'][cols_to_keep].copy()\n",
    "#     ctakes = data[data[\"system\"]=='ctakes'][cols_to_keep].copy()\n",
    "#     clamp = data[data[\"system\"]=='clamp'][cols_to_keep].copy()\n",
    "#     metamap = data[data[\"system\"]=='metamap'][cols_to_keep].copy()\n",
    "#     quickumls = data[data[\"system\"]=='quick_umls'][cols_to_keep].copy()\n",
    "\n",
    "#     print('systems:', len(biomedicus), len(clamp), len(ctakes), len(metamap), len(quickumls))\n",
    "\n",
    "#     b9 = set()\n",
    "#     cl = set()\n",
    "#     ct = set()\n",
    "#     mm = set()\n",
    "#     qu = set()\n",
    "\n",
    "#     b9 = df_to_set(get_sys_data('biomedicus', analysis_type, corpus), analysis_type)\n",
    "#     print(len(b9))\n",
    "\n",
    "#     ct = df_to_set(get_sys_data('ctakes', analysis_type, corpus), analysis_type)\n",
    "#     print(len(ct))\n",
    "\n",
    "#     cl = df_to_set(get_sys_data('clamp', analysis_type, corpus), analysis_type)\n",
    "#     print(len(cl))\n",
    "\n",
    "#     mm = df_to_set(get_sys_data('metamap', analysis_type, corpus), analysis_type)\n",
    "#     print(len(mm))\n",
    "\n",
    "#     qu = df_to_set(get_sys_data('quick_umls', analysis_type, corpus), analysis_type)\n",
    "#     print(len(qu))\n",
    "    \n",
    "#     print('various merges:')\n",
    "#     print(len(b9), len(cl), len(ct), len(mm), len(qu))\n",
    "#     print(len(mm.intersection(b9.intersection(cl.intersection(ct)))))\n",
    "#     print(len(mm.union(b9.intersection(cl.intersection(ct)))))\n",
    "#     print(len(mm.union(b9.union(cl.intersection(ct)))))\n",
    "#     print(len(mm.union(b9.union(cl.union(ct)))))\n",
    "#     print(len(b9.intersection(ct)))\n",
    "\n",
    "#     sys_m = b9.intersection(ct.intersection(qu))\n",
    "#     print('sys_m:', len(sys_m))\n",
    "\n",
    "#     # Get match merges:\n",
    "#     ct, _ = get_system_matches('ctakes', analysis_type, corpus)\n",
    "#     cl, _ = get_system_matches('clamp', analysis_type, corpus)\n",
    "#     b9, _ = get_system_matches('biomedicus', analysis_type, corpus)\n",
    "#     mm, _ = get_system_matches('metamap', analysis_type, corpus)\n",
    "#     qu, _ = get_system_matches('quick_umls', analysis_type, corpus)\n",
    "\n",
    "#     match_m = b9.intersection(ct.intersection(qu))\n",
    "#     print('match_m:', len(match_m))\n",
    "#     # reference df to set\n",
    "#     if 'entity' in analysis_type: \n",
    "#         cols_to_keep = ['end', 'start','file']\n",
    "#     elif 'cui' in analysis_type:\n",
    "#         cols_to_keep = ['value','file']\n",
    "#     elif 'full' in analysis_type:\n",
    "#         cols_to_keep = ['end', 'start', 'value','file']\n",
    "\n",
    "#     ref = df_to_set(ref_ann[cols_to_keep], analysis_type, 'ref')\n",
    "\n",
    "#     print('ref:', len(ref))\n",
    "\n",
    "#     # test difference:\n",
    "#     print('FP:', len(sys_m - match_m), len(sys_m - ref))\n",
    "#     assert len(sys_m - match_m) == len(sys_m - ref), 'FP mismatch'\n",
    "#     print('FN:', len(ref - match_m), len(ref - sys_m))\n",
    "#     assert len(ref - match_m) == len(ref - sys_m), 'FN mismatch'\n",
    "    \n",
    "#     test_metrics(ref, sys_m, match_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_offset(case, begin, end):\n",
    "#     directory_to_parse = \"/Users/gms/development/nlp/nlpie/data/clinical_trial2/data_in/\"\n",
    "#     os.chdir(directory_to_parse)\n",
    "#     with open(case + '.txt') as f:\n",
    "#         line = f.readline()\n",
    "#         cnt = 0 # count number lines\n",
    "#         l = 0 # get cummulative length\n",
    "#         while line:\n",
    "#             if begin <= l + len(line):\n",
    "#                 x=begin-l-cnt*(2+1) # handle position in file by line number and deal with extra windows chars\n",
    "#                 y=end-l-cnt*(2+1)\n",
    "#                 print('line off:', cnt, line[x-1:y])\n",
    "                \n",
    "\n",
    "#                 return line[x-1:y]\n",
    "#                 break\n",
    "#             else:\n",
    "#                 l += len(line)\n",
    "#                 line = f.readline()\n",
    "#                 cnt += 1\n",
    "\n",
    "# sql = \"select substring(sofa, 145, 160-145) from clinical_trial.sofas where corpus = 'clinical_trial2' and note_id = 'NCT00006180_criteria'\"  \n",
    "# #test = pd.read_sql(sql, params={\"sys.name\":sys.name}, con=engine) \n",
    "# test = pd.read_sql(sql, con=engine) \n",
    "# print(test)\n",
    "\n",
    "import pandas as pd\n",
    "# use to generate FN/FP\n",
    "def span_to_text(gold, sys, type='FN'):\n",
    "    \n",
    "    #gold = gold[gold['note_id']=='NCT00042380_criteria'].drop_duplicates()\n",
    "    #sys = sys[sys['note_id']=='NCT00042380_criteria']\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    if type == 'FN':\n",
    "        #iterate through gold as tuples\n",
    "        for g in gold.itertuples():\n",
    "            g_begin = int(g.begin)\n",
    "            g_end = int(g.end)\n",
    "            g_case = g.note_id\n",
    "            mMatch = False # flag when there is a match\n",
    "            for s in sys.itertuples():\n",
    "                s_begin = int(s.begin)\n",
    "                s_end = int(s.end)\n",
    "                s_case = s.note_id\n",
    "                if (((g_begin <= s_end and g_end >= s_begin) or \n",
    "                     (g_end >= s_begin and s_end >= g_begin)) and g_case == s_case):\n",
    "                        mMatch = True\n",
    "                        # create a df of TP here if wanted\n",
    "                        # break once a match is found\n",
    "                        break\n",
    "                        \n",
    "            # no match found, write list of tuple elements to df\n",
    "            if mMatch == False:\n",
    "                l = [g.begin,g.end,g.semtype,g.note_id,g.text,'FN']\n",
    "                fn = pd.DataFrame(columns=['begin','end','semtypes','note_id','text','value'], data=None)\n",
    "                fn.loc[fn.shape[0]] = l \n",
    "                frames = [df,fn]\n",
    "                #print(fn)\n",
    "                df = pd.concat(frames)\n",
    "                \n",
    "    else:    \n",
    "        for s in sys.itertuples(index=False):\n",
    "            s_begin = int(s.begin)\n",
    "            s_end = int(s.end)\n",
    "            s_case = s.note_id\n",
    "            mMatch = False\n",
    "            for g in gold.itertuples():\n",
    "                g_begin = int(g.begin)\n",
    "                g_end = int(g.end)\n",
    "                g_case = g.note_id\n",
    "                if (((g_begin <= s_end and g_end >= s_begin) or \n",
    "                     (g_end >= s_begin and s_end >= g_begin)) and g_case == s_case):\n",
    "                        mMatch = True\n",
    "                        break\n",
    "\n",
    "            if mMatch == False:\n",
    "                l = [s.begin,s.end,s.semtypes,s.note_id,s.text,s.type,s.system, 'FP']\n",
    "                fp = pd.DataFrame(columns=['begin','end','semtypes','note_id','text','type','system','value'], data=None)\n",
    "                fp.loc[fp.shape[0]] = l \n",
    "                frames = [df,fp]\n",
    "                df = pd.concat(frames)\n",
    "                #print(fp)\n",
    "       \n",
    "    return df\n",
    "\n",
    "# path/file\n",
    "gold='~/Desktop/clinical_trial2_reference.csv' \n",
    "system='~/development/nlp/nlpie/data/ensembling-u01/output/analytical_clinical_trial2.csv'\n",
    "analysis_type = 'FP'  # default for method is 'FN'\n",
    "\n",
    "#df = span_to_text(pd.read_csv(gold).drop_duplicates(),pd.read_csv(system).drop_duplicates(), analysis_type)\n",
    "#print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
