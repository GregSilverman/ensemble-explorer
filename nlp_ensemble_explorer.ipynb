{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  Copyright (c) 2019 Regents of the University of Minnesota.\n",
    " \n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    " \n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gevent\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pymysql\n",
    "import time \n",
    "import functools as ft\n",
    "import glob   \n",
    "import operator as op\n",
    "import shelve\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from pathlib import Path\n",
    "from itertools import combinations, product, permutations\n",
    "from sqlalchemy.engine import create_engine\n",
    "from datetime import datetime\n",
    "from ast import literal_eval\n",
    "from scipy import stats  \n",
    "from scipy.stats.mstats import gmean\n",
    "from pythonds.basic.stack import Stack\n",
    "from pythonds.trees.binaryTree import BinaryTree\n",
    "from collections import defaultdict\n",
    "from typing import List, Set, Tuple "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The cell below contains the configurable parameters to ensure that our ensemble explorer runs properaly on your machine. Please read carfully through steps (1-7) before running the rest of the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-1: CHOOSE YOUR CORPUS\n",
    "corpus = 'fairview' #options include 'fairview', 'mipacq' OR 'i2b2'\n",
    "#corpora = ['fairview', 'mipacq', 'i2b2']\n",
    "#corpora = ['i2b2']\n",
    "# STEP-2: CHOOSE YOUR DATA DIRECTORY; this is where output data will be saved on your machine\n",
    "data_directory = '/Users/gms/development/nlp/nlpie/data/ensembling-u01/output/' \n",
    "\n",
    "# STEP-4: CHOOSE WHAT SYSTEM YOU'D LIKE TO RUN ON THE CORPUS\n",
    "rtype = 2      # OPTIONS INCLUDE: 1->Single systems; 2->Ensemble; 3->Tests; 4-> MM Test\n",
    "               # The Ensemble includes ['ctakes','biomedicus','clamp','metamap','quick_umls']\n",
    "    \n",
    "# STEP-5: CHOOSE WHAT TYPE OF ANALYSIS YOU'D LIKE TO RUN ON THE CORPUS\n",
    "analysis_type = 'entity' #options include 'entity' OR 'full'\n",
    "\n",
    "# STEP-(6A): ENTER DETAILS FOR ACCESSING MANUAL ANNOTATION DATA\n",
    "database_type = 'mysql+pymysql' # We use mysql+pymql as default\n",
    "database_username = 'gms'\n",
    "database_password = 'nej123' \n",
    "database_url = 'localhost' # HINT: use localhost if you're running database on your local machine\n",
    "database_name = 'concepts' # Enter database name\n",
    "table_name = corpus + '_all' # Enter the table within the database where your reference data is store\n",
    "\n",
    "# STEP-(6B): ENTER DIRECTORY FOR ACCESSING SYSTEM ANNOTATION DATA\n",
    "# TODO: allow for list of corpora\n",
    "system_annotation = 'analytical_'+corpus+'.csv' # OPTIONS include 'analytical_cui_mipacq_concepts.csv' OR 'analytical_cui_i2b2_concepts.csv' \n",
    "\n",
    "# STEP-7: WE'LL CREATE AN 'SYSTEM OUTPUT'DIRECTORY FOR YOU INSIDE THE DIRECTORY YOU SPECIFIED IN (STEP 2)\n",
    "single_sys_dir = Path(data_directory + \"single_system_out\")\n",
    "single_sys_dir.mkdir(parents=True, exist_ok=True)\n",
    "dir_out = Path(data_directory + 'single_system_out/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config class for analysis\n",
    "class AnalysisConfig(object):\n",
    "    \"\"\"\n",
    "    Configuration object:\n",
    "    notes by test, full per corpus\n",
    "    paths by output, gold and system location\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self = self    \n",
    "       \n",
    "        self.systems = ['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls']\n",
    "        #self.systems = ['biomedicus', 'clamp', 'ctakes', 'metamap']\n",
    "        #self.systems = ['metamap']\n",
    "        #self.systems = ['biomedicus']\n",
    "        #self.systems = ['quick_umls']\n",
    "        self.data_dir = data_directory\n",
    "    \n",
    "    def corpus_config(self): \n",
    "        usys_data = system_annotation\n",
    "        ref_data = database_name+'.'+table_name\n",
    "        return usys_data, ref_data\n",
    "        \n",
    "\n",
    "analysisConf =  AnalysisConfig()\n",
    "usys, ref = analysisConf.corpus_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation class for UIMA systems\n",
    "class AnnotationSystems(object):\n",
    "    \"\"\"   \n",
    "    CAS XMI Annotations of interest\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\" \n",
    "        annotation base types\n",
    "        \"\"\"   \n",
    "        \n",
    "        self.biomedicus_types = [\"biomedicus.v2.UmlsConcept\"]\n",
    "                                 #\"biomedicus.v2.Negated\"\n",
    "                                 #\"biomedicus.v2.Acronym\",\n",
    "                                 #\"biomedicus.v2.DictionaryTerm\",\n",
    "                                 #\"biomedicus.v2.Historical\"]\n",
    "        \n",
    "        self.clamp_types = [\"edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA\"]\n",
    "                            #\"org.apache.ctakes.typesystem.type.syntax.ConllDependencyNode\",\n",
    "                            #\"edu.uth.clamp.nlp.typesystem.ClampRelationUIMA\"]    \n",
    "        \n",
    "        self.ctakes_types = ['ctakes_mentions']#\"org.apache.ctakes.typesystem.type.textspan.Sentence\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.DiseaseDisorderMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.MedicationMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.ProcedureMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.refsem.UmlsConcept\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.SignSymptomMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.AnatomicalSiteMention\"]\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.MeasurementAnnotation\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.EventMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.EntityMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.Predicate\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.syntax.WordToken\"]\n",
    "        \n",
    "        self.metamap_types = [#\"org.metamap.uima.ts.Utterance\",\n",
    "                              #\"org.metamap.uima.ts.Span\",\n",
    "                              #\"org.metamap.uima.ts.Phrase\"]\n",
    "                              \"org.metamap.uima.ts.Candidate\"]\n",
    "                              #\"org.metamap.uima.ts.CuiConcept\",\n",
    "                              #\"org.metamap.uima.ts.Negation\"]\n",
    "                \n",
    "        self.quick_umls_types = [#'concept']#,\n",
    "                                #'concept_cosine_length_false',\n",
    "                                #'concept_cosine_length_true',\n",
    "                                #'concept_cosine_score_false',\n",
    "                                #'concept_cosine_score_true',\n",
    "                                #'concept_dice_length_false',\n",
    "                                #'concept_dice_length_true',\n",
    "                                #'concept_dice_score_false',\n",
    "                                #'concept_dice_score_true',\n",
    "                                #'concept_jaccard_length_false',\n",
    "                                #'concept_jaccard_length_true',\n",
    "                                #'concept']\n",
    "                                 'concept_jaccard_score_False']\n",
    "                                #'concept_jaccard_score_true']\n",
    "                \n",
    "        '''\n",
    "\n",
    "        self.biomedicus_dir = \"biomedicus_out/\"\n",
    "        self.biomedicus_types = [#\"biomedicus.v2.UmlsConcept\"]\n",
    "                                  #\"biomedicus.v2.Negated\"\n",
    "                                 \"biomedicus.v2.Acronym\",\n",
    "                                 \"biomedicus.v2.DictionaryTerm\",\n",
    "                                 \"biomedicus.v2.Historical\"]\n",
    "        \n",
    "        \n",
    "        self.clamp_dir = \"clamp_out/\"\n",
    "        #self.clamp_types = [#\"edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA\"]\n",
    "                             #\"org.apache.ctakes.typesystem.type.syntax.ConllDependencyNode\",\n",
    "                             #\"edu.uth.clamp.nlp.typesystem.ClampRelationUIMA\"]\n",
    "        \n",
    "        \n",
    "        self.ctakes_dir = \"ctakes_out/\"\n",
    "        self.ctakes_types = [\"org.apache.ctakes.typesystem.type.textspan.Sentence\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.DiseaseDisorderMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.MedicationMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.ProcedureMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.refsem.UmlsConcept\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.SignSymptomMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.AnatomicalSiteMention\"]\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.MeasurementAnnotation\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.EventMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.EntityMention\",\n",
    "                             \"org.apache.ctakes.typesystem.type.textsem.Predicate\",\n",
    "                             \"org.apache.ctakes.typesystem.type.syntax.WordToken\"]\n",
    "        \n",
    "        self.metamap_dir = \"metamap_out/\"\n",
    "        self.metamap_types = [\"org.metamap.uima.ts.Utterance\",\n",
    "                              \"org.metamap.uima.ts.Span\",\n",
    "                              \"org.metamap.uima.ts.Phrase\"]\n",
    "                              #\"org.metamap.uima.ts.Candidate\"]\n",
    "                              #\"org.metamap.uima.ts.CuiConcept\",\n",
    "                              #\"org.metamap.uima.ts.Negation\"]\n",
    "                              \n",
    "        '''\n",
    "       \n",
    "    def get_system_type(self, system):\n",
    "        \n",
    "        \"\"\"\n",
    "        return system types\n",
    "        \"\"\"\n",
    "        \n",
    "        if system == \"biomedicus\":\n",
    "            view = \"Analysis\"\n",
    "        else:\n",
    "            view = \"_InitialView\"\n",
    "\n",
    "        if system == 'biomedicus':\n",
    "            types = self.biomedicus_types\n",
    "\n",
    "        elif system == 'clamp':\n",
    "            types = self.clamp_types\n",
    "\n",
    "        elif system == 'ctakes':\n",
    "            types = self.ctakes_types\n",
    "\n",
    "        elif system == 'metamap':\n",
    "            types = self.metamap_types\n",
    "        \n",
    "        elif system == \"quick_umls\":\n",
    "            types = self.quick_umls_types\n",
    "            \n",
    "        return types, view\n",
    "    \n",
    "annSys = AnnotationSystems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def get_notes(analysis_type: str, corpus: str) -> List[str]:\n",
    "    \n",
    "    if 'test' in analysis_type:\n",
    "        # test set of notes\n",
    "        if corpus == 'mipacq':\n",
    "            notes = ['522412787',\n",
    "             '617637585',\n",
    "             '3307880735-8',\n",
    "             '9080688558',\n",
    "             '618370565',\n",
    "             '573718188',\n",
    "             '534584',\n",
    "             '60891',\n",
    "             '62620',\n",
    "             '616172834']\n",
    "            \n",
    "        elif corpus == 'i2b2':\n",
    "            print('TODO')\n",
    "        \n",
    "        print('TEST NOTES!')\n",
    "        #,\n",
    "#          '4130154172-4',\n",
    "#          '3580478614',\n",
    "#          '5024581165-5',\n",
    "#          '4486835700-9',\n",
    "#          '534828617',\n",
    "#          '8154986253',\n",
    "#          '533855209',\n",
    "#          '60118',\n",
    "#          '3537704982-3',\n",
    "#          '617637585',\n",
    "#          '60118',\n",
    "#          '9045889026',\n",
    "#          '8819868493-8',\n",
    "#          '533698',\n",
    "#          '535978760']\n",
    "     \n",
    "    else:\n",
    "        \n",
    "        if corpus == 'mipacq':\n",
    "        # these did not meet the minimal criteria for parsing\n",
    "            notes = [\"0595040941-0\",\n",
    "                    \"0778429553-0\",\n",
    "                    \"1014681675\",\n",
    "                    \"2889522952-2\",\n",
    "                    \"3080383448-5\",\n",
    "                    \"3300000926-3\",\n",
    "                    \"3360037185-3\",\n",
    "                    \"3580973392\",\n",
    "                    \"3627629462-3\",\n",
    "                    \"4323116051-4\",\n",
    "                    \"477704053-4\",\n",
    "                    \"528317073\",\n",
    "                    \"531702602\",\n",
    "                    \"534061073\",\n",
    "                    \"54832076\",\n",
    "                    \"5643725437-6\",\n",
    "                    \"5944412090-5\",\n",
    "                    \"6613169476-6\",\n",
    "                    \"7261075903-7\",\n",
    "                    \"7504944368-7\",\n",
    "                    \"7999462393-7\",\n",
    "                    \"8131081430\",\n",
    "                    \"8171084310\",\n",
    "                    \"8193787896\",\n",
    "                    \"8295055184-8\",\n",
    "                    \"8823185307-8\"]\n",
    "            \n",
    "        elif corpus == 'i2b2':\n",
    "            # these notes were not processed \n",
    "            notes = ['0081', \n",
    "                     '0401']\n",
    "\n",
    "        else:\n",
    "            notes = None\n",
    "            \n",
    "    return notes# training_notes\n",
    "print(get_notes('entity', corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np # access to Numpy from Python layer\n",
    "import math\n",
    "\n",
    "class Metrics(object):\n",
    "    \"\"\"\n",
    "    metrics class:\n",
    "    returns an instance with confusion matrix metrics\n",
    "    \"\"\"\n",
    "    def __init__(self, system_only, gold_only, gold_system_match, system_n, neither = 0): # neither: no sys or manual annotation\n",
    "\n",
    "        self = self    \n",
    "        self.system_only = system_only\n",
    "        self.gold_only = gold_only\n",
    "        self.gold_system_match = gold_system_match\n",
    "        self.system_n = system_n\n",
    "        self.neither = neither\n",
    "        \n",
    "    def get_confusion_metrics(self, corpus = None, test = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        compute confusion matrix measures, as per  \n",
    "        https://stats.stackexchange.com/questions/51296/how-do-you-calculate-precision-and-recall-for-multiclass-classification-using-co\n",
    "        \"\"\"\n",
    "        cdef:\n",
    "            int TP, FP, FN\n",
    "            double TM\n",
    "\n",
    "        TP = self.gold_system_match\n",
    "        FP = self.system_only\n",
    "        FN = self.gold_only\n",
    "        \n",
    "        TM = TP/math.sqrt(self.system_n) # TigMetric\n",
    "       \n",
    "        if not test:\n",
    "            \n",
    "            if corpus == 'casi':\n",
    "                recall = TP/(TP + FN)\n",
    "                precision = TP/(TP + FP)\n",
    "                F = 2*(precision*recall)/(precision + recall)\n",
    "            else:\n",
    "                if self.neither == 0:\n",
    "                    confusion = [[0, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "                else:\n",
    "                    confusion = [[self.neither, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "                c = np.asarray(confusion)\n",
    "                recall = np.diag(c) / np.sum(c, axis = 1)\n",
    "                precision = np.diag(c) / np.sum(c, axis = 0)\n",
    "                F = 2*(precision*recall)/(precision + recall)\n",
    "        else:\n",
    "            precision = TP/(TP+FP)\n",
    "            recall = TP/(TP+FN)\n",
    "            F = 2*(precision*recall)/(precision + recall)\n",
    "        \n",
    "        # Tignanelli Metric\n",
    "        if FN == 0:\n",
    "            TP_FN_R = TP\n",
    "        elif FN > 0:\n",
    "            TP_FN_R = TP/FN\n",
    " \n",
    "        return F, recall, precision, TP, FP, FN, TP_FN_R, TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out(name: str, analysis_type: str, c: object):\n",
    "   \n",
    "    \"\"\"\n",
    "    write matching and reference-only sets to file for ease in merging combinations\n",
    "    \"\"\"\n",
    "    \n",
    "    # write output to file\n",
    "    dir_out = analysisConf.data_dir + 'single_system_out/'\n",
    "    with open(dir_out + name + '_' + analysis_type + '_' + c.corpus + '_matches.txt', 'w') as f:\n",
    "        for item in list(c.matches):\n",
    "            f.write(\"%s\\n\" % str(item))\n",
    "\n",
    "    # write to file\n",
    "    with open(dir_out + name + '_' + analysis_type + '_' + c.corpus + '_ref_only.txt', 'w') as f:\n",
    "        for item in list(c.false_negatives):\n",
    "            f.write(\"%s\\n\" % str(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%cython \n",
    "\n",
    "#from __main__ import write_out\n",
    "\n",
    "#import numpy as np # access to Numpy from Python layer\n",
    "def label_vector(doc: str, ann: List[int], labels: List[str]) -> np.array:\n",
    "\n",
    "    #print(ann, doc, labels)\n",
    "\n",
    "    v = np.zeros(doc)\n",
    "    labels = list(labels)\n",
    "    \n",
    "    for (i, lab) in enumerate(labels):\n",
    "        i += 1  # 0 is reserved for no label\n",
    "        idxs = [np.arange(a.begin, a.end) for a in ann if a.label == lab]\n",
    "            \n",
    "        idxs = [j for mask in idxs for j in mask]\n",
    "        v[idxs] = i\n",
    "\n",
    "    return v\n",
    "\n",
    "# test confusion matrix elements for vectorized annotation set; includes TN\n",
    "def confused(sys1, ann1):\n",
    "    TP = np.sum(np.logical_and(sys1 >= 1, ann1 == sys1 ))\n",
    "\n",
    "    # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "    TN = np.sum(np.logical_and(sys1 == 0, ann1 == 0))\n",
    "\n",
    "    # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "    FP = np.sum(np.logical_and(sys1 >= 1, ann1 == 0))\n",
    "\n",
    "    # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "    FN = np.sum(np.logical_and(sys1 == 0, ann1 >= 1))\n",
    "    \n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "\n",
    "def get_cooccurences(ref, sys, analysis_type: str, corpus: str, single_sys = True, name = None):\n",
    "    \"\"\"\n",
    "    get coocurences between system and reference; exact match; TODO: add relaxed\n",
    "    \"\"\"\n",
    "    # test cooccurences\n",
    "    class Coocurences(object):\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.ref_system_match = 0\n",
    "            self.ref_only = 0\n",
    "            self.system_only = 0\n",
    "            self.system_n = 0\n",
    "            self.ref_n = 0\n",
    "            self.matches = set()\n",
    "            self.false_negatives = set()\n",
    "            self.corpus = corpus\n",
    "            self.cases = set(ref[\"file\"].tolist()) # cases to label \n",
    "\n",
    "    c = Coocurences()\n",
    "    \n",
    "    # test for converting to vectorization and i-o labeling\n",
    "    def test_io():\n",
    "        test = c.cases\n",
    "        if analysis_type == 'entity':\n",
    "            docs = [(x, len(open(\"/Users/gms/development/nlp/nlpie/data/ensembling-u01/i2b2/source_data/test_data/\" + x + \".txt\", 'r').read())) for x in test]\n",
    "\n",
    "        ann = ref.copy()\n",
    "        ann = ann.rename(index=str, columns={\"start\": \"begin\", \"file\": \"case\"}).copy()\n",
    "        cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "        if analysis_type == 'entity':\n",
    "            labels = [\"concept\"]\n",
    "            ann[\"label\"] = 'concept'\n",
    "            ann = ann[cols_to_keep].copy()\n",
    "\n",
    "        sys_ = sys.rename(index=str, columns={\"note_id\": \"case\"}).copy()\n",
    "        \n",
    "        # need for enttity-only\n",
    "        if analysis_type == 'entity':\n",
    "            sys_[\"label\"] = 'concept'\n",
    "        \n",
    "        sys_ = sys_[cols_to_keep]\n",
    "       \n",
    "        tp = []\n",
    "        tn = []\n",
    "        fp = []\n",
    "        fn = []\n",
    "        cvals = []\n",
    "        out = []\n",
    "        t = []\n",
    "        d = defaultdict(list)\n",
    "        \n",
    "        for n in range(len(docs)):\n",
    "            a1 = [i for i in ann[ann[\"case\"] == docs[n][0]].copy().itertuples(index=False)]\n",
    "            s1 = [i for i in sys_[sys_[\"case\"] == docs[n][0]].copy().itertuples(index=False)]\n",
    "\n",
    "            ann1 = label_vector(docs[n][1], a1, labels)\n",
    "            sys1 = label_vector(docs[n][1], s1, labels)\n",
    "            \n",
    "            TP, TN, FP, FN = confused(sys1, ann1)\n",
    "            cvals.append([TP, TN, FP, FN])\n",
    "                 \n",
    "            d['sys'].append(list([int(i) for i in sys1]))\n",
    "            d['oracle'].append(list([int(i) for i in ann1]))\n",
    "            d['case'].append(docs[n][0])\n",
    "            \n",
    "            '''\n",
    "            print(\"tn:\", np.intersect1d(np.where(ann1 == 0)[0], np.where(sys1 == 0)[0]),  \n",
    "                  \"tp:\", np.intersect1d(np.where(ann1 == 1)[0], np.where(sys1 == 1)[0]), \n",
    "                  \"fn:\", np.intersect1d(np.where(ann1 == 1)[0], np.where(sys1 == 0)[0]), \n",
    "                  \"fp:\", np.intersect1d(np.where(ann1 == 0)[0], np.where(sys1 == 1)[0]))\n",
    "            '''\n",
    "        d['labels'] = labels\n",
    "        \n",
    "        corp = shelve.open('/Users/gms/Desktop/' + sys.name + '_' + corpus + '.dat')\n",
    "        \n",
    "        for k in d:\n",
    "            corp[k] = d[k]\n",
    "        \n",
    "        corp.close()\n",
    "       \n",
    "        return cvals\n",
    "    \n",
    "    if corpus == 'i2b2':\n",
    "        TP, TN, FP, FN = np.sum(test_io(), axis=0)\n",
    "        F, recall, precision, TP, FP, FN, TP_FN_R, TM = Metrics(FP, FN, TP, len(sys), TN).get_confusion_metrics() #no TN\n",
    "        print('test_io():', TP, TN, FP, FN, np.mean(F), np.mean(recall), np.mean(precision))\n",
    "    \n",
    "    # non-vectorized:\n",
    "    \n",
    "    if corpus != 'casi':\n",
    "        if 'entity' in analysis_type and single_sys: # mipacq n -> 16793\n",
    "            cols_to_keep = ['begin', 'end', 'note_id']\n",
    "            sys = sys[cols_to_keep].drop_duplicates()\n",
    "            ref = ref[['start', 'end', 'file']].drop_duplicates()\n",
    "            sys.name = name\n",
    "        elif 'cui' in analysis_type and single_sys: # mipacq n -> 10799\n",
    "            cols_to_keep = ['cui', 'note_id']\n",
    "            sys = sys[cols_to_keep].drop_duplicates()\n",
    "            # do not overestimate FP\n",
    "            sys = sys[~sys['cui'].isnull()] \n",
    "            ref = ref[['value', 'file']].drop_duplicates()\n",
    "            ref = ref[~ref['value'].isnull()]\n",
    "            sys.name = name\n",
    "        elif 'full' in analysis_type and single_sys: # mipacq n -> 17393\n",
    "            cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "            sys = sys[cols_to_keep].drop_duplicates()\n",
    "            sys = sys[~sys['cui'].isnull()]\n",
    "            ref = ref[['start', 'end', 'value', 'file']].drop_duplicates()\n",
    "            ref = ref[~ref['value'].isnull()]\n",
    "            sys.name = name\n",
    "\n",
    "        # matches via inner join\n",
    "        matches = pd.merge(sys, ref, how = 'inner', left_on=['begin','end','note_id'], right_on = ['start','end','file']) \n",
    "        # reference-only via left outer join\n",
    "        fn = pd.merge(ref, sys, how = 'left', left_on=['start','end','file'], right_on = ['begin','end','note_id']) \n",
    "\n",
    "        fn = fn[fn['begin'].isnull()] # get as outer join with no match\n",
    "\n",
    "        if 'entity' in analysis_type and single_sys:\n",
    "            cols_to_keep = ['start', 'end', 'file']\n",
    "        else:\n",
    "            cols_to_keep = ['start', 'end', 'value', 'file']\n",
    "\n",
    "        matches = matches[cols_to_keep]\n",
    "        fn = fn[cols_to_keep]\n",
    "\n",
    "        # use for metrics \n",
    "        c.matches = c.matches.union(df_to_set(matches, analysis_type, 'ref'))\n",
    "        c.false_negatives = c.false_negatives.union(df_to_set(fn, analysis_type, 'ref'))\n",
    "        c.ref_system_match = len(c.matches)\n",
    "        c.system_only = len(sys) - len(c.matches)\n",
    "        c.system_n = len(sys)\n",
    "        c.ref_n = len(ref)\n",
    "        c.ref_only = len(c.false_negatives)\n",
    "        \n",
    "    else:\n",
    "        #matches = df_to_set(pd.read_sql(\"select `case` from test.amia_2019_analytical_v where overlap = 1;\", con=engine), 'entity', 'sys', 'casi')\n",
    "        \n",
    "        sql = \"select `case` from test.amia_2019_analytical_v where overlap = 1 and `system` = %(sys.name)s\"  \n",
    "        #ref_ann = pd.read_sql(sql, params={\"training_notes\":training_notes}, con=engine)\n",
    "        \n",
    "        matches = pd.read_sql(sql, params={\"sys.name\":sys.name}, con=engine)\n",
    "        \n",
    "        sql = \"select `case` from test.amia_2019_analytical_v where (overlap = 0 or overlap is null) and `system` = %(sys.name)s\"  \n",
    "        #ref_ann = pd.read_sql(sql, params={\"training_notes\":training_notes}, con=engine)\n",
    "        \n",
    "        fn = pd.read_sql(sql, params={\"sys.name\":sys.name}, con=engine)\n",
    "        \n",
    "        c.matches = df_to_set(matches, 'entity', 'sys', 'casi')\n",
    "        c.fn = df_to_set(fn, 'entity', 'sys', 'casi')\n",
    "        c.ref_system_match = len(c.matches)\n",
    "        c.system_only = len(sys) - len(c.matches)\n",
    "        c.system_n = len(matches) + len(fn)\n",
    "        c.ref_n = len(matches) + len(fn)\n",
    "        c.ref_only = len(fn)\n",
    "        \n",
    "        print('cooc', c.ref_system_match, c.system_only, c.ref_n, c.ref_n, c.ref_only)\n",
    "        \n",
    "    # sanity check\n",
    "    if len(ref) - c.ref_system_match < 0:\n",
    "        print('Error: ref_system_match > len(ref)!')\n",
    "    if len(ref) != c.ref_system_match + c.ref_only:\n",
    "        print('Error: ref count mismatch!')\n",
    "   \n",
    "    # save TP/FN\n",
    "    if single_sys and corpus != 'casi':\n",
    "        print(analysis_type)\n",
    "        write_out(sys.name, analysis_type, c)\n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging test for i-o labeled data\n",
    "import numpy as np\n",
    "import shelve\n",
    "# load shelve\n",
    "def read_shelve():\n",
    "    corp = shelve.open('/Users/gms/Desktop/test.dat')\n",
    "\n",
    "    return corp\n",
    "        \n",
    "test = read_shelve()\n",
    "\n",
    "#l0 = list(t0)\n",
    "#l1 = list(t1)\n",
    "\n",
    "def test_merge_vector(test):\n",
    "    # get sample for testing\n",
    "    for case in test['case'][3:5]:\n",
    "        for i in range(len(test['case'][3:5])):\n",
    "            if i == 3:\n",
    "                t0 = test['oracle'][3][0:750]\n",
    "            else:\n",
    "                t1 = test['oracle'][4][0:750]\n",
    "\n",
    "            #print('case:', case, test['sys'][i], test['oracle'][i], confused(np.array(test['sys'][i]), np.array(test['oracle'][i])))\n",
    "        #print(t0, t1)\n",
    "\n",
    "    t0 = np.array(test['oracle'][3][0:750])\n",
    "    t1 = np.array(test['oracle'][5][0:750])\n",
    "\n",
    "    l0 = list(t0)\n",
    "    l1 = list(t1)\n",
    "    \n",
    "    l0 = [0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0] \n",
    "    l1 = [0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0]\n",
    "    \n",
    "    print(l0, l1)\n",
    "\n",
    "    def intersection(lst1, lst2): \n",
    "        out = list()\n",
    "        if isinstance(lst1, set) and isinstance(lst2, set):\n",
    "            out = (set(lst1) & set(lst2))\n",
    "        elif isinstance(lst1, set) and isinstance(lst2, np.int64):\n",
    "            out = (set(lst1) & set([lst2]))\n",
    "        elif isinstance(lst1, np.int64) and isinstance(lst2, set):\n",
    "            out = (set([lst1]) & set(lst2))\n",
    "        elif isinstance(lst1, np.int64) and isinstance(lst2, np.int64):\n",
    "            out = (set([lst1]) & set([lst2]))\n",
    "        #if len(out) > 1:\n",
    "        return out\n",
    "        #elif len(out) == 1:\n",
    "        #    return out[0]\n",
    "        #else:\n",
    "        #    return 0\n",
    "\n",
    "    def union(lst1, lst2): \n",
    "        out = list()\n",
    "        if isinstance(lst1, set) and isinstance(lst2, set):\n",
    "            out = set(lst1) | set(lst2)\n",
    "        elif isinstance(lst1, set) and isinstance(lst2, np.int64):\n",
    "            out = set(lst1) | set([lst2])\n",
    "        elif isinstance(lst1, np.int64) and isinstance(lst2, set):\n",
    "            out = set([lst1]) | set(lst2)\n",
    "        elif isinstance(lst1, np.int64) and isinstance(lst2, np.int64):\n",
    "            out = set([lst1]) | set([lst2])\n",
    "        #if len(out) == 1:\n",
    "        #    #out = out[0]\n",
    "        return out\n",
    "\n",
    "    # union and intersect\n",
    "    def umerges(l0, l1):\n",
    "        #un = [0]*len(l0)\n",
    "        #for i in range(len(l0)):\n",
    "        #    un[i] = union(l0[i], l1[i])\n",
    "\n",
    "        return [union(l0[i], l1[i]) for i in range(len(l0))]\n",
    "\n",
    "    %timeit un = umerges(l0, l1)\n",
    "    \n",
    "    x = umerges(l0, l1)\n",
    "    \n",
    "\n",
    "    #l2 = [1, {1, 4}, {3}, {2, 4}, {1}, 0, 2, 3, {0, 8}, {1, 8}]\n",
    "    \n",
    "    #print(umerges(x, l2))\n",
    "    \n",
    "    def imerges(l0, l1):\n",
    "        #inter = [0]*len(l0)\n",
    "        #for i in range(len(l0)):\n",
    "        \n",
    "\n",
    "        return [intersection(l0[i], l1[i]) for i in range(len(l0))]\n",
    "    \n",
    "    %timeit x = imerges(l0, l1)\n",
    "    \n",
    "    '''\n",
    "    union = [\n",
    "        ( [set(x) | set(y)] if isinstance(x, list) and isinstance(y, list)\n",
    "          else [set(x) | set([y])] if isinstance(x, list) and isinstance(y, int)\n",
    "          else [set([x]) | set(y)] if isinstance(x, int) and isinstance(y, list)\n",
    "          else [set([x]) | set([y])])\n",
    "\n",
    "         for x, y in zip(l0, l1)\n",
    "    ]\n",
    "\n",
    "    # unpack map object\n",
    "    #*y, = list(map(list, zip(*union)))\n",
    "    #%timeit list(map(list, zip(*union)))\n",
    "\n",
    "    intersection = [\n",
    "        ( [set(x) & set(y)] if isinstance(x, list) and isinstance(y, list)\n",
    "          else [set(x) & set([y])] if isinstance(x, list) and isinstance(y, int)\n",
    "          else [set([x]) & set(y)] if isinstance(x, int) and isinstance(y, list)\n",
    "          else [set([x]) & set([y])])\n",
    "          for x, y in zip(l0, l1)\n",
    "\n",
    "    ]\n",
    "\n",
    "    #*x, = list(map(list, zip(*intersection)))\n",
    "    #%timeit list(map(list, zip(*intersection)))\n",
    "    '''\n",
    "#test_merge_vector(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blah -=\n",
    "# https://kawahara.ca/how-to-compute-truefalse-positives-and-truefalse-negatives-in-python-for-binary-classification-problems/\n",
    "def confusing(sys1, ann1):\n",
    "    TP = np.sum(np.logical_and(sys1 == 1, ann1 == sys1))\n",
    "\n",
    "    # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "    TN = np.sum(np.logical_and(sys1 == 0, ann1 == 0))\n",
    "\n",
    "    # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "    FP = np.sum(np.logical_and(sys1 == 1, ann1 == 0))\n",
    "\n",
    "    # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "    FN = np.sum(np.logical_and(np.logical_or(sys1 == 0, sys1 is None), ann1 == 1))\n",
    "    \n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "#%%cython\n",
    "#import numpy as np # access to Numpy from Python layer\n",
    "#import time\n",
    "#from __main__ import read_shelve\n",
    "def imerge(l0, l1):\n",
    "\n",
    "    return [\n",
    "        ( [\n",
    "            set(x) & set(y)] if isinstance(x, list) and  isinstance(y, list)\n",
    "            else [set(x) & set([y])] if isinstance(x, list) and  isinstance(y, np.int64)\n",
    "            else [set(x) & y] if isinstance(x, list) and isinstance(y, set)\n",
    "            else [x & y] if isinstance(x, set) and isinstance(y, set)\n",
    "            else [x & set(y)] if isinstance(x, set) and isinstance(y, list)\n",
    "            else [x & set([y])] if isinstance(x, set) and isinstance(y, np.int64)\n",
    "            else [set([x]) & set(y)] if isinstance(x, np.int64) and  isinstance(y, list)\n",
    "            else [set([x]) & y] if isinstance(x, np.int64) and isinstance(y, set)\n",
    "            else [set([x]) & set([y])])\n",
    "        for x, y in zip(l0, l1)\n",
    "    ]\n",
    "\n",
    "def umerge(l0, l1):\n",
    "\n",
    "    return [\n",
    "        ( [\n",
    "            set(x) | set(y)] if isinstance(x, list) and  isinstance(y, list)\n",
    "            else [set(x) | set([y])] if isinstance(x, list) and  isinstance(y, np.int64)\n",
    "            else [set(x) | y] if isinstance(x, list) and isinstance(y, set)\n",
    "            else [x | y] if isinstance(x, set) and isinstance(y, set)\n",
    "            else [x | set(y)] if isinstance(x, set) and isinstance(y, list)\n",
    "            else [x | set([y])] if isinstance(x, set) and isinstance(y, np.int64)\n",
    "            else [set([x]) | y] if isinstance(x, np.int64) and isinstance(y, set)\n",
    "            else [set([x]) | set(y)] if isinstance(x, np.int64) and  isinstance(y, list)\n",
    "            else [set([x]) | set([y])])\n",
    "        for x, y in zip(l0, l1)\n",
    "    ]\n",
    "\n",
    "def imerge_int(l0, l1):\n",
    "    return [\n",
    "        ( [\n",
    "            set(x) & set(y)] if isinstance(x, list) and  isinstance(y, list)\n",
    "            else [set(x) & set([y])] if isinstance(x, list) and isinstance(y, int)\n",
    "            else [set(x) & y] if isinstance(x, list) and isinstance(y, set)\n",
    "            else [x & y] if isinstance(x, set) and isinstance(y, set)\n",
    "            else [x & set(y)] if isinstance(x, set) and isinstance(y, list)\n",
    "            else [x & set([y])] if isinstance(x, set) and isinstance(y, int)\n",
    "            else [set([x]) & set(y)] if isinstance(x, int) and  isinstance(y, list)\n",
    "            else [set([x]) & y] if isinstance(x, int) and isinstance(y, set)\n",
    "            else [set([x]) & set([y])])\n",
    "        for x, y in zip(l0, l1)\n",
    "    ]\n",
    "\n",
    "def umerge_int(l0, l1):\n",
    "    return [\n",
    "        ( [\n",
    "            set(x) | set(y)] if isinstance(x, list) and  isinstance(y, list)\n",
    "            else [set(x) | set([y])] if isinstance(x, list) and  isinstance(y, int)\n",
    "            else [set(x) | y] if isinstance(x, list) and isinstance(y, set)\n",
    "            else [x | y] if isinstance(x, set) and isinstance(y, set)\n",
    "            else [x | set(y)] if isinstance(x, set) and isinstance(y, list)\n",
    "            else [x | set([y])] if isinstance(x, set) and isinstance(y, int)\n",
    "            else [set([x]) | y] if isinstance(x, int) and isinstance(y, set)\n",
    "            else [set([x]) | set(y)] if isinstance(x, int) and  isinstance(y, list)\n",
    "            else [set([x]) | set([y])])\n",
    "        for x, y in zip(l0, l1)\n",
    "    ]\n",
    "\n",
    "def test_merge_shelve():\n",
    "    ctakes = shelve.open('/Users/gms/Desktop/ctakes_' + corpus + '.dat')\n",
    "    clamp = shelve.open('/Users/gms/Desktop/clamp_' + corpus + '.dat')\n",
    "    mm = shelve.open('/Users/gms/Desktop/metamap_' + corpus + '.dat')\n",
    "\n",
    "    print(test['case'][0:2])\n",
    "\n",
    "    #t0 = np.array(test['oracle'][3][0:750])\n",
    "    #t1 = np.array(test['oracle'][5][0:750])\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    sys = []\n",
    "    oracles = []\n",
    "    confuzz = []\n",
    "    for i in range(len(ctakes['case'])):\n",
    "        t0 = np.array(ctakes['sys'][i])\n",
    "        t1 = np.array(clamp['sys'][i])\n",
    "        t2 = np.array(mm['sys'][i])\n",
    "        oracle = np.array(ctakes['oracle'][i])\n",
    "\n",
    "        #print(ctakes['case'][i])\n",
    "\n",
    "        l0 = list(t0)\n",
    "        l1 = list(t1)\n",
    "        l2 = list(t2)\n",
    "\n",
    "        z = *map(list, zip(*umerge(l0, l1))),\n",
    "\n",
    "        #%time  *map(list, zip(*umerge(l0, l1))),\n",
    "        #%time  *map(list, zip(*umerge(z[0], l1))),\n",
    "\n",
    "        t = *map(list, zip(*umerge(z[0], l2))),\n",
    "\n",
    "        #results = *map(list, zip(*imerge(t[0], oracle))),\n",
    "\n",
    "        from functools import reduce \n",
    "        import itertools\n",
    "        import operator\n",
    "\n",
    "        test = [list(i) for i in t[0]]\n",
    "\n",
    "        replaced = [[none] if len(wd) == 0  else wd for wd in t[0]]\n",
    "\n",
    "        #print('truth:', {1, 0} in replaced)\n",
    "        replaced = [[1] if wd == {0, 1}  else wd for wd in replaced]\n",
    "\n",
    "        #print('len', len(replaced), len(oracle))\n",
    "\n",
    "        sys.append(replaced)\n",
    "        #oracles.append(list(oracle))\n",
    "\n",
    "        #print(len(np.array(list(itertools.chain.from_iterable(replaced)))), len(np.array(list(itertools.chain.from_iterable(t[0])))), len(oracle), len(t[0]))\n",
    "        tp, tn, fp, fn = confusing(np.array(list(itertools.chain.from_iterable(replaced))), oracle)\n",
    "\n",
    "        confuzz.append((tp, tn, fp, fn))\n",
    "        f, recall, precision, tp, fp, fn, tp_fn_r, tm = Metrics(fp, fn, tp, len(oracle), tn).get_confusion_metrics() #no tn\n",
    "        #print('test_io():', tp, tn, fp, fn, np.mean(f), np.mean(recall), np.mean(precision))\n",
    "\n",
    "        #print(tp, tn, fp, fn, np.array(test), oracle, np.array(list(itertools.chain.from_iterable(replaced))))\n",
    "\n",
    "        #print(oracle[0:2], np.array(t[0][0:2]))\n",
    "        #print(t[0][0:1])\n",
    "\n",
    "        #start = time.perf_counter()\n",
    "        #z = *map(list, zip(*imerge(l0, l1))),\n",
    "        #elapsed = (time.perf_counter() - start)\n",
    "        #print('time 2:', elapsed)\n",
    "        #%time *map(list, zip(*imerge(l0, l1))),\n",
    "        #%time *map(list, zip(*imerge(z[0], l1))),\n",
    "        #print( *map(list, zip(*imerge(z[0], l1))),)\n",
    "    print(' --- ')\n",
    "    #print(results)\n",
    "\n",
    "    print(len(list(itertools.chain.from_iterable(oracles))), len(list(itertools.chain.from_iterable(sys))))\n",
    "    #tp, tn, fp, fn = confusing(np.array(list(itertools.chain.from_iterable(sys))), np.array(list(itertools.chain.from_iterable(oracles))))\n",
    "    #f, recall, precision, tp, fp, fn, tp_fn_r, tm = metrics(fp, fn, tp, len(list(itertools.chain.from_iterable(oracles))), tn).get_confusion_metrics() #no tn\n",
    "    #print('test_io():', tp, tn, fp, fn, np.mean(f), np.mean(recall), np.mean(precision))\n",
    "    #print(confuzz)\n",
    "\n",
    "    print(list(map(sum, zip(*confuzz))))\n",
    "    z = list(map(sum, zip(*confuzz)))\n",
    "    # tp, tn, fp, fn -> (fp, fn, tp, len(oracle), tn)\n",
    "    f, recall, precision, tp, fp, fn, tp_fn_r, tm = Metrics(z[2], z[3], z[0], len(list(itertools.chain.from_iterable(sys))), z[1]).get_confusion_metrics() #no tn\n",
    "    print('test_io():', tp, tn, fp, fn, np.mean(f), np.mean(recall), np.mean(precision))\n",
    "    elapsed = (time.perf_counter() - start)\n",
    "    print('time 1:', elapsed)\n",
    "    # test_io(): 420721 2408 520533 16586 0.6283008747980883 0.7236264392738099 0.7071374894816047\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ft.lru_cache(maxsize=None)\n",
    "def get_metric_data(training_notes: List[str], analysis_type: str, corpus: str):\n",
    "    engine_request = str(database_type)+'://'+database_username+':'+database_password+\"@\"+database_url+'/'+database_name\n",
    "    engine = create_engine(engine_request, pool_pre_ping=True, pool_size=20, max_overflow=30)\n",
    "   \n",
    "    usys_file, ref_table = AnalysisConfig().corpus_config()\n",
    "    systems = AnalysisConfig().systems\n",
    "    \n",
    "    sys_ann = pd.read_csv(analysisConf.data_dir + usys_file, dtype={'note_id': str})\n",
    "    \n",
    "    if 'test' not in analysis_type:\n",
    "        if corpus != 'fairview':\n",
    "            sql = \"SELECT * FROM \" + ref_table + \" where file not in %(training_notes)s\"  \n",
    "            sys_ann = sys_ann[~sys_ann['note_id'].isin(training_notes)]\n",
    "        else:\n",
    "            sql = \"SELECT * FROM \" + ref_table  \n",
    "            sys_ann = sys_ann\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        sql = \"SELECT * FROM \" + ref_table + \" where file in %(training_notes)s\"  \n",
    "        sys_ann = sys_ann[sys_ann['note_id'].isin(training_notes)]\n",
    "    \n",
    "    ref_ann = pd.read_sql(sql, params={\"training_notes\":training_notes}, con=engine)\n",
    "    sys_ann = sys_ann.drop_duplicates()\n",
    "    \n",
    "    return ref_ann, sys_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "def geometric_mean(metrics):\n",
    "    \"\"\"\n",
    "    1. Get rank average of F1, TP/FN, TM\n",
    "        http://www.datasciencemadesimple.com/rank-dataframe-python-pandas-min-max-dense-rank-group/\n",
    "        https://stackoverflow.com/questions/46686315/in-pandas-how-to-create-a-new-column-with-a-rank-according-to-the-mean-values-o?rq=1\n",
    "    2. Take geomean of 2.\n",
    "        https://stackoverflow.com/questions/42436577/geometric-mean-applied-on-row\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.DataFrame() \n",
    "\n",
    "    metrics['F1 rank']=metrics['F'].rank(ascending=0,method='average')\n",
    "    metrics['TP/FN rank']=metrics['TP/FN'].rank(ascending=0,method='average')\n",
    "    metrics['TM rank']=metrics['TM'].rank(ascending=0,method='average')\n",
    "    metrics['Gmean'] = gmean(metrics.iloc[:,-3:],axis=1)\n",
    "\n",
    "    return metrics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metrics(analysis_type: str, corpus: str, single_sys = None):\n",
    "    start = time.time()\n",
    "\n",
    "    systems = AnalysisConfig().systems\n",
    "    metrics = pd.DataFrame()\n",
    "\n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    ref_ann, sys_ann = get_metric_data(training_notes, analysis_type, corpus)\n",
    "    \n",
    "    for sys in systems:\n",
    "            types, _ = AnnotationSystems().get_system_type(sys) # system types for iterable\n",
    "            for t in types:\n",
    "                print(t)\n",
    "                system = pd.DataFrame()\n",
    "                \n",
    "                system_annotations = sys_ann.copy()\n",
    "                \n",
    "                system = system_annotations[system_annotations['type'] == str(t)]\n",
    "            \n",
    "                if sys == 'quick_umls':\n",
    "                    system = system[system.score.astype(float) >= .8]\n",
    "                \n",
    "                if sys == 'metamap':\n",
    "                    system = system[system.score.abs().astype(int) >= 800]\n",
    "            \n",
    "                system = system.drop_duplicates()\n",
    "                system.name = sys\n",
    "                \n",
    "                c = get_cooccurences(ref_ann, system, analysis_type, corpus, True, system.name) # get matches, FN, etc.\n",
    "                \n",
    "                print(c.ref_n, c.ref_only, c.system_n, c.system_only, c.ref_system_match)\n",
    "                \n",
    "            if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "                F, recall, precision, TP, FP, FN, TP_FN_R, TM = Metrics(c.system_only, c.ref_only, c.ref_system_match, c.system_n).get_confusion_metrics(corpus)\n",
    "                \n",
    "                if corpus == 'casi':\n",
    "                    if sys == 'biomedicus':\n",
    "                        t = 'biomedicus.v2.Acronym'\n",
    "                        \n",
    "                    d = {'system': sys, \n",
    "                         'type': t, \n",
    "                         'F': F, \n",
    "                         'precision': precision, \n",
    "                         'recall': recall, \n",
    "                         'FN': FN, \n",
    "                         'TP/FN': TP_FN_R,\n",
    "                         'n_gold': c.ref_n, \n",
    "                         'n_sys': c.system_n, \n",
    "                         'TM': TM}\n",
    "                else:\n",
    "                    d = {'system': sys, \n",
    "                         'type': t, \n",
    "                         'F': F[1], \n",
    "                         'precision': precision[1], \n",
    "                         'recall': recall[1], \n",
    "                         'TP': TP, \n",
    "                         'FN': FN, \n",
    "                         'FP': FP, \n",
    "                         'TP/FN': TP_FN_R,\n",
    "                         'n_gold': c.ref_n, \n",
    "                         'n_sys': c.system_n, \n",
    "                         'TM': TM}\n",
    "\n",
    "                data = pd.DataFrame(d,  index=[0])\n",
    "                metrics = pd.concat([metrics, data], ignore_index=True)\n",
    "                metrics.drop_duplicates(keep='last', inplace=True)\n",
    "            else:\n",
    "                print(\"NO EXACT MATCHES FOR\", t)\n",
    "            elapsed = (time.time() - start)\n",
    "            print(\"elapsed:\", sys, elapsed)\n",
    "     \n",
    "    elapsed = (time.time() - start)\n",
    "    print(geometric_mean(metrics))\n",
    "    \n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    \n",
    "    if single_sys is None:\n",
    "        file_name = 'metrics_'\n",
    "    \n",
    "    metrics.to_csv(analysisConf.data_dir + corpus + '_' + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "    \n",
    "    print(\"total elapsed time:\", elapsed) \n",
    "\n",
    "# use to iterate through mm scores\n",
    "def generate_metrics_mm(analysis_type: str, corpus: str, single_sys = None):\n",
    "    start = time.time()\n",
    "    from pandas.api.types import is_numeric_dtype\n",
    "    #systems = [\"biomedicus\",\"ctakes\",\"metamap\",\"clamp\",\"quick_umls\"]\n",
    "    systems = AnalysisConfig().systems\n",
    "    #systems = [\"quick_umls\"]\n",
    "    metrics = pd.DataFrame()\n",
    "\n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    ref_ann, sys_ann = get_metric_data(training_notes, analysis_type, corpus)\n",
    "    \n",
    "    sys_ann = sys_ann[(sys_ann.score.notnull()) & (sys_ann['system'] == 'metamap')]\n",
    "    sys_ann = sys_ann[['begin', 'end', 'note_id', 'system', 'score']].drop_duplicates()\n",
    "    sys_ann.score = sys_ann.score.astype(int)\n",
    "    \n",
    "    for sys in systems:\n",
    "        types, __ = AnnotationSystems().get_system_type(sys) # system types for iterable\n",
    "        for t in types:\n",
    "            print(t)\n",
    "\n",
    "            for i in range(500, 1050, 50): \n",
    "\n",
    "                sys_ann = sys_ann[(sys_ann[\"score\"] >= i)].copy()\n",
    "\n",
    "                sys_ann.name = sys + str(i)\n",
    "\n",
    "                c = get_cooccurences(ref_ann, sys_ann, analysis_type, corpus, True, sys_ann.name) # get matches, FN, etc.\n",
    "\n",
    "                print(c.ref_n, c.ref_only, c.system_n, c.system_only, c.ref_system_match)\n",
    "\n",
    "                #print(i, len(system))\n",
    "\n",
    "                if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "                    F, recall, precision, TP, FP, FN, TP_FN_R, TM = Metrics(c.system_only, c.ref_only, c.ref_system_match, c.system_n).get_confusion_metrics()\n",
    "                    d = {'system': sys + '_score_' + str(i), \n",
    "                         'type': t, \n",
    "                         'F': F[1], \n",
    "                         'precision': precision[1], \n",
    "                         'recall': recall[1], \n",
    "                         'TP': TP, \n",
    "                         'FN': FN, \n",
    "                         'FP': FP, \n",
    "                         'TP/FN': TP_FN_R,\n",
    "                         'n_gold': c.ref_n, \n",
    "                         'n_sys': c.system_n, \n",
    "                         'TM': TM}\n",
    "\n",
    "                    data = pd.DataFrame(d,  index=[0])\n",
    "                    metrics = pd.concat([metrics, data], ignore_index=True)\n",
    "                    metrics.drop_duplicates(keep='last', inplace=True)\n",
    "                else:\n",
    "                    print(\"NO EXACT MATCHES FOR\", t)\n",
    "                elapsed = (time.time() - start)\n",
    "                print(\"elapsed:\", sys, elapsed)\n",
    "     \n",
    "    elapsed = (time.time() - start)\n",
    "    print(geometric_mean(metrics))\n",
    "    \n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    # UIMA or QuickUMLS\n",
    "    if single_sys is None:\n",
    "        file_name = 'mm_metrics_'\n",
    "    metrics.to_csv(analysisConf.data_dir + corpus + '_' + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "    \n",
    "    print(\"total elapsed time:\", elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in system matches from file\n",
    "def get_ref_n(analysis_type: str, corpus) -> int:\n",
    "    \n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    ref_ann, _ = get_metric_data(training_notes, analysis_type, corpus)\n",
    "    \n",
    "    if corpus == 'casi':\n",
    "        return len(ref_ann)\n",
    "        \n",
    "    else:\n",
    "        # do not overestimate fn\n",
    "        if 'entity' in analysis_type:\n",
    "            ref_ann = ref_ann[['start', 'end', 'file']].drop_duplicates()\n",
    "        elif 'cui' in analysis_type:\n",
    "            ref_ann = ref_ann[['value', 'file']].drop_duplicates()\n",
    "        elif 'full' in analysis_type:\n",
    "            ref_ann = ref_ann[['start', 'end', 'value', 'file']].drop_duplicates()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        ref_n = len(ref_ann.drop_duplicates())\n",
    "\n",
    "        return ref_n\n",
    "\n",
    "def get_sys_data(system: str, analysis_type: str, corpus: str) -> pd.DataFrame:\n",
    "   \n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    _, data = get_metric_data(training_notes, analysis_type, corpus)\n",
    "    \n",
    "    out = data[data['system'] == system].copy()\n",
    "    \n",
    "    if corpus == 'casi':\n",
    "        cols_to_keep = ['case', 'overlap'] \n",
    "        #cols_to_keep = ['case', 'begin', 'end'] \n",
    "        out = out[cols_to_keep].drop_duplicates()\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    else:\n",
    "        out = data[data['system']== system].copy()\n",
    "\n",
    "        if system == 'quick_umls':\n",
    "            out = out[(out.score.astype(float) >= 0.8) & (out[\"type\"] == 'concept_jaccard_score_False')]\n",
    "        \n",
    "        if system == 'metamap':\n",
    "            out = out[out.score.abs().astype(int) >= 800]\n",
    "\n",
    "        if 'entity' in analysis_type:\n",
    "            cols_to_keep = ['begin', 'end', 'note_id']\n",
    "        elif 'cui' in analysis_type:\n",
    "            cols_to_keep = ['cui', 'note_id']\n",
    "        elif 'full' in analysis_type:\n",
    "            cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "\n",
    "        out = out[cols_to_keep]\n",
    "\n",
    "        return out.drop_duplicates()\n",
    "\n",
    "def get_system_matches(system: str, analysis_type: str, corpus: str):\n",
    "   \n",
    "    if corpus == 'casi':\n",
    "        \n",
    "        sql = \"select `case`, overlap from test.amia_2019_cases where overlap = 1 and `system` = %(system)s\"  \n",
    "        #ref_ann = pd.read_sql(sql, params={\"training_notes\":training_notes}, con=engine)\n",
    "        \n",
    "        data_matches = df_to_set(pd.read_sql(sql, params={\"system\":system}, con=engine), 'entity', 'sys', 'casi')\n",
    "        \n",
    "        sql = \"select `case`, overlap from test.amia_2019_cases where (overlap = 0 or overlap is null) and `system` = %(system)s\"  \n",
    "        #ref_ann = pd.read_sql(sql, params={\"training_notes\":training_notes}, con=engine)\n",
    "        \n",
    "        data_fn = df_to_set(pd.read_sql(sql, params={\"system\":system}, con=engine), 'entity', 'sys', 'casi')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        dir_test = analysisConf.data_dir + 'single_system_out/'\n",
    "\n",
    "        file = dir_test + system + '_' + analysis_type + '_' + corpus + '_matches.txt'\n",
    "        data_matches = set(literal_eval(line.strip()) for line in open(file))\n",
    "\n",
    "        file = dir_test + system + '_' + analysis_type + '_' + corpus + '_ref_only.txt'\n",
    "        data_fn = set(literal_eval(line.strip()) for line in open(file)) #{ f for f in file.readlines() }\n",
    "\n",
    "    return data_matches, data_fn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GENERATE merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetTotals(object):\n",
    "    \"\"\" \n",
    "    returns an instance with merged match set numbers using either union or intersection of elements in set \n",
    "    \"\"\"\n",
    "    def __init__(self, ref_n, sys_n, match_set):\n",
    "\n",
    "        self = self    \n",
    "        self.ref_ann = ref_n\n",
    "        self.sys_n = sys_n\n",
    "        self.match_set = match_set\n",
    "\n",
    "    def get_ref_sys(self):\n",
    "\n",
    "        ref_only = self.ref_ann - len(self.match_set)\n",
    "        sys_only = self.sys_n - len(self.match_set)\n",
    "\n",
    "        return ref_only, sys_only, len(self.match_set), self.match_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_eval(ref_only: int, system_only: int, ref_system_match: int, system_n: int, ref_n: int) -> dict:\n",
    "    \"\"\"\n",
    "    Generate confusion matrix params\n",
    "    :params: ref_only, system_only, reference_system_match -> sets\n",
    "    matches, system_n, reference_n -> counts\n",
    "    :return: dictionary object\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if ref_only + ref_system_match != ref_n:\n",
    "        print('ERROR!')\n",
    "\n",
    "    # get evaluation metrics\n",
    "    d = {}\n",
    "    \n",
    "    F, recall, precision, TP, FP, FN, TP_FN_R, TM  = Metrics(system_only, ref_only, ref_system_match, system_n).get_confusion_metrics()\n",
    "\n",
    "    d = {\n",
    "         'F': F[1], \n",
    "         'precision': precision[1], \n",
    "         'recall': recall[1], \n",
    "         'TP': TP, \n",
    "         'FN': FN, \n",
    "         'FP': FP, \n",
    "         'TP/FN': TP_FN_R,\n",
    "         'n_gold': ref_n, \n",
    "         'n_sys': system_n, \n",
    "         'TM': TM\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if system_n - FP != TP:\n",
    "        print('inconsistent system n!')\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_multi(self, df, on):\n",
    "    return self.reset_index().join(df,on=on).set_index(self.index.names)\n",
    "pd.DataFrame.merge_multi = merge_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%cython\n",
    "\n",
    "import operator as op\n",
    "import pandas as pd\n",
    "import gevent\n",
    "from __main__ import get_system_matches, get_sys_data, merge_multi\n",
    "\n",
    "def process_sentence(pt, sentence, analysis_type, corpus):\n",
    "    \"\"\"\n",
    "    Recursively evaluate parse tree, \n",
    "    with check for existence before build\n",
    "       :param sentence: to process\n",
    "       :return class of merged annotations, boolean operated system df \n",
    "    \"\"\"\n",
    "    \n",
    "    class Results(object):\n",
    "        def __init__(self):\n",
    "            self.results = set()\n",
    "            #self.operations = []\n",
    "            self.system_merges = pd.DataFrame()\n",
    "            \n",
    "    r = Results()\n",
    "    \n",
    "    if 'entity' in analysis_type and corpus != 'casi': \n",
    "        cols_to_keep = ['begin', 'end', 'note_id'] # entity only\n",
    "    elif 'full' in analysis_type: \n",
    "        cols_to_keep = ['cui', 'begin', 'end', 'note_id'] # entity only\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['cui', 'note_id'] # entity only\n",
    "    elif corpus == 'casi':\n",
    "        cols_to_keep = ['case', 'overlap']\n",
    "    \n",
    "    def evaluate(parseTree):\n",
    "        oper = {'&': op.and_, '|': op.or_}\n",
    "        \n",
    "        if parseTree:\n",
    "            #leftC = evaluate(parseTree.getLeftChild())\n",
    "            #rightC = evaluate(parseTree.getRightChild())\n",
    "            leftC = gevent.spawn(evaluate, parseTree.getLeftChild())\n",
    "            rightC = gevent.spawn(evaluate, parseTree.getRightChild())\n",
    "            \n",
    "            if leftC.get() and rightC.get():\n",
    "                query = set()\n",
    "                system_query = pd.DataFrame()\n",
    "                fn = oper[parseTree.getRootVal()]\n",
    "                \n",
    "                if isinstance(leftC.get(), str):\n",
    "                    \n",
    "                    # get system as leaf node \n",
    "                    left, _ = get_system_matches(leftC.get(), analysis_type, corpus)\n",
    "                    left_sys = get_sys_data(leftC.get(), analysis_type, corpus)\n",
    "                \n",
    "                elif isinstance(leftC.get(), tuple):\n",
    "                    left = leftC.get()[0]\n",
    "                    l_sys = leftC.get()[1]\n",
    "                \n",
    "                if isinstance(rightC.get(), str):\n",
    "                    # get system as leaf node\n",
    "                    right, _ = get_system_matches(rightC.get(), analysis_type, corpus)\n",
    "                    right_sys = get_sys_data(rightC.get(), analysis_type, corpus)\n",
    "                    \n",
    "                elif isinstance(rightC.get(), tuple):\n",
    "                    right = rightC.get()[0]\n",
    "                    r_sys = rightC.get()[1]\n",
    "                    \n",
    "                # create match set based on boolean operation\n",
    "                match_set = fn(left, right)\n",
    "               \n",
    "                if corpus != 'casi':\n",
    "                    if fn == op.or_:\n",
    "                        r.results = r.results.union(match_set)\n",
    "\n",
    "                        if isinstance(leftC.get(), str) and isinstance(rightC.get(), str):\n",
    "                            frames = [left_sys, right_sys]\n",
    "                            df = pd.concat(frames,  ignore_index=True)\n",
    "                            \n",
    "                            #df = left_sys.append(right_sys)\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                        elif isinstance(leftC.get(), str) and isinstance(rightC.get(), tuple):\n",
    "                            frames = [left_sys, r_sys]\n",
    "                            df = pd.concat(frames,  ignore_index=True)\n",
    "                            #df = left_sys.append(r_sys)\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                        elif isinstance(leftC.get(), tuple) and isinstance(rightC.get(), str):\n",
    "                            frames = [l_sys, right_sys]\n",
    "                            df = pd.concat(frames,  ignore_index=True)\n",
    "                            #df = right_sys.append(l_sys)\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                        elif isinstance(leftC.get(), tuple) and isinstance(rightC.get(), tuple):\n",
    "                            frames = [l_sys, r_sys]\n",
    "                            df = pd.concat(frames,  ignore_index=True)\n",
    "                            #df = l_sys.append(r_sys)\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                    if fn == op.and_:\n",
    "                        if len(r.results) == 0:\n",
    "                            r.results = match_set\n",
    "                        r.results = r.results.intersection(match_set)\n",
    "\n",
    "                        if isinstance(leftC.get(), str) and isinstance(rightC.get(), str):\n",
    "                            df = left_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                            #df = left_sys.join(right_sys, on=cols_to_keep, how='inner')\n",
    "                            #df = left_sys.merge_multi(right_sys,on=cols_to_keep)\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                        elif isinstance(leftC.get(), str) and isinstance(rightC.get(), tuple):\n",
    "                            df = left_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                            #df = left_sys.Join(r_sys, on=cols_to_keep, how='inner')\n",
    "                            #df = left_sys.merge_multi(r_sys, on=cols_to_keep)\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                        elif isinstance(leftC.get(), tuple) and isinstance(rightC.get(), str):\n",
    "                            df = l_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                            #df = l_sys.join(right_sys, on=cols_to_keep, how='inner')\n",
    "                            #df = l_sys.merge_multi(right_sys, on=cols_to_keep)\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                        elif isinstance(leftC.get(), tuple) and isinstance(rightC.get(), tuple):\n",
    "                            df = l_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                            #df = l_sys.join(r_sys, on=cols_to_keep, how='inner')\n",
    "                            #df = l_sys.merge_multi(r_sys, on=cols_to_keep)\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "                else:\n",
    "                    if fn == op.or_:\n",
    "                        r.results = r.results.union(match_set)\n",
    "\n",
    "                        if isinstance(leftC, str) and isinstance(rightC, str):\n",
    "                            df = left_sys.append(right_sys)\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, str) and isinstance(rightC, tuple):\n",
    "                            df = left_sys.append(r_sys)\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, tuple) and isinstance(rightC, str):\n",
    "                            df = right_sys.append(l_sys)\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, tuple) and isinstance(rightC, tuple):\n",
    "                            df = l_sys.append(r_sys)\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                    if fn == op.and_:\n",
    "                        if len(r.results) == 0:\n",
    "                            r.results = match_set\n",
    "                        r.results = r.results.intersection(match_set)\n",
    "\n",
    "                        if isinstance(leftC, str) and isinstance(rightC, str):\n",
    "                            df = left_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, str) and isinstance(rightC, tuple):\n",
    "                            df = left_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, tuple) and isinstance(rightC, str):\n",
    "                            df = l_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, tuple) and isinstance(rightC, tuple):\n",
    "                            df = l_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df.drop_duplicates()\n",
    "                \n",
    "                # get matched results\n",
    "                query.update(r.results)\n",
    "                \n",
    "                # get combined system results\n",
    "                r.system_merges = df\n",
    "                \n",
    "                if len(df) > 0:\n",
    "                    system_query = system_query.append(df)\n",
    "                else:\n",
    "                    print('wtf!')\n",
    "                    \n",
    "                return query, system_query\n",
    "            else:\n",
    "                return parseTree.getRootVal()\n",
    "    \n",
    "    if sentence.n_or > 0 or sentence.n_and > 0:\n",
    "        evaluate(pt)  \n",
    "    \n",
    "    # trivial case\n",
    "    elif sentence.n_or == 0 and sentence.n_and == 0:\n",
    "        r.results, _ = get_system_matches(sentence.sentence, analysis_type, corpus)\n",
    "        r.system_merges = get_sys_data(sentence.sentence, analysis_type, corpus)\n",
    "        print('trivial:', sentence.sentence, len(r.results), len(r.system_merges))\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Incoming Boolean sentences are parsed into a binary tree.\n",
    "\n",
    "Test expressions to parse:\n",
    "\n",
    "sentence = '((((A&B)|C)|D)&E)'\n",
    "\n",
    "sentence = '(E&(D|(C|(A&B))))'\n",
    "\n",
    "sentence = '(((A|(B&C))|(D&(E&F)))|(H&I))'\n",
    "\n",
    "\"\"\"\n",
    "# build parse tree from passed sentence\n",
    "# using grammatical rules of Boolean logic\n",
    "def buildParseTree(fpexp):\n",
    "    \"\"\"\n",
    "       Iteratively build parse tree from passed sentence using grammatical rules of Boolean logic\n",
    "       :param fpexp: sentence to parse\n",
    "       :return eTree: parse tree representation\n",
    "       Incoming Boolean sentences are parsed into a binary tree.\n",
    "       Test expressions to parse:\n",
    "       sentence = '(A&B)'\n",
    "       sentence = '(A|B)'\n",
    "       sentence = '((A|B)&C)'\n",
    "       \n",
    "    \"\"\"\n",
    "    fplist = fpexp.split()\n",
    "    pStack = Stack()\n",
    "    eTree = BinaryTree('')\n",
    "    pStack.push(eTree)\n",
    "    currentTree = eTree\n",
    "\n",
    "    for i in fplist:\n",
    "\n",
    "        if i == '(':\n",
    "            currentTree.insertLeft('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getLeftChild()\n",
    "        elif i not in ['&', '|', ')']:\n",
    "            currentTree.setRootVal(i)\n",
    "            parent = pStack.pop()\n",
    "            currentTree = parent\n",
    "        elif i in ['&', '|']:\n",
    "            currentTree.setRootVal(i)\n",
    "            currentTree.insertRight('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getRightChild()\n",
    "        elif i == ')':\n",
    "            currentTree = pStack.pop()\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    return eTree\n",
    "\n",
    "def make_parse_tree(payload):\n",
    "    \"\"\"\n",
    "    Ensure data to create tree are in standard form\n",
    "    :param sentence: sentence to preprocess\n",
    "    :return pt, parse tree graph\n",
    "            sentence, processed sentence to build tree\n",
    "            a: order\n",
    "    \"\"\"\n",
    "    def preprocess_sentence(sentence):\n",
    "        # prepare statement for case when a boolean AND/OR is given\n",
    "        sentence = payload.replace('(', ' ( '). \\\n",
    "            replace(')', ' ) '). \\\n",
    "            replace('&', ' & '). \\\n",
    "            replace('|', ' | '). \\\n",
    "            replace('  ', ' ')\n",
    "        return sentence\n",
    "\n",
    "    sentence = preprocess_sentence(payload)\n",
    "    print(sentence)\n",
    "    \n",
    "    pt = buildParseTree(sentence)\n",
    "    #pt.postorder() \n",
    "    \n",
    "    return pt\n",
    "\n",
    "class Sentence(object):\n",
    "\n",
    "    def __init__(self, sentence):\n",
    "        self = self\n",
    "        self.n_and = sentence.count('&')\n",
    "        self.n_or = sentence.count('|')\n",
    "        self.sentence = sentence\n",
    "    \n",
    "def get_metrics(boolean_expression: str, analysis_type: str, corpus: str):\n",
    "    \"\"\"\n",
    "    Traverse binary parse tree representation of Boolean sentence\n",
    "        :params: boolean expression in form of '(<annotator_engine_name1><boolean operator><annotator_engine_name2>)'\n",
    "                 analysis_type (string value of: 'test', 'entity', 'cui', 'full') used to filter set of reference and system annotations \n",
    "        :return: dictionary with values needed for confusion matrix\n",
    "    \"\"\"\n",
    "    sentence = Sentence(boolean_expression)   \n",
    "\n",
    "    pt = make_parse_tree(sentence.sentence)\n",
    "\n",
    "    r = process_sentence(pt, sentence, analysis_type, corpus)\n",
    "    \n",
    "    print('len sys merges:', len(r.system_merges))\n",
    "    system_n = len(r.system_merges)\n",
    "    reference_n = get_ref_n(analysis_type, corpus)\n",
    "\n",
    "    reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, r.results).get_ref_sys()\n",
    "\n",
    "    # get overall TP/TF and various other counts for running confusion matrix metric analysis\n",
    "    return merge_eval(reference_only, system_only, reference_system_match, system_n, reference_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all combinations of given list of annotators:\n",
    "def expressions(l, n):\n",
    "    for (operations, *operands), operators in product(\n",
    "            combinations(l, n), product(('&', '|'), repeat=n - 1)):\n",
    "        for operation in zip(operators, operands):\n",
    "            operations = [operations, *operation]\n",
    "        yield operations\n",
    "        \n",
    "def run_ensemble(l, analysis_type, corpus):\n",
    "\n",
    "    metrics = pd.DataFrame()\n",
    "\n",
    "    print('CORPUS---->', corpus)\n",
    "    \n",
    "    #for i in l:\n",
    "    #    d = get_metrics(i, analysis_type, corpus)\n",
    "    \n",
    "    #for s in list(permutations(l)):\n",
    "    for i in range(1, len(l)+1):\n",
    "        test = list(expressions(l, i))\n",
    "        for t  in test:\n",
    "            if i > 1:\n",
    "                # format Boolean sentence for parse tree \n",
    "                t = '(' + \" \".join(str(x) for x in t).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "\n",
    "            d = get_metrics(t, analysis_type, corpus)\n",
    "            d['merge'] = t\n",
    "            frames = [metrics, pd.DataFrame(d, index=[0]) ]\n",
    "            metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "    \n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    \n",
    "    file_name = corpus + '_all_merge_metrics_'\n",
    "        \n",
    "    geometric_mean(metrics).to_csv(analysisConf.data_dir + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "    print(geometric_mean(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_set(df, analysis_type = 'entity', df_type = 'sys', corpus = None):\n",
    "    \n",
    "    #print(df[0:10])\n",
    "    \n",
    "    # get values for creation of series of type tuple\n",
    "    if 'entity' in analysis_type: \n",
    "        if corpus == 'casi':\n",
    "            arg = df.case, df.overlap\n",
    "        else:    \n",
    "            if df_type == 'sys':\n",
    "                arg = df.begin, df.end, df.note_id\n",
    "            else:\n",
    "                arg = df.start, df.end, df.file\n",
    "            \n",
    "    elif 'cui' in analysis_type:\n",
    "        if df_type == 'sys':\n",
    "            arg = df.cui, df.note_id\n",
    "        else:\n",
    "            arg = df.value, df.file\n",
    "    elif 'full' in analysis_type:\n",
    "        if df_type == 'sys':\n",
    "            arg = df.begin, df.end, df.cui, df.note_id\n",
    "        else:\n",
    "            arg = df.start, df.end, df.value, df.file\n",
    "    \n",
    "    return set(list(zip(*arg)))\n",
    "#     if corpus == 'casi':\n",
    "#         return set(arg)\n",
    "#     else:\n",
    "#         return set(list(zip(*arg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTS -> ensemble:\n",
    "def test_match_consistency(matches, ref_only, ref_n, sys):\n",
    "    \"\"\"test for reference only/match set consistency:\n",
    "        params: match, system and reference only sets\"\"\"\n",
    "   \n",
    "    print('len', len(sys), len(matches), len(matches.union(sys)), len(matches.intersection(sys)))\n",
    "    assert len(matches.union(ref_only)) == ref_n, 'Reference annotation mismatch union'\n",
    "    assert len(matches.intersection(sys)) == len(matches), 'System annotation mismatch intersect'\n",
    "    assert len(matches.union(sys)) == len(sys), 'System annotation mismatch union'\n",
    "    assert len(matches.intersection(ref_only)) == 0, 'Reference annotation mismatch intersect'\n",
    "\n",
    "def test_systems(analysis_type, systems, corpus):\n",
    "    sys = df_to_set(get_sys_data(systems[0], analysis_type, corpus), analysis_type)\n",
    "    test_match_consistency(*get_system_matches(systems[0], analysis_type, corpus), get_ref_n(analysis_type), sys)\n",
    "    print('Match consistency:', len(sys),get_ref_n(analysis_type))\n",
    "\n",
    "def test_metrics(ref, sys_m, match_m):\n",
    "    test = True\n",
    "    reference_n = len(ref)\n",
    "    system_n = len(sys_m)\n",
    "\n",
    "    print('Test metrics:', type(reference_n), type(system_n), type(match_m))\n",
    "\n",
    "    reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, match_m).get_ref_sys()\n",
    "    F, recall, precision, _, _, _, _, _ = Metrics(system_only, reference_only, reference_system_match, system_n).get_confusion_metrics()\n",
    "    F_, recall_, precision_, _, _, _, _, _ = Metrics(system_only, reference_only, reference_system_match, system_n).get_confusion_metrics(test)\n",
    "\n",
    "    assert F[1] == F_, 'F1 issue'\n",
    "    assert recall[1] == recall_, 'recall issue'\n",
    "    assert precision[1] == precision_, 'precision issue'\n",
    "    print(F[1], F_)\n",
    "    print(recall[1], recall_)\n",
    "    print(precision[1], precision_)\n",
    "\n",
    "def test_count(analysis_type, corpus):\n",
    "    # test match counts:\n",
    "    ctakes, _ = get_system_matches('ctakes', analysis_type, corpus)\n",
    "    clamp, _ = get_system_matches('clamp', analysis_type, corpus)\n",
    "    b9, _ = get_system_matches('biomedicus', analysis_type, corpus)\n",
    "    mm, _ = get_system_matches('metamap', analysis_type, corpus)\n",
    "\n",
    "    print('count:', len(mm.intersection(b9.intersection(clamp.intersection(ctakes)))))\n",
    "    \n",
    "def test_ensemble(analysis_type, corpus):\n",
    "    \n",
    "    print('ensemble:')\n",
    "    # Get mixed system_n\n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    ref_ann, data = get_metric_data(training_notes, analysis_type, corpus)\n",
    "\n",
    "    names = ['ctakes', 'biomedicus', 'metamap', 'clamp']\n",
    "    if 'entity' in analysis_type: \n",
    "        cols_to_keep = ['begin', 'end', 'note_id']\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['cui', 'note_id']\n",
    "    elif 'full' in analysis_type:\n",
    "        cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "\n",
    "    biomedicus = data[data[\"system\"]=='biomedicus'][cols_to_keep].copy()\n",
    "    ctakes = data[data[\"system\"]=='ctakes'][cols_to_keep].copy()\n",
    "    clamp = data[data[\"system\"]=='clamp'][cols_to_keep].copy()\n",
    "    metamap = data[data[\"system\"]=='metamap'][cols_to_keep].copy()\n",
    "    quickumls = data[data[\"system\"]=='quick_umls'][cols_to_keep].copy()\n",
    "\n",
    "    print('systems:', len(biomedicus), len(clamp), len(ctakes), len(metamap), len(quickumls))\n",
    "\n",
    "    b9 = set()\n",
    "    cl = set()\n",
    "    ct = set()\n",
    "    mm = set()\n",
    "    qu = set()\n",
    "\n",
    "    b9 = df_to_set(get_sys_data('biomedicus', analysis_type, corpus), analysis_type)\n",
    "    print(len(b9))\n",
    "\n",
    "    ct = df_to_set(get_sys_data('ctakes', analysis_type, corpus), analysis_type)\n",
    "    print(len(ct))\n",
    "\n",
    "    cl = df_to_set(get_sys_data('clamp', analysis_type, corpus), analysis_type)\n",
    "    print(len(cl))\n",
    "\n",
    "    mm = df_to_set(get_sys_data('metamap', analysis_type, corpus), analysis_type)\n",
    "    print(len(mm))\n",
    "\n",
    "    qu = df_to_set(get_sys_data('quick_umls', analysis_type, corpus), analysis_type)\n",
    "    print(len(qu))\n",
    "    \n",
    "    print('various merges:')\n",
    "    print(len(b9), len(cl), len(ct), len(mm), len(qu))\n",
    "    print(len(mm.intersection(b9.intersection(cl.intersection(ct)))))\n",
    "    print(len(mm.union(b9.intersection(cl.intersection(ct)))))\n",
    "    print(len(mm.union(b9.union(cl.intersection(ct)))))\n",
    "    print(len(mm.union(b9.union(cl.union(ct)))))\n",
    "    print(len(b9.intersection(ct)))\n",
    "\n",
    "    sys_m = b9.intersection(ct.intersection(qu))\n",
    "    print('sys_m:', len(sys_m))\n",
    "\n",
    "    # Get match merges:\n",
    "    ct, _ = get_system_matches('ctakes', analysis_type, corpus)\n",
    "    cl, _ = get_system_matches('clamp', analysis_type, corpus)\n",
    "    b9, _ = get_system_matches('biomedicus', analysis_type, corpus)\n",
    "    mm, _ = get_system_matches('metamap', analysis_type, corpus)\n",
    "    qu, _ = get_system_matches('quick_umls', analysis_type, corpus)\n",
    "\n",
    "    match_m = b9.intersection(ct.intersection(qu))\n",
    "    print('match_m:', len(match_m))\n",
    "    # reference df to set\n",
    "    if 'entity' in analysis_type: \n",
    "        cols_to_keep = ['end', 'start','file']\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['value','file']\n",
    "    elif 'full' in analysis_type:\n",
    "        cols_to_keep = ['end', 'start', 'value','file']\n",
    "\n",
    "    ref = df_to_set(ref_ann[cols_to_keep], analysis_type, 'ref')\n",
    "\n",
    "    print('ref:', len(ref))\n",
    "\n",
    "    # test difference:\n",
    "    print('FP:', len(sys_m - match_m), len(sys_m - ref))\n",
    "    assert len(sys_m - match_m) == len(sys_m - ref), 'FP mismatch'\n",
    "    print('FN:', len(ref - match_m), len(ref - sys_m))\n",
    "    assert len(ref - match_m) == len(ref - sys_m), 'FN mismatch'\n",
    "    \n",
    "    test_metrics(ref, sys_m, match_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls'] ('analytical_fairview.csv', 'concepts.fairview_all')\n",
      "corpus: fairview systems ('ctakes', 'biomedicus', 'clamp')\n",
      "CORPUS----> fairview\n",
      "----------> fairview\n",
      "ctakes\n",
      "trivial: ctakes 12946 133350\n",
      "len sys merges: 133350\n",
      "biomedicus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trivial: biomedicus 10364 105945\n",
      "len sys merges: 105945\n",
      "clamp\n",
      "trivial: clamp 10922 73230\n",
      "len sys merges: 73230\n",
      " ( ctakes & biomedicus ) \n",
      "len sys merges: 65629\n",
      " ( ctakes | biomedicus ) \n",
      "len sys merges: 173666\n",
      " ( ctakes & clamp ) \n",
      "len sys merges: 31922\n",
      " ( ctakes | clamp ) \n",
      "len sys merges: 174658\n",
      " ( biomedicus & clamp ) \n",
      "len sys merges: 23262\n",
      " ( biomedicus | clamp ) \n",
      "len sys merges: 155913\n",
      " ( ( ctakes & biomedicus ) & clamp ) \n",
      "len sys merges: 21121\n",
      " ( ( ctakes & biomedicus ) | clamp ) \n",
      "len sys merges: 117738\n",
      " ( ( ctakes | biomedicus ) & clamp ) \n",
      "len sys merges: 34063\n",
      " ( ( ctakes | biomedicus ) | clamp ) \n",
      "len sys merges: 212833\n",
      "           F  precision    recall     TP     FN      FP     TP/FN  n_gold  \\\n",
      "0   0.163476   0.097083  0.517137  12946  12088  120404  1.070979   25034   \n",
      "1   0.158254   0.097824  0.413997  10364  14670   95581  0.706476   25034   \n",
      "2   0.222299   0.149147  0.436287  10922  14112   62308  0.773951   25034   \n",
      "3   0.202398   0.139801  0.366502   9175  15859   56454  0.578536   25034   \n",
      "4   0.142275   0.081392  0.564632  14135  10899  159531  1.296908   25034   \n",
      "5   0.318105   0.283785  0.361868   9059  15975   22863  0.567074   25034   \n",
      "6   0.148318   0.084789  0.591555  14809  10225  159849  1.448313   25034   \n",
      "7   0.290335   0.301393  0.280059   7011  18023   16251  0.389003   25034   \n",
      "8   0.157781   0.091557  0.570224  14275  10759  141638  1.326796   25034   \n",
      "9   0.282440   0.308603  0.260366   6518  18516   14603  0.352020   25034   \n",
      "10  0.190219   0.115332  0.542422  13579  11455  104159  1.185421   25034   \n",
      "11  0.323265   0.280422  0.381561   9552  15482   24511  0.616975   25034   \n",
      "12  0.130367   0.072851  0.619358  15505   9529  197328  1.627138   25034   \n",
      "\n",
      "     n_sys         TM                        merge  F1 rank  TP/FN rank  \\\n",
      "0   133350  35.451865                       ctakes      8.0         6.0   \n",
      "1   105945  31.841035                   biomedicus      9.0         8.0   \n",
      "2    73230  40.360604                        clamp      5.0         7.0   \n",
      "3    65629  35.814441          (ctakes&biomedicus)      6.0        10.0   \n",
      "4   173666  33.918639          (ctakes|biomedicus)     12.0         4.0   \n",
      "5    31922  50.703182               (ctakes&clamp)      2.0        11.0   \n",
      "6   174658  35.434923               (ctakes|clamp)     11.0         2.0   \n",
      "7    23262  45.968088           (biomedicus&clamp)      3.0        12.0   \n",
      "8   155913  36.152218           (biomedicus|clamp)     10.0         3.0   \n",
      "9    21121  44.849450  ((ctakes&biomedicus)&clamp)      4.0        13.0   \n",
      "10  117738  39.573956  ((ctakes&biomedicus)|clamp)      7.0         5.0   \n",
      "11   34063  51.755066  ((ctakes|biomedicus)&clamp)      1.0         9.0   \n",
      "12  212833  33.608744  ((ctakes|biomedicus)|clamp)     13.0         1.0   \n",
      "\n",
      "    TM rank      Gmean  \n",
      "0       9.0   7.418159  \n",
      "1      13.0  10.057411  \n",
      "2       5.0   5.806518  \n",
      "3       8.0   8.556162  \n",
      "4      11.0   7.084879  \n",
      "5       2.0   4.266582  \n",
      "6      10.0   4.942491  \n",
      "7       3.0   5.555248  \n",
      "8       7.0   4.997630  \n",
      "9       4.0   6.754041  \n",
      "10      6.0   5.628571  \n",
      "11      1.0   2.655264  \n",
      "12     12.0   4.012431  \n",
      "corpus: fairview systems ('ctakes', 'clamp', 'biomedicus')\n",
      "CORPUS----> fairview\n",
      "----------> fairview\n",
      "ctakes\n",
      "trivial: ctakes 12946 133350\n",
      "len sys merges: 133350\n",
      "clamp\n",
      "trivial: clamp 10922 73230\n",
      "len sys merges: 73230\n",
      "biomedicus\n",
      "trivial: biomedicus 10364 105945\n",
      "len sys merges: 105945\n",
      " ( ctakes & clamp ) \n",
      "len sys merges: 31922\n",
      " ( ctakes | clamp ) \n",
      "len sys merges: 174658\n",
      " ( ctakes & biomedicus ) \n",
      "len sys merges: 65629\n",
      " ( ctakes | biomedicus ) \n",
      "len sys merges: 173666\n",
      " ( clamp & biomedicus ) \n",
      "len sys merges: 23262\n",
      " ( clamp | biomedicus ) \n",
      "len sys merges: 155913\n",
      " ( ( ctakes & clamp ) & biomedicus ) \n",
      "len sys merges: 21121\n",
      " ( ( ctakes & clamp ) | biomedicus ) \n",
      "len sys merges: 116746\n",
      " ( ( ctakes | clamp ) & biomedicus ) \n",
      "len sys merges: 67770\n",
      " ( ( ctakes | clamp ) | biomedicus ) \n",
      "len sys merges: 212833\n",
      "           F  precision    recall     TP     FN      FP     TP/FN  n_gold  \\\n",
      "0   0.163476   0.097083  0.517137  12946  12088  120404  1.070979   25034   \n",
      "1   0.222299   0.149147  0.436287  10922  14112   62308  0.773951   25034   \n",
      "2   0.158254   0.097824  0.413997  10364  14670   95581  0.706476   25034   \n",
      "3   0.318105   0.283785  0.361868   9059  15975   22863  0.567074   25034   \n",
      "4   0.148318   0.084789  0.591555  14809  10225  159849  1.448313   25034   \n",
      "5   0.202398   0.139801  0.366502   9175  15859   56454  0.578536   25034   \n",
      "6   0.142275   0.081392  0.564632  14135  10899  159531  1.296908   25034   \n",
      "7   0.290335   0.301393  0.280059   7011  18023   16251  0.389003   25034   \n",
      "8   0.157781   0.091557  0.570224  14275  10759  141638  1.326796   25034   \n",
      "9   0.282440   0.308603  0.260366   6518  18516   14603  0.352020   25034   \n",
      "10  0.182043   0.110539  0.515499  12905  12129  103841  1.063979   25034   \n",
      "11  0.208353   0.142659  0.386195   9668  15366   58102  0.629181   25034   \n",
      "12  0.130367   0.072851  0.619358  15505   9529  197328  1.627138   25034   \n",
      "\n",
      "     n_sys         TM                        merge  F1 rank  TP/FN rank  \\\n",
      "0   133350  35.451865                       ctakes      8.0         5.0   \n",
      "1    73230  40.360604                        clamp      4.0         7.0   \n",
      "2   105945  31.841035                   biomedicus      9.0         8.0   \n",
      "3    31922  50.703182               (ctakes&clamp)      1.0        11.0   \n",
      "4   174658  35.434923               (ctakes|clamp)     11.0         2.0   \n",
      "5    65629  35.814441          (ctakes&biomedicus)      6.0        10.0   \n",
      "6   173666  33.918639          (ctakes|biomedicus)     12.0         4.0   \n",
      "7    23262  45.968088           (clamp&biomedicus)      2.0        12.0   \n",
      "8   155913  36.152218           (clamp|biomedicus)     10.0         3.0   \n",
      "9    21121  44.849450  ((ctakes&clamp)&biomedicus)      3.0        13.0   \n",
      "10  116746  37.769132  ((ctakes&clamp)|biomedicus)      7.0         6.0   \n",
      "11   67770  37.137948  ((ctakes|clamp)&biomedicus)      5.0         9.0   \n",
      "12  212833  33.608744  ((ctakes|clamp)|biomedicus)     13.0         1.0   \n",
      "\n",
      "    TM rank      Gmean  \n",
      "0       9.0   6.840761  \n",
      "1       4.0   5.129522  \n",
      "2      13.0  10.057411  \n",
      "3       1.0   2.902961  \n",
      "4      10.0   4.942491  \n",
      "5       8.0   8.556162  \n",
      "6      11.0   7.084879  \n",
      "7       2.0   4.434810  \n",
      "8       7.0   4.997630  \n",
      "9       3.0   5.756431  \n",
      "10      5.0   5.628571  \n",
      "11      6.0   7.040704  \n",
      "12     12.0   4.012431  \n",
      "corpus: fairview systems ('biomedicus', 'clamp', 'ctakes')\n",
      "CORPUS----> fairview\n",
      "----------> fairview\n",
      "biomedicus\n",
      "trivial: biomedicus 10364 105945\n",
      "len sys merges: 105945\n",
      "clamp\n",
      "trivial: clamp 10922 73230\n",
      "len sys merges: 73230\n",
      "ctakes\n",
      "trivial: ctakes 12946 133350\n",
      "len sys merges: 133350\n",
      " ( biomedicus & clamp ) \n",
      "len sys merges: 23262\n",
      " ( biomedicus | clamp ) \n",
      "len sys merges: 155913\n",
      " ( biomedicus & ctakes ) \n",
      "len sys merges: 65629\n",
      " ( biomedicus | ctakes ) \n",
      "len sys merges: 173666\n",
      " ( clamp & ctakes ) \n",
      "len sys merges: 31922\n",
      " ( clamp | ctakes ) \n",
      "len sys merges: 174658\n",
      " ( ( biomedicus & clamp ) & ctakes ) \n",
      "len sys merges: 21121\n",
      " ( ( biomedicus & clamp ) | ctakes ) \n",
      "len sys merges: 135491\n",
      " ( ( biomedicus | clamp ) & ctakes ) \n",
      "len sys merges: 76430\n",
      " ( ( biomedicus | clamp ) | ctakes ) \n",
      "len sys merges: 212833\n",
      "           F  precision    recall     TP     FN      FP     TP/FN  n_gold  \\\n",
      "0   0.158254   0.097824  0.413997  10364  14670   95581  0.706476   25034   \n",
      "1   0.222299   0.149147  0.436287  10922  14112   62308  0.773951   25034   \n",
      "2   0.163476   0.097083  0.517137  12946  12088  120404  1.070979   25034   \n",
      "3   0.290335   0.301393  0.280059   7011  18023   16251  0.389003   25034   \n",
      "4   0.157781   0.091557  0.570224  14275  10759  141638  1.326796   25034   \n",
      "5   0.202398   0.139801  0.366502   9175  15859   56454  0.578536   25034   \n",
      "6   0.142275   0.081392  0.564632  14135  10899  159531  1.296908   25034   \n",
      "7   0.318105   0.283785  0.361868   9059  15975   22863  0.567074   25034   \n",
      "8   0.148318   0.084789  0.591555  14809  10225  159849  1.448313   25034   \n",
      "9   0.282440   0.308603  0.260366   6518  18516   14603  0.352020   25034   \n",
      "10  0.167438   0.099187  0.536830  13439  11595  122052  1.159034   25034   \n",
      "11  0.230939   0.153291  0.468004  11716  13318   64714  0.879712   25034   \n",
      "12  0.130367   0.072851  0.619358  15505   9529  197328  1.627138   25034   \n",
      "\n",
      "     n_sys         TM                        merge  F1 rank  TP/FN rank  \\\n",
      "0   105945  31.841035                   biomedicus      9.0         9.0   \n",
      "1    73230  40.360604                        clamp      5.0         8.0   \n",
      "2   133350  35.451865                       ctakes      8.0         6.0   \n",
      "3    23262  45.968088           (biomedicus&clamp)      2.0        12.0   \n",
      "4   155913  36.152218           (biomedicus|clamp)     10.0         3.0   \n",
      "5    65629  35.814441          (biomedicus&ctakes)      6.0        10.0   \n",
      "6   173666  33.918639          (biomedicus|ctakes)     12.0         4.0   \n",
      "7    31922  50.703182               (clamp&ctakes)      1.0        11.0   \n",
      "8   174658  35.434923               (clamp|ctakes)     11.0         2.0   \n",
      "9    21121  44.849450  ((biomedicus&clamp)&ctakes)      3.0        13.0   \n",
      "10  135491  36.509991  ((biomedicus&clamp)|ctakes)      7.0         5.0   \n",
      "11   76430  42.378681  ((biomedicus|clamp)&ctakes)      4.0         7.0   \n",
      "12  212833  33.608744  ((biomedicus|clamp)|ctakes)     13.0         1.0   \n",
      "\n",
      "    TM rank      Gmean  \n",
      "0      13.0  10.597921  \n",
      "1       5.0   6.161550  \n",
      "2       9.0   7.418159  \n",
      "3       2.0   4.434810  \n",
      "4       7.0   4.997630  \n",
      "5       8.0   8.556162  \n",
      "6      11.0   7.084879  \n",
      "7       1.0   2.902961  \n",
      "8      10.0   4.942491  \n",
      "9       3.0   5.756431  \n",
      "10      6.0   5.628571  \n",
      "11      4.0   5.129522  \n",
      "12     12.0   4.012431  \n",
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         48329977 function calls (42227947 primitive calls) in 38.092 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "  2027754   10.142    0.000   10.142    0.000 {built-in method builtins.compile}\n",
       "8111016/2027754    4.823    0.000    6.766    0.000 ast.py:64(_convert)\n",
       "      687    4.100    0.006    4.100    0.006 {method 'copy' of 'numpy.ndarray' objects}\n",
       "      162    3.738    0.023    3.738    0.023 {pandas._libs.ops.scalar_compare}\n",
       " 28665890    2.274    0.000    2.324    0.000 {built-in method builtins.isinstance}\n",
       "  2027754    1.952    0.000   19.519    0.000 ast.py:38(literal_eval)\n",
       "       81    1.078    0.013   12.866    0.159 <ipython-input-15-edee5e438fd2>:25(get_sys_data)\n",
       "      402    0.829    0.002    0.831    0.002 {method 'factorize' of 'pandas._libs.hashtable.Int64HashTable' objects}\n",
       "  1103571    0.819    0.000   11.319    0.000 <ipython-input-15-edee5e438fd2>:81(<genexpr>)\n",
       "   924345    0.707    0.000    9.940    0.000 <ipython-input-15-edee5e438fd2>:78(<genexpr>)\n",
       "      243    0.654    0.003    0.654    0.003 {pandas._libs.algos.take_2d_axis1_object_object}\n",
       "     5376    0.575    0.000    0.575    0.000 {built-in method numpy.empty}\n",
       "      201    0.541    0.003    0.542    0.003 {method 'factorize' of 'pandas._libs.hashtable.StringHashTable' objects}\n",
       "      126    0.489    0.004    0.490    0.004 {method 'factorize' of 'pandas._libs.hashtable.Int64Factorizer' objects}\n",
       "       81    0.407    0.005   21.689    0.268 <ipython-input-15-edee5e438fd2>:59(get_system_matches)\n",
       "  2027754    0.396    0.000   10.538    0.000 ast.py:30(parse)\n",
       "      201    0.329    0.002    0.329    0.002 {pandas._libs.hashtable.duplicated_int64}\n",
       "   258/30    0.283    0.001   32.986    1.100 <ipython-input-19-f1005eda34b6>:33(evaluate)\n",
       "      468    0.260    0.001    0.260    0.001 {pandas._libs.algos.take_2d_axis1_int64_int64}\n",
       "     1632    0.216    0.000    0.225    0.000 {pandas._libs.lib.infer_dtype}\n",
       "  2027755    0.191    0.000    0.191    0.000 {method 'strip' of 'str' objects}\n",
       "      261    0.186    0.001    0.186    0.001 {pandas._libs.algos.take_2d_axis0_float64_float64}\n",
       "      525    0.147    0.000    0.147    0.000 {method 'take' of 'numpy.ndarray' objects}\n",
       "      201    0.147    0.001    0.198    0.001 sorting.py:20(get_group_index)\n",
       "       39    0.143    0.004   37.203    0.954 <ipython-input-19-f1005eda34b6>:8(process_sentence)\n",
       "      375    0.140    0.000    0.140    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
       "       42    0.136    0.003    0.136    0.003 {method 'factorize' of 'pandas._libs.hashtable.Factorizer' objects}\n",
       "      603    0.122    0.000    1.915    0.003 algorithms.py:559(factorize)\n",
       "      303    0.096    0.000    0.096    0.000 {pandas._libs.algos.take_2d_axis0_object_object}\n",
       "      201    0.091    0.000    3.110    0.015 frame.py:4605(drop_duplicates)\n",
       "       21    0.089    0.004    0.101    0.005 {pandas._libs.join.inner_join}\n",
       "      525    0.087    0.000    0.097    0.000 indexing.py:2575(maybe_convert_indices)\n",
       "      525    0.087    0.000    0.087    0.000 {built-in method numpy.concatenate}\n",
       "      228    0.070    0.000    0.070    0.000 {pandas._libs.algos.take_2d_axis0_int64_int64}\n",
       "     1899    0.067    0.000    0.067    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
       "       21    0.054    0.003    0.054    0.003 {built-in method _operator.or_}\n",
       "       21    0.047    0.002    0.047    0.002 {built-in method _operator.and_}\n",
       "      525    0.043    0.000    2.369    0.005 managers.py:1329(take)\n",
       "      603    0.040    0.000    0.046    0.000 sorting.py:55(maybe_lift)\n",
       "     2058    0.039    0.000    1.982    0.001 algorithms.py:1544(take_nd)\n",
       "     5442    0.038    0.000    0.038    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
       "       21    0.034    0.002    0.036    0.002 merge.py:1701(_get_join_keys)\n",
       "      603    0.034    0.000    1.627    0.003 algorithms.py:434(_factorize_array)\n",
       "   117090    0.032    0.000    0.046    0.000 generic.py:7(_check)\n",
       "    19140    0.031    0.000    0.086    0.000 dtypes.py:68(find)\n",
       "       21    0.030    0.001    0.839    0.040 merge.py:730(_get_join_indexers)\n",
       "       39    0.029    0.001   37.857    0.971 <ipython-input-20-edee81b75678>:88(get_metrics)\n",
       "    25620    0.028    0.000    0.046    0.000 {method 'format' of 'str' objects}\n",
       "   182613    0.025    0.000    0.028    0.000 {built-in method builtins.getattr}\n",
       "      165    0.024    0.000    0.026    0.000 {built-in method io.open}\n",
       "       21    0.024    0.001    0.809    0.039 merge.py:1104(_get_join_indexers)\n",
       "1527/1254    0.023    0.000    0.132    0.000 base.py:253(__new__)\n",
       "    40236    0.022    0.000    0.023    0.000 {built-in method builtins.hasattr}\n",
       "12552/11802    0.022    0.000    0.036    0.000 {built-in method numpy.array}\n",
       "    23100    0.022    0.000    0.048    0.000 common.py:1845(_is_dtype_type)\n",
       "    10254    0.020    0.000    0.049    0.000 _dtype.py:319(_name_get)\n",
       "    28569    0.020    0.000    0.084    0.000 base.py:75(is_dtype)\n",
       "     1065    0.020    0.000    0.048    0.000 managers.py:186(_rebuild_blknos_and_blklocs)\n",
       "75890/61124    0.017    0.000    0.028    0.000 {built-in method builtins.len}\n",
       "       84    0.017    0.000    0.648    0.008 merge.py:1617(_factorize_keys)\n",
       "      831    0.017    0.000    2.730    0.003 frame.py:2893(__getitem__)\n",
       "    18372    0.016    0.000    0.086    0.000 common.py:1702(is_extension_array_dtype)\n",
       "    98883    0.015    0.000    0.015    0.000 {built-in method builtins.issubclass}\n",
       "       39    0.014    0.000    0.610    0.016 <ipython-input-15-edee5e438fd2>:2(get_ref_n)\n",
       "      201    0.014    0.000    2.578    0.013 frame.py:4639(duplicated)\n",
       "    13566    0.014    0.000    0.038    0.000 common.py:1981(pandas_dtype)\n",
       "      228    0.014    0.000    0.014    0.000 {built-in method posix.stat}\n",
       "     4122    0.013    0.000    0.019    0.000 generic.py:5069(__setattr__)\n",
       "      201    0.012    0.000    0.012    0.000 {built-in method _operator.inv}\n",
       "       42    0.012    0.000    0.012    0.000 {method 'update' of 'set' objects}\n",
       "     2058    0.012    0.000    0.031    0.000 algorithms.py:1417(_get_take_nd_function)\n",
       "     7425    0.011    0.000    0.011    0.000 {built-in method _codecs.utf_8_decode}\n",
       "    20508    0.011    0.000    0.016    0.000 numerictypes.py:293(issubclass_)\n",
       "    10254    0.011    0.000    0.028    0.000 numerictypes.py:365(issubdtype)\n",
       "       21    0.011    0.001    0.011    0.001 {method 'union' of 'set' objects}\n",
       "       21    0.010    0.000    0.010    0.000 {method 'intersection' of 'set' objects}\n",
       "      408    0.010    0.000    0.010    0.000 {pandas._libs.algos.take_1d_int64_int64}\n",
       "     7425    0.010    0.000    0.022    0.000 codecs.py:319(decode)\n",
       "     3744    0.010    0.000    0.022    0.000 blocks.py:78(__init__)\n",
       "     1377    0.010    0.000    0.096    0.000 series.py:152(__init__)\n",
       "     2784    0.009    0.000    0.009    0.000 {built-in method numpy.arange}\n",
       "     1713    0.009    0.000    0.029    0.000 cast.py:255(maybe_promote)\n",
       "     1230    0.009    0.000    1.939    0.002 blocks.py:1217(take_nd)\n",
       "     5823    0.009    0.000    0.019    0.000 common.py:160(is_sparse)\n",
       "     3744    0.008    0.000    0.063    0.000 blocks.py:3080(make_block)\n",
       "      406    0.007    0.000    0.007    0.000 socket.py:337(send)\n",
       "    17064    0.007    0.000    0.020    0.000 integer.py:80(construct_from_string)\n",
       "     5067    0.007    0.000    0.021    0.000 dtypes.py:973(is_dtype)\n",
       "    15384    0.007    0.000    0.023    0.000 <frozen importlib._bootstrap>:1009(_handle_fromlist)\n",
       "     3903    0.007    0.000    0.018    0.000 managers.py:139(shape)\n",
       "     2115    0.007    0.000    0.016    0.000 base.py:504(_simple_new)\n",
       "    12762    0.006    0.000    0.045    0.000 common.py:434(is_datetime64tz_dtype)\n",
       "     9651    0.006    0.000    0.028    0.000 common.py:1809(_get_dtype)\n",
       "     2406    0.006    0.000    0.012    0.000 dtypes.py:786(construct_from_string)\n",
       "     2493    0.006    0.000    0.018    0.000 _dtype.py:46(__str__)\n",
       "     1014    0.006    0.000    0.056    0.000 construction.py:537(sanitize_array)\n",
       "    13116    0.006    0.000    0.008    0.000 base.py:652(__len__)\n",
       "     4848    0.006    0.000    0.055    0.000 common.py:1578(is_bool_dtype)\n",
       "      525    0.006    0.000    2.409    0.005 generic.py:3323(_take)\n",
       "     6618    0.006    0.000    0.022    0.000 common.py:131(is_object_dtype)\n",
       "     1458    0.006    0.000    0.036    0.000 blocks.py:3034(get_block_type)\n",
       "     1023    0.006    0.000    0.106    0.000 managers.py:97(__init__)\n",
       "     5241    0.005    0.000    0.018    0.000 dtypes.py:827(is_dtype)\n",
       "      162    0.005    0.000    3.819    0.024 ops.py:1660(wrapper)\n",
       "     4284    0.005    0.000    0.011    0.000 dtypes.py:672(construct_from_string)\n",
       "     3744    0.005    0.000    0.007    0.000 blocks.py:199(mgr_locs)\n",
       "      987    0.005    0.000    0.114    0.000 frame.py:378(__init__)\n",
       "      792    0.005    0.000    0.016    0.000 managers.py:963(iget)\n",
       "      201    0.005    0.000    0.007    0.000 concat.py:22(get_mgr_concatenation_plan)\n",
       "      189    0.005    0.000    0.218    0.001 managers.py:1241(_slice_take_blocks_ax0)\n",
       "     1902    0.005    0.000    0.036    0.000 base.py:4051(equals)\n",
       "      147    0.005    0.000    0.005    0.000 {built-in method pandas._libs.missing.isnaobj}\n",
       "     2364    0.005    0.000    0.005    0.000 generic.py:127(__init__)\n",
       "     2715    0.005    0.000    0.029    0.000 blocks.py:225(make_block_same_class)\n",
       "      237    0.004    0.000    0.020    0.000 concat.py:261(get_empty_dtype_and_na)\n",
       "     3849    0.004    0.000    0.012    0.000 inference.py:253(is_list_like)\n",
       "     5601    0.004    0.000    0.004    0.000 {built-in method _abc._abc_instancecheck}\n",
       "     1692    0.004    0.000    0.004    0.000 {method 'get_loc' of 'pandas._libs.index.IndexEngine' objects}\n",
       "     3744    0.004    0.000    0.017    0.000 common.py:403(is_datetime64_dtype)\n",
       "      201    0.004    0.000    0.049    0.000 {pandas._libs.lib.clean_index_list}\n",
       "     4353    0.004    0.000    0.006    0.000 {pandas._libs.lib.is_scalar}\n",
       "     1377    0.004    0.000    0.029    0.000 managers.py:1443(__init__)\n",
       "     1119    0.004    0.000    0.011    0.000 blocks.py:2626(__init__)\n",
       "    17013    0.004    0.000    0.006    0.000 common.py:119(<lambda>)\n",
       "9495/8754    0.004    0.000    0.034    0.000 numeric.py:469(asarray)\n",
       "      162    0.004    0.000    3.743    0.023 ops.py:1591(_comp_method_OBJECT_ARRAY)\n",
       "     7605    0.004    0.000    0.024    0.000 common.py:572(is_categorical_dtype)\n",
       "      585    0.004    0.000    0.009    0.000 cast.py:832(maybe_castable)\n",
       "     4464    0.004    0.000    0.038    0.000 {method 'any' of 'numpy.ndarray' objects}\n",
       "    17013    0.004    0.000    0.004    0.000 common.py:117(classes)\n",
       "      834    0.004    0.000    0.013    0.000 managers.py:306(_verify_integrity)\n",
       "     4416    0.004    0.000    0.087    0.000 base.py:5318(ensure_index)\n",
       "      603    0.004    0.000    0.135    0.000 algorithms.py:132(_reconstruct_data)\n",
       "     2406    0.004    0.000    0.006    0.000 dtypes.py:929(construct_from_string)\n",
       "      123    0.003    0.000    0.286    0.002 managers.py:2029(concatenate_block_managers)\n",
       "      546    0.003    0.000    2.031    0.004 managers.py:1198(reindex_indexer)\n",
       "  807/204    0.003    0.000    0.041    0.000 <frozen importlib._bootstrap>:978(_find_and_load)\n",
       "     6570    0.003    0.000    0.024    0.000 common.py:536(is_interval_dtype)\n",
       "     4851    0.003    0.000    0.003    0.000 {method 'view' of 'numpy.ndarray' objects}\n",
       "     2775    0.003    0.000    0.003    0.000 {method 'match' of 're.Pattern' objects}\n",
       "     1065    0.003    0.000    0.014    0.000 missing.py:360(array_equivalent)\n",
       "     2493    0.003    0.000    0.033    0.000 blocks.py:312(ftype)\n",
       "     3309    0.003    0.000    0.003    0.000 dtypes.py:452(construct_from_string)\n",
       "     5241    0.003    0.000    0.021    0.000 common.py:503(is_period_dtype)\n",
       "      798    0.003    0.000    0.012    0.000 base.py:566(_shallow_copy)\n",
       "      324    0.003    0.000    0.004    0.000 {method 'get_indexer' of 'pandas._libs.index.IndexEngine' objects}\n",
       "    10227    0.003    0.000    0.003    0.000 {method 'get' of 'dict' objects}\n",
       "     1287    0.003    0.000    0.011    0.000 common.py:222(asarray_tuplesafe)\n",
       "      909    0.003    0.000    0.048    0.000 generic.py:3056(_get_item_cache)\n",
       "    11709    0.003    0.000    0.011    0.000 managers.py:141(<genexpr>)\n",
       "     1224    0.003    0.000    0.025    0.000 algorithms.py:38(_ensure_data)\n",
       "     1023    0.003    0.000    0.038    0.000 managers.py:599(_consolidate_check)\n",
       "     2361    0.003    0.000    0.003    0.000 generic.py:5053(__getattr__)\n",
       "      297    0.003    0.000    0.006    0.000 base.py:1658(is_unique)\n",
       "     4488    0.003    0.000    0.010    0.000 base.py:3608(values)\n",
       "     3579    0.003    0.000    0.011    0.000 common.py:472(is_timedelta64_dtype)\n",
       "     3057    0.003    0.000    0.004    0.000 generic.py:363(_get_axis_name)\n",
       "      747    0.003    0.000    0.023    0.000 managers.py:934(get)\n",
       "      834    0.003    0.000    0.013    0.000 common.py:93(is_bool_indexer)\n",
       "      642    0.003    0.000    0.005    0.000 blocks.py:3100(_extend_blocks)\n",
       "    11196    0.003    0.000    0.003    0.000 blocks.py:195(mgr_locs)\n",
       "      228    0.003    0.000    0.020    0.000 <frozen importlib._bootstrap_external>:1356(find_spec)\n",
       "      183    0.003    0.000    4.066    0.022 managers.py:318(apply)\n",
       "      129    0.003    0.000    0.020    0.000 managers.py:1696(form_blocks)\n",
       "     1923    0.003    0.000    0.020    0.000 missing.py:105(_isna_new)\n",
       "      525    0.003    0.000    0.192    0.000 base.py:784(take)\n",
       "     2898    0.003    0.000    0.009    0.000 common.py:923(is_signed_integer_dtype)\n",
       "     2559    0.002    0.000    0.002    0.000 {method 'fill' of 'numpy.ndarray' objects}\n",
       "     1377    0.002    0.000    0.003    0.000 series.py:354(_set_axis)\n",
       "      204    0.002    0.000    0.026    0.000 <frozen importlib._bootstrap>:882(_find_spec)\n",
       "     1749    0.002    0.000    0.005    0.000 base.py:3918(__contains__)\n",
       "     1014    0.002    0.000    0.032    0.000 construction.py:684(_try_cast)\n",
       "      363    0.002    0.000    2.328    0.006 frame.py:2952(_getitem_bool_array)\n",
       "     2625    0.002    0.000    0.007    0.000 {pandas._libs.lib.values_from_object}\n",
       "     6087    0.002    0.000    0.003    0.000 common.py:127(<lambda>)\n",
       "     1317    0.002    0.000    0.004    0.000 base.py:3940(__getitem__)\n",
       "      324    0.002    0.000    0.076    0.000 base.py:2715(get_indexer)\n",
       "      363    0.002    0.000    1.755    0.005 managers.py:1233(<listcomp>)\n",
       "     1758    0.002    0.000    0.004    0.000 series.py:392(name)\n",
       "     4464    0.002    0.000    0.034    0.000 _methods.py:42(_any)\n",
       "      363    0.002    0.000    0.011    0.000 base.py:786(array)\n",
       "     2994    0.002    0.000    0.007    0.000 generic.py:377(_get_axis)\n",
       "      603    0.002    0.000    1.922    0.003 frame.py:4666(f)\n",
       "      594    0.002    0.000    0.011    0.000 cast.py:953(maybe_cast_to_datetime)\n",
       "     8383    0.002    0.000    0.002    0.000 {method 'startswith' of 'str' objects}\n",
       "      804    0.002    0.000    0.049    0.000 frame.py:742(iteritems)\n",
       "      102    0.002    0.000    0.036    0.000 concat.py:237(__init__)\n",
       "      417    0.002    0.000    0.004    0.000 concat.py:117(needs_filling)\n",
       "     2550    0.002    0.000    0.005    0.000 common.py:1545(is_float_dtype)\n",
       "     5601    0.002    0.000    0.006    0.000 abc.py:137(__instancecheck__)\n",
       "      429    0.002    0.000    0.014    0.000 missing.py:183(_isna_ndarraylike)\n",
       "      807    0.002    0.000    0.004    0.000 <frozen importlib._bootstrap>:157(_get_module_lock)\n",
       "     1797    0.002    0.000    0.003    0.000 generic.py:349(_get_axis_number)\n",
       "      747    0.002    0.000    0.019    0.000 frame.py:3342(_box_item_values)\n",
       "     3744    0.002    0.000    0.002    0.000 blocks.py:89(_check_ndim)\n",
       "      396    0.002    0.000    0.036    0.000 base.py:584(_shallow_copy_with_infer)\n",
       "      573    0.002    0.000    0.021    0.000 concat.py:137(is_na)\n",
       "     2151    0.002    0.000    0.003    0.000 <frozen importlib._bootstrap>:416(parent)\n",
       "     2154    0.002    0.000    0.003    0.000 range.py:510(__len__)\n",
       "     2103    0.002    0.000    0.005    0.000 common.py:746(is_dtype_equal)\n",
       "     7338    0.002    0.000    0.002    0.000 blocks.py:308(dtype)\n",
       "     2178    0.002    0.000    0.003    0.000 __init__.py:221(iteritems)\n",
       "     2586    0.002    0.000    0.003    0.000 managers.py:1522(dtype)\n",
       "      528    0.002    0.000    4.033    0.008 blocks.py:749(copy)\n",
       "      465    0.002    0.000    0.008    0.000 numeric.py:34(__new__)\n",
       "      462    0.002    0.000    0.005    0.000 numeric.py:2656(seterr)\n",
       "      162    0.002    0.000    0.111    0.001 indexing.py:1271(_convert_to_indexer)\n",
       "     3240    0.002    0.000    0.004    0.000 {built-in method builtins.max}\n",
       "     1023    0.002    0.000    0.003    0.000 managers.py:98(<listcomp>)\n",
       "      525    0.002    0.000    0.002    0.000 concat.py:425(combine_concat_plans)\n",
       "  807/204    0.002    0.000    0.036    0.000 <frozen importlib._bootstrap>:948(_find_and_load_unlocked)\n",
       "      588    0.002    0.000    0.038    0.000 numeric.py:67(_shallow_copy)\n",
       "      792    0.002    0.000    0.014    0.000 frame.py:3349(_box_col_values)\n",
       "     2154    0.002    0.000    0.020    0.000 common.py:1078(is_datetime64_any_dtype)\n",
       "      363    0.002    0.000    0.004    0.000 numpy_.py:35(__init__)\n",
       "     1245    0.002    0.000    0.002    0.000 _internal.py:886(npy_ctypes_check)\n",
       "     1680    0.002    0.000    0.006    0.000 base.py:2650(get_loc)\n",
       "      474    0.002    0.000    0.004    0.000 cast.py:1193(construct_1d_object_array_from_listlike)\n",
       "     5058    0.002    0.000    0.002    0.000 managers.py:143(ndim)\n",
       "      243    0.002    0.000    0.010    0.000 concat.py:20(get_dtype_kinds)\n",
       "      162    0.002    0.000    0.099    0.001 indexing.py:1108(_get_listlike_indexer)\n",
       "       39    0.002    0.000    0.003    0.000 <ipython-input-17-99252b473894>:1(merge_eval)\n",
       "      102    0.002    0.000    0.256    0.003 concat.py:383(get_result)\n",
       "     2133    0.002    0.000    0.003    0.000 sparse.py:196(construct_from_string)\n",
       "     1023    0.002    0.000    0.034    0.000 managers.py:600(<listcomp>)\n",
       "      462    0.002    0.000    0.002    0.000 numeric.py:2758(geterr)\n",
       "      417    0.002    0.000    0.035    0.000 concat.py:165(get_reindexed_values)\n",
       "        3    0.001    0.000   38.092   12.697 <ipython-input-21-ca51fc37fe1a>:9(run_ensemble)\n",
       "     1227    0.001    0.000    0.003    0.000 base.py:547(_get_attributes_dict)\n",
       "     2586    0.001    0.000    0.005    0.000 series.py:406(dtype)\n",
       "     1098    0.001    0.000    0.002    0.000 generic.py:5036(__finalize__)\n",
       "      804    0.001    0.000    0.053    0.000 frame.py:4685(<genexpr>)\n",
       "      300    0.001    0.000    0.004    0.000 fromnumeric.py:69(_wrapreduction)\n",
       "     1902    0.001    0.000    0.002    0.000 base.py:613(is_)\n",
       "      162    0.001    0.000    0.004    0.000 indexing.py:1209(_validate_read_indexer)\n",
       "      183    0.001    0.000    4.079    0.022 generic.py:5699(copy)\n",
       "     4554    0.001    0.000    0.002    0.000 format.py:301(len)\n",
       "       60    0.001    0.000    0.002    0.000 numeric.py:2551(array_equal)\n",
       "     3186    0.001    0.000    0.001    0.000 {method 'rpartition' of 'str' objects}\n",
       "     1671    0.001    0.000    0.004    0.000 common.py:980(is_unsigned_integer_dtype)\n",
       "      807    0.001    0.000    0.001    0.000 <frozen importlib._bootstrap>:78(acquire)\n",
       "     2100    0.001    0.000    0.010    0.000 common.py:262(is_categorical)\n",
       "     1596    0.001    0.000    0.002    0.000 series.py:399(name)\n",
       "      621    0.001    0.000    0.221    0.000 algorithms.py:217(_get_data_algo)\n",
       "      406    0.001    0.000    0.010    0.000 iostream.py:195(schedule)\n",
       "     1608    0.001    0.000    0.013    0.000 managers.py:927(_consolidate_inplace)\n",
       "      756    0.001    0.000    0.018    0.000 generic.py:5135(f)\n",
       "      858    0.001    0.000    0.001    0.000 generic.py:144(_init_mgr)\n",
       "     3009    0.001    0.000    0.001    0.000 {pandas._libs.algos.ensure_int64}\n",
       "     2178    0.001    0.000    0.001    0.000 base.py:633(_reset_identity)\n",
       "      756    0.001    0.000    0.019    0.000 generic.py:5122(_protect_consolidate)\n",
       "     2781    0.001    0.000    0.002    0.000 managers.py:308(<genexpr>)\n",
       "     4122    0.001    0.000    0.002    0.000 {built-in method builtins.hash}\n",
       "     1458    0.001    0.000    0.006    0.000 common.py:1784(_is_dtype)\n",
       "     4134    0.001    0.000    0.001    0.000 managers.py:1488(_block)\n",
       "      365    0.001    0.000    0.012    0.000 iostream.py:382(write)\n",
       "      603    0.001    0.000    1.916    0.003 _decorators.py:146(wrapper)\n",
       "     1674    0.001    0.000    0.026    0.000 {built-in method builtins.all}\n",
       "     6087    0.001    0.000    0.001    0.000 common.py:122(classes_and_not_datetimelike)\n",
       "      888    0.001    0.000    0.022    0.000 concat.py:379(<genexpr>)\n",
       "     1203    0.001    0.000    0.004    0.000 common.py:868(is_integer_dtype)\n",
       "     1122    0.001    0.000    0.003    0.000 {built-in method builtins.sum}\n",
       "      363    0.001    0.000    0.005    0.000 numpy_.py:127(__init__)\n",
       "     2874    0.001    0.000    0.001    0.000 managers.py:206(items)\n",
       "      807    0.001    0.000    0.001    0.000 <frozen importlib._bootstrap>:103(release)\n",
       "     2349    0.001    0.000    0.001    0.000 inference.py:438(is_hashable)\n",
       "      807    0.001    0.000    0.002    0.000 <frozen importlib._bootstrap>:58(__init__)\n",
       "      201    0.001    0.000    0.036    0.000 generic.py:1439(__neg__)\n",
       "      429    0.001    0.000    0.007    0.000 cast.py:1151(construct_1d_arraylike_from_scalar)\n",
       "      792    0.001    0.000    0.002    0.000 generic.py:3070(_set_as_cached)\n",
       "     1404    0.001    0.000    0.001    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
       "      363    0.001    0.000    0.014    0.000 series.py:669(__array__)\n",
       "      204    0.001    0.000    0.021    0.000 <frozen importlib._bootstrap_external>:1240(_get_spec)\n",
       "     1200    0.001    0.000    0.006    0.000 inference.py:304(is_array_like)\n",
       "      363    0.001    0.000    0.021    0.000 indexing.py:2475(check_bool_indexer)\n",
       "      321    0.001    0.000    0.001    0.000 base.py:643(_engine)\n",
       "      429    0.001    0.000    0.001    0.000 generic.py:3175(_set_is_copy)\n",
       "     1227    0.001    0.000    0.001    0.000 base.py:551(<dictcomp>)\n",
       "      183    0.001    0.000    0.023    0.000 base.py:3089(reindex)\n",
       "     4020    0.001    0.000    0.001    0.000 {method 'items' of 'dict' objects}\n",
       "     2364    0.001    0.000    0.001    0.000 managers.py:591(is_consolidated)\n",
       "     1923    0.001    0.000    0.021    0.000 missing.py:25(isna)\n",
       "     1140    0.001    0.000    0.002    0.000 <frozen importlib._bootstrap_external>:56(_path_join)\n",
       "     1236    0.001    0.000    0.005    0.000 {built-in method builtins.any}\n",
       "      129    0.001    0.000    0.105    0.001 construction.py:170(init_dict)\n",
       "     2190    0.001    0.000    0.001    0.000 {built-in method __new__ of type object at 0x1034cb778}\n",
       "      237    0.001    0.000    0.080    0.000 concat.py:230(concatenate_join_units)\n",
       "      363    0.001    0.000    0.002    0.000 managers.py:1556(get_values)\n",
       "     2133    0.001    0.000    0.001    0.000 {method 'search' of 're.Pattern' objects}\n",
       "      756    0.001    0.000    0.020    0.000 generic.py:5132(_consolidate_inplace)\n",
       "     2130    0.001    0.000    0.003    0.000 base.py:3663(get_values)\n",
       "      429    0.001    0.000    0.001    0.000 cast.py:341(infer_dtype_from_scalar)\n",
       "      201    0.001    0.000    0.001    0.000 sorting.py:47(_int64_cut_off)\n",
       "      183    0.001    0.000    4.075    0.022 managers.py:710(copy)\n",
       "      114    0.001    0.000    0.006    0.000 blocks.py:3131(_merge_blocks)\n",
       "      654    0.001    0.000    0.002    0.000 generic.py:381(_get_block_manager_axis)\n",
       "      621    0.001    0.000    0.001    0.000 managers.py:1546(external_values)\n",
       "      957    0.001    0.000    0.001    0.000 common.py:316(apply_if_callable)\n",
       "      792    0.001    0.000    0.001    0.000 blocks.py:332(iget)\n",
       "      315    0.001    0.000    0.024    0.000 concat.py:367(is_uniform_join_units)\n",
       "      210    0.001    0.000    0.004    0.000 base.py:1117(__iter__)\n",
       "      201    0.001    0.000    0.019    0.000 series.py:730(__array_wrap__)\n",
       "     1494    0.001    0.000    0.001    0.000 {built-in method pandas._libs.missing.checknull}\n",
       "     3393    0.001    0.000    0.001    0.000 {pandas._libs.lib.is_float}\n",
       "      180    0.001    0.000    0.011    0.000 concat.py:101(_concat_compat)\n",
       "     1575    0.001    0.000    0.001    0.000 generic.py:450(ndim)\n",
       "      462    0.001    0.000    0.001    0.000 {built-in method numpy.seterrobj}\n",
       "     4554    0.001    0.000    0.001    0.000 __init__.py:291(strlen)\n",
       "      807    0.001    0.000    0.006    0.000 <frozen importlib._bootstrap>:147(__enter__)\n",
       "      468    0.001    0.000    0.002    0.000 blocks.py:128(_consolidate_key)\n",
       "      117    0.001    0.000    0.002    0.000 managers.py:1841(_stack_arrays)\n",
       "     1311    0.001    0.000    0.001    0.000 common.py:144(cast_scalar_indexer)\n",
       "      429    0.001    0.000    0.005    0.000 cast.py:846(maybe_infer_to_datetimelike)\n",
       "      549    0.001    0.000    0.001    0.000 indexing.py:2450(convert_to_index_sliceable)\n",
       "      678    0.001    0.000    0.004    0.000 base.py:963(_ndarray_values)\n",
       "      195    0.001    0.000    0.002    0.000 numeric.py:676(require)\n",
       "      183    0.001    0.000    0.002    0.000 ops.py:43(get_op_result_name)\n",
       "      201    0.001    0.000    0.011    0.000 base.py:2445(difference)\n",
       "     1014    0.001    0.000    0.004    0.000 arrays.py:7(extract_array)\n",
       "     1140    0.001    0.000    0.001    0.000 <frozen importlib._bootstrap_external>:58(<listcomp>)\n",
       "     2313    0.001    0.000    0.001    0.000 {built-in method builtins.iter}\n",
       "      162    0.001    0.000    0.002    0.000 indexing.py:256(_convert_scalar_indexer)\n",
       "      144    0.001    0.000    0.001    0.000 {pandas._libs.algos.take_2d_axis1_float64_float64}\n",
       "     1977    0.001    0.000    0.001    0.000 blocks.py:304(shape)\n",
       "     1614    0.001    0.000    0.001    0.000 {pandas._libs.algos.ensure_platform_int}\n",
       "      756    0.001    0.000    0.016    0.000 managers.py:911(consolidate)\n",
       "      807    0.001    0.000    0.001    0.000 <frozen importlib._bootstrap>:176(cb)\n",
       "     1035    0.001    0.000    0.001    0.000 {built-in method builtins.min}\n",
       "      126    0.001    0.000    0.010    0.000 generic.py:1657(_get_label_or_level_values)\n",
       "      162    0.001    0.000    0.001    0.000 {built-in method _locale.nl_langinfo}\n",
       "      867    0.001    0.000    0.002    0.000 frame.py:937(__len__)\n",
       "     4758    0.001    0.000    0.001    0.000 {method 'append' of 'list' objects}\n",
       "     1614    0.001    0.000    0.001    0.000 {built-in method _thread.allocate_lock}\n",
       "      231    0.001    0.000    0.003    0.000 numeric.py:3063(__exit__)\n",
       "      324    0.001    0.000    0.001    0.000 base.py:4459(_maybe_promote)\n",
       "     1206    0.001    0.000    0.003    0.000 generic.py:1895(<genexpr>)\n",
       "     1377    0.001    0.000    0.001    0.000 series.py:382(_set_subtyp)\n",
       "       42    0.001    0.000    0.001    0.000 base.py:1587(is_monotonic_increasing)\n",
       "       18    0.001    0.000    0.001    0.000 {pandas._libs.algos.rank_1d_float64}\n",
       "      100    0.001    0.000    0.039    0.000 {built-in method builtins.print}\n",
       "      366    0.001    0.000    0.008    0.000 base.py:700(view)\n",
       "  603/201    0.001    0.000    0.035    0.000 {built-in method builtins.__import__}\n",
       "      642    0.001    0.000    0.008    0.000 common.py:1643(is_extension_type)\n",
       "      162    0.001    0.000    0.001    0.000 _bootlocale.py:33(getpreferredencoding)\n",
       "      351    0.001    0.000    0.001    0.000 format.py:1019(base_formatter)\n",
       "      402    0.001    0.000    0.004    0.000 generic.py:1848(empty)\n",
       "      363    0.001    0.000    0.001    0.000 generic.py:1814(__hash__)\n",
       "      162    0.001    0.000    3.745    0.023 ops.py:1615(na_op)\n",
       "        3    0.001    0.000    0.001    0.000 {method 'writelines' of '_io._IOBase' objects}\n",
       "      948    0.001    0.000    0.006    0.000 common.py:605(is_string_dtype)\n",
       "      102    0.001    0.000    0.292    0.003 concat.py:24(concat)\n",
       "       27    0.001    0.000    0.007    0.000 format.py:1045(get_result_as_array)\n",
       "       39    0.001    0.000    0.001    0.000 {built-in method builtins.__build_class__}\n",
       "      222    0.001    0.000    0.004    0.000 fromnumeric.py:2664(prod)\n",
       "      102    0.001    0.000    0.016    0.000 concat.py:440(_get_new_axes)\n",
       "      948    0.001    0.000    0.003    0.000 common.py:634(condition)\n",
       "       21    0.001    0.000    0.961    0.046 merge.py:37(merge)\n",
       "      406    0.001    0.000    0.001    0.000 threading.py:1080(is_alive)\n",
       "      519    0.001    0.000    0.001    0.000 managers.py:1549(internal_values)\n",
       "     1233    0.001    0.000    0.001    0.000 {method 'update' of 'dict' objects}\n",
       "      129    0.001    0.000    0.029    0.000 construction.py:254(_homogenize)\n",
       "      363    0.001    0.000    0.001    0.000 common.py:1118(is_datetime64_ns_dtype)\n",
       "      228    0.001    0.000    0.001    0.000 numerictypes.py:654(<listcomp>)\n",
       "      189    0.001    0.000    0.002    0.000 generic.py:1546(_is_label_reference)\n",
       "     1401    0.001    0.000    0.001    0.000 blocks.py:191(fill_value)\n",
       "     2178    0.001    0.000    0.001    0.000 {method 'lower' of 'str' objects}\n",
       "      363    0.001    0.000    0.003    0.000 series.py:490(get_values)\n",
       "      822    0.001    0.000    0.001    0.000 inference.py:121(is_iterator)\n",
       "      408    0.001    0.000    0.001    0.000 config.py:78(_get_single_key)\n",
       "      189    0.001    0.000    0.001    0.000 managers.py:2015(_preprocess_slice_or_indexer)\n",
       "      756    0.001    0.000    0.001    0.000 frame.py:361(_constructor)\n",
       "       21    0.000    0.000    0.889    0.042 merge.py:541(get_result)\n",
       "       21    0.000    0.000    0.001    0.000 merge.py:647(_maybe_add_join_keys)\n",
       "      603    0.000    0.000    0.004    0.000 algorithms.py:163(_ensure_arraylike)\n",
       "      228    0.000    0.000    0.001    0.000 numerictypes.py:602(find_common_type)\n",
       "      129    0.000    0.000    0.028    0.000 managers.py:1663(create_block_manager_from_arrays)\n",
       "      621    0.000    0.000    0.002    0.000 series.py:434(values)\n",
       "       27    0.000    0.000    0.005    0.000 format.py:1060(format_values_with)\n",
       "        3    0.000    0.000    0.000    0.000 {pandas._libs.writers.write_csv_rows}\n",
       "      924    0.000    0.000    0.000    0.000 {built-in method numpy.geterrobj}\n",
       "     1158    0.000    0.000    0.003    0.000 numeric.py:541(asanyarray)\n",
       "      231    0.000    0.000    0.001    0.000 numeric.py:3054(__init__)\n",
       "     1350    0.000    0.000    0.001    0.000 {method 'join' of 'str' objects}\n",
       "      402    0.000    0.000    0.000    0.000 {pandas._libs.internals.get_blkno_placements}\n",
       "     3222    0.000    0.000    0.000    0.000 {pandas._libs.lib.is_integer}\n",
       "      252    0.000    0.000    0.001    0.000 generic.py:1513(_is_level_reference)\n",
       "      162    0.000    0.000    0.001    0.000 base.py:2849(_convert_scalar_indexer)\n",
       "      324    0.000    0.000    0.004    0.000 base.py:1669(is_boolean)\n",
       "      102    0.000    0.000    0.002    0.000 concat.py:84(_get_frame_result_type)\n",
       "       42    0.000    0.000    0.010    0.000 managers.py:1887(_consolidate)\n",
       "      366    0.000    0.000    0.008    0.000 managers.py:729(<lambda>)\n",
       "      807    0.000    0.000    0.002    0.000 <frozen importlib._bootstrap>:151(__exit__)\n",
       "     2964    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}\n",
       "       21    0.000    0.000    0.001    0.000 function_base.py:4220(delete)\n",
       "       21    0.000    0.000    0.070    0.003 merge.py:777(_get_merge_keys)\n",
       "     2430    0.000    0.000    0.000    0.000 {built-in method _imp.acquire_lock}\n",
       "      462    0.000    0.000    0.001    0.000 common.py:1198(is_datetime_or_timedelta_dtype)\n",
       "      573    0.000    0.000    0.009    0.000 base.py:1729(inferred_type)\n",
       "      363    0.000    0.000    0.001    0.000 blocks.py:184(to_dense)\n",
       "      417    0.000    0.000    0.004    0.000 concat.py:126(dtype)\n",
       "       78    0.000    0.000    0.005    0.000 managers.py:1810(_multi_blockify)\n",
       "      594    0.000    0.000    0.001    0.000 cast.py:1218(construct_1d_ndarray_preserving_na)\n",
       "     1278    0.000    0.000    0.000    0.000 base.py:676(dtype)\n",
       "       21    0.000    0.000    0.072    0.003 merge.py:474(__init__)\n",
       "      129    0.000    0.000    0.080    0.001 construction.py:43(arrays_to_mgr)\n",
       "     1248    0.000    0.000    0.001    0.000 format.py:1401(just)\n",
       "       39    0.000    0.000    0.001    0.000 <ipython-input-20-edee81b75678>:15(buildParseTree)\n",
       "      237    0.000    0.000    0.036    0.000 concat.py:240(<listcomp>)\n",
       "     1872    0.000    0.000    0.001    0.000 format.py:1418(_is_number)\n",
       "        6    0.000    0.000    0.000    0.000 {method 'close' of '_io.TextIOWrapper' objects}\n",
       "      462    0.000    0.000    0.005    0.000 common.py:1431(needs_i8_conversion)\n",
       "      807    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:143(__init__)\n",
       "       21    0.000    0.000    0.000    0.000 base.py:221(_inner_indexer)\n",
       "      231    0.000    0.000    0.003    0.000 numeric.py:3058(__enter__)\n",
       "       78    0.000    0.000    0.000    0.000 blocks.py:3145(<listcomp>)\n",
       "     1374    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
       "      399    0.000    0.000    0.000    0.000 {pandas._libs.lib.array_equivalent_object}\n",
       "      420    0.000    0.000    0.001    0.000 managers.py:291(__len__)\n",
       "      402    0.000    0.000    0.001    0.000 managers.py:174(_is_single_block)\n",
       "      519    0.000    0.000    0.001    0.000 series.py:476(_values)\n",
       "     1140    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:222(_verbose_message)\n",
       "      342    0.000    0.000    0.003    0.000 base.py:646(<lambda>)\n",
       "     2280    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}\n",
       "      810    0.000    0.000    0.000    0.000 config.py:561(_get_deprecated_option)\n",
       "      126    0.000    0.000    0.001    0.000 generic.py:1607(_check_label_or_level_ambiguity)\n",
       "      162    0.000    0.000    0.003    0.000 base.py:2965(_convert_listlike_indexer)\n",
       "      363    0.000    0.000    0.001    0.000 numpy_.py:170(__array__)\n",
       "       48    0.000    0.000    0.013    0.000 format.py:848(format_array)\n",
       "      102    0.000    0.000    0.009    0.000 concat.py:475(_get_concat_axis)\n",
       "       51    0.000    0.000    0.004    0.000 indexing.py:960(_getitem_lowerdim)\n",
       "       24    0.000    0.000    0.006    0.000 managers.py:1019(set)\n",
       "      365    0.000    0.000    0.000    0.000 iostream.py:307(_is_master_process)\n",
       "      402    0.000    0.000    0.000    0.000 config.py:546(_get_root)\n",
       "        3    0.000    0.000    0.023    0.008 format.py:503(_to_str_columns)\n",
       "      465    0.000    0.000    0.000    0.000 base.py:3806(_coerce_to_ndarray)\n",
       "       96    0.000    0.000    0.005    0.000 format.py:1384(_make_fixed_width)\n",
       "     1728    0.000    0.000    0.000    0.000 base.py:3632(_values)\n",
       "      363    0.000    0.000    0.001    0.000 common.py:1167(is_timedelta64_ns_dtype)\n",
       "     2430    0.000    0.000    0.000    0.000 {built-in method _imp.release_lock}\n",
       "      888    0.000    0.000    0.000    0.000 concat.py:376(<genexpr>)\n",
       "      162    0.000    0.000    0.068    0.000 base.py:4447(get_indexer_for)\n",
       "      462    0.000    0.000    0.002    0.000 {method 'all' of 'numpy.ndarray' objects}\n",
       "      204    0.000    0.000    0.017    0.000 generic.py:5140(_consolidate)\n",
       "     1344    0.000    0.000    0.001    0.000 format.py:1392(<genexpr>)\n",
       "      165    0.000    0.000    0.010    0.000 generic.py:178(_validate_dtype)\n",
       "       42    0.000    0.000    0.085    0.002 frame.py:6558(append)\n",
       "      402    0.000    0.000    0.002    0.000 config.py:96(_get_option)\n",
       "      660    0.000    0.000    0.000    0.000 concat.py:104(__init__)\n",
       "      204    0.000    0.000    0.022    0.000 <frozen importlib._bootstrap_external>:1272(find_spec)\n",
       "      204    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:369(__init__)\n",
       "      222    0.000    0.000    0.000    0.000 shape_base.py:83(atleast_2d)\n",
       "      162    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
       "     1386    0.000    0.000    0.000    0.000 numeric.py:113(is_all_dates)\n",
       "       78    0.000    0.000    0.083    0.001 blocks.py:323(concat_same_type)\n",
       "      300    0.000    0.000    0.000    0.000 fromnumeric.py:70(<dictcomp>)\n",
       "      345    0.000    0.000    0.000    0.000 missing.py:530(clean_reindex_fill_method)\n",
       "      456    0.000    0.000    0.000    0.000 numerictypes.py:578(_can_coerce_all)\n",
       "      426    0.000    0.000    0.000    0.000 _validators.py:221(validate_bool_kwarg)\n",
       "       78    0.000    0.000    0.000    0.000 {method 'argsort' of 'numpy.ndarray' objects}\n",
       "      406    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
       "      406    0.000    0.000    0.000    0.000 iostream.py:93(_event_pipe)\n",
       "      492    0.000    0.000    0.000    0.000 base.py:1237(_get_names)\n",
       "      102    0.000    0.000    0.001    0.000 concat.py:309(<listcomp>)\n",
       "      315    0.000    0.000    0.001    0.000 common.py:1472(is_numeric_dtype)\n",
       "      102    0.000    0.000    0.005    0.000 concat.py:464(_get_comb_axis)\n",
       "      406    0.000    0.000    0.001    0.000 threading.py:1038(_wait_for_tstate_lock)\n",
       "       63    0.000    0.000    0.011    0.000 base.py:3988(append)\n",
       "      618    0.000    0.000    0.000    0.000 frame.py:474(axes)\n",
       "      210    0.000    0.000    0.003    0.000 common.py:702(is_datetimelike)\n",
       "      630    0.000    0.000    0.000    0.000 concat.py:450(_next_or_none)\n",
       "       60    0.000    0.000    0.001    0.000 range.py:69(__new__)\n",
       "       60    0.000    0.000    0.001    0.000 base.py:5408(default_index)\n",
       "       57    0.000    0.000    0.004    0.000 api.py:128(_union_indexes)\n",
       "     1245    0.000    0.000    0.000    0.000 {pandas._libs.algos.ensure_object}\n",
       "  603/201    0.000    0.000    0.035    0.000 <frozen importlib._bootstrap>:211(_call_with_frames_removed)\n",
       "       63    0.000    0.000    0.000    0.000 range.py:136(_simple_new)\n",
       "      228    0.000    0.000    0.014    0.000 <frozen importlib._bootstrap_external>:74(_path_stat)\n",
       "      204    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:873(_find_spec_legacy)\n",
       "      162    0.000    0.000    0.002    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
       "       96    0.000    0.000    0.001    0.000 format.py:1407(<listcomp>)\n",
       "      816    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:855(__enter__)\n",
       "      231    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1203(_path_importer_cache)\n",
       "      474    0.000    0.000    0.000    0.000 concat.py:391(<genexpr>)\n",
       "      378    0.000    0.000    0.001    0.000 generic.py:1576(<genexpr>)\n",
       "       21    0.000    0.000    0.962    0.046 frame.py:6858(merge)\n",
       "     1266    0.000    0.000    0.000    0.000 {pandas._libs.lib.is_bool}\n",
       "       45    0.000    0.000    0.018    0.000 format.py:702(_format_col)\n",
       "      381    0.000    0.000    0.000    0.000 series.py:338(_constructor)\n",
       "     1614    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}\n",
       "      816    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:859(__exit__)\n",
       "      102    0.000    0.000    0.000    0.000 api.py:73(_get_distinct_objs)\n",
       "       81    0.000    0.000    0.001    0.000 format.py:1422(<listcomp>)\n",
       "      432    0.000    0.000    0.001    0.000 blocks.py:175(get_values)\n",
       "      429    0.000    0.000    0.001    0.000 managers.py:1844(_asarray_compat)\n",
       "      957    0.000    0.000    0.000    0.000 {built-in method builtins.callable}\n",
       "       21    0.000    0.000    0.005    0.000 base.py:2357(intersection)\n",
       "       60    0.000    0.000    0.006    0.000 range.py:272(_shallow_copy)\n",
       "      204    0.000    0.000    0.001    0.000 frame.py:491(shape)\n",
       "      201    0.000    0.000    0.000    0.000 base.py:978(empty)\n",
       "      162    0.000    0.000    0.003    0.000 base.py:1672(is_integer)\n",
       "      708    0.000    0.000    0.000    0.000 concat.py:381(<genexpr>)\n",
       "        1    0.000    0.000   38.092   38.092 <ipython-input-25-8ba185739980>:8(main)\n",
       "      195    0.000    0.000    0.000    0.000 numeric.py:748(<setcomp>)\n",
       "      237    0.000    0.000    0.001    0.000 concat.py:388(is_uniform_reindex)\n",
       "      102    0.000    0.000    0.004    0.000 api.py:87(_get_combined_index)\n",
       "       63    0.000    0.000    0.001    0.000 {built-in method builtins.sorted}\n",
       "      135    0.000    0.000    0.000    0.000 printing.py:59(<listcomp>)\n",
       "       21    0.000    0.000    0.005    0.000 base.py:4939(drop)\n",
       "       12    0.000    0.000    0.001    0.000 printing.py:15(adjoin)\n",
       "      123    0.000    0.000    0.007    0.000 managers.py:2041(<listcomp>)\n",
       "       54    0.000    0.000    0.000    0.000 format.py:1427(<listcomp>)\n",
       "      444    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
       "       21    0.000    0.000    0.014    0.001 generic.py:3787(_drop_axis)\n",
       "      129    0.000    0.000    0.000    0.000 common.py:212(dict_keys_to_ordered_list)\n",
       "      102    0.000    0.000    0.001    0.000 generic.py:337(_from_axes)\n",
       "       90    0.000    0.000    0.013    0.000 construction.py:284(extract_index)\n",
       "       21    0.000    0.000    0.842    0.040 merge.py:737(_get_join_info)\n",
       "      183    0.000    0.000    0.008    0.000 managers.py:730(<listcomp>)\n",
       "        3    0.000    0.000    0.003    0.001 csvs.py:290(_save_chunk)\n",
       "       63    0.000    0.000    0.010    0.000 base.py:4017(_concat)\n",
       "      402    0.000    0.000    0.002    0.000 config.py:226(__call__)\n",
       "      222    0.000    0.000    0.001    0.000 base.py:2590(_assert_can_do_setop)\n",
       "      468    0.000    0.000    0.002    0.000 managers.py:1893(<lambda>)\n",
       "       21    0.000    0.000    0.007    0.000 generic.py:4113(reindex)\n",
       "       57    0.000    0.000    0.000    0.000 api.py:262(<setcomp>)\n",
       "      126    0.000    0.000    0.007    0.000 generic.py:3461(xs)\n",
       "      297    0.000    0.000    0.001    0.000 indexing.py:2689(is_list_like_indexer)\n",
       "      462    0.000    0.000    0.002    0.000 _methods.py:45(_all)\n",
       "      753    0.000    0.000    0.001    0.000 {built-in method builtins.next}\n",
       "      102    0.000    0.000    0.005    0.000 api.py:44(_get_objs_combined_axis)\n",
       "       21    0.000    0.000    0.002    0.000 concat.py:486(_concat_index_asobject)\n",
       "      630    0.000    0.000    0.000    0.000 format.py:541(<genexpr>)\n",
       "      365    0.000    0.000    0.001    0.000 iostream.py:320(_schedule_flush)\n",
       "       93    0.000    0.000    0.000    0.000 format.py:356(_get_formatter)\n",
       "       12    0.000    0.000    0.000    0.000 index_tricks.py:316(__getitem__)\n",
       "       18    0.000    0.000    0.000    0.000 function_base.py:1149(diff)\n",
       "      162    0.000    0.000    0.000    0.000 base.py:802(_assert_take_fillable)\n",
       "      402    0.000    0.000    0.000    0.000 managers.py:706(nblocks)\n",
       "        6    0.000    0.000    0.019    0.003 {_cython_magic_a8678e4eba7059b7e523b0769dde009b.geometric_mean}\n",
       "       78    0.000    0.000    0.001    0.000 twodim_base.py:216(diag)\n",
       "      138    0.000    0.000    0.000    0.000 range.py:330(equals)\n",
       "       51    0.000    0.000    0.009    0.000 indexing.py:1485(__getitem__)\n",
       "       27    0.000    0.000    0.001    0.000 format.py:1078(<listcomp>)\n",
       "      228    0.000    0.000    0.000    0.000 _config.py:49(getter)\n",
       "      120    0.000    0.000    0.000    0.000 <ipython-input-6-8d057f9c05bf>:1(get_notes)\n",
       "      162    0.000    0.000    0.003    0.000 base.py:2999(_convert_arr_indexer)\n",
       "      204    0.000    0.000    0.000    0.000 six.py:184(find_module)\n",
       "      204    0.000    0.000    0.000    0.000 {built-in method _imp.is_frozen}\n",
       "       72    0.000    0.000    0.000    0.000 printing.py:55(<listcomp>)\n",
       "       21    0.000    0.000    0.001    0.000 merge.py:889(_maybe_coerce_merge_keys)\n",
       "      807    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}\n",
       "      363    0.000    0.000    0.001    0.000 series.py:591(__len__)\n",
       "       45    0.000    0.000    0.002    0.000 frame.py:2829(_ixs)\n",
       "      228    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:36(_relax_case)\n",
       "     1215    0.000    0.000    0.000    0.000 {method 'rjust' of 'str' objects}\n",
       "      180    0.000    0.000    0.000    0.000 concat.py:126(<listcomp>)\n",
       "       21    0.000    0.000    0.005    0.000 generic.py:4469(_reindex_with_indexers)\n",
       "      147    0.000    0.000    0.000    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
       "       42    0.000    0.000    0.004    0.000 concat.py:481(_concat_index_same_dtype)\n",
       "      810    0.000    0.000    0.000    0.000 format.py:1423(<genexpr>)\n",
       "      597    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}\n",
       "      564    0.000    0.000    0.000    0.000 blocks.py:165(internal_values)\n",
       "       12    0.000    0.000    0.005    0.000 managers.py:1134(insert)\n",
       "      990    0.000    0.000    0.000    0.000 {method 'ljust' of 'str' objects}\n",
       "      402    0.000    0.000    0.000    0.000 config.py:602(_warn_if_deprecated)\n",
       "       51    0.000    0.000    0.006    0.000 indexing.py:2205(_getitem_axis)\n",
       "      621    0.000    0.000    0.000    0.000 blocks.py:161(external_values)\n",
       "       57    0.000    0.000    0.000    0.000 api.py:205(_sanitize_and_check)\n",
       "       81    0.000    0.000    0.001    0.000 format.py:1421(_cond)\n",
       "      204    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:792(find_spec)\n",
       "      201    0.000    0.000    0.000    0.000 base.py:759(size)\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method posix.listdir}\n",
       "      573    0.000    0.000    0.000    0.000 base.py:1396(nlevels)\n",
       "      354    0.000    0.000    0.001    0.000 missing.py:259(notna)\n",
       "      204    0.000    0.000    0.001    0.000 concat.py:92(<genexpr>)\n",
       "       51    0.000    0.000    0.001    0.000 indexing.py:217(_has_valid_tuple)\n",
       "      162    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
       "       42    0.000    0.000    0.001    0.000 base.py:1272(set_names)\n",
       "      408    0.000    0.000    0.000    0.000 config.py:589(_translate_key)\n",
       "      180    0.000    0.000    0.001    0.000 concat.py:151(<listcomp>)\n",
       "       24    0.000    0.000    0.001    0.000 frame.py:3565(_sanitize_column)\n",
       "       78    0.000    0.000    0.001    0.000 fromnumeric.py:1966(sum)\n",
       "       21    0.000    0.000    0.059    0.003 generic.py:1729(_drop_labels_or_levels)\n",
       "      408    0.000    0.000    0.000    0.000 config.py:528(_select_options)\n",
       "      360    0.000    0.000    0.000    0.000 concat.py:136(<genexpr>)\n",
       "       78    0.000    0.000    0.001    0.000 shape_base.py:229(vstack)\n",
       "       39    0.000    0.000    0.015    0.000 <ipython-input-19-f1005eda34b6>:17(__init__)\n",
       "       51    0.000    0.000    0.000    0.000 indexing.py:2089(_is_scalar_access)\n",
       "      207    0.000    0.000    0.001    0.000 printing.py:50(justify)\n",
       "       84    0.000    0.000    0.000    0.000 printing.py:185(as_escaped_unicode)\n",
       "       57    0.000    0.000    0.000    0.000 api.py:243(_get_consensus_names)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method posix.getcwd}\n",
       "       42    0.000    0.000    0.006    0.000 numeric.py:110(_concat_same_dtype)\n",
       "       84    0.000    0.000    0.000    0.000 printing.py:156(pprint_thing)\n",
       "      102    0.000    0.000    0.000    0.000 api.py:67(<listcomp>)\n",
       "       78    0.000    0.000    0.000    0.000 fromnumeric.py:1395(diagonal)\n",
       "       78    0.000    0.000    0.000    0.000 {method 'diagonal' of 'numpy.ndarray' objects}\n",
       "       18    0.000    0.000    0.000    0.000 format.py:1140(<listcomp>)\n",
       "      345    0.000    0.000    0.000    0.000 missing.py:71(clean_fill_method)\n",
       "       21    0.000    0.000    0.019    0.001 generic.py:3759(drop)\n",
       "      360    0.000    0.000    0.000    0.000 concat.py:120(is_nonempty)\n",
       "      237    0.000    0.000    0.000    0.000 blocks.py:255(__len__)\n",
       "      126    0.000    0.000    0.001    0.000 generic.py:1844(__contains__)\n",
       "       39    0.000    0.000    0.005    0.000 <ipython-input-20-edee81b75678>:55(make_parse_tree)\n",
       "       60    0.000    0.000    0.001    0.000 common.py:1320(is_datetimelike_v_numeric)\n",
       "      207    0.000    0.000    0.001    0.000 format.py:304(justify)\n",
       "       78    0.000    0.000    0.001    0.000 shape_base.py:283(<listcomp>)\n",
       "      102    0.000    0.000    0.000    0.000 common.py:162(_not_none)\n",
       "      756    0.000    0.000    0.000    0.000 format.py:1424(<genexpr>)\n",
       "      120    0.000    0.000    0.000    0.000 range.py:89(ensure_int)\n",
       "       24    0.000    0.000    0.007    0.000 frame.py:3356(__setitem__)\n",
       "       51    0.000    0.000    0.008    0.000 indexing.py:2141(_getitem_tuple)\n",
       "       27    0.000    0.000    0.000    0.000 format.py:1430(<listcomp>)\n",
       "      102    0.000    0.000    0.001    0.000 indexing.py:2056(_validate_key)\n",
       "       48    0.000    0.000    0.011    0.000 format.py:927(get_result)\n",
       "      365    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
       "      162    0.000    0.000    0.002    0.000 _methods.py:34(_sum)\n",
       "      183    0.000    0.000    0.000    0.000 base.py:5381(_ensure_has_len)\n",
       "       21    0.000    0.000    0.006    0.000 generic.py:3840(_update_inplace)\n",
       "       42    0.000    0.000    0.008    0.000 concat.py:531(_concat_indexes)\n",
       "      681    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}\n",
       "      204    0.000    0.000    0.001    0.000 concat.py:93(<genexpr>)\n",
       "       63    0.000    0.000    0.001    0.000 generic.py:1578(_is_label_or_level_reference)\n",
       "       27    0.000    0.000    0.002    0.000 format.py:1412(_trim_zeros)\n",
       "      147    0.000    0.000    0.000    0.000 common.py:1513(is_string_like_dtype)\n",
       "       21    0.000    0.000    0.007    0.000 frame.py:3794(reindex)\n",
       "        3    0.000    0.000    0.000    0.000 {pandas._libs.lib.map_infer}\n",
       "      162    0.000    0.000    0.000    0.000 base.py:3035(_convert_list_indexer)\n",
       "       81    0.000    0.000    0.000    0.000 generic.py:3149(_clear_item_cache)\n",
       "       18    0.000    0.000    0.003    0.000 generic.py:8324(ranker)\n",
       "       21    0.000    0.000    0.000    0.000 generic.py:297(_construct_axes_from_arguments)\n",
       "       90    0.000    0.000    0.000    0.000 indexing.py:2116(_validate_integer)\n",
       "      222    0.000    0.000    0.000    0.000 base.py:2249(_validate_sort_keyword)\n",
       "      406    0.000    0.000    0.000    0.000 threading.py:507(is_set)\n",
       "       78    0.000    0.000    0.000    0.000 fromnumeric.py:942(argsort)\n",
       "      306    0.000    0.000    0.000    0.000 common.py:164(<genexpr>)\n",
       "      162    0.000    0.000    0.000    0.000 indexing.py:2676(is_nested_tuple)\n",
       "       21    0.000    0.000    0.001    0.000 blocks.py:1982(to_native_types)\n",
       "       42    0.000    0.000    0.000    0.000 base.py:1240(_set_names)\n",
       "       12    0.000    0.000    0.000    0.000 common.py:120(_stringify_path)\n",
       "       21    0.000    0.000    0.000    0.000 merge.py:614(_maybe_restore_index_levels)\n",
       "       48    0.000    0.000    0.000    0.000 <ipython-input-21-ca51fc37fe1a>:2(expressions)\n",
       "      228    0.000    0.000    0.000    0.000 _config.py:140(get)\n",
       "       39    0.000    0.000    0.000    0.000 <ipython-input-20-edee81b75678>:82(__init__)\n",
       "      390    0.000    0.000    0.000    0.000 managers.py:1814(<lambda>)\n",
       "      378    0.000    0.000    0.000    0.000 format.py:1107(<genexpr>)\n",
       "       78    0.000    0.000    0.000    0.000 shape_base.py:220(_warn_for_nonsequence)\n",
       "       21    0.000    0.000    0.002    0.000 base.py:2354(_wrap_setop_result)\n",
       "      360    0.000    0.000    0.000    0.000 concat.py:137(<genexpr>)\n",
       "        3    0.000    0.000    0.001    0.000 format.py:931(_format_strings)\n",
       "       51    0.000    0.000    0.001    0.000 format.py:338(_get_adjustment)\n",
       "        6    0.000    0.000    0.000    0.000 _methods.py:58(_mean)\n",
       "       24    0.000    0.000    0.007    0.000 frame.py:3433(_set_item)\n",
       "        9    0.000    0.000    0.001    0.000 blocks.py:730(to_native_types)\n",
       "      102    0.000    0.000    0.000    0.000 concat.py:507(<listcomp>)\n",
       "       21    0.000    0.000    0.000    0.000 merge.py:1704(<lambda>)\n",
       "       21    0.000    0.000    0.006    0.000 frame.py:3729(_reindex_axes)\n",
       "       51    0.000    0.000    0.000    0.000 indexing.py:229(_is_nested_tuple_indexer)\n",
       "        3    0.000    0.000    0.002    0.001 format.py:642(_join_multiline)\n",
       "       39    0.000    0.000    0.000    0.000 <ipython-input-20-edee81b75678>:63(preprocess_sentence)\n",
       "      438    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
       "       39    0.000    0.000    0.002    0.000 managers.py:1796(_simple_blockify)\n",
       "      129    0.000    0.000    0.000    0.000 construction.py:210(<listcomp>)\n",
       "      102    0.000    0.000    0.000    0.000 concat.py:434(_get_result_dim)\n",
       "      406    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
       "       78    0.000    0.000    0.000    0.000 fromnumeric.py:54(_wrapfunc)\n",
       "       21    0.000    0.000    0.001    0.000 base.py:4909(delete)\n",
       "      204    0.000    0.000    0.000    0.000 concat.py:97(<genexpr>)\n",
       "       24    0.000    0.000    0.006    0.000 generic.py:3171(_set_item)\n",
       "      153    0.000    0.000    0.000    0.000 indexing.py:1487(<genexpr>)\n",
       "      363    0.000    0.000    0.000    0.000 common.py:1195(<lambda>)\n",
       "      156    0.000    0.000    0.000    0.000 common.py:183(_any_not_none)\n",
       "       21    0.000    0.000    0.006    0.000 frame.py:3754(_reindex_columns)\n",
       "      132    0.000    0.000    0.000    0.000 generic.py:426(_info_axis)\n",
       "      234    0.000    0.000    0.000    0.000 format.py:1139(<lambda>)\n",
       "       21    0.000    0.000    0.000    0.000 copy.py:66(copy)\n",
       "       21    0.000    0.000    0.000    0.000 sorting.py:124(is_int64_overflow_possible)\n",
       "       12    0.000    0.000    0.000    0.000 blocks.py:335(set)\n",
       "        3    0.000    0.000    0.001    0.000 format.py:651(<listcomp>)\n",
       "       39    0.000    0.000    0.000    0.000 <ipython-input-16-09e278ad2131>:12(get_ref_sys)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'getvalue' of '_io.StringIO' objects}\n",
       "      186    0.000    0.000    0.000    0.000 base.py:475(<genexpr>)\n",
       "      123    0.000    0.000    0.000    0.000 binaryTree.py:17(__init__)\n",
       "       21    0.000    0.000    0.000    0.000 {built-in method numpy.copyto}\n",
       "      120    0.000    0.000    0.000    0.000 <ipython-input-21-ca51fc37fe1a>:25(<genexpr>)\n",
       "      204    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:719(find_spec)\n",
       "      177    0.000    0.000    0.000    0.000 printing.py:62(_join_unicode)\n",
       "      102    0.000    0.000    0.000    0.000 indexing.py:2695(is_label_like)\n",
       "       12    0.000    0.000    0.001    0.000 managers.py:2008(_fast_count_smallints)\n",
       "       48    0.000    0.000    0.000    0.000 format.py:912(__init__)\n",
       "       63    0.000    0.000    0.000    0.000 base.py:4012(<setcomp>)\n",
       "       21    0.000    0.000    0.020    0.001 frame.py:3819(drop)\n",
       "        6    0.000    0.000    0.003    0.000 generic.py:3155(_slice)\n",
       "       60    0.000    0.000    0.000    0.000 common.py:1752(is_complex_dtype)\n",
       "        6    0.000    0.000    0.000    0.000 stats.py:256(gmean)\n",
       "       39    0.000    0.000    0.000    0.000 <ipython-input-16-09e278ad2131>:5(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 {pandas._libs.lib.maybe_convert_objects}\n",
       "        3    0.000    0.000    0.001    0.000 csvs.py:30(__init__)\n",
       "       42    0.000    0.000    0.001    0.000 base.py:1346(rename)\n",
       "       12    0.000    0.000    0.004    0.000 base.py:4919(insert)\n",
       "      189    0.000    0.000    0.000    0.000 generic.py:1567(<listcomp>)\n",
       "       42    0.000    0.000    0.000    0.000 merge.py:1738(_any)\n",
       "      147    0.000    0.000    0.000    0.000 {method 'pop' of 'list' objects}\n",
       "       24    0.000    0.000    0.000    0.000 base.py:3926(contains)\n",
       "       90    0.000    0.000    0.000    0.000 inference.py:470(is_sequence)\n",
       "       21    0.000    0.000    0.005    0.000 managers.py:1959(items_overlap_with_suffix)\n",
       "        3    0.000    0.000    0.002    0.001 format.py:739(_get_formatted_column_labels)\n",
       "        3    0.000    0.000    0.000    0.000 format.py:1553(_binify)\n",
       "       21    0.000    0.000    0.000    0.000 range.py:180(_int64index)\n",
       "       39    0.000    0.000    0.000    0.000 format.py:945(_format)\n",
       "      123    0.000    0.000    0.000    0.000 stack.py:14(push)\n",
       "      123    0.000    0.000    0.000    0.000 stack.py:17(pop)\n",
       "       27    0.000    0.000    0.004    0.000 range.py:176(_data)\n",
       "       18    0.000    0.000    0.003    0.000 generic.py:8282(rank)\n",
       "       12    0.000    0.000    0.001    0.000 format.py:307(adjoin)\n",
       "       27    0.000    0.000    0.007    0.000 format.py:1128(_format_strings)\n",
       "       63    0.000    0.000    0.000    0.000 merge.py:1731(_should_fill)\n",
       "       81    0.000    0.000    0.000    0.000 {method 'clear' of 'dict' objects}\n",
       "       21    0.000    0.000    0.000    0.000 {built-in method pandas._libs.lib.is_bool_array}\n",
       "       57    0.000    0.000    0.000    0.000 api.py:226(<setcomp>)\n",
       "      153    0.000    0.000    0.000    0.000 indexing.py:230(<genexpr>)\n",
       "       45    0.000    0.000    0.000    0.000 managers.py:1552(formatting_values)\n",
       "        3    0.000    0.000    0.026    0.009 format.py:582(to_string)\n",
       "      147    0.000    0.000    0.000    0.000 common.py:1542(<lambda>)\n",
       "       21    0.000    0.000    0.000    0.000 _validators.py:230(validate_axis_style_args)\n",
       "      117    0.000    0.000    0.000    0.000 managers.py:1850(_shape_compat)\n",
       "        6    0.000    0.000    0.001    0.000 base.py:998(_format_with_header)\n",
       "       42    0.000    0.000    0.001    0.000 generic.py:4341(<genexpr>)\n",
       "       21    0.000    0.000    0.000    0.000 generic.py:4378(_needs_reindex_multi)\n",
       "      234    0.000    0.000    0.000    0.000 concat.py:383(<genexpr>)\n",
       "       42    0.000    0.000    0.000    0.000 binaryTree.py:22(insertLeft)\n",
       "      156    0.000    0.000    0.000    0.000 binaryTree.py:50(getLeftChild)\n",
       "        3    0.000    0.000    0.007    0.002 csvs.py:130(save)\n",
       "      126    0.000    0.000    0.000    0.000 generic.py:1693(<listcomp>)\n",
       "       63    0.000    0.000    0.000    0.000 merge.py:799(<lambda>)\n",
       "       78    0.000    0.000    0.000    0.000 {method 'count' of 'str' objects}\n",
       "       42    0.000    0.000    0.000    0.000 function_base.py:258(iterable)\n",
       "      195    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
       "       84    0.000    0.000    0.000    0.000 common.py:273(maybe_make_list)\n",
       "       42    0.000    0.000    0.000    0.000 concat.py:483(<listcomp>)\n",
       "        3    0.000    0.000    0.027    0.009 frame.py:614(__unicode__)\n",
       "       21    0.000    0.000    0.000    0.000 generic.py:3115(_maybe_update_cacher)\n",
       "       51    0.000    0.000    0.000    0.000 format.py:298(__init__)\n",
       "       27    0.000    0.000    0.000    0.000 format.py:992(__init__)\n",
       "       78    0.000    0.000    0.000    0.000 blocks.py:327(<listcomp>)\n",
       "      318    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
       "       21    0.000    0.000    0.000    0.000 common.py:246(index_labels_to_array)\n",
       "       57    0.000    0.000    0.000    0.000 common.py:279(is_null_slice)\n",
       "       18    0.000    0.000    0.000    0.000 frame.py:3585(reindexer)\n",
       "       21    0.000    0.000    0.001    0.000 generic.py:1765(<listcomp>)\n",
       "      162    0.000    0.000    0.000    0.000 indexing.py:937(_convert_for_reindex)\n",
       "       21    0.000    0.000    0.000    0.000 merge.py:1011(_validate_specification)\n",
       "       24    0.000    0.000    0.000    0.000 function_base.py:4641(append)\n",
       "       21    0.000    0.000    0.007    0.000 _decorators.py:195(wrapper)\n",
       "      126    0.000    0.000    0.000    0.000 generic.py:1627(<listcomp>)\n",
       "        3    0.000    0.000    0.008    0.003 generic.py:2882(to_csv)\n",
       "       45    0.000    0.000    0.002    0.000 indexing.py:143(_get_loc)\n",
       "      177    0.000    0.000    0.000    0.000 concat.py:510(<genexpr>)\n",
       "       42    0.000    0.000    0.000    0.000 binaryTree.py:34(insertRight)\n",
       "       39    0.000    0.000    0.000    0.000 stack.py:8(__init__)\n",
       "       21    0.000    0.000    0.000    0.000 numeric.py:175(ones)\n",
       "        3    0.000    0.000    0.027    0.009 frame.py:678(to_string)\n",
       "        6    0.000    0.000    0.003    0.000 managers.py:684(get_slice)\n",
       "       90    0.000    0.000    0.000    0.000 format.py:535(<genexpr>)\n",
       "        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1404(_fill_cache)\n",
       "       78    0.000    0.000    0.000    0.000 shape_base.py:209(_arrays_for_stack_dispatcher)\n",
       "       24    0.000    0.000    0.000    0.000 apipkg.py:133(__makeattr)\n",
       "      183    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
       "       12    0.000    0.000    0.002    0.000 base.py:3830(_coerce_scalar_to_index)\n",
       "       21    0.000    0.000    0.000    0.000 blocks.py:2633(is_bool)\n",
       "       78    0.000    0.000    0.000    0.000 managers.py:2058(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 format.py:381(__init__)\n",
       "       21    0.000    0.000    0.000    0.000 generic.py:1775(<listcomp>)\n",
       "       78    0.000    0.000    0.000    0.000 blocks.py:3146(<listcomp>)\n",
       "       18    0.000    0.000    0.000    0.000 generic.py:277(_construct_axes_dict)\n",
       "        3    0.000    0.000    0.000    0.000 csvs.py:191(_save_header)\n",
       "      150    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
       "        6    0.000    0.000    0.000    0.000 base.py:1050(_format_native_types)\n",
       "      183    0.000    0.000    0.000    0.000 managers.py:376(<dictcomp>)\n",
       "       12    0.000    0.000    0.001    0.000 printing.py:36(<listcomp>)\n",
       "       21    0.000    0.000    0.002    0.000 base.py:4025(_concat_same_dtype)\n",
       "       21    0.000    0.000    0.000    0.000 concat.py:503(<listcomp>)\n",
       "       45    0.000    0.000    0.000    0.000 series.py:483(_formatting_values)\n",
       "       39    0.000    0.000    0.000    0.000 <ipython-input-19-f1005eda34b6>:16(Results)\n",
       "        9    0.000    0.000    0.000    0.000 parse.py:409(urlsplit)\n",
       "       18    0.000    0.000    0.001    0.000 algorithms.py:835(rank)\n",
       "        3    0.000    0.000    0.000    0.000 format.py:769(<listcomp>)\n",
       "       18    0.000    0.000    0.000    0.000 format.py:1138(_format_strings)\n",
       "      114    0.000    0.000    0.000    0.000 binaryTree.py:56(getRootVal)\n",
       "      114    0.000    0.000    0.000    0.000 timeout.py:56(stop)\n",
       "       21    0.000    0.000    0.000    0.000 {built-in method numpy.can_cast}\n",
       "       42    0.000    0.000    0.001    0.000 base.py:1580(is_monotonic)\n",
       "        3    0.000    0.000    0.001    0.000 format.py:801(_get_formatted_index)\n",
       "       39    0.000    0.000    0.000    0.000 format.py:943(<lambda>)\n",
       "      123    0.000    0.000    0.000    0.000 binaryTree.py:53(setRootVal)\n",
       "        9    0.000    0.000    0.000    0.000 parse.py:361(urlparse)\n",
       "      114    0.000    0.000    0.000    0.000 timeout.py:260(_start_new_or_dummy)\n",
       "       24    0.000    0.000    0.000    0.000 fromnumeric.py:1583(ravel)\n",
       "       21    0.000    0.000    0.000    0.000 generic.py:1778(<listcomp>)\n",
       "       60    0.000    0.000    0.000    0.000 range.py:165(_validate_dtype)\n",
       "        3    0.000    0.000    0.004    0.001 common.py:314(_get_handle)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method _csv.writer}\n",
       "       39    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.normalize_axis_index}\n",
       "       63    0.000    0.000    0.000    0.000 {method 'get_count' of 'pandas._libs.hashtable.Int64Factorizer' objects}\n",
       "        3    0.000    0.000    0.003    0.001 csvs.py:272(_save)\n",
       "       60    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}\n",
       "      228    0.000    0.000    0.000    0.000 numerictypes.py:655(<listcomp>)\n",
       "       60    0.000    0.000    0.000    0.000 common.py:175(_all_none)\n",
       "       42    0.000    0.000    0.000    0.000 {method 'add' of 'pandas._libs.internals.BlockPlacement' objects}\n",
       "       63    0.000    0.000    0.000    0.000 merge.py:800(<lambda>)\n",
       "      156    0.000    0.000    0.000    0.000 binaryTree.py:47(getRightChild)\n",
       "       48    0.000    0.000    0.000    0.000 _methods.py:26(_amax)\n",
       "       21    0.000    0.000    0.000    0.000 common.py:199(count_not_none)\n",
       "       45    0.000    0.000    0.000    0.000 generic.py:3205(_check_setitem_copy)\n",
       "       18    0.000    0.000    0.000    0.000 common.py:94(_expand_user)\n",
       "       42    0.000    0.000    0.000    0.000 concat.py:523(_maybe_check_integrity)\n",
       "       96    0.000    0.000    0.000    0.000 numerictypes.py:587(<listcomp>)\n",
       "       48    0.000    0.000    0.000    0.000 {method 'max' of 'numpy.ndarray' objects}\n",
       "       18    0.000    0.000    0.000    0.000 generic.py:279(<dictcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'writerow' of '_csv.writer' objects}\n",
       "       30    0.000    0.000    0.000    0.000 {method 'transpose' of 'numpy.ndarray' objects}\n",
       "        3    0.000    0.000    0.000    0.000 base.py:1010(<listcomp>)\n",
       "       21    0.000    0.000    0.000    0.000 base.py:4071(identical)\n",
       "       24    0.000    0.000    0.000    0.000 frame.py:3416(_ensure_valid_index)\n",
       "       45    0.000    0.000    0.000    0.000 blocks.py:171(formatting_values)\n",
       "       42    0.000    0.000    0.000    0.000 merge.py:1742(validate_operand)\n",
       "        4    0.000    0.000    0.000    0.000 <ipython-input-25-8ba185739980>:2(partly_unordered_permutations)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method now}\n",
       "       12    0.000    0.000    0.000    0.000 {built-in method numpy.bincount}\n",
       "        6    0.000    0.000    0.000    0.000 base.py:2897(_convert_slice_indexer)\n",
       "       21    0.000    0.000    0.000    0.000 concat.py:496(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 range.py:520(__getitem__)\n",
       "       27    0.000    0.000    0.000    0.000 format.py:1004(_value_formatter)\n",
       "        3    0.000    0.000    0.000    0.000 console.py:47(get_console_size)\n",
       "        6    0.000    0.000    0.001    0.000 base.py:1024(to_native_types)\n",
       "        6    0.000    0.000    0.003    0.001 indexing.py:2170(_get_slice_axis)\n",
       "       12    0.000    0.000    0.000    0.000 blocks.py:2011(should_store)\n",
       "        6    0.000    0.000    0.000    0.000 managers.py:736(as_array)\n",
       "        6    0.000    0.000    0.000    0.000 common.py:260(_infer_compression)\n",
       "        6    0.000    0.000    0.000    0.000 construction.py:509(sanitize_index)\n",
       "        3    0.000    0.000    0.000    0.000 {pandas._libs.algos.take_1d_object_object}\n",
       "       21    0.000    0.000    0.000    0.000 ops.py:66(_maybe_match_name)\n",
       "       18    0.000    0.000    0.000    0.000 base.py:658(__array__)\n",
       "       21    0.000    0.000    0.000    0.000 generic.py:334(<dictcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 format.py:648(<listcomp>)\n",
       "        3    0.000    0.000    0.001    0.000 generic.py:5457(dtypes)\n",
       "        6    0.000    0.000    0.000    0.000 indexing.py:264(_convert_slice_indexer)\n",
       "        1    0.000    0.000   38.092   38.092 <string>:1(<module>)\n",
       "        1    0.000    0.000   38.092   38.092 {built-in method builtins.exec}\n",
       "       12    0.000    0.000    0.000    0.000 posixpath.py:232(expanduser)\n",
       "        6    0.000    0.000    0.001    0.000 base.py:983(format)\n",
       "        3    0.000    0.000    0.000    0.000 common.py:179(get_filepath_or_buffer)\n",
       "       18    0.000    0.000    0.000    0.000 parse.py:109(_coerce_args)\n",
       "        3    0.000    0.000    0.000    0.000 console.py:149(in_ipython_frontend)\n",
       "       21    0.000    0.000    0.000    0.000 base.py:100(_reset_cache)\n",
       "        6    0.000    0.000    0.000    0.000 generic.py:5250(values)\n",
       "        3    0.000    0.000    0.000    0.000 format.py:431(_chk_truncate)\n",
       "       63    0.000    0.000    0.000    0.000 base.py:704(ndim)\n",
       "       21    0.000    0.000    0.000    0.000 base.py:3072(_can_reindex)\n",
       "       33    0.000    0.000    0.000    0.000 csvs.py:112(<genexpr>)\n",
       "       48    0.000    0.000    0.000    0.000 base.py:5398(<genexpr>)\n",
       "        3    0.000    0.000    0.000    0.000 managers.py:225(get_dtypes)\n",
       "       24    0.000    0.000    0.000    0.000 fromnumeric.py:2847(ndim)\n",
       "        6    0.000    0.000    0.000    0.000 _methods.py:48(_count_reduce_items)\n",
       "        6    0.000    0.000    0.003    0.000 indexing.py:148(_slice)\n",
       "        3    0.000    0.000    0.000    0.000 managers.py:226(<listcomp>)\n",
       "        6    0.000    0.000    0.000    0.000 managers.py:627(is_view)\n",
       "        3    0.000    0.000    0.000    0.000 common.py:76(_is_url)\n",
       "        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1433(<setcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method pandas._libs.lib.is_datetime_array}\n",
       "        6    0.000    0.000    0.000    0.000 generic.py:5337(get_values)\n",
       "        1    0.000    0.000    0.000    0.000 warnings.py:35(_formatwarnmsg_impl)\n",
       "       21    0.000    0.000    0.000    0.000 base.py:540(_constructor)\n",
       "       18    0.000    0.000    0.000    0.000 base.py:4698(_validate_indexer)\n",
       "        3    0.000    0.000    0.000    0.000 frame.py:606(_info_repr)\n",
       "        6    0.000    0.000    0.000    0.000 format.py:789(show_row_idx_names)\n",
       "        3    0.000    0.000    0.000    0.000 common.py:544(UnicodeWriter)\n",
       "        6    0.000    0.000    0.000    0.000 base.py:1681(is_object)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:5393(_trim_front)\n",
       "        6    0.000    0.000    0.000    0.000 range.py:248(dtype)\n",
       "        6    0.000    0.000    0.000    0.000 format.py:781(has_index_names)\n",
       "        6    0.000    0.000    0.000    0.000 {method 'mean' of 'numpy.ndarray' objects}\n",
       "        6    0.000    0.000    0.000    0.000 config.py:170(get_default_val)\n",
       "        3    0.000    0.000    0.000    0.000 console.py:90(in_interactive_session)\n",
       "        3    0.000    0.000    0.027    0.009 base.py:48(__str__)\n",
       "        6    0.000    0.000    0.000    0.000 generic.py:3110(_is_view)\n",
       "        6    0.000    0.000    0.000    0.000 blocks.py:136(is_view)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method _imp.is_builtin}\n",
       "        3    0.000    0.000    0.000    0.000 codecs.py:186(__init__)\n",
       "       18    0.000    0.000    0.000    0.000 {pandas._libs.algos.ensure_float64}\n",
       "       63    0.000    0.000    0.000    0.000 common.py:201(<genexpr>)\n",
       "       12    0.000    0.000    0.000    0.000 base.py:4683(_maybe_cast_indexer)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:5399(<listcomp>)\n",
       "       12    0.000    0.000    0.000    0.000 managers.py:1045(value_getitem)\n",
       "        3    0.000    0.000    0.000    0.000 format.py:785(has_column_names)\n",
       "        3    0.000    0.000    0.000    0.000 format.py:816(<listcomp>)\n",
       "       12    0.000    0.000    0.000    0.000 {method 'insert' of 'list' objects}\n",
       "       12    0.000    0.000    0.000    0.000 {built-in method posix.fspath}\n",
       "        3    0.000    0.000    0.000    0.000 base.py:1736(is_all_dates)\n",
       "        6    0.000    0.000    0.000    0.000 {built-in method pandas._libs.internals.slice_len}\n",
       "        3    0.000    0.000    0.000    0.000 common.py:171(is_gcs_url)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'find' of 'str' objects}\n",
       "        1    0.000    0.000    0.000    0.000 warnings.py:20(_showwarnmsg_impl)\n",
       "        1    0.000    0.000    0.000    0.000 warnings.py:419(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'timestamp' of 'datetime.datetime' objects}\n",
       "        3    0.000    0.000    0.000    0.000 generic.py:1818(__iter__)\n",
       "        6    0.000    0.000    0.000    0.000 indexing.py:2700(need_slice)\n",
       "        9    0.000    0.000    0.000    0.000 format.py:1434(_has_names)\n",
       "        3    0.000    0.000    0.000    0.000 common.py:163(is_s3_url)\n",
       "        1    0.000    0.000    0.000    0.000 warnings.py:96(_showwarnmsg)\n",
       "        1    0.000    0.000    0.000    0.000 <ipython-input-3-3890d44f8490>:8(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 linecache.py:15(getline)\n",
       "       18    0.000    0.000    0.000    0.000 parse.py:98(_noop)\n",
       "        9    0.000    0.000    0.000    0.000 <string>:1(__new__)\n",
       "       21    0.000    0.000    0.000    0.000 {method 'get_count' of 'pandas._libs.hashtable.Factorizer' objects}\n",
       "        6    0.000    0.000    0.000    0.000 config.py:578(_get_registered_option)\n",
       "        6    0.000    0.000    0.000    0.000 generic.py:1904(__array__)\n",
       "        3    0.000    0.000    0.000    0.000 format.py:351(should_show_dimensions)\n",
       "        1    0.000    0.000    0.000    0.000 warnings.py:117(_formatwarnmsg)\n",
       "        1    0.000    0.000    0.000    0.000 linecache.py:37(getlines)\n",
       "        3    0.000    0.000    0.000    0.000 parse.py:394(_checknetloc)\n",
       "        3    0.000    0.000    0.000    0.000 interactiveshell.py:698(get_ipython)\n",
       "        1    0.000    0.000    0.000    0.000 <ipython-input-3-3890d44f8490>:18(corpus_config)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        3    0.000    0.000    0.000    0.000 __init__.py:275(u)\n",
       "        3    0.000    0.000    0.000    0.000 format.py:795(show_col_idx_names)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%time\n",
    "def partly_unordered_permutations(lst, k):\n",
    "    elems = set(lst)\n",
    "    for c in combinations(lst, k):\n",
    "        for d in permutations(elems - set(c)):\n",
    "            yield c + d\n",
    "            \n",
    "def main():\n",
    "    \n",
    "    #rtype = int(input(\"Run: 1->Single systems; 2->Ensemble; 3->Tests; 4-> MM Test\"))\n",
    "   \n",
    "    '''\n",
    "        corpora: i2b2, mipacq, fv017\n",
    "        analyses: entity only (exact span), cui by document, full (aka (entity and cui on exaact span/exact cui)\n",
    "                  NB: add \"_test\" using mipacq to egnerate small test sample \n",
    "        systems: ctakes, biomedicus, clamp, metamap, quick_umls\n",
    "        \n",
    "        TODO -> Vectorization (entity only and full):\n",
    "                add switch for use of TN on single system performance evaluations \n",
    "                add switch for overlap matching versus exact span\n",
    "             -> Other tasks besides concept extraction\n",
    "             -> Use of https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html\n",
    "        \n",
    "    ''' \n",
    "    analysisConf =  AnalysisConfig()\n",
    "    print(analysisConf.systems, analysisConf.corpus_config())\n",
    "    \n",
    "    if (rtype == 1):\n",
    "        generate_metrics(analysis_type, corpus)\n",
    "    elif (rtype == 2):\n",
    "        \n",
    "        \n",
    "        #systems = ['ctakes','biomedicus','clamp','metamap','quick_umls']\n",
    "        systems = ['ctakes','biomedicus','clamp']\n",
    "        \n",
    "        #l = ['metamap', 'clamp', 'biomedicus']\n",
    "        #for corpus in corpora:\n",
    "        for l in partly_unordered_permutations(systems, 2):\n",
    "            print('corpus:', corpus, 'systems', l)\n",
    "            run_ensemble(l, analysis_type, corpus) \n",
    "            \n",
    "    elif (rtype == 3):\n",
    "        systems = ['biomedicus']\n",
    "        t = ['concept_jaccard_score_false']\n",
    "        test_systems(analysis_type, systems, corpus)  \n",
    "        test_count(analysis_type, corpus)\n",
    "        test_ensemble(analysis_type, corpus)\n",
    "    elif (rtype == 4):\n",
    "        generate_metrics_test(analysis_type, corpus)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    %prun main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
