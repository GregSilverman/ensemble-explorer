{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  Copyright (c) 2019 Regents of the University of Minnesota.\n",
    " \n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    " \n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gevent\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pymysql\n",
    "import time \n",
    "import functools as ft\n",
    "import glob   \n",
    "import operator as op\n",
    "import shelve\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from pathlib import Path\n",
    "from itertools import combinations, product, permutations\n",
    "from sqlalchemy.engine import create_engine\n",
    "from datetime import datetime\n",
    "from ast import literal_eval\n",
    "from scipy import stats  \n",
    "from scipy.stats.mstats import gmean\n",
    "from pythonds.basic.stack import Stack\n",
    "from pythonds.trees.binaryTree import BinaryTree\n",
    "from collections import defaultdict\n",
    "from typing import List, Set, Tuple "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The cell below contains the configurable parameters to ensure that our ensemble explorer runs properaly on your machine. Please read carfully through steps (1-7) before running the rest of the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-1: CHOOSE YOUR CORPUS\n",
    "corpus = 'fairview' #options include 'fairview', 'mipacq' OR 'i2b2'\n",
    "#corpora = ['fairview', 'mipacq', 'i2b2']\n",
    "#corpora = ['i2b2']\n",
    "# STEP-2: CHOOSE YOUR DATA DIRECTORY; this is where output data will be saved on your machine\n",
    "data_directory = '/Users/gms/development/nlp/nlpie/data/ensembling-u01/output/' \n",
    "\n",
    "# STEP-4: CHOOSE WHAT SYSTEM YOU'D LIKE TO RUN ON THE CORPUS\n",
    "rtype = 1      # OPTIONS INCLUDE: 1->Single systems; 2->Ensemble; 3->Tests; 4-> MM Test\n",
    "               # The Ensemble includes ['ctakes','biomedicus','clamp','metamap','quick_umls']\n",
    "    \n",
    "# STEP-5: CHOOSE WHAT TYPE OF ANALYSIS YOU'D LIKE TO RUN ON THE CORPUS\n",
    "analysis_type = 'entity' #options include 'entity' OR 'full'\n",
    "\n",
    "# STEP-(6A): ENTER DETAILS FOR ACCESSING MANUAL ANNOTATION DATA\n",
    "database_type = 'mysql+pymysql' # We use mysql+pymql as default\n",
    "database_username = 'gms'\n",
    "database_password = 'nej123' \n",
    "database_url = 'localhost' # HINT: use localhost if you're running database on your local machine\n",
    "database_name = 'concepts' # Enter database name\n",
    "table_name = corpus + '_all' # Enter the table within the database where your reference data is store\n",
    "\n",
    "# STEP-(6B): ENTER DIRECTORY FOR ACCESSING SYSTEM ANNOTATION DATA\n",
    "# TODO: allow for list of corpora\n",
    "system_annotation = 'analytical_'+corpus+'.csv' # OPTIONS include 'analytical_cui_mipacq_concepts.csv' OR 'analytical_cui_i2b2_concepts.csv' \n",
    "\n",
    "# STEP-7: WE'LL CREATE AN 'SYSTEM OUTPUT'DIRECTORY FOR YOU INSIDE THE DIRECTORY YOU SPECIFIED IN (STEP 2)\n",
    "single_sys_dir = Path(data_directory + \"single_system_out\")\n",
    "single_sys_dir.mkdir(parents=True, exist_ok=True)\n",
    "dir_out = Path(data_directory + 'single_system_out/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config class for analysis\n",
    "class AnalysisConfig(object):\n",
    "    \"\"\"\n",
    "    Configuration object:\n",
    "    notes by test, full per corpus\n",
    "    paths by output, gold and system location\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self = self    \n",
    "       \n",
    "        self.systems = ['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls']\n",
    "        #self.systems = ['biomedicus', 'clamp', 'ctakes', 'metamap']\n",
    "        #self.systems = ['metamap']\n",
    "        #self.systems = ['biomedicus']\n",
    "        #self.systems = ['quick_umls']\n",
    "        self.data_dir = data_directory\n",
    "    \n",
    "    def corpus_config(self): \n",
    "        usys_data = system_annotation\n",
    "        ref_data = database_name+'.'+table_name\n",
    "        return usys_data, ref_data\n",
    "        \n",
    "\n",
    "analysisConf =  AnalysisConfig()\n",
    "usys, ref = analysisConf.corpus_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation class for UIMA systems\n",
    "class AnnotationSystems(object):\n",
    "    \"\"\"   \n",
    "    CAS XMI Annotations of interest\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\" \n",
    "        annotation base types\n",
    "        \"\"\"   \n",
    "        \n",
    "        self.biomedicus_types = [\"biomedicus.v2.UmlsConcept\"]\n",
    "                                 #\"biomedicus.v2.Negated\"\n",
    "                                 #\"biomedicus.v2.Acronym\",\n",
    "                                 #\"biomedicus.v2.DictionaryTerm\",\n",
    "                                 #\"biomedicus.v2.Historical\"]\n",
    "        \n",
    "        self.clamp_types = [\"edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA\"]\n",
    "                            #\"org.apache.ctakes.typesystem.type.syntax.ConllDependencyNode\",\n",
    "                            #\"edu.uth.clamp.nlp.typesystem.ClampRelationUIMA\"]    \n",
    "        \n",
    "        self.ctakes_types = ['ctakes_mentions']#\"org.apache.ctakes.typesystem.type.textspan.Sentence\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.DiseaseDisorderMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.MedicationMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.ProcedureMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.refsem.UmlsConcept\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.SignSymptomMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.AnatomicalSiteMention\"]\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.MeasurementAnnotation\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.EventMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.EntityMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.Predicate\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.syntax.WordToken\"]\n",
    "        \n",
    "        self.metamap_types = [#\"org.metamap.uima.ts.Utterance\",\n",
    "                              #\"org.metamap.uima.ts.Span\",\n",
    "                              #\"org.metamap.uima.ts.Phrase\"]\n",
    "                              \"org.metamap.uima.ts.Candidate\"]\n",
    "                              #\"org.metamap.uima.ts.CuiConcept\",\n",
    "                              #\"org.metamap.uima.ts.Negation\"]\n",
    "                \n",
    "        self.quick_umls_types = [#'concept']#,\n",
    "                                #'concept_cosine_length_false',\n",
    "                                #'concept_cosine_length_true',\n",
    "                                #'concept_cosine_score_false',\n",
    "                                #'concept_cosine_score_true',\n",
    "                                #'concept_dice_length_false',\n",
    "                                #'concept_dice_length_true',\n",
    "                                #'concept_dice_score_false',\n",
    "                                #'concept_dice_score_true',\n",
    "                                #'concept_jaccard_length_false',\n",
    "                                #'concept_jaccard_length_true',\n",
    "                                #'concept']\n",
    "                                 'concept_jaccard_score_False']\n",
    "                                #'concept_jaccard_score_true']\n",
    "                \n",
    "        '''\n",
    "\n",
    "        self.biomedicus_dir = \"biomedicus_out/\"\n",
    "        self.biomedicus_types = [#\"biomedicus.v2.UmlsConcept\"]\n",
    "                                  #\"biomedicus.v2.Negated\"\n",
    "                                 \"biomedicus.v2.Acronym\",\n",
    "                                 \"biomedicus.v2.DictionaryTerm\",\n",
    "                                 \"biomedicus.v2.Historical\"]\n",
    "        \n",
    "        \n",
    "        self.clamp_dir = \"clamp_out/\"\n",
    "        #self.clamp_types = [#\"edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA\"]\n",
    "                             #\"org.apache.ctakes.typesystem.type.syntax.ConllDependencyNode\",\n",
    "                             #\"edu.uth.clamp.nlp.typesystem.ClampRelationUIMA\"]\n",
    "        \n",
    "        \n",
    "        self.ctakes_dir = \"ctakes_out/\"\n",
    "        self.ctakes_types = [\"org.apache.ctakes.typesystem.type.textspan.Sentence\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.DiseaseDisorderMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.MedicationMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.ProcedureMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.refsem.UmlsConcept\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.SignSymptomMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.AnatomicalSiteMention\"]\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.MeasurementAnnotation\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.EventMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.EntityMention\",\n",
    "                             \"org.apache.ctakes.typesystem.type.textsem.Predicate\",\n",
    "                             \"org.apache.ctakes.typesystem.type.syntax.WordToken\"]\n",
    "        \n",
    "        self.metamap_dir = \"metamap_out/\"\n",
    "        self.metamap_types = [\"org.metamap.uima.ts.Utterance\",\n",
    "                              \"org.metamap.uima.ts.Span\",\n",
    "                              \"org.metamap.uima.ts.Phrase\"]\n",
    "                              #\"org.metamap.uima.ts.Candidate\"]\n",
    "                              #\"org.metamap.uima.ts.CuiConcept\",\n",
    "                              #\"org.metamap.uima.ts.Negation\"]\n",
    "                              \n",
    "        '''\n",
    "       \n",
    "    def get_system_type(self, system):\n",
    "        \n",
    "        \"\"\"\n",
    "        return system types\n",
    "        \"\"\"\n",
    "        \n",
    "        if system == \"biomedicus\":\n",
    "            view = \"Analysis\"\n",
    "        else:\n",
    "            view = \"_InitialView\"\n",
    "\n",
    "        if system == 'biomedicus':\n",
    "            types = self.biomedicus_types\n",
    "\n",
    "        elif system == 'clamp':\n",
    "            types = self.clamp_types\n",
    "\n",
    "        elif system == 'ctakes':\n",
    "            types = self.ctakes_types\n",
    "\n",
    "        elif system == 'metamap':\n",
    "            types = self.metamap_types\n",
    "        \n",
    "        elif system == \"quick_umls\":\n",
    "            types = self.quick_umls_types\n",
    "            \n",
    "        return types, view\n",
    "    \n",
    "annSys = AnnotationSystems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def get_notes(analysis_type: str, corpus: str) -> List[str]:\n",
    "    \n",
    "    if 'test' in analysis_type:\n",
    "        # test set of notes\n",
    "        if corpus == 'mipacq':\n",
    "            notes = ['522412787',\n",
    "             '617637585',\n",
    "             '3307880735-8',\n",
    "             '9080688558',\n",
    "             '618370565',\n",
    "             '573718188',\n",
    "             '534584',\n",
    "             '60891',\n",
    "             '62620',\n",
    "             '616172834']\n",
    "            \n",
    "        elif corpus == 'i2b2':\n",
    "            print('TODO')\n",
    "        \n",
    "        print('TEST NOTES!')\n",
    "        #,\n",
    "#          '4130154172-4',\n",
    "#          '3580478614',\n",
    "#          '5024581165-5',\n",
    "#          '4486835700-9',\n",
    "#          '534828617',\n",
    "#          '8154986253',\n",
    "#          '533855209',\n",
    "#          '60118',\n",
    "#          '3537704982-3',\n",
    "#          '617637585',\n",
    "#          '60118',\n",
    "#          '9045889026',\n",
    "#          '8819868493-8',\n",
    "#          '533698',\n",
    "#          '535978760']\n",
    "     \n",
    "    else:\n",
    "        \n",
    "        if corpus == 'mipacq':\n",
    "        # these did not meet the minimal criteria for parsing\n",
    "            notes = [\"0595040941-0\",\n",
    "                    \"0778429553-0\",\n",
    "                    \"1014681675\",\n",
    "                    \"2889522952-2\",\n",
    "                    \"3080383448-5\",\n",
    "                    \"3300000926-3\",\n",
    "                    \"3360037185-3\",\n",
    "                    \"3580973392\",\n",
    "                    \"3627629462-3\",\n",
    "                    \"4323116051-4\",\n",
    "                    \"477704053-4\",\n",
    "                    \"528317073\",\n",
    "                    \"531702602\",\n",
    "                    \"534061073\",\n",
    "                    \"54832076\",\n",
    "                    \"5643725437-6\",\n",
    "                    \"5944412090-5\",\n",
    "                    \"6613169476-6\",\n",
    "                    \"7261075903-7\",\n",
    "                    \"7504944368-7\",\n",
    "                    \"7999462393-7\",\n",
    "                    \"8131081430\",\n",
    "                    \"8171084310\",\n",
    "                    \"8193787896\",\n",
    "                    \"8295055184-8\",\n",
    "                    \"8823185307-8\"]\n",
    "            \n",
    "        elif corpus == 'i2b2':\n",
    "            # these notes were not processed \n",
    "            notes = ['0081', \n",
    "                     '0401']\n",
    "\n",
    "        else:\n",
    "            notes = None\n",
    "            \n",
    "    return notes# training_notes\n",
    "print(get_notes('entity', corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np # access to Numpy from Python layer\n",
    "import math\n",
    "\n",
    "class Metrics(object):\n",
    "    \"\"\"\n",
    "    metrics class:\n",
    "    returns an instance with confusion matrix metrics\n",
    "    \"\"\"\n",
    "    def __init__(self, system_only, gold_only, gold_system_match, system_n, neither = 0): # neither: no sys or manual annotation\n",
    "\n",
    "        self = self    \n",
    "        self.system_only = system_only\n",
    "        self.gold_only = gold_only\n",
    "        self.gold_system_match = gold_system_match\n",
    "        self.system_n = system_n\n",
    "        self.neither = neither\n",
    "        \n",
    "    def get_confusion_metrics(self, corpus = None, test = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        compute confusion matrix measures, as per  \n",
    "        https://stats.stackexchange.com/questions/51296/how-do-you-calculate-precision-and-recall-for-multiclass-classification-using-co\n",
    "        \"\"\"\n",
    "        cdef:\n",
    "            int TP, FP, FN\n",
    "            double TM\n",
    "\n",
    "        TP = self.gold_system_match\n",
    "        FP = self.system_only\n",
    "        FN = self.gold_only\n",
    "        \n",
    "        TM = TP/math.sqrt(self.system_n) # TigMetric\n",
    "       \n",
    "        if not test:\n",
    "            \n",
    "            if corpus == 'casi':\n",
    "                recall = TP/(TP + FN)\n",
    "                precision = TP/(TP + FP)\n",
    "                F = 2*(precision*recall)/(precision + recall)\n",
    "            else:\n",
    "                if self.neither == 0:\n",
    "                    confusion = [[0, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "                else:\n",
    "                    confusion = [[self.neither, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "                c = np.asarray(confusion)\n",
    "                recall = np.diag(c) / np.sum(c, axis = 1)\n",
    "                precision = np.diag(c) / np.sum(c, axis = 0)\n",
    "                F = 2*(precision*recall)/(precision + recall)\n",
    "        else:\n",
    "            precision = TP/(TP+FP)\n",
    "            recall = TP/(TP+FN)\n",
    "            F = 2*(precision*recall)/(precision + recall)\n",
    "        \n",
    "        # Tignanelli Metric\n",
    "        if FN == 0:\n",
    "            TP_FN_R = TP\n",
    "        elif FN > 0:\n",
    "            TP_FN_R = TP/FN\n",
    " \n",
    "        return F, recall, precision, TP, FP, FN, TP_FN_R, TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out(name: str, analysis_type: str, c: object):\n",
    "   \n",
    "    \"\"\"\n",
    "    write matching and reference-only sets to file for ease in merging combinations\n",
    "    \"\"\"\n",
    "    \n",
    "    # write output to file\n",
    "    dir_out = analysisConf.data_dir + 'single_system_out/'\n",
    "    with open(dir_out + name + '_' + analysis_type + '_' + c.corpus + '_matches.txt', 'w') as f:\n",
    "        for item in list(c.matches):\n",
    "            f.write(\"%s\\n\" % str(item))\n",
    "\n",
    "    # write to file\n",
    "    with open(dir_out + name + '_' + analysis_type + '_' + c.corpus + '_ref_only.txt', 'w') as f:\n",
    "        for item in list(c.false_negatives):\n",
    "            f.write(\"%s\\n\" % str(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%cython \n",
    "\n",
    "#from __main__ import write_out\n",
    "\n",
    "#import numpy as np # access to Numpy from Python layer\n",
    "def label_vector(doc: str, ann: List[int], labels: List[str]) -> np.array:\n",
    "\n",
    "    #print(ann, doc, labels)\n",
    "\n",
    "    v = np.zeros(doc)\n",
    "    labels = list(labels)\n",
    "    \n",
    "    for (i, lab) in enumerate(labels):\n",
    "        i += 1  # 0 is reserved for no label\n",
    "        idxs = [np.arange(a.begin, a.end) for a in ann if a.label == lab]\n",
    "            \n",
    "        idxs = [j for mask in idxs for j in mask]\n",
    "        v[idxs] = i\n",
    "\n",
    "    return v\n",
    "\n",
    "# test confusion matrix elements for vectorized annotation set; includes TN\n",
    "def confused(sys1, ann1):\n",
    "    TP = np.sum(np.logical_and(sys1 >= 1, ann1 == sys1 ))\n",
    "\n",
    "    # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "    TN = np.sum(np.logical_and(sys1 == 0, ann1 == 0))\n",
    "\n",
    "    # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "    FP = np.sum(np.logical_and(sys1 >= 1, ann1 == 0))\n",
    "\n",
    "    # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "    FN = np.sum(np.logical_and(sys1 == 0, ann1 >= 1))\n",
    "    \n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "\n",
    "def get_cooccurences(ref, sys, analysis_type: str, corpus: str, single_sys = True, name = None):\n",
    "    \"\"\"\n",
    "    get coocurences between system and reference; exact match; TODO: add relaxed\n",
    "    \"\"\"\n",
    "    # test cooccurences\n",
    "    class Coocurences(object):\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.ref_system_match = 0\n",
    "            self.ref_only = 0\n",
    "            self.system_only = 0\n",
    "            self.system_n = 0\n",
    "            self.ref_n = 0\n",
    "            self.matches = set()\n",
    "            self.false_negatives = set()\n",
    "            self.corpus = corpus\n",
    "            self.cases = set(ref[\"file\"].tolist()) # cases to label \n",
    "\n",
    "    c = Coocurences()\n",
    "    \n",
    "    # test for converting to vectorization and i-o labeling\n",
    "    def test_io():\n",
    "        test = c.cases\n",
    "        if analysis_type == 'entity':\n",
    "            docs = [(x, len(open(\"/Users/gms/development/nlp/nlpie/data/ensembling-u01/i2b2/source_data/test_data/\" + x + \".txt\", 'r').read())) for x in test]\n",
    "\n",
    "        ann = ref.copy()\n",
    "        ann = ann.rename(index=str, columns={\"start\": \"begin\", \"file\": \"case\"}).copy()\n",
    "        cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "        if analysis_type == 'entity':\n",
    "            labels = [\"concept\"]\n",
    "            ann[\"label\"] = 'concept'\n",
    "            ann = ann[cols_to_keep].copy()\n",
    "\n",
    "        sys_ = sys.rename(index=str, columns={\"note_id\": \"case\"}).copy()\n",
    "        \n",
    "        # need for enttity-only\n",
    "        if analysis_type == 'entity':\n",
    "            sys_[\"label\"] = 'concept'\n",
    "        \n",
    "        sys_ = sys_[cols_to_keep]\n",
    "       \n",
    "        tp = []\n",
    "        tn = []\n",
    "        fp = []\n",
    "        fn = []\n",
    "        cvals = []\n",
    "        out = []\n",
    "        t = []\n",
    "        d = defaultdict(list)\n",
    "        \n",
    "        for n in range(len(docs)):\n",
    "            a1 = [i for i in ann[ann[\"case\"] == docs[n][0]].copy().itertuples(index=False)]\n",
    "            s1 = [i for i in sys_[sys_[\"case\"] == docs[n][0]].copy().itertuples(index=False)]\n",
    "\n",
    "            ann1 = label_vector(docs[n][1], a1, labels)\n",
    "            sys1 = label_vector(docs[n][1], s1, labels)\n",
    "            \n",
    "            TP, TN, FP, FN = confused(sys1, ann1)\n",
    "            cvals.append([TP, TN, FP, FN])\n",
    "                 \n",
    "            d['sys'].append(list([int(i) for i in sys1]))\n",
    "            d['oracle'].append(list([int(i) for i in ann1]))\n",
    "            d['case'].append(docs[n][0])\n",
    "            \n",
    "            '''\n",
    "            print(\"tn:\", np.intersect1d(np.where(ann1 == 0)[0], np.where(sys1 == 0)[0]),  \n",
    "                  \"tp:\", np.intersect1d(np.where(ann1 == 1)[0], np.where(sys1 == 1)[0]), \n",
    "                  \"fn:\", np.intersect1d(np.where(ann1 == 1)[0], np.where(sys1 == 0)[0]), \n",
    "                  \"fp:\", np.intersect1d(np.where(ann1 == 0)[0], np.where(sys1 == 1)[0]))\n",
    "            '''\n",
    "        d['labels'] = labels\n",
    "        \n",
    "        corp = shelve.open('/Users/gms/Desktop/' + sys.name + '_' + corpus + '.dat')\n",
    "        \n",
    "        for k in d:\n",
    "            corp[k] = d[k]\n",
    "        \n",
    "        corp.close()\n",
    "       \n",
    "        return cvals\n",
    "    \n",
    "    if corpus == 'i2b2':\n",
    "        TP, TN, FP, FN = np.sum(test_io(), axis=0)\n",
    "        F, recall, precision, TP, FP, FN, TP_FN_R, TM = Metrics(FP, FN, TP, len(sys), TN).get_confusion_metrics() #no TN\n",
    "        print('test_io():', TP, TN, FP, FN, np.mean(F), np.mean(recall), np.mean(precision))\n",
    "    \n",
    "    # non-vectorized:\n",
    "    \n",
    "    if corpus != 'casi':\n",
    "        if 'entity' in analysis_type and single_sys: # mipacq n -> 16793\n",
    "            cols_to_keep = ['begin', 'end', 'note_id']\n",
    "            sys = sys[cols_to_keep].drop_duplicates()\n",
    "            ref = ref[['start', 'end', 'file']].drop_duplicates()\n",
    "            sys.name = name\n",
    "        elif 'cui' in analysis_type and single_sys: # mipacq n -> 10799\n",
    "            cols_to_keep = ['cui', 'note_id']\n",
    "            sys = sys[cols_to_keep].drop_duplicates()\n",
    "            # do not overestimate FP\n",
    "            sys = sys[~sys['cui'].isnull()] \n",
    "            ref = ref[['value', 'file']].drop_duplicates()\n",
    "            ref = ref[~ref['value'].isnull()]\n",
    "            sys.name = name\n",
    "        elif 'full' in analysis_type and single_sys: # mipacq n -> 17393\n",
    "            cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "            sys = sys[cols_to_keep].drop_duplicates()\n",
    "            sys = sys[~sys['cui'].isnull()]\n",
    "            ref = ref[['start', 'end', 'value', 'file']].drop_duplicates()\n",
    "            ref = ref[~ref['value'].isnull()]\n",
    "            sys.name = name\n",
    "\n",
    "        # matches via inner join\n",
    "        matches = pd.merge(sys, ref, how = 'inner', left_on=['begin','end','note_id'], right_on = ['start','end','file']) \n",
    "        # reference-only via left outer join\n",
    "        fn = pd.merge(ref, sys, how = 'left', left_on=['start','end','file'], right_on = ['begin','end','note_id']) \n",
    "\n",
    "        fn = fn[fn['begin'].isnull()] # get as outer join with no match\n",
    "\n",
    "        if 'entity' in analysis_type and single_sys:\n",
    "            cols_to_keep = ['start', 'end', 'file']\n",
    "        else:\n",
    "            cols_to_keep = ['start', 'end', 'value', 'file']\n",
    "\n",
    "        matches = matches[cols_to_keep]\n",
    "        fn = fn[cols_to_keep]\n",
    "\n",
    "        # use for metrics \n",
    "        c.matches = c.matches.union(df_to_set(matches, analysis_type, 'ref'))\n",
    "        c.false_negatives = c.false_negatives.union(df_to_set(fn, analysis_type, 'ref'))\n",
    "        c.ref_system_match = len(c.matches)\n",
    "        c.system_only = len(sys) - len(c.matches)\n",
    "        c.system_n = len(sys)\n",
    "        c.ref_n = len(ref)\n",
    "        c.ref_only = len(c.false_negatives)\n",
    "        \n",
    "    else:\n",
    "        #matches = df_to_set(pd.read_sql(\"select `case` from test.amia_2019_analytical_v where overlap = 1;\", con=engine), 'entity', 'sys', 'casi')\n",
    "        \n",
    "        sql = \"select `case` from test.amia_2019_analytical_v where overlap = 1 and `system` = %(sys.name)s\"  \n",
    "        #ref_ann = pd.read_sql(sql, params={\"training_notes\":training_notes}, con=engine)\n",
    "        \n",
    "        matches = pd.read_sql(sql, params={\"sys.name\":sys.name}, con=engine)\n",
    "        \n",
    "        sql = \"select `case` from test.amia_2019_analytical_v where (overlap = 0 or overlap is null) and `system` = %(sys.name)s\"  \n",
    "        #ref_ann = pd.read_sql(sql, params={\"training_notes\":training_notes}, con=engine)\n",
    "        \n",
    "        fn = pd.read_sql(sql, params={\"sys.name\":sys.name}, con=engine)\n",
    "        \n",
    "        c.matches = df_to_set(matches, 'entity', 'sys', 'casi')\n",
    "        c.fn = df_to_set(fn, 'entity', 'sys', 'casi')\n",
    "        c.ref_system_match = len(c.matches)\n",
    "        c.system_only = len(sys) - len(c.matches)\n",
    "        c.system_n = len(matches) + len(fn)\n",
    "        c.ref_n = len(matches) + len(fn)\n",
    "        c.ref_only = len(fn)\n",
    "        \n",
    "        print('cooc', c.ref_system_match, c.system_only, c.ref_n, c.ref_n, c.ref_only)\n",
    "        \n",
    "    # sanity check\n",
    "    if len(ref) - c.ref_system_match < 0:\n",
    "        print('Error: ref_system_match > len(ref)!')\n",
    "    if len(ref) != c.ref_system_match + c.ref_only:\n",
    "        print('Error: ref count mismatch!')\n",
    "   \n",
    "    # save TP/FN\n",
    "    if single_sys and corpus != 'casi':\n",
    "        print(analysis_type)\n",
    "        write_out(sys.name, analysis_type, c)\n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging test for i-o labeled data\n",
    "import numpy as np\n",
    "import shelve\n",
    "# load shelve\n",
    "def read_shelve():\n",
    "    corp = shelve.open('/Users/gms/Desktop/test.dat')\n",
    "\n",
    "    return corp\n",
    "        \n",
    "test = read_shelve()\n",
    "\n",
    "#l0 = list(t0)\n",
    "#l1 = list(t1)\n",
    "\n",
    "def test_merge_vector(test):\n",
    "    # get sample for testing\n",
    "    for case in test['case'][3:5]:\n",
    "        for i in range(len(test['case'][3:5])):\n",
    "            if i == 3:\n",
    "                t0 = test['oracle'][3][0:750]\n",
    "            else:\n",
    "                t1 = test['oracle'][4][0:750]\n",
    "\n",
    "            #print('case:', case, test['sys'][i], test['oracle'][i], confused(np.array(test['sys'][i]), np.array(test['oracle'][i])))\n",
    "        #print(t0, t1)\n",
    "\n",
    "    t0 = np.array(test['oracle'][3][0:750])\n",
    "    t1 = np.array(test['oracle'][5][0:750])\n",
    "\n",
    "    l0 = list(t0)\n",
    "    l1 = list(t1)\n",
    "    \n",
    "    l0 = [0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0] \n",
    "    l1 = [0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0]\n",
    "    \n",
    "    print(l0, l1)\n",
    "\n",
    "    def intersection(lst1, lst2): \n",
    "        out = list()\n",
    "        if isinstance(lst1, set) and isinstance(lst2, set):\n",
    "            out = (set(lst1) & set(lst2))\n",
    "        elif isinstance(lst1, set) and isinstance(lst2, np.int64):\n",
    "            out = (set(lst1) & set([lst2]))\n",
    "        elif isinstance(lst1, np.int64) and isinstance(lst2, set):\n",
    "            out = (set([lst1]) & set(lst2))\n",
    "        elif isinstance(lst1, np.int64) and isinstance(lst2, np.int64):\n",
    "            out = (set([lst1]) & set([lst2]))\n",
    "        #if len(out) > 1:\n",
    "        return out\n",
    "        #elif len(out) == 1:\n",
    "        #    return out[0]\n",
    "        #else:\n",
    "        #    return 0\n",
    "\n",
    "    def union(lst1, lst2): \n",
    "        out = list()\n",
    "        if isinstance(lst1, set) and isinstance(lst2, set):\n",
    "            out = set(lst1) | set(lst2)\n",
    "        elif isinstance(lst1, set) and isinstance(lst2, np.int64):\n",
    "            out = set(lst1) | set([lst2])\n",
    "        elif isinstance(lst1, np.int64) and isinstance(lst2, set):\n",
    "            out = set([lst1]) | set(lst2)\n",
    "        elif isinstance(lst1, np.int64) and isinstance(lst2, np.int64):\n",
    "            out = set([lst1]) | set([lst2])\n",
    "        #if len(out) == 1:\n",
    "        #    #out = out[0]\n",
    "        return out\n",
    "\n",
    "    # union and intersect\n",
    "    def umerges(l0, l1):\n",
    "        #un = [0]*len(l0)\n",
    "        #for i in range(len(l0)):\n",
    "        #    un[i] = union(l0[i], l1[i])\n",
    "\n",
    "        return [union(l0[i], l1[i]) for i in range(len(l0))]\n",
    "\n",
    "    %timeit un = umerges(l0, l1)\n",
    "    \n",
    "    x = umerges(l0, l1)\n",
    "    \n",
    "\n",
    "    #l2 = [1, {1, 4}, {3}, {2, 4}, {1}, 0, 2, 3, {0, 8}, {1, 8}]\n",
    "    \n",
    "    #print(umerges(x, l2))\n",
    "    \n",
    "    def imerges(l0, l1):\n",
    "        #inter = [0]*len(l0)\n",
    "        #for i in range(len(l0)):\n",
    "        \n",
    "\n",
    "        return [intersection(l0[i], l1[i]) for i in range(len(l0))]\n",
    "    \n",
    "    %timeit x = imerges(l0, l1)\n",
    "    \n",
    "    '''\n",
    "    union = [\n",
    "        ( [set(x) | set(y)] if isinstance(x, list) and isinstance(y, list)\n",
    "          else [set(x) | set([y])] if isinstance(x, list) and isinstance(y, int)\n",
    "          else [set([x]) | set(y)] if isinstance(x, int) and isinstance(y, list)\n",
    "          else [set([x]) | set([y])])\n",
    "\n",
    "         for x, y in zip(l0, l1)\n",
    "    ]\n",
    "\n",
    "    # unpack map object\n",
    "    #*y, = list(map(list, zip(*union)))\n",
    "    #%timeit list(map(list, zip(*union)))\n",
    "\n",
    "    intersection = [\n",
    "        ( [set(x) & set(y)] if isinstance(x, list) and isinstance(y, list)\n",
    "          else [set(x) & set([y])] if isinstance(x, list) and isinstance(y, int)\n",
    "          else [set([x]) & set(y)] if isinstance(x, int) and isinstance(y, list)\n",
    "          else [set([x]) & set([y])])\n",
    "          for x, y in zip(l0, l1)\n",
    "\n",
    "    ]\n",
    "\n",
    "    #*x, = list(map(list, zip(*intersection)))\n",
    "    #%timeit list(map(list, zip(*intersection)))\n",
    "    '''\n",
    "#test_merge_vector(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blah -=\n",
    "# https://kawahara.ca/how-to-compute-truefalse-positives-and-truefalse-negatives-in-python-for-binary-classification-problems/\n",
    "def confusing(sys1, ann1):\n",
    "    TP = np.sum(np.logical_and(sys1 == 1, ann1 == sys1))\n",
    "\n",
    "    # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "    TN = np.sum(np.logical_and(sys1 == 0, ann1 == 0))\n",
    "\n",
    "    # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "    FP = np.sum(np.logical_and(sys1 == 1, ann1 == 0))\n",
    "\n",
    "    # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "    FN = np.sum(np.logical_and(np.logical_or(sys1 == 0, sys1 is None), ann1 == 1))\n",
    "    \n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "#%%cython\n",
    "#import numpy as np # access to Numpy from Python layer\n",
    "#import time\n",
    "#from __main__ import read_shelve\n",
    "def imerge(l0, l1):\n",
    "\n",
    "    return [\n",
    "        ( [\n",
    "            set(x) & set(y)] if isinstance(x, list) and  isinstance(y, list)\n",
    "            else [set(x) & set([y])] if isinstance(x, list) and  isinstance(y, np.int64)\n",
    "            else [set(x) & y] if isinstance(x, list) and isinstance(y, set)\n",
    "            else [x & y] if isinstance(x, set) and isinstance(y, set)\n",
    "            else [x & set(y)] if isinstance(x, set) and isinstance(y, list)\n",
    "            else [x & set([y])] if isinstance(x, set) and isinstance(y, np.int64)\n",
    "            else [set([x]) & set(y)] if isinstance(x, np.int64) and  isinstance(y, list)\n",
    "            else [set([x]) & y] if isinstance(x, np.int64) and isinstance(y, set)\n",
    "            else [set([x]) & set([y])])\n",
    "        for x, y in zip(l0, l1)\n",
    "    ]\n",
    "\n",
    "def umerge(l0, l1):\n",
    "\n",
    "    return [\n",
    "        ( [\n",
    "            set(x) | set(y)] if isinstance(x, list) and  isinstance(y, list)\n",
    "            else [set(x) | set([y])] if isinstance(x, list) and  isinstance(y, np.int64)\n",
    "            else [set(x) | y] if isinstance(x, list) and isinstance(y, set)\n",
    "            else [x | y] if isinstance(x, set) and isinstance(y, set)\n",
    "            else [x | set(y)] if isinstance(x, set) and isinstance(y, list)\n",
    "            else [x | set([y])] if isinstance(x, set) and isinstance(y, np.int64)\n",
    "            else [set([x]) | y] if isinstance(x, np.int64) and isinstance(y, set)\n",
    "            else [set([x]) | set(y)] if isinstance(x, np.int64) and  isinstance(y, list)\n",
    "            else [set([x]) | set([y])])\n",
    "        for x, y in zip(l0, l1)\n",
    "    ]\n",
    "\n",
    "def imerge_int(l0, l1):\n",
    "    return [\n",
    "        ( [\n",
    "            set(x) & set(y)] if isinstance(x, list) and  isinstance(y, list)\n",
    "            else [set(x) & set([y])] if isinstance(x, list) and isinstance(y, int)\n",
    "            else [set(x) & y] if isinstance(x, list) and isinstance(y, set)\n",
    "            else [x & y] if isinstance(x, set) and isinstance(y, set)\n",
    "            else [x & set(y)] if isinstance(x, set) and isinstance(y, list)\n",
    "            else [x & set([y])] if isinstance(x, set) and isinstance(y, int)\n",
    "            else [set([x]) & set(y)] if isinstance(x, int) and  isinstance(y, list)\n",
    "            else [set([x]) & y] if isinstance(x, int) and isinstance(y, set)\n",
    "            else [set([x]) & set([y])])\n",
    "        for x, y in zip(l0, l1)\n",
    "    ]\n",
    "\n",
    "def umerge_int(l0, l1):\n",
    "    return [\n",
    "        ( [\n",
    "            set(x) | set(y)] if isinstance(x, list) and  isinstance(y, list)\n",
    "            else [set(x) | set([y])] if isinstance(x, list) and  isinstance(y, int)\n",
    "            else [set(x) | y] if isinstance(x, list) and isinstance(y, set)\n",
    "            else [x | y] if isinstance(x, set) and isinstance(y, set)\n",
    "            else [x | set(y)] if isinstance(x, set) and isinstance(y, list)\n",
    "            else [x | set([y])] if isinstance(x, set) and isinstance(y, int)\n",
    "            else [set([x]) | y] if isinstance(x, int) and isinstance(y, set)\n",
    "            else [set([x]) | set(y)] if isinstance(x, int) and  isinstance(y, list)\n",
    "            else [set([x]) | set([y])])\n",
    "        for x, y in zip(l0, l1)\n",
    "    ]\n",
    "\n",
    "def test_merge_shelve():\n",
    "    ctakes = shelve.open('/Users/gms/Desktop/ctakes_' + corpus + '.dat')\n",
    "    clamp = shelve.open('/Users/gms/Desktop/clamp_' + corpus + '.dat')\n",
    "    mm = shelve.open('/Users/gms/Desktop/metamap_' + corpus + '.dat')\n",
    "\n",
    "    print(test['case'][0:2])\n",
    "\n",
    "    #t0 = np.array(test['oracle'][3][0:750])\n",
    "    #t1 = np.array(test['oracle'][5][0:750])\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    sys = []\n",
    "    oracles = []\n",
    "    confuzz = []\n",
    "    for i in range(len(ctakes['case'])):\n",
    "        t0 = np.array(ctakes['sys'][i])\n",
    "        t1 = np.array(clamp['sys'][i])\n",
    "        t2 = np.array(mm['sys'][i])\n",
    "        oracle = np.array(ctakes['oracle'][i])\n",
    "\n",
    "        #print(ctakes['case'][i])\n",
    "\n",
    "        l0 = list(t0)\n",
    "        l1 = list(t1)\n",
    "        l2 = list(t2)\n",
    "\n",
    "        z = *map(list, zip(*umerge(l0, l1))),\n",
    "\n",
    "        #%time  *map(list, zip(*umerge(l0, l1))),\n",
    "        #%time  *map(list, zip(*umerge(z[0], l1))),\n",
    "\n",
    "        t = *map(list, zip(*umerge(z[0], l2))),\n",
    "\n",
    "        #results = *map(list, zip(*imerge(t[0], oracle))),\n",
    "\n",
    "        from functools import reduce \n",
    "        import itertools\n",
    "        import operator\n",
    "\n",
    "        test = [list(i) for i in t[0]]\n",
    "\n",
    "        replaced = [[none] if len(wd) == 0  else wd for wd in t[0]]\n",
    "\n",
    "        #print('truth:', {1, 0} in replaced)\n",
    "        replaced = [[1] if wd == {0, 1}  else wd for wd in replaced]\n",
    "\n",
    "        #print('len', len(replaced), len(oracle))\n",
    "\n",
    "        sys.append(replaced)\n",
    "        #oracles.append(list(oracle))\n",
    "\n",
    "        #print(len(np.array(list(itertools.chain.from_iterable(replaced)))), len(np.array(list(itertools.chain.from_iterable(t[0])))), len(oracle), len(t[0]))\n",
    "        tp, tn, fp, fn = confusing(np.array(list(itertools.chain.from_iterable(replaced))), oracle)\n",
    "\n",
    "        confuzz.append((tp, tn, fp, fn))\n",
    "        f, recall, precision, tp, fp, fn, tp_fn_r, tm = Metrics(fp, fn, tp, len(oracle), tn).get_confusion_metrics() #no tn\n",
    "        #print('test_io():', tp, tn, fp, fn, np.mean(f), np.mean(recall), np.mean(precision))\n",
    "\n",
    "        #print(tp, tn, fp, fn, np.array(test), oracle, np.array(list(itertools.chain.from_iterable(replaced))))\n",
    "\n",
    "        #print(oracle[0:2], np.array(t[0][0:2]))\n",
    "        #print(t[0][0:1])\n",
    "\n",
    "        #start = time.perf_counter()\n",
    "        #z = *map(list, zip(*imerge(l0, l1))),\n",
    "        #elapsed = (time.perf_counter() - start)\n",
    "        #print('time 2:', elapsed)\n",
    "        #%time *map(list, zip(*imerge(l0, l1))),\n",
    "        #%time *map(list, zip(*imerge(z[0], l1))),\n",
    "        #print( *map(list, zip(*imerge(z[0], l1))),)\n",
    "    print(' --- ')\n",
    "    #print(results)\n",
    "\n",
    "    print(len(list(itertools.chain.from_iterable(oracles))), len(list(itertools.chain.from_iterable(sys))))\n",
    "    #tp, tn, fp, fn = confusing(np.array(list(itertools.chain.from_iterable(sys))), np.array(list(itertools.chain.from_iterable(oracles))))\n",
    "    #f, recall, precision, tp, fp, fn, tp_fn_r, tm = metrics(fp, fn, tp, len(list(itertools.chain.from_iterable(oracles))), tn).get_confusion_metrics() #no tn\n",
    "    #print('test_io():', tp, tn, fp, fn, np.mean(f), np.mean(recall), np.mean(precision))\n",
    "    #print(confuzz)\n",
    "\n",
    "    print(list(map(sum, zip(*confuzz))))\n",
    "    z = list(map(sum, zip(*confuzz)))\n",
    "    # tp, tn, fp, fn -> (fp, fn, tp, len(oracle), tn)\n",
    "    f, recall, precision, tp, fp, fn, tp_fn_r, tm = Metrics(z[2], z[3], z[0], len(list(itertools.chain.from_iterable(sys))), z[1]).get_confusion_metrics() #no tn\n",
    "    print('test_io():', tp, tn, fp, fn, np.mean(f), np.mean(recall), np.mean(precision))\n",
    "    elapsed = (time.perf_counter() - start)\n",
    "    print('time 1:', elapsed)\n",
    "    # test_io(): 420721 2408 520533 16586 0.6283008747980883 0.7236264392738099 0.7071374894816047\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_data(training_notes: List[str], analysis_type: str, corpus: str):\n",
    "    engine_request = str(database_type)+'://'+database_username+':'+database_password+\"@\"+database_url+'/'+database_name\n",
    "    engine = create_engine(engine_request, pool_pre_ping=True, pool_size=20, max_overflow=30)\n",
    "   \n",
    "    usys_file, ref_table = AnalysisConfig().corpus_config()\n",
    "    systems = AnalysisConfig().systems\n",
    "    \n",
    "    sys_ann = pd.read_csv(analysisConf.data_dir + usys_file, dtype={'note_id': str})\n",
    "    \n",
    "    if 'test' not in analysis_type:\n",
    "        if corpus != 'fairview':\n",
    "            sql = \"SELECT * FROM \" + ref_table + \" where file not in %(training_notes)s\"  \n",
    "            sys_ann = sys_ann[~sys_ann['note_id'].isin(training_notes)]\n",
    "        else:\n",
    "            sql = \"SELECT * FROM \" + ref_table  \n",
    "            sys_ann = sys_ann\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        sql = \"SELECT * FROM \" + ref_table + \" where file in %(training_notes)s\"  \n",
    "        sys_ann = sys_ann[sys_ann['note_id'].isin(training_notes)]\n",
    "    \n",
    "    ref_ann = pd.read_sql(sql, params={\"training_notes\":training_notes}, con=engine)\n",
    "    sys_ann = sys_ann.drop_duplicates()\n",
    "    \n",
    "    return ref_ann, sys_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "def geometric_mean(metrics):\n",
    "    \"\"\"\n",
    "    1. Get rank average of F1, TP/FN, TM\n",
    "        http://www.datasciencemadesimple.com/rank-dataframe-python-pandas-min-max-dense-rank-group/\n",
    "        https://stackoverflow.com/questions/46686315/in-pandas-how-to-create-a-new-column-with-a-rank-according-to-the-mean-values-o?rq=1\n",
    "    2. Take geomean of 2.\n",
    "        https://stackoverflow.com/questions/42436577/geometric-mean-applied-on-row\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.DataFrame() \n",
    "\n",
    "    metrics['F1 rank']=metrics['F'].rank(ascending=0,method='average')\n",
    "    metrics['TP/FN rank']=metrics['TP/FN'].rank(ascending=0,method='average')\n",
    "    metrics['TM rank']=metrics['TM'].rank(ascending=0,method='average')\n",
    "    metrics['Gmean'] = gmean(metrics.iloc[:,-3:],axis=1)\n",
    "\n",
    "    return metrics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metrics(analysis_type: str, corpus: str, single_sys = None):\n",
    "    start = time.time()\n",
    "\n",
    "    systems = AnalysisConfig().systems\n",
    "    metrics = pd.DataFrame()\n",
    "\n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    ref_ann, sys_ann = get_metric_data(training_notes, analysis_type, corpus)\n",
    "    \n",
    "    for sys in systems:\n",
    "            types, _ = AnnotationSystems().get_system_type(sys) # system types for iterable\n",
    "            for t in types:\n",
    "                print(t)\n",
    "                system = pd.DataFrame()\n",
    "                \n",
    "                system_annotations = sys_ann.copy()\n",
    "                \n",
    "                system = system_annotations[system_annotations['type'] == str(t)]\n",
    "            \n",
    "                if sys == 'quick_umls':\n",
    "                    system = system[system.score.astype(float) >= .8]\n",
    "                \n",
    "                if sys == 'metamap':\n",
    "                    system = system[system.score.abs().astype(int) >= 800]\n",
    "            \n",
    "                system = system.drop_duplicates()\n",
    "                system.name = sys\n",
    "                \n",
    "                c = get_cooccurences(ref_ann, system, analysis_type, corpus, True, system.name) # get matches, FN, etc.\n",
    "                \n",
    "                print(c.ref_n, c.ref_only, c.system_n, c.system_only, c.ref_system_match)\n",
    "                \n",
    "            if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "                F, recall, precision, TP, FP, FN, TP_FN_R, TM = Metrics(c.system_only, c.ref_only, c.ref_system_match, c.system_n).get_confusion_metrics(corpus)\n",
    "                \n",
    "                if corpus == 'casi':\n",
    "                    if sys == 'biomedicus':\n",
    "                        t = 'biomedicus.v2.Acronym'\n",
    "                        \n",
    "                    d = {'system': sys, \n",
    "                         'type': t, \n",
    "                         'F': F, \n",
    "                         'precision': precision, \n",
    "                         'recall': recall, \n",
    "                         'FN': FN, \n",
    "                         'TP/FN': TP_FN_R,\n",
    "                         'n_gold': c.ref_n, \n",
    "                         'n_sys': c.system_n, \n",
    "                         'TM': TM}\n",
    "                else:\n",
    "                    d = {'system': sys, \n",
    "                         'type': t, \n",
    "                         'F': F[1], \n",
    "                         'precision': precision[1], \n",
    "                         'recall': recall[1], \n",
    "                         'TP': TP, \n",
    "                         'FN': FN, \n",
    "                         'FP': FP, \n",
    "                         'TP/FN': TP_FN_R,\n",
    "                         'n_gold': c.ref_n, \n",
    "                         'n_sys': c.system_n, \n",
    "                         'TM': TM}\n",
    "\n",
    "                data = pd.DataFrame(d,  index=[0])\n",
    "                metrics = pd.concat([metrics, data], ignore_index=True)\n",
    "                metrics.drop_duplicates(keep='last', inplace=True)\n",
    "            else:\n",
    "                print(\"NO EXACT MATCHES FOR\", t)\n",
    "            elapsed = (time.time() - start)\n",
    "            print(\"elapsed:\", sys, elapsed)\n",
    "     \n",
    "    elapsed = (time.time() - start)\n",
    "    print(geometric_mean(metrics))\n",
    "    \n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    \n",
    "    if single_sys is None:\n",
    "        file_name = 'metrics_'\n",
    "    \n",
    "    metrics.to_csv(analysisConf.data_dir + corpus + '_' + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "    \n",
    "    print(\"total elapsed time:\", elapsed) \n",
    "\n",
    "# use to iterate through mm scores\n",
    "def generate_metrics_mm(analysis_type: str, corpus: str, single_sys = None):\n",
    "    start = time.time()\n",
    "    from pandas.api.types import is_numeric_dtype\n",
    "    #systems = [\"biomedicus\",\"ctakes\",\"metamap\",\"clamp\",\"quick_umls\"]\n",
    "    systems = AnalysisConfig().systems\n",
    "    #systems = [\"quick_umls\"]\n",
    "    metrics = pd.DataFrame()\n",
    "\n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    ref_ann, sys_ann = get_metric_data(training_notes, analysis_type, corpus)\n",
    "    \n",
    "    sys_ann = sys_ann[(sys_ann.score.notnull()) & (sys_ann['system'] == 'metamap')]\n",
    "    sys_ann = sys_ann[['begin', 'end', 'note_id', 'system', 'score']].drop_duplicates()\n",
    "    sys_ann.score = sys_ann.score.astype(int)\n",
    "    \n",
    "    for sys in systems:\n",
    "        types, __ = AnnotationSystems().get_system_type(sys) # system types for iterable\n",
    "        for t in types:\n",
    "            print(t)\n",
    "\n",
    "            for i in range(500, 1050, 50): \n",
    "\n",
    "                sys_ann = sys_ann[(sys_ann[\"score\"] >= i)].copy()\n",
    "\n",
    "                sys_ann.name = sys + str(i)\n",
    "\n",
    "                c = get_cooccurences(ref_ann, sys_ann, analysis_type, corpus, True, sys_ann.name) # get matches, FN, etc.\n",
    "\n",
    "                print(c.ref_n, c.ref_only, c.system_n, c.system_only, c.ref_system_match)\n",
    "\n",
    "                #print(i, len(system))\n",
    "\n",
    "                if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "                    F, recall, precision, TP, FP, FN, TP_FN_R, TM = Metrics(c.system_only, c.ref_only, c.ref_system_match, c.system_n).get_confusion_metrics()\n",
    "                    d = {'system': sys + '_score_' + str(i), \n",
    "                         'type': t, \n",
    "                         'F': F[1], \n",
    "                         'precision': precision[1], \n",
    "                         'recall': recall[1], \n",
    "                         'TP': TP, \n",
    "                         'FN': FN, \n",
    "                         'FP': FP, \n",
    "                         'TP/FN': TP_FN_R,\n",
    "                         'n_gold': c.ref_n, \n",
    "                         'n_sys': c.system_n, \n",
    "                         'TM': TM}\n",
    "\n",
    "                    data = pd.DataFrame(d,  index=[0])\n",
    "                    metrics = pd.concat([metrics, data], ignore_index=True)\n",
    "                    metrics.drop_duplicates(keep='last', inplace=True)\n",
    "                else:\n",
    "                    print(\"NO EXACT MATCHES FOR\", t)\n",
    "                elapsed = (time.time() - start)\n",
    "                print(\"elapsed:\", sys, elapsed)\n",
    "     \n",
    "    elapsed = (time.time() - start)\n",
    "    print(geometric_mean(metrics))\n",
    "    \n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    # UIMA or QuickUMLS\n",
    "    if single_sys is None:\n",
    "        file_name = 'mm_metrics_'\n",
    "    metrics.to_csv(analysisConf.data_dir + corpus + '_' + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "    \n",
    "    print(\"total elapsed time:\", elapsed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in system matches from file\n",
    "def get_ref_n(analysis_type: str, corpus) -> int:\n",
    "    \n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    ref_ann, _ = get_metric_data(training_notes, analysis_type, corpus)\n",
    "    \n",
    "    if corpus == 'casi':\n",
    "        return len(ref_ann)\n",
    "        \n",
    "    else:\n",
    "        # do not overestimate fn\n",
    "        if 'entity' in analysis_type:\n",
    "            ref_ann = ref_ann[['start', 'end', 'file']].drop_duplicates()\n",
    "        elif 'cui' in analysis_type:\n",
    "            ref_ann = ref_ann[['value', 'file']].drop_duplicates()\n",
    "        elif 'full' in analysis_type:\n",
    "            ref_ann = ref_ann[['start', 'end', 'value', 'file']].drop_duplicates()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        ref_n = len(ref_ann.drop_duplicates())\n",
    "\n",
    "        return ref_n\n",
    "\n",
    "def get_sys_data(system: str, analysis_type: str, corpus: str) -> int: \n",
    "   \n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    _, data = get_metric_data(training_notes, analysis_type, corpus)\n",
    "    \n",
    "    out = data[data['system'] == system].copy()\n",
    "    \n",
    "    if corpus == 'casi':\n",
    "        cols_to_keep = ['case', 'overlap'] \n",
    "        #cols_to_keep = ['case', 'begin', 'end'] \n",
    "        out = out[cols_to_keep].drop_duplicates()\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    else:\n",
    "        out = data[data['system']== system].copy()\n",
    "\n",
    "        if system == 'quick_umls':\n",
    "            out = out[(out.score.astype(float) >= 0.8) & (out[\"type\"] == 'concept_jaccard_score_False')]\n",
    "        \n",
    "        if system == 'metamap':\n",
    "            out = out[out.score.abs().astype(int) >= 800]\n",
    "\n",
    "        if 'entity' in analysis_type:\n",
    "            cols_to_keep = ['begin', 'end', 'note_id']\n",
    "        elif 'cui' in analysis_type:\n",
    "            cols_to_keep = ['cui', 'note_id']\n",
    "        elif 'full' in analysis_type:\n",
    "            cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "\n",
    "        out = out[cols_to_keep]\n",
    "\n",
    "        return out.drop_duplicates()\n",
    "\n",
    "def get_system_matches(system: str, analysis_type: str, corpus: str):\n",
    "   \n",
    "    if corpus == 'casi':\n",
    "        \n",
    "        sql = \"select `case`, overlap from test.amia_2019_cases where overlap = 1 and `system` = %(system)s\"  \n",
    "        #ref_ann = pd.read_sql(sql, params={\"training_notes\":training_notes}, con=engine)\n",
    "        \n",
    "        data_matches = df_to_set(pd.read_sql(sql, params={\"system\":system}, con=engine), 'entity', 'sys', 'casi')\n",
    "        \n",
    "        sql = \"select `case`, overlap from test.amia_2019_cases where (overlap = 0 or overlap is null) and `system` = %(system)s\"  \n",
    "        #ref_ann = pd.read_sql(sql, params={\"training_notes\":training_notes}, con=engine)\n",
    "        \n",
    "        data_fn = df_to_set(pd.read_sql(sql, params={\"system\":system}, con=engine), 'entity', 'sys', 'casi')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        dir_test = analysisConf.data_dir + 'single_system_out/'\n",
    "\n",
    "        file = dir_test + system + '_' + analysis_type + '_' + corpus + '_matches.txt'\n",
    "        data_matches = set(literal_eval(line.strip()) for line in open(file))\n",
    "\n",
    "        file = dir_test + system + '_' + analysis_type + '_' + corpus + '_ref_only.txt'\n",
    "        data_fn = set(literal_eval(line.strip()) for line in open(file)) #{ f for f in file.readlines() }\n",
    "\n",
    "    return data_matches, data_fn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GENERATE merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetTotals(object):\n",
    "    \"\"\" \n",
    "    returns an instance with merged match set numbers using either union or intersection of elements in set \n",
    "    \"\"\"\n",
    "    def __init__(self, ref_n, sys_n, match_set):\n",
    "\n",
    "        self = self    \n",
    "        self.ref_ann = ref_n\n",
    "        self.sys_n = sys_n\n",
    "        self.match_set = match_set\n",
    "\n",
    "    def get_ref_sys(self):\n",
    "\n",
    "        ref_only = self.ref_ann - len(self.match_set)\n",
    "        sys_only = self.sys_n - len(self.match_set)\n",
    "\n",
    "        return ref_only, sys_only, len(self.match_set), self.match_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_eval(ref_only: int, system_only: int, ref_system_match: int, system_n: int, ref_n: int) -> dict:\n",
    "    \"\"\"\n",
    "    Generate confusion matrix params\n",
    "    :params: ref_only, system_only, reference_system_match -> sets\n",
    "    matches, system_n, reference_n -> counts\n",
    "    :return: dictionary object\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if ref_only + ref_system_match != ref_n:\n",
    "        print('ERROR!')\n",
    "\n",
    "    # get evaluation metrics\n",
    "    d = {}\n",
    "    \n",
    "    F, recall, precision, TP, FP, FN, TP_FN_R, TM  = Metrics(system_only, ref_only, ref_system_match, system_n).get_confusion_metrics()\n",
    "\n",
    "    d = {\n",
    "         'F': F[1], \n",
    "         'precision': precision[1], \n",
    "         'recall': recall[1], \n",
    "         'TP': TP, \n",
    "         'FN': FN, \n",
    "         'FP': FP, \n",
    "         'TP/FN': TP_FN_R,\n",
    "         'n_gold': ref_n, \n",
    "         'n_sys': system_n, \n",
    "         'TM': TM\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if system_n - FP != TP:\n",
    "        print('inconsistent system n!')\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import operator as op\n",
    "import pandas as pd\n",
    "import gevent\n",
    "from __main__ import get_system_matches, get_sys_data\n",
    "\n",
    "def process_sentence(pt, sentence, analysis_type, corpus):\n",
    "    \"\"\"\n",
    "    Recursively evaluate parse tree, \n",
    "    with check for existence before build\n",
    "       :param sentence: to process\n",
    "       :return class of merged annotations, boolean operated system df \n",
    "    \"\"\"\n",
    "    \n",
    "    class Results(object):\n",
    "        def __init__(self):\n",
    "            self.results = set()\n",
    "            #self.operations = []\n",
    "            self.system_merges = pd.DataFrame()\n",
    "            \n",
    "    r = Results()\n",
    "    \n",
    "    if 'entity' in analysis_type and corpus != 'casi': \n",
    "        cols_to_keep = ['begin', 'end', 'note_id'] # entity only\n",
    "    elif 'full' in analysis_type: \n",
    "        cols_to_keep = ['cui', 'begin', 'end', 'note_id'] # entity only\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['cui', 'note_id'] # entity only\n",
    "    elif corpus == 'casi':\n",
    "        cols_to_keep = ['case', 'overlap']\n",
    "    \n",
    "    def evaluate(parseTree):\n",
    "        oper = {'&': op.and_, '|': op.or_}\n",
    "        \n",
    "        if parseTree:\n",
    "            #leftC = evaluate(parseTree.getLeftChild())\n",
    "            #rightC = evaluate(parseTree.getRightChild())\n",
    "            leftC = gevent.spawn(evaluate, parseTree.getLeftChild())\n",
    "            rightC = gevent.spawn(evaluate, parseTree.getRightChild())\n",
    "            \n",
    "            if leftC.get() and rightC.get():\n",
    "                query = set()\n",
    "                system_query = pd.DataFrame()\n",
    "                fn = oper[parseTree.getRootVal()]\n",
    "                \n",
    "                if isinstance(leftC.get(), str):\n",
    "                    \n",
    "                    # get system as leaf node \n",
    "                    left, _ = get_system_matches(leftC.get(), analysis_type, corpus)\n",
    "                    left_sys = get_sys_data(leftC.get(), analysis_type, corpus)\n",
    "                \n",
    "                elif isinstance(leftC.get(), tuple):\n",
    "                    left = leftC.get()[0]\n",
    "                    l_sys = leftC.get()[1]\n",
    "                \n",
    "                if isinstance(rightC.get(), str):\n",
    "                    # get system as leaf node\n",
    "                    right, _ = get_system_matches(rightC.get(), analysis_type, corpus)\n",
    "                    right_sys = get_sys_data(rightC.get(), analysis_type, corpus)\n",
    "                    \n",
    "                elif isinstance(rightC.get(), tuple):\n",
    "                    right = rightC.get()[0]\n",
    "                    r_sys = rightC.get()[1]\n",
    "                    \n",
    "                # create match set based on boolean operation\n",
    "                match_set = fn(left, right)\n",
    "               \n",
    "                if corpus != 'casi':\n",
    "                    if fn == op.or_:\n",
    "                        r.results = r.results.union(match_set)\n",
    "\n",
    "                        if isinstance(leftC.get(), str) and isinstance(rightC.get(), str):\n",
    "                            frames = [left_sys, right_sys]\n",
    "                            df = pd.concat(frames,  ignore_index=True)\n",
    "                            \n",
    "                            #df = left_sys.append(right_sys)\n",
    "                            df = df[cols_to_keep].drop_duplicates(cols_to_keep)\n",
    "\n",
    "                        elif isinstance(leftC.get(), str) and isinstance(rightC.get(), tuple):\n",
    "                            frames = [left_sys, r_sys]\n",
    "                            df = pd.concat(frames,  ignore_index=True)\n",
    "                            #df = left_sys.append(r_sys)\n",
    "                            df = df[cols_to_keep].drop_duplicates(cols_to_keep)\n",
    "\n",
    "                        elif isinstance(leftC.get(), tuple) and isinstance(rightC.get(), str):\n",
    "                            frames = [l_sys, right_sys]\n",
    "                            df = pd.concat(frames,  ignore_index=True)\n",
    "                            #df = right_sys.append(l_sys)\n",
    "                            df = df[cols_to_keep].drop_duplicates(cols_to_keep)\n",
    "\n",
    "                        elif isinstance(leftC.get(), tuple) and isinstance(rightC.get(), tuple):\n",
    "                            frames = [l_sys, r_sys]\n",
    "                            df = pd.concat(frames,  ignore_index=True)\n",
    "                            #df = l_sys.append(r_sys)\n",
    "                            df = df[cols_to_keep].drop_duplicates(cols_to_keep)\n",
    "\n",
    "                    if fn == op.and_:\n",
    "                        if len(r.results) == 0:\n",
    "                            r.results = match_set\n",
    "                        r.results = r.results.intersection(match_set)\n",
    "\n",
    "                        if isinstance(leftC.get(), str) and isinstance(rightC.get(), str):\n",
    "                            df = left_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates(cols_to_keep)\n",
    "\n",
    "                        elif isinstance(leftC.get(), str) and isinstance(rightC.get(), tuple):\n",
    "                            df = left_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates(cols_to_keep)\n",
    "\n",
    "                        elif isinstance(leftC.get(), tuple) and isinstance(rightC.get(), str):\n",
    "                            df = l_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates(cols_to_keep)\n",
    "\n",
    "                        elif isinstance(leftC.get(), tuple) and isinstance(rightC.get(), tuple):\n",
    "                            df = l_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates(cols_to_keep)\n",
    "                else:\n",
    "                    if fn == op.or_:\n",
    "                        r.results = r.results.union(match_set)\n",
    "\n",
    "                        if isinstance(leftC, str) and isinstance(rightC, str):\n",
    "                            df = left_sys.append(right_sys)\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, str) and isinstance(rightC, tuple):\n",
    "                            df = left_sys.append(r_sys)\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, tuple) and isinstance(rightC, str):\n",
    "                            df = right_sys.append(l_sys)\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, tuple) and isinstance(rightC, tuple):\n",
    "                            df = l_sys.append(r_sys)\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                    if fn == op.and_:\n",
    "                        if len(r.results) == 0:\n",
    "                            r.results = match_set\n",
    "                        r.results = r.results.intersection(match_set)\n",
    "\n",
    "                        if isinstance(leftC, str) and isinstance(rightC, str):\n",
    "                            df = left_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, str) and isinstance(rightC, tuple):\n",
    "                            df = left_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, tuple) and isinstance(rightC, str):\n",
    "                            df = l_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, tuple) and isinstance(rightC, tuple):\n",
    "                            df = l_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df.drop_duplicates()\n",
    "                \n",
    "                # get matched results\n",
    "                query.update(r.results)\n",
    "                \n",
    "                # get combined system results\n",
    "                r.system_merges = df\n",
    "                \n",
    "                if len(df) > 0:\n",
    "                    system_query = system_query.append(df)\n",
    "                else:\n",
    "                    print('wtf!')\n",
    "                    \n",
    "                return query, system_query\n",
    "            else:\n",
    "                return parseTree.getRootVal()\n",
    "    \n",
    "    if sentence.n_or > 0 or sentence.n_and > 0:\n",
    "        evaluate(pt)  \n",
    "    \n",
    "    # trivial case\n",
    "    elif sentence.n_or == 0 and sentence.n_and == 0:\n",
    "        r.results, _ = get_system_matches(sentence.sentence, analysis_type, corpus)\n",
    "        r.system_merges = get_sys_data(sentence.sentence, analysis_type, corpus)\n",
    "        print('trivial:', sentence.sentence, len(r.results), len(r.system_merges))\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Incoming Boolean sentences are parsed into a binary tree.\n",
    "\n",
    "Test expressions to parse:\n",
    "\n",
    "sentence = '((((A&B)|C)|D)&E)'\n",
    "\n",
    "sentence = '(E&(D|(C|(A&B))))'\n",
    "\n",
    "sentence = '(((A|(B&C))|(D&(E&F)))|(H&I))'\n",
    "\n",
    "\"\"\"\n",
    "# build parse tree from passed sentence\n",
    "# using grammatical rules of Boolean logic\n",
    "def buildParseTree(fpexp):\n",
    "    \"\"\"\n",
    "       Iteratively build parse tree from passed sentence using grammatical rules of Boolean logic\n",
    "       :param fpexp: sentence to parse\n",
    "       :return eTree: parse tree representation\n",
    "       Incoming Boolean sentences are parsed into a binary tree.\n",
    "       Test expressions to parse:\n",
    "       sentence = '(A&B)'\n",
    "       sentence = '(A|B)'\n",
    "       sentence = '((A|B)&C)'\n",
    "       \n",
    "    \"\"\"\n",
    "\n",
    "    fplist = fpexp.split()\n",
    "    pStack = Stack()\n",
    "    eTree = BinaryTree('')\n",
    "    pStack.push(eTree)\n",
    "    currentTree = eTree\n",
    "\n",
    "    for i in fplist:\n",
    "\n",
    "        if i == '(':\n",
    "            currentTree.insertLeft('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getLeftChild()\n",
    "        elif i not in ['&', '|', ')']:\n",
    "            currentTree.setRootVal(i)\n",
    "            parent = pStack.pop()\n",
    "            currentTree = parent\n",
    "        elif i in ['&', '|']:\n",
    "            currentTree.setRootVal(i)\n",
    "            currentTree.insertRight('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getRightChild()\n",
    "        elif i == ')':\n",
    "            currentTree = pStack.pop()\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    return eTree\n",
    "\n",
    "def make_parse_tree(payload):\n",
    "    \"\"\"\n",
    "    Ensure data to create tree are in standard form\n",
    "    :param sentence: sentence to preprocess\n",
    "    :return pt, parse tree graph\n",
    "            sentence, processed sentence to build tree\n",
    "            a: order\n",
    "    \"\"\"\n",
    "    def preprocess_sentence(sentence):\n",
    "        # prepare statement for case when a boolean AND/OR is given\n",
    "        sentence = payload.replace('(', ' ( '). \\\n",
    "            replace(')', ' ) '). \\\n",
    "            replace('&', ' & '). \\\n",
    "            replace('|', ' | '). \\\n",
    "            replace('  ', ' ')\n",
    "        return sentence\n",
    "\n",
    "    sentence = preprocess_sentence(payload)\n",
    "    print(sentence)\n",
    "    \n",
    "    pt = buildParseTree(sentence)\n",
    "    #pt.postorder() \n",
    "    \n",
    "    return pt\n",
    "\n",
    "class Sentence(object):\n",
    "\n",
    "    def __init__(self, sentence):\n",
    "        self = self\n",
    "        self.n_and = sentence.count('&')\n",
    "        self.n_or = sentence.count('|')\n",
    "        self.sentence = sentence\n",
    "    \n",
    "def get_metrics(boolean_expression: str, analysis_type: str, corpus: str):\n",
    "    \"\"\"\n",
    "    Traverse binary parse tree representation of Boolean sentence\n",
    "        :params: boolean expression in form of '(<annotator_engine_name1><boolean operator><annotator_engine_name2>)'\n",
    "                 analysis_type (string value of: 'test', 'entity', 'cui', 'full') used to filter set of reference and system annotations \n",
    "        :return: dictionary with values needed for confusion matrix\n",
    "    \"\"\"\n",
    "    sentence = Sentence(boolean_expression)   \n",
    "\n",
    "    pt = make_parse_tree(sentence.sentence)\n",
    "\n",
    "    r = process_sentence(pt, sentence, analysis_type, corpus)\n",
    "    \n",
    "    print('len sys merges:', len(r.system_merges))\n",
    "    system_n = len(r.system_merges)\n",
    "    reference_n = get_ref_n(analysis_type, corpus)\n",
    "\n",
    "    reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, r.results).get_ref_sys()\n",
    "\n",
    "    # get overall TP/TF and various other counts for running confusion matrix metric analysis\n",
    "    return merge_eval(reference_only, system_only, reference_system_match, system_n, reference_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all combinations of given list of annotators:\n",
    "def expressions(l, n):\n",
    "    for (operations, *operands), operators in product(\n",
    "            combinations(l, n), product(('&', '|'), repeat=n - 1)):\n",
    "        for operation in zip(operators, operands):\n",
    "            operations = [operations, *operation]\n",
    "        yield operations\n",
    "\n",
    "def run_ensemble(l, analysis_type, corpus):\n",
    "\n",
    "    metrics = pd.DataFrame()\n",
    "\n",
    "    print('CORPUS---->', corpus)\n",
    "    print('---------->', corpus)\n",
    "    \n",
    "    #for i in l:\n",
    "    #    d = get_metrics(i, analysis_type, corpus)\n",
    "    \n",
    "    #for s in list(permutations(l)):\n",
    "    for i in range(1, len(l)+1):\n",
    "        test = list(expressions(l, i))\n",
    "        for t  in test:\n",
    "            if i > 1:\n",
    "                # format Boolean sentence for parse tree \n",
    "                t = '(' + \" \".join(str(x) for x in t).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "\n",
    "            d = get_metrics(t, analysis_type, corpus)\n",
    "            d['merge'] = t\n",
    "            frames = [metrics, pd.DataFrame(d, index=[0]) ]\n",
    "            metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "    \n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    \n",
    "    file_name = corpus + '_all_merge_metrics_'\n",
    "        \n",
    "    geometric_mean(metrics).to_csv(analysisConf.data_dir + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "    print(geometric_mean(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_set(df, analysis_type = 'entity', df_type = 'sys', corpus = None):\n",
    "    \n",
    "    #print(df[0:10])\n",
    "    \n",
    "    # get values for creation of series of type tuple\n",
    "    if 'entity' in analysis_type: \n",
    "        if corpus == 'casi':\n",
    "            arg = df.case, df.overlap\n",
    "        else:    \n",
    "            if df_type == 'sys':\n",
    "                arg = df.begin, df.end, df.note_id\n",
    "            else:\n",
    "                arg = df.start, df.end, df.file\n",
    "            \n",
    "    elif 'cui' in analysis_type:\n",
    "        if df_type == 'sys':\n",
    "            arg = df.cui, df.note_id\n",
    "        else:\n",
    "            arg = df.value, df.file\n",
    "    elif 'full' in analysis_type:\n",
    "        if df_type == 'sys':\n",
    "            arg = df.begin, df.end, df.cui, df.note_id\n",
    "        else:\n",
    "            arg = df.start, df.end, df.value, df.file\n",
    "    \n",
    "    return set(list(zip(*arg)))\n",
    "#     if corpus == 'casi':\n",
    "#         return set(arg)\n",
    "#     else:\n",
    "#         return set(list(zip(*arg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTS -> ensemble:\n",
    "def test_match_consistency(matches, ref_only, ref_n, sys):\n",
    "    \"\"\"test for reference only/match set consistency:\n",
    "        params: match, system and reference only sets\"\"\"\n",
    "   \n",
    "    print('len', len(sys), len(matches), len(matches.union(sys)), len(matches.intersection(sys)))\n",
    "    assert len(matches.union(ref_only)) == ref_n, 'Reference annotation mismatch union'\n",
    "    assert len(matches.intersection(sys)) == len(matches), 'System annotation mismatch intersect'\n",
    "    assert len(matches.union(sys)) == len(sys), 'System annotation mismatch union'\n",
    "    assert len(matches.intersection(ref_only)) == 0, 'Reference annotation mismatch intersect'\n",
    "\n",
    "def test_systems(analysis_type, systems, corpus):\n",
    "    sys = df_to_set(get_sys_data(systems[0], analysis_type, corpus), analysis_type)\n",
    "    test_match_consistency(*get_system_matches(systems[0], analysis_type, corpus), get_ref_n(analysis_type), sys)\n",
    "    print('Match consistency:', len(sys),get_ref_n(analysis_type))\n",
    "\n",
    "def test_metrics(ref, sys_m, match_m):\n",
    "    test = True\n",
    "    reference_n = len(ref)\n",
    "    system_n = len(sys_m)\n",
    "\n",
    "    print('Test metrics:', type(reference_n), type(system_n), type(match_m))\n",
    "\n",
    "    reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, match_m).get_ref_sys()\n",
    "    F, recall, precision, _, _, _, _, _ = Metrics(system_only, reference_only, reference_system_match, system_n).get_confusion_metrics()\n",
    "    F_, recall_, precision_, _, _, _, _, _ = Metrics(system_only, reference_only, reference_system_match, system_n).get_confusion_metrics(test)\n",
    "\n",
    "    assert F[1] == F_, 'F1 issue'\n",
    "    assert recall[1] == recall_, 'recall issue'\n",
    "    assert precision[1] == precision_, 'precision issue'\n",
    "    print(F[1], F_)\n",
    "    print(recall[1], recall_)\n",
    "    print(precision[1], precision_)\n",
    "\n",
    "def test_count(analysis_type, corpus):\n",
    "    # test match counts:\n",
    "    ctakes, _ = get_system_matches('ctakes', analysis_type, corpus)\n",
    "    clamp, _ = get_system_matches('clamp', analysis_type, corpus)\n",
    "    b9, _ = get_system_matches('biomedicus', analysis_type, corpus)\n",
    "    mm, _ = get_system_matches('metamap', analysis_type, corpus)\n",
    "\n",
    "    print('count:', len(mm.intersection(b9.intersection(clamp.intersection(ctakes)))))\n",
    "    \n",
    "def test_ensemble(analysis_type, corpus):\n",
    "    \n",
    "    print('ensemble:')\n",
    "    # Get mixed system_n\n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    ref_ann, data = get_metric_data(training_notes, analysis_type, corpus)\n",
    "\n",
    "    names = ['ctakes', 'biomedicus', 'metamap', 'clamp']\n",
    "    if 'entity' in analysis_type: \n",
    "        cols_to_keep = ['begin', 'end', 'note_id']\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['cui', 'note_id']\n",
    "    elif 'full' in analysis_type:\n",
    "        cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "\n",
    "    biomedicus = data[data[\"system\"]=='biomedicus'][cols_to_keep].copy()\n",
    "    ctakes = data[data[\"system\"]=='ctakes'][cols_to_keep].copy()\n",
    "    clamp = data[data[\"system\"]=='clamp'][cols_to_keep].copy()\n",
    "    metamap = data[data[\"system\"]=='metamap'][cols_to_keep].copy()\n",
    "    quickumls = data[data[\"system\"]=='quick_umls'][cols_to_keep].copy()\n",
    "\n",
    "    print('systems:', len(biomedicus), len(clamp), len(ctakes), len(metamap), len(quickumls))\n",
    "\n",
    "    b9 = set()\n",
    "    cl = set()\n",
    "    ct = set()\n",
    "    mm = set()\n",
    "    qu = set()\n",
    "\n",
    "    b9 = df_to_set(get_sys_data('biomedicus', analysis_type, corpus), analysis_type)\n",
    "    print(len(b9))\n",
    "\n",
    "    ct = df_to_set(get_sys_data('ctakes', analysis_type, corpus), analysis_type)\n",
    "    print(len(ct))\n",
    "\n",
    "    cl = df_to_set(get_sys_data('clamp', analysis_type, corpus), analysis_type)\n",
    "    print(len(cl))\n",
    "\n",
    "    mm = df_to_set(get_sys_data('metamap', analysis_type, corpus), analysis_type)\n",
    "    print(len(mm))\n",
    "\n",
    "    qu = df_to_set(get_sys_data('quick_umls', analysis_type, corpus), analysis_type)\n",
    "    print(len(qu))\n",
    "    \n",
    "    print('various merges:')\n",
    "    print(len(b9), len(cl), len(ct), len(mm), len(qu))\n",
    "    print(len(mm.intersection(b9.intersection(cl.intersection(ct)))))\n",
    "    print(len(mm.union(b9.intersection(cl.intersection(ct)))))\n",
    "    print(len(mm.union(b9.union(cl.intersection(ct)))))\n",
    "    print(len(mm.union(b9.union(cl.union(ct)))))\n",
    "    print(len(b9.intersection(ct)))\n",
    "\n",
    "    sys_m = b9.intersection(ct.intersection(qu))\n",
    "    print('sys_m:', len(sys_m))\n",
    "\n",
    "    # Get match merges:\n",
    "    ct, _ = get_system_matches('ctakes', analysis_type, corpus)\n",
    "    cl, _ = get_system_matches('clamp', analysis_type, corpus)\n",
    "    b9, _ = get_system_matches('biomedicus', analysis_type, corpus)\n",
    "    mm, _ = get_system_matches('metamap', analysis_type, corpus)\n",
    "    qu, _ = get_system_matches('quick_umls', analysis_type, corpus)\n",
    "\n",
    "    match_m = b9.intersection(ct.intersection(qu))\n",
    "    print('match_m:', len(match_m))\n",
    "    # reference df to set\n",
    "    if 'entity' in analysis_type: \n",
    "        cols_to_keep = ['end', 'start','file']\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['value','file']\n",
    "    elif 'full' in analysis_type:\n",
    "        cols_to_keep = ['end', 'start', 'value','file']\n",
    "\n",
    "    ref = df_to_set(ref_ann[cols_to_keep], analysis_type, 'ref')\n",
    "\n",
    "    print('ref:', len(ref))\n",
    "\n",
    "    # test difference:\n",
    "    print('FP:', len(sys_m - match_m), len(sys_m - ref))\n",
    "    assert len(sys_m - match_m) == len(sys_m - ref), 'FP mismatch'\n",
    "    print('FN:', len(ref - match_m), len(ref - sys_m))\n",
    "    assert len(ref - match_m) == len(ref - sys_m), 'FN mismatch'\n",
    "    \n",
    "    test_metrics(ref, sys_m, match_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls'] ('analytical_fairview.csv', 'concepts.fairview_all')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/cassis/lib/python3.7/site-packages/ipykernel_launcher.py:62: DtypeWarning: Columns (8,9,10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biomedicus.v2.UmlsConcept\n",
      "entity\n",
      "25034 14670 105945 95581 10364\n",
      "elapsed: biomedicus 3.0123350620269775\n",
      "edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/cassis/lib/python3.7/site-packages/ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity\n",
      "25034 14112 73230 62308 10922\n",
      "elapsed: clamp 3.3748300075531006\n",
      "ctakes_mentions\n",
      "entity\n",
      "25034 12088 133350 120404 12946\n",
      "elapsed: ctakes 3.7853219509124756\n",
      "org.metamap.uima.ts.Candidate\n",
      "entity\n",
      "25034 18765 58881 52612 6269\n",
      "elapsed: metamap 4.140478849411011\n",
      "concept_jaccard_score_False\n",
      "entity\n",
      "25034 19712 81256 75934 5322\n",
      "elapsed: quick_umls 4.4985740184783936\n",
      "       system                                              type         F  \\\n",
      "0  biomedicus                         biomedicus.v2.UmlsConcept  0.158254   \n",
      "1       clamp  edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA  0.222299   \n",
      "2      ctakes                                   ctakes_mentions  0.163476   \n",
      "3     metamap                     org.metamap.uima.ts.Candidate  0.149413   \n",
      "4  quick_umls                       concept_jaccard_score_False  0.100141   \n",
      "\n",
      "   precision    recall     TP     FN      FP     TP/FN  n_gold   n_sys  \\\n",
      "0   0.097824  0.413997  10364  14670   95581  0.706476   25034  105945   \n",
      "1   0.149147  0.436287  10922  14112   62308  0.773951   25034   73230   \n",
      "2   0.097083  0.517137  12946  12088  120404  1.070979   25034  133350   \n",
      "3   0.106469  0.250419   6269  18765   52612  0.334079   25034   58881   \n",
      "4   0.065497  0.212591   5322  19712   75934  0.269988   25034   81256   \n",
      "\n",
      "          TM  F1 rank  TP/FN rank  TM rank     Gmean  \n",
      "0  31.841035      3.0         3.0      3.0  3.000000  \n",
      "1  40.360604      1.0         2.0      1.0  1.259921  \n",
      "2  35.451865      2.0         1.0      2.0  1.587401  \n",
      "3  25.835132      4.0         4.0      4.0  4.000000  \n",
      "4  18.670122      5.0         5.0      5.0  5.000000  \n",
      "total elapsed time: 4.498992919921875\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "def partly_unordered_permutations(lst, k):\n",
    "    elems = set(lst)\n",
    "    for c in combinations(lst, k):\n",
    "        for d in permutations(elems - set(c)):\n",
    "            yield c + d\n",
    "            \n",
    "def main():\n",
    "    \n",
    "    #rtype = int(input(\"Run: 1->Single systems; 2->Ensemble; 3->Tests; 4-> MM Test\"))\n",
    "   \n",
    "    '''\n",
    "        corpora: i2b2, mipacq, fv017\n",
    "        analyses: entity only (exact span), cui by document, full (aka (entity and cui on exaact span/exact cui)\n",
    "                  NB: add \"_test\" using mipacq to egnerate small test sample \n",
    "        systems: ctakes, biomedicus, clamp, metamap, quick_umls\n",
    "        \n",
    "        TODO -> Vectorization (entity only and full):\n",
    "                add switch for use of TN on single system performance evaluations \n",
    "                add switch for overlap matching versus exact span\n",
    "             -> Other tasks besides concept extraction\n",
    "             -> Use of https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html\n",
    "        \n",
    "    ''' \n",
    "    analysisConf =  AnalysisConfig()\n",
    "    print(analysisConf.systems, analysisConf.corpus_config())\n",
    "    \n",
    "    if (rtype == 1):\n",
    "        generate_metrics(analysis_type, corpus)\n",
    "    elif (rtype == 2):\n",
    "        systems = [['ctakes','biomedicus','clamp','metamap','quick_umls'],\n",
    "             ['ctakes','clamp','biomedicus','metamap','quick_umls'],\n",
    "             ['ctakes','metamap','biomedicus','clamp','quick_umls'],\n",
    "             ['ctakes','quick_umls','biomedicus','clamp','metamap'],\n",
    "             ['biomedicus','clamp','ctakes','metamap','quick_umls'],\n",
    "             ['biomedicus','metamap','ctakes','clamp','quick_umls'],\n",
    "             ['biomedicus','quick_umls','ctakes','clamp','metamap'],\n",
    "             ['clamp','metamap','biomedicus','ctakes','quick_umls'],\n",
    "             ['clamp','quick_umls','ctakes','biomedicus','metamap'],\n",
    "             ['metamap','quick_umls','ctakes','biomedicus','clamp']]\n",
    "        \n",
    "        l = ['ctakes','biomedicus','clamp','metamap','quick_umls']  \n",
    "        \n",
    "        systems = ['ctakes','biomedicus','clamp','metamap','quick_umls']\n",
    "        \n",
    "        #l = ['metamap', 'clamp', 'biomedicus']\n",
    "        #for corpus in corpora:\n",
    "        for l in partly_unordered_permutations(systems, 2):\n",
    "            print('corpus:', corpus, l)\n",
    "            run_ensemble(l, analysis_type, corpus) \n",
    "            \n",
    "    elif (rtype == 3):\n",
    "        systems = ['biomedicus']\n",
    "        t = ['concept_jaccard_score_false']\n",
    "        test_systems(analysis_type, systems, corpus)  \n",
    "        test_count(analysis_type, corpus)\n",
    "        test_ensemble(analysis_type, corpus)\n",
    "    elif (rtype == 4):\n",
    "        generate_metrics_test(analysis_type, corpus)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
