{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  Copyright (c) 2019 Regents of the University of Minnesota.\n",
    " \n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    " \n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gevent\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pymysql\n",
    "import time \n",
    "import functools as ft\n",
    "import glob   \n",
    "import operator as op\n",
    "import shelve\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from pathlib import Path\n",
    "from itertools import combinations, product, permutations\n",
    "from sqlalchemy.engine import create_engine\n",
    "from datetime import datetime\n",
    "from ast import literal_eval\n",
    "from scipy import stats  \n",
    "from scipy.stats.mstats import gmean\n",
    "from pythonds.basic.stack import Stack\n",
    "from pythonds.trees.binaryTree import BinaryTree\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "from typing import List, Set, Tuple "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The cell below contains the configurable parameters to ensure that our ensemble explorer runs properaly on your machine. \n",
    "Please read carfully through steps (1-11) before running the rest of the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-1: CHOOSE YOUR CORPUS\n",
    "# TODO: allow for list of corpora\n",
    "corpus = 'fairview' #options include 'fairview', 'mipacq' OR 'i2b2'\n",
    "\n",
    "# STEP-2: CHOOSE YOUR DATA DIRECTORY; this is where output data will be saved on your machine\n",
    "data_directory = '/Users/gms/development/nlp/nlpie/data/ensembling-u01/output/' \n",
    "\n",
    "# STEP-3: CHOOSE WHICH SYSTEMS YOU'D LIKE TO EVALUATE AGAINST THE CORPUS REFERENCE SET\n",
    "#systems = ['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls']\n",
    "#systems = ['biomedicus', 'clamp']\n",
    "#systems = ['biomedicus', 'clamp']\n",
    "systems = ['clamp', 'quick_umls', 'biomedicus']\n",
    "\n",
    "# STEP-4: CHOOSE TYPE OF RUN\n",
    "rtype = 2      # OPTIONS INCLUDE: 1->Single systems; 2->Ensemble; 3->Tests; 4 -> generate signiicance test data;  \n",
    "               # 5 -> signiicance testing\n",
    "               # The Ensemble can include the max system set ['ctakes','biomedicus','clamp','metamap','quick_umls']\n",
    "    \n",
    "# STEP-5: CHOOSE WHAT TYPE OF ANALYSIS YOU'D LIKE TO RUN ON THE CORPUS\n",
    "analysis_type = 'entity' #options include 'entity' OR 'full'\n",
    "\n",
    "# STEP-(6A): ENTER DETAILS FOR ACCESSING MANUAL ANNOTATION DATA\n",
    "database_type = 'mysql+pymysql' # We use mysql+pymql as default\n",
    "database_username = 'gms'\n",
    "database_password = 'nej123' \n",
    "database_url = 'localhost' # HINT: use localhost if you're running database on your local machine\n",
    "database_name = 'concepts' # Enter database name\n",
    "table_name = corpus + '_all' # Enter the table within the database where your reference data is stored\n",
    "\n",
    "# STEP-(6B): ENTER DETAILS FOR ACCESSING SYSTEM ANNOTATION DATA\n",
    "system_annotation = 'analytical_'+corpus+'.csv' # OPTIONS include 'analytical_cui_mipacq_concepts.csv' OR 'analytical_cui_i2b2_concepts.csv' \n",
    "\n",
    "# STEP-7: WE'LL CREATE A 'SYSTEM OUTPUT' DIRECTORY FOR YOU INSIDE THE DIRECTORY YOU SPECIFIED IN (STEP 2)\n",
    "single_sys_dir = Path(data_directory + \"single_system_out\")\n",
    "single_sys_dir.mkdir(parents=True, exist_ok=True)\n",
    "dir_out = Path(data_directory + 'single_system_out/')\n",
    "\n",
    "# STEP-8: CREATE A DB CONNECTION POOL\n",
    "engine_request = str(database_type)+'://'+database_username+':'+database_password+\"@\"+database_url+'/'+database_name\n",
    "engine = create_engine(engine_request, pool_pre_ping=True, pool_size=20, max_overflow=30)\n",
    "\n",
    "# STEP-(9A): FILTER BY SEMTYPE\n",
    "filter_semtype = None\n",
    "\n",
    "# STEP-(9B): IF STEP-(9A) == True -> GET REFERENCE SEMTYPES\n",
    "\n",
    "### Fairview -> ['drug', 'finding', 'anatomy', 'procedure']\n",
    "### i2b2 -> ['test, treatment', 'problem']\n",
    "### MiPACQ -> ['procedures', 'disorders, sign_symptom', 'anatomy', 'chemicals_and_drugs']\n",
    "\n",
    "# semtypes = ['Anatomy']\n",
    "\n",
    "if filter_semtype:\n",
    "    if corpus == 'fairview':\n",
    "        semtypes = ['Drug', 'Finding', 'Anatomy', 'Procedure']\n",
    "    elif corpus == 'i2b2':\n",
    "        semtypes = ['test,treatment', 'problem']\n",
    "    elif corpus == 'mipacq':\n",
    "        semtypes = ['Procedures', 'Disorders,Sign_Symptom', 'Anatomy', 'Chemicals_and_drugs']\n",
    "\n",
    "# STEP-10: Set data directory/table for source documents for vectorization\n",
    "src_table = 'sofa'\n",
    "\n",
    "# STEP-11: Specificy match type from {'exact', 'overlap'}\n",
    "run_type = 'overlap'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "****** TODO \n",
    "-> add 'semtype' to output file name -> done\n",
    "-> filter for case when a system has no equivalent semtypes for a given reference -> done: still need to validate that all semtypes in corpus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if filter_semtype:\n",
    "    print(semtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config class for analysis\n",
    "class AnalysisConfig():\n",
    "    \"\"\"\n",
    "    Configuration object:\n",
    "    systems to use\n",
    "    notes by corpus\n",
    "    paths by output, gold and system location\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self = self    \n",
    "        self.systems = systems\n",
    "        self.data_dir = data_directory\n",
    "    \n",
    "    def corpus_config(self): \n",
    "        usys_data = system_annotation\n",
    "        ref_data = database_name+'.'+table_name\n",
    "        return usys_data, ref_data\n",
    "\n",
    "analysisConf =  AnalysisConfig()\n",
    "usys, ref = analysisConf.corpus_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'treatment', 'test'}\n"
     ]
    }
   ],
   "source": [
    "class SemanticTypes(object):\n",
    "    '''\n",
    "    Filter semantic types based on: https://metamap.nlm.nih.gov/SemanticTypesAndGroups.shtml\n",
    "    :params: semtypes list from corpus, system to query\n",
    "    :return: list of equivalent system semtypes \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, semtypes, corpus):\n",
    "        self = self\n",
    "        \n",
    "        sql = \"SELECT st.tui, abbreviation, clamp_name, ctakes_name FROM concepts.semantic_groups sg join semantic_types st on sg.tui = st.tui where \" + corpus + \"_name in ({})\"\\\n",
    "           .format(', '.join(['%s' for _ in semtypes]))  \n",
    "        \n",
    "        stypes = pd.read_sql(sql, params=[semtypes], con=engine) \n",
    "       \n",
    "        if len(stypes['tui'].tolist()) > 0:\n",
    "            self.biomedicus_types = set(stypes['tui'].tolist())\n",
    "            self.qumls_types = set(stypes['tui'].tolist())\n",
    "        else:\n",
    "            self.biomedicus_types = None\n",
    "            self.qumls_types = None\n",
    "        \n",
    "        if stypes['clamp_name'].dropna(inplace=True) or len(stypes['clamp_name']) == 0:\n",
    "            self.clamp_types = None\n",
    "        else:\n",
    "            self.clamp_types = set(stypes['clamp_name'].tolist()[0].split(','))\n",
    "            \n",
    "        if len(stypes['ctakes_name'].tolist()) > 0:\n",
    "            self.ctakes_types = set(stypes['ctakes_name'].tolist()[0].split(','))\n",
    "        else:\n",
    "            self.ctakes_types = None\n",
    "            \n",
    "        if len(stypes['abbreviation'].tolist()) > 0:\n",
    "            self.metamap_types = set(stypes['abbreviation'].tolist())\n",
    "        else:\n",
    "            self.metamap_types = None\n",
    "            \n",
    "        self.reference_types =  set(semtypes)\n",
    "    \n",
    "    def get_system_type(self, system):  \n",
    "        \n",
    "        if system == 'biomedicus':\n",
    "            semtypes = self.biomedicus_types\n",
    "        elif system == 'ctakes':\n",
    "            semtypes = self.ctakes_types\n",
    "        elif system == 'clamp':\n",
    "            semtypes = self.clamp_types\n",
    "        elif system == 'metamap':\n",
    "            semtypes = self.metamap_types\n",
    "        elif system == 'quick_umls':\n",
    "            semtypes = self.qumls_types\n",
    "        elif system == 'reference':\n",
    "            semtypes = self.reference_types\n",
    "            \n",
    "        return semtypes\n",
    "    \n",
    "# print(SemanticTypes(['Drug'], corpus).get_system_type('biomedicus'))\n",
    "# print(SemanticTypes(['Drug'], corpus).get_system_type('quick_umls'))\n",
    "# print(SemanticTypes(['Anatomy'], corpus).get_system_type('clamp'))\n",
    "print(SemanticTypes(['test,treatment'], 'i2b2').get_system_type('clamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semtypes = ['test,treatment']\n",
    "# semtypes = ['problem']\n",
    "# corpus = 'i2b2'\n",
    "# sys = 'clamp'\n",
    "\n",
    "# is semantic type in particular system\n",
    "def system_semtype_check(sys, semtype, corpus):\n",
    "    st = SemanticTypes([semtype], corpus).get_system_type(sys)\n",
    "    if st:\n",
    "        return sys\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# print(system_semtype_check(sys, semtypes, corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation class for systems\n",
    "class AnnotationSystems():\n",
    "    \"\"\"   \n",
    "    System annotations of interest for UMLS concept extraction\n",
    "    NB: ctakes combines all \"mentions\" annotation types\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\" \n",
    "        annotation base types\n",
    "        \"\"\"   \n",
    "        \n",
    "        self.biomedicus_types = [\"biomedicus.v2.UmlsConcept\"]\n",
    "        self.clamp_types = [\"edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA\"]\n",
    "        self.ctakes_types = [\"ctakes_mentions\"]\n",
    "        self.metamap_types = [\"org.metamap.uima.ts.Candidate\"]\n",
    "        self.qumls_types = [\"concept_jaccard_score_False\"]\n",
    "       \n",
    "    def get_system_type(self, system):\n",
    "        \n",
    "        \"\"\"\n",
    "        return system types\n",
    "        \"\"\"\n",
    "        \n",
    "        if system == \"biomedicus\":\n",
    "            view = \"Analysis\"\n",
    "        else:\n",
    "            view = \"_InitialView\"\n",
    "\n",
    "        if system == 'biomedicus':\n",
    "            types = self.biomedicus_types\n",
    "\n",
    "        elif system == 'clamp':\n",
    "            types = self.clamp_types\n",
    "\n",
    "        elif system == 'ctakes':\n",
    "            types = self.ctakes_types\n",
    "\n",
    "        elif system == 'metamap':\n",
    "            types = self.metamap_types\n",
    "        \n",
    "        elif system == \"quick_umls\":\n",
    "            types = self.qumls_types\n",
    "            \n",
    "        return types, view\n",
    "    \n",
    "annSys = AnnotationSystems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np # access to Numpy from Python layer\n",
    "import math\n",
    "\n",
    "class Metrics(object):\n",
    "    \"\"\"\n",
    "    metrics class:\n",
    "    returns an instance with confusion matrix metrics\n",
    "    \"\"\"\n",
    "    def __init__(self, system_only, gold_only, gold_system_match, system_n, neither = 0): # neither: no sys or manual annotation\n",
    "\n",
    "        self = self    \n",
    "        self.system_only = system_only\n",
    "        self.gold_only = gold_only\n",
    "        self.gold_system_match = gold_system_match\n",
    "        self.system_n = system_n\n",
    "        self.neither = neither\n",
    "        \n",
    "    def get_confusion_metrics(self, corpus = None, test = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        compute confusion matrix measures, as per  \n",
    "        https://stats.stackexchange.com/questions/51296/how-do-you-calculate-precision-and-recall-for-multiclass-classification-using-co\n",
    "        \"\"\"\n",
    "        cdef:\n",
    "            int TP, FP, FN\n",
    "            double TM\n",
    "\n",
    "        TP = self.gold_system_match\n",
    "        FP = self.system_only\n",
    "        FN = self.gold_only\n",
    "        \n",
    "        TM = TP/math.sqrt(self.system_n) # TigMetric\n",
    "       \n",
    "        if not test:\n",
    "            \n",
    "            if corpus == 'casi':\n",
    "                recall = TP/(TP + FN)\n",
    "                precision = TP/(TP + FP)\n",
    "                F = 2*(precision*recall)/(precision + recall)\n",
    "            else:\n",
    "                if self.neither == 0:\n",
    "                    confusion = [[0, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "                else:\n",
    "                    confusion = [[self.neither, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "                c = np.asarray(confusion)\n",
    "                recall = np.diag(c) / np.sum(c, axis = 1)\n",
    "                precision = np.diag(c) / np.sum(c, axis = 0)\n",
    "                F = 2*(precision*recall)/(precision + recall)\n",
    "        else:\n",
    "            precision = TP/(TP+FP)\n",
    "            recall = TP/(TP+FN)\n",
    "            F = 2*(precision*recall)/(precision + recall)\n",
    "        \n",
    "        # Tignanelli Metric\n",
    "        if FN == 0:\n",
    "            TP_FN_R = TP\n",
    "        elif FN > 0:\n",
    "            TP_FN_R = TP/FN\n",
    " \n",
    "        return F, recall, precision, TP, FP, FN, TP_FN_R, TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out(name: str, analysis_type: str, c: object):\n",
    "   \n",
    "    \"\"\"\n",
    "    write matching and reference-only sets to file for use in merging combinations\n",
    "    \"\"\"\n",
    "    \n",
    "    # write output to file\n",
    "    dir_out = analysisConf.data_dir + 'single_system_out/'\n",
    "    with open(dir_out + name + '_' + analysis_type + '_' + c.corpus + '_matches.txt', 'w') as f:\n",
    "        for item in list(c.matches):\n",
    "            f.write(\"%s\\n\" % str(item))\n",
    "\n",
    "    # write to file\n",
    "    with open(dir_out + name + '_' + analysis_type + '_' + c.corpus + '_ref_only.txt', 'w') as f:\n",
    "        for item in list(c.false_negatives):\n",
    "            f.write(\"%s\\n\" % str(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_set(df, analysis_type = 'entity', df_type = 'sys', corpus = None):\n",
    "    \n",
    "    # get values for creation of series of type tuple\n",
    "    if 'entity' in analysis_type: \n",
    "        if corpus == 'casi':\n",
    "            arg = df.case, df.overlap\n",
    "        else:    \n",
    "            if df_type == 'sys':\n",
    "                arg = df.begin, df.end, df.note_id\n",
    "            else:\n",
    "                arg = df.start, df.end, df.file\n",
    "            \n",
    "    elif 'cui' in analysis_type:\n",
    "        if df_type == 'sys':\n",
    "            arg = df.cui, df.note_id\n",
    "        else:\n",
    "            arg = df.value, df.file\n",
    "    elif 'full' in analysis_type:\n",
    "        if df_type == 'sys':\n",
    "            arg = df.begin, df.end, df.cui, df.note_id\n",
    "        else:\n",
    "            arg = df.start, df.end, df.value, df.file\n",
    "    \n",
    "    return set(list(zip(*arg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython \n",
    "\n",
    "from __main__ import write_out, df_to_set, engine\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "def get_cooccurences(ref, sys, analysis_type: str, corpus: str, single_sys = True, name = None):\n",
    "    \"\"\"\n",
    "    get coocurences between system and reference; exact match; TODO: add relaxed -> done in single system evals during ensemble run\n",
    "    \"\"\"\n",
    "    # cooccurences\n",
    "    class Coocurences(object):\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.ref_system_match = 0\n",
    "            self.ref_only = 0\n",
    "            self.system_only = 0\n",
    "            self.system_n = 0\n",
    "            self.ref_n = 0\n",
    "            self.matches = set()\n",
    "            self.false_negatives = set()\n",
    "            self.corpus = corpus\n",
    "\n",
    "    c = Coocurences()\n",
    "    \n",
    "    if c.corpus != 'casi':\n",
    "        if 'entity' in analysis_type and single_sys: # mipacq n -> 16793\n",
    "            cols_to_keep = ['begin', 'end', 'note_id']\n",
    "            sys = sys[cols_to_keep].drop_duplicates()\n",
    "            ref = ref[['start', 'end', 'file']].drop_duplicates()\n",
    "            sys.name = name\n",
    "        elif 'cui' in analysis_type and single_sys: # mipacq n -> 10799\n",
    "            cols_to_keep = ['cui', 'note_id']\n",
    "            sys = sys[cols_to_keep].drop_duplicates()\n",
    "            # do not overestimate FP\n",
    "            sys = sys[~sys['cui'].isnull()] \n",
    "            ref = ref[['value', 'file']].drop_duplicates()\n",
    "            ref = ref[~ref['value'].isnull()]\n",
    "            sys.name = name\n",
    "        elif 'full' in analysis_type and single_sys: # mipacq n -> 17393\n",
    "            cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "            sys = sys[cols_to_keep].drop_duplicates()\n",
    "            sys = sys[~sys['cui'].isnull()]\n",
    "            ref = ref[['start', 'end', 'value', 'file']].drop_duplicates()\n",
    "            ref = ref[~ref['value'].isnull()]\n",
    "            sys.name = name\n",
    "\n",
    "        # matches via inner join\n",
    "        matches = pd.merge(sys, ref, how = 'inner', left_on=['begin','end','note_id'], right_on = ['start','end','file']) \n",
    "        # reference-only via left outer join\n",
    "        fn = pd.merge(ref, sys, how = 'left', left_on=['start','end','file'], right_on = ['begin','end','note_id']) \n",
    "\n",
    "        fn = fn[fn['begin'].isnull()] # get as outer join with no match\n",
    "\n",
    "        if 'entity' in analysis_type:\n",
    "            cols_to_keep = ['start', 'end', 'file']\n",
    "        else:\n",
    "            cols_to_keep = ['start', 'end', 'value', 'file']\n",
    "\n",
    "        matches = matches[cols_to_keep]\n",
    "        fn = fn[cols_to_keep]\n",
    "\n",
    "        # use for metrics \n",
    "        c.matches = c.matches.union(df_to_set(matches, analysis_type, 'ref'))\n",
    "        c.false_negatives = c.false_negatives.union(df_to_set(fn, analysis_type, 'ref'))\n",
    "        c.ref_system_match = len(c.matches)\n",
    "        c.system_only = len(sys) - len(c.matches)\n",
    "        c.system_n = len(sys)\n",
    "        c.ref_n = len(ref)\n",
    "        c.ref_only = len(c.false_negatives)\n",
    "        \n",
    "    else:\n",
    "        sql = \"select `case` from test.amia_2019_analytical_v where overlap = 1 and `system` = %(sys.name)s\"  \n",
    "        \n",
    "        matches = pd.read_sql(sql, params={\"sys.name\":sys.name}, con=engine)\n",
    "        \n",
    "        sql = \"select `case` from test.amia_2019_analytical_v where (overlap = 0 or overlap is null) and `system` = %(sys.name)s\"  \n",
    "        \n",
    "        fn = pd.read_sql(sql, params={\"sys.name\":sys.name}, con=engine)\n",
    "        \n",
    "        c.matches = df_to_set(matches, 'entity', 'sys', 'casi')\n",
    "        c.fn = df_to_set(fn, 'entity', 'sys', 'casi')\n",
    "        c.ref_system_match = len(c.matches)\n",
    "        c.system_only = len(sys) - len(c.matches)\n",
    "        c.system_n = len(matches) + len(fn)\n",
    "        c.ref_n = len(matches) + len(fn)\n",
    "        c.ref_only = len(fn)\n",
    "        \n",
    "    # sanity check\n",
    "    if len(ref) - c.ref_system_match < 0:\n",
    "        print('Error: ref_system_match > len(ref)!')\n",
    "    if len(ref) != c.ref_system_match + c.ref_only:\n",
    "        print('Error: ref count mismatch!')\n",
    "   \n",
    "    # save TP/FN\n",
    "    if single_sys and corpus != 'casi':\n",
    "        print(analysis_type)\n",
    "        write_out(sys.name, analysis_type, c)\n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_vector(doc: str, ann: List[int], labels: List[str]) -> np.array:\n",
    "\n",
    "    v = np.zeros(doc)\n",
    "    labels = list(labels)\n",
    "    \n",
    "    for (i, lab) in enumerate(labels):\n",
    "        i += 1  # 0 is reserved for no label\n",
    "        idxs = [np.arange(a.begin, a.end) for a in ann if a.label == lab]\n",
    "        idxs = [j for mask in idxs for j in mask]\n",
    "        v[idxs] = i\n",
    "\n",
    "    return v\n",
    "\n",
    "# test confusion matrix elements for vectorized annotation set; includes TN\n",
    "# https://kawahara.ca/how-to-compute-truefalse-positives-and-truefalse-negatives-in-python-for-binary-classification-problems/\n",
    "def confused(sys1, ann1):\n",
    "    TP = np.sum(np.logical_and(ann1 == 1, sys1 == 1))\n",
    "\n",
    "    # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "    TN = np.sum(np.logical_and(ann1 == 0, sys1 == 0))\n",
    "\n",
    "    # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "    FP = np.sum(np.logical_and(ann1 == 0, sys1 == 1))\n",
    "\n",
    "    # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "    FN = np.sum(np.logical_and(ann1 == 1, sys1 == 0))\n",
    "    \n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def vectorized_coocurences(r: object, analysis_type: str, corpus: str, filter_semtype = None, semtype = None) -> np.int64:\n",
    "    docs = get_docs(corpus)\n",
    "    if filter_semtype:\n",
    "        ann = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "    else: \n",
    "        ann = get_ref_ann(analysis_type, corpus)\n",
    "        \n",
    "    sys = get_sys_ann(r)\n",
    "    cvals = []\n",
    "    labels = [\"concept\"]\n",
    "\n",
    "    for n in range(len(docs)):\n",
    "        a1 = list(ann.loc[ann[\"case\"] == docs[n][0]].itertuples(index=False))\n",
    "        s1 = list(sys.loc[sys[\"case\"] == docs[n][0]].itertuples(index=False))\n",
    "        ann1 = label_vector(docs[n][1], a1, labels)\n",
    "        sys1 = label_vector(docs[n][1], s1, labels)\n",
    "\n",
    "        TP, TN, FP, FN = confused(sys1, ann1)\n",
    "        cvals.append([TP, TN, FP, FN])\n",
    "\n",
    "    return np.sum(cvals, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm_dict(ref_only: int, system_only: int, ref_system_match: int, system_n: int, ref_n: int) -> dict:\n",
    "    \"\"\"\n",
    "    Generate dictionary of confusion matrix params and measures\n",
    "    :params: ref_only, system_only, reference_system_match -> sets\n",
    "    matches, system_n, reference_n -> counts\n",
    "    :return: dictionary object\n",
    "    \"\"\"\n",
    "\n",
    "    if ref_only + ref_system_match != ref_n:\n",
    "        print('ERROR!')\n",
    "\n",
    "    # get evaluation metrics\n",
    "    F, recall, precision, TP, FP, FN, TP_FN_R, TM  = Metrics(system_only, ref_only, ref_system_match, system_n).get_confusion_metrics()\n",
    "\n",
    "    d = {\n",
    "         'F': F[1], \n",
    "         'precision': precision[1], \n",
    "         'recall': recall[1], \n",
    "         'TP': TP, \n",
    "         'FN': FN, \n",
    "         'FP': FP, \n",
    "         'TP/FN': TP_FN_R,\n",
    "         'n_gold': ref_n, \n",
    "         'n_sys': system_n, \n",
    "         'TM': TM\n",
    "    }\n",
    "    \n",
    "    if system_n - FP != TP:\n",
    "        print('inconsistent system n!')\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ft.lru_cache(maxsize=None)\n",
    "def get_metric_data(analysis_type: str, corpus: str):\n",
    "   \n",
    "    usys_file, ref_table = AnalysisConfig().corpus_config()\n",
    "    systems = AnalysisConfig().systems\n",
    "    \n",
    "    sys_ann = pd.read_csv(analysisConf.data_dir + usys_file, dtype={'note_id': str})\n",
    "    \n",
    "    sql = \"SELECT * FROM \" + ref_table  \n",
    "    \n",
    "    ref_ann = pd.read_sql(sql, con=engine)\n",
    "    sys_ann = sys_ann.drop_duplicates()\n",
    "    \n",
    "    return ref_ann, sys_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "def geometric_mean(metrics):\n",
    "    \"\"\"\n",
    "    1. Get rank average of F1, TP/FN, TM\n",
    "        http://www.datasciencemadesimple.com/rank-dataframe-python-pandas-min-max-dense-rank-group/\n",
    "        https://stackoverflow.com/questions/46686315/in-pandas-how-to-create-a-new-column-with-a-rank-according-to-the-mean-values-o?rq=1\n",
    "    2. Take geomean of 2.\n",
    "        https://stackoverflow.com/questions/42436577/geometric-mean-applied-on-row\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.DataFrame() \n",
    "\n",
    "    metrics['F1 rank']=metrics['F'].rank(ascending=0,method='average')\n",
    "    metrics['TP/FN rank']=metrics['TP/FN'].rank(ascending=0,method='average')\n",
    "    metrics['TM rank']=metrics['TM'].rank(ascending=0,method='average')\n",
    "    metrics['Gmean'] = gmean(metrics.iloc[:,-3:],axis=1)\n",
    "\n",
    "    return metrics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metrics(analysis_type: str, corpus: str, filter_semtype = None, single_sys = None):\n",
    "    start = time.time()\n",
    "\n",
    "    systems = AnalysisConfig().systems\n",
    "    metrics = pd.DataFrame()\n",
    "\n",
    "    ref_ann, sys_ann = get_metric_data(analysis_type, corpus)\n",
    "    \n",
    "    for sys in systems:\n",
    "        \n",
    "        if filter_semtype:\n",
    "            st = SemanticTypes(semtypes, corpus).get_system_type(sys)\n",
    "            ref_ann = ref_ann[ref_ann['semtype'].isin(SemanticTypes(semtypes, corpus).get_system_type('reference'))]\n",
    "            \n",
    "        types, _ = AnnotationSystems().get_system_type(sys) # system types for iterable\n",
    "        for t in types:\n",
    "            print(t)\n",
    "\n",
    "            if filter_semtype:\n",
    "                system_annotations = sys_ann[sys_ann['semtypes'].isin(st)].copy()\n",
    "            else:\n",
    "                system_annotations = sys_ann.copy()\n",
    "\n",
    "            system = system_annotations[system_annotations['type'] == str(t)]\n",
    "\n",
    "            if sys == 'quick_umls':\n",
    "                system = system[system.score.astype(float) >= .8]\n",
    "\n",
    "            if sys == 'metamap':\n",
    "                system = system[system.score.abs().astype(int) >= 800]\n",
    "\n",
    "            system = system.drop_duplicates()\n",
    "            system.name = sys\n",
    "\n",
    "            c = get_cooccurences(ref_ann, system, analysis_type, corpus, True, system.name) # get matches, FN, etc.\n",
    "\n",
    "        if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "            if corpus == 'casi':\n",
    "                if sys == 'biomedicus':\n",
    "                    t = 'biomedicus.v2.Acronym'\n",
    "            \n",
    "            # get dictionary of confusion matrix metrics\n",
    "            d = cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n)\n",
    "            d['system'] = sys\n",
    "            d['type'] = t\n",
    "                \n",
    "            data = pd.DataFrame(d,  index=[0])\n",
    "            metrics = pd.concat([metrics, data], ignore_index=True)\n",
    "            metrics.drop_duplicates(keep='last', inplace=True)\n",
    "        else:\n",
    "            print(\"NO EXACT MATCHES FOR\", t)\n",
    "        elapsed = (time.time() - start)\n",
    "        print(\"elapsed:\", sys, elapsed)\n",
    "     \n",
    "    elapsed = (time.time() - start)\n",
    "    print(geometric_mean(metrics))\n",
    "    \n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    \n",
    "    if single_sys is None:\n",
    "        file_name = 'metrics_'\n",
    "    \n",
    "    metrics.to_csv(analysisConf.data_dir + corpus + '_' + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "    \n",
    "    print(\"total elapsed time:\", elapsed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ft.lru_cache(maxsize=None)\n",
    "def get_ref_n(analysis_type: str, corpus: str, filter_semtype = None) -> int:\n",
    "    \n",
    "    ref_ann, _ = get_metric_data(analysis_type, corpus)\n",
    "    \n",
    "    if filter_semtype:\n",
    "        ref_ann = ref_ann[ref_ann['semtype'].isin(SemanticTypes(semtypes, corpus).get_system_type('reference'))]\n",
    "            \n",
    "    if corpus == 'casi':\n",
    "        return len(ref_ann)\n",
    "        \n",
    "    else:\n",
    "        # do not overestimate fn\n",
    "        if 'entity' in analysis_type:\n",
    "            ref_ann = ref_ann[['start', 'end', 'file']].drop_duplicates()\n",
    "        elif 'cui' in analysis_type:\n",
    "            ref_ann = ref_ann[['value', 'file']].drop_duplicates()\n",
    "        elif 'full' in analysis_type:\n",
    "            ref_ann = ref_ann[['start', 'end', 'value', 'file']].drop_duplicates()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        ref_n = len(ref_ann.drop_duplicates())\n",
    "\n",
    "        return ref_n\n",
    "    \n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_sys_data(system: str, analysis_type: str, corpus: str, filter_semtype = None, semtype = None) -> pd.DataFrame:\n",
    "   \n",
    "    _, data = get_metric_data(analysis_type, corpus)\n",
    "    \n",
    "    out = data[data['system'] == system].copy()\n",
    "    \n",
    "    if filter_semtype:\n",
    "        st = SemanticTypes([semtype], corpus).get_system_type(system)\n",
    "    \n",
    "    if corpus == 'casi':\n",
    "        cols_to_keep = ['case', 'overlap'] \n",
    "        out = out[cols_to_keep].drop_duplicates()\n",
    "        return out\n",
    "        \n",
    "    else:\n",
    "        if filter_semtype:\n",
    "            out = out[out['semtypes'].isin(st)].copy()\n",
    "            \n",
    "        else:\n",
    "            out = out[out['system']== system].copy()\n",
    "        \n",
    "        if system == 'quick_umls':\n",
    "            out = out[(out.score.astype(float) >= 0.8) & (out[\"type\"] == 'concept_jaccard_score_False')]\n",
    "            # fix for leading spacxe on semantic type field\n",
    "            out = out.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x) \n",
    "        \n",
    "        if system == 'metamap':\n",
    "            out = out[out.score.abs().astype(int) >= 800]\n",
    "            \n",
    "        if 'entity' in analysis_type:\n",
    "            cols_to_keep = ['begin', 'end', 'note_id']\n",
    "        elif 'cui' in analysis_type:\n",
    "            cols_to_keep = ['cui', 'note_id']\n",
    "        elif 'full' in analysis_type:\n",
    "            cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "\n",
    "        #out = out[cols_to_keep]\n",
    "\n",
    "        return out.drop_duplicates()\n",
    "\n",
    "# read in system/reference matches from file\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_system_matches(system: str, analysis_type: str, corpus: str):\n",
    "   \n",
    "    if corpus == 'casi':\n",
    "        \n",
    "        sql = \"select `case`, overlap from test.amia_2019_cases where overlap = 1 and `system` = %(system)s\"  \n",
    "        data_matches = df_to_set(pd.read_sql(sql, params={\"system\":system}, con=engine), 'entity', 'sys', 'casi')\n",
    "        \n",
    "        sql = \"select `case`, overlap from test.amia_2019_cases where (overlap = 0 or overlap is null) and `system` = %(system)s\"  \n",
    "        data_fn = df_to_set(pd.read_sql(sql, params={\"system\":system}, con=engine), 'entity', 'sys', 'casi')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        dir_test = analysisConf.data_dir + 'single_system_out/'\n",
    "\n",
    "        file = dir_test + system + '_' + analysis_type + '_' + corpus + '_matches.txt'\n",
    "        data_matches = set(literal_eval(line.strip()) for line in open(file))\n",
    "\n",
    "        file = dir_test + system + '_' + analysis_type + '_' + corpus + '_ref_only.txt'\n",
    "        data_fn = set(literal_eval(line.strip()) for line in open(file)) \n",
    "\n",
    "    return data_matches, data_fn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GENERATE merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetTotals(object):\n",
    "    \"\"\" \n",
    "    returns an instance with merged match set numbers using either union or intersection of elements in set \n",
    "    \"\"\"\n",
    "    def __init__(self, ref_n, sys_n, match_set):\n",
    "\n",
    "        self = self    \n",
    "        self.ref_ann = ref_n\n",
    "        self.sys_n = sys_n\n",
    "        self.match_set = match_set\n",
    "\n",
    "    def get_ref_sys(self):\n",
    "\n",
    "        ref_only = self.ref_ann - len(self.match_set)\n",
    "        sys_only = self.sys_n - len(self.match_set)\n",
    "\n",
    "        return ref_only, sys_only, len(self.match_set), self.match_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ft.lru_cache(maxsize=None)\n",
    "def process_sentence(pt, sentence, analysis_type, corpus, filter_semtype = None, semtype = None):\n",
    "    \"\"\"\n",
    "    Recursively evaluate parse tree, \n",
    "    with check for existence before build\n",
    "       :param sentence: to process\n",
    "       :return class of merged annotations, boolean operated system df \n",
    "    \"\"\"\n",
    "    \n",
    "    class Results(object):\n",
    "        def __init__(self):\n",
    "            self.results = set()\n",
    "            self.system_merges = pd.DataFrame()\n",
    "            \n",
    "    r = Results()\n",
    "    \n",
    "    if 'entity' in analysis_type and corpus != 'casi': \n",
    "        cols_to_keep = ['begin', 'end', 'note_id'] # entity only\n",
    "    elif 'full' in analysis_type: \n",
    "        cols_to_keep = ['cui', 'begin', 'end', 'note_id'] # entity only\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['cui', 'note_id'] # entity only\n",
    "    elif corpus == 'casi':\n",
    "        cols_to_keep = ['case', 'overlap']\n",
    "    \n",
    "    def evaluate(parseTree):\n",
    "        oper = {'&': op.and_, '|': op.or_}\n",
    "        \n",
    "        if parseTree:\n",
    "            leftC = gevent.spawn(evaluate, parseTree.getLeftChild())\n",
    "            rightC = gevent.spawn(evaluate, parseTree.getRightChild())\n",
    "            \n",
    "            if leftC.get() and rightC.get():\n",
    "                query = set()\n",
    "                system_query = pd.DataFrame()\n",
    "                fn = oper[parseTree.getRootVal()]\n",
    "                \n",
    "                if isinstance(leftC.get(), str):\n",
    "                    # get system as leaf node \n",
    "                    left, _ = get_system_matches(leftC.get(), analysis_type, corpus)\n",
    "                    if filter_semtype:\n",
    "                        left_sys = get_sys_data(leftC.get(), analysis_type, corpus, filter_semtype, semtype)\n",
    "                    else:\n",
    "                        left_sys = get_sys_data(leftC.get(), analysis_type, corpus)\n",
    "                \n",
    "                elif isinstance(leftC.get(), tuple):\n",
    "                    left = leftC.get()[0]\n",
    "                    l_sys = leftC.get()[1]\n",
    "                \n",
    "                if isinstance(rightC.get(), str):\n",
    "                    # get system as leaf node\n",
    "                    right, _ = get_system_matches(rightC.get(), analysis_type, corpus)\n",
    "                    if filter_semtype:\n",
    "                        right_sys = get_sys_data(rightC.get(), analysis_type, corpus, filter_semtype, semtype)\n",
    "                    else:\n",
    "                        right_sys = get_sys_data(rightC.get(), analysis_type, corpus)\n",
    "                    \n",
    "                elif isinstance(rightC.get(), tuple):\n",
    "                    right = rightC.get()[0]\n",
    "                    r_sys = rightC.get()[1]\n",
    "                    \n",
    "                # create match set based on boolean operation\n",
    "                match_set = fn(left, right)\n",
    "               \n",
    "                if fn == op.or_:\n",
    "                    r.results = r.results.union(match_set)\n",
    "\n",
    "                    if isinstance(leftC.get(), str) and isinstance(rightC.get(), str):\n",
    "                        frames = [left_sys, right_sys]\n",
    "                        df = pd.concat(frames,  ignore_index=True)\n",
    "                        df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                    elif isinstance(leftC.get(), str) and isinstance(rightC.get(), tuple):\n",
    "                        frames = [left_sys, r_sys]\n",
    "                        df = pd.concat(frames,  ignore_index=True)\n",
    "                        df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                    elif isinstance(leftC.get(), tuple) and isinstance(rightC.get(), str):\n",
    "                        frames = [l_sys, right_sys]\n",
    "                        df = pd.concat(frames,  ignore_index=True)\n",
    "                        df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                    elif isinstance(leftC.get(), tuple) and isinstance(rightC.get(), tuple):\n",
    "                        frames = [l_sys, r_sys]\n",
    "                        df = pd.concat(frames,  ignore_index=True)\n",
    "                        df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                if fn == op.and_:\n",
    "                    if len(r.results) == 0:\n",
    "                        r.results = match_set\n",
    "                    r.results = r.results.intersection(match_set)\n",
    "\n",
    "                    if isinstance(leftC.get(), str) and isinstance(rightC.get(), str):\n",
    "                        df = left_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                        df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                    elif isinstance(leftC.get(), str) and isinstance(rightC.get(), tuple):\n",
    "                        df = left_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                        df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                    elif isinstance(leftC.get(), tuple) and isinstance(rightC.get(), str):\n",
    "                        df = l_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                        df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                    elif isinstance(leftC.get(), tuple) and isinstance(rightC.get(), tuple):\n",
    "                        df = l_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                        df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "                \n",
    "                # get matched results\n",
    "                query.update(r.results)\n",
    "                \n",
    "                # get combined system results\n",
    "                r.system_merges = df\n",
    "                \n",
    "                if len(df) > 0:\n",
    "                    system_query = system_query.append(df)\n",
    "                else:\n",
    "                    print('wtf!')\n",
    "                    \n",
    "                return query, system_query\n",
    "            else:\n",
    "                return parseTree.getRootVal()\n",
    "    \n",
    "    if sentence.n_or > 0 or sentence.n_and > 0:\n",
    "        evaluate(pt)  \n",
    "    \n",
    "    # trivial case\n",
    "    elif sentence.n_or == 0 and sentence.n_and == 0:\n",
    "        r.results, _ = get_system_matches(sentence.sentence, analysis_type, corpus)\n",
    "        if filter_semtype:\n",
    "            r.system_merges = get_sys_data(sentence.sentence, analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "            r.system_merges = get_sys_data(sentence.sentence, analysis_type, corpus)\n",
    "            \n",
    "        #print('trivial:', sentence.sentence, len(r.results), len(r.system_merges))\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Incoming Boolean sentences are parsed into a binary tree.\n",
    "\n",
    "Test expressions to parse:\n",
    "\n",
    "sentence = '((((A&B)|C)|D)&E)'\n",
    "\n",
    "sentence = '(E&(D|(C|(A&B))))'\n",
    "\n",
    "sentence = '(((A|(B&C))|(D&(E&F)))|(H&I))'\n",
    "\n",
    "\"\"\"\n",
    "# build parse tree from passed sentence\n",
    "# using grammatical rules of Boolean logic\n",
    "def buildParseTree(fpexp):\n",
    "    \"\"\"\n",
    "       Iteratively build parse tree from passed sentence using grammatical rules of Boolean logic\n",
    "       :param fpexp: sentence to parse\n",
    "       :return eTree: parse tree representation\n",
    "       Incoming Boolean sentences are parsed into a binary tree.\n",
    "       Test expressions to parse:\n",
    "       sentence = '(A&B)'\n",
    "       sentence = '(A|B)'\n",
    "       sentence = '((A|B)&C)'\n",
    "       \n",
    "    \"\"\"\n",
    "    fplist = fpexp.split()\n",
    "    pStack = Stack()\n",
    "    eTree = BinaryTree('')\n",
    "    pStack.push(eTree)\n",
    "    currentTree = eTree\n",
    "\n",
    "    for i in fplist:\n",
    "\n",
    "        if i == '(':\n",
    "            currentTree.insertLeft('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getLeftChild()\n",
    "        elif i not in ['&', '|', ')']:\n",
    "            currentTree.setRootVal(i)\n",
    "            parent = pStack.pop()\n",
    "            currentTree = parent\n",
    "        elif i in ['&', '|']:\n",
    "            currentTree.setRootVal(i)\n",
    "            currentTree.insertRight('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getRightChild()\n",
    "        elif i == ')':\n",
    "            currentTree = pStack.pop()\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    return eTree\n",
    "\n",
    "def make_parse_tree(payload):\n",
    "    \"\"\"\n",
    "    Ensure data to create tree are in standard form\n",
    "    :param sentence: sentence to preprocess\n",
    "    :return pt, parse tree graph\n",
    "            sentence, processed sentence to build tree\n",
    "            a: order\n",
    "    \"\"\"\n",
    "    def preprocess_sentence(sentence):\n",
    "        # prepare statement for case when a boolean AND/OR is given\n",
    "        sentence = payload.replace('(', ' ( '). \\\n",
    "            replace(')', ' ) '). \\\n",
    "            replace('&', ' & '). \\\n",
    "            replace('|', ' | '). \\\n",
    "            replace('  ', ' ')\n",
    "        return sentence\n",
    "\n",
    "    sentence = preprocess_sentence(payload)\n",
    "    print(sentence)\n",
    "    \n",
    "    pt = buildParseTree(sentence)\n",
    "    #pt.postorder() \n",
    "    \n",
    "    return pt\n",
    "\n",
    "class Sentence(object):\n",
    "    def __init__(self, sentence):\n",
    "        self = self\n",
    "        self.n_and = sentence.count('&')\n",
    "        self.n_or = sentence.count('|')\n",
    "        self.sentence = sentence\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_docs(corpus):\n",
    "    sql = 'select distinct note_id, sofa from sofas where corpus = %(corpus)s order by note_id'\n",
    "    df = pd.read_sql(sql, params={\"corpus\":corpus}, con=engine)\n",
    "    df.drop_duplicates()\n",
    "    df['len_doc'] = df['sofa'].apply(len)\n",
    "    \n",
    "    subset = df[['note_id', 'len_doc']]\n",
    "    docs = [tuple(x) for x in subset.to_numpy()]\n",
    "    \n",
    "    return docs\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_ref_ann(analysis_type, corpus, filter_semtype = None, semtype = None):\n",
    "    \n",
    "    if filter_semtype:\n",
    "        if ',' in semtype:\n",
    "            semtype = semtype.split(',')\n",
    "        else:\n",
    "            semtype = [semtype]\n",
    "        \n",
    "    ann, _ = get_metric_data(analysis_type, corpus)\n",
    "    ann = ann.rename(index=str, columns={\"start\": \"begin\", \"file\": \"case\"})\n",
    "    \n",
    "    if filter_semtype:\n",
    "        ann = ann[ann['semtype'].isin(semtype)]\n",
    "        \n",
    "    ann[\"label\"] = 'concept'\n",
    "    cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "    ann = ann[cols_to_keep]\n",
    "    \n",
    "    return ann\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_sys_ann(r):\n",
    "    sys = r.system_merges    \n",
    "    sys = sys.rename(index=str, columns={\"note_id\": \"case\"})\n",
    "    sys[\"label\"] = 'concept'\n",
    "    cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "    sys = sys[cols_to_keep]\n",
    "\n",
    "    return sys\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_metrics(boolean_expression: str, analysis_type: str, corpus: str, run_type: str, filter_semtype = None, semtype = None):\n",
    "    \"\"\"\n",
    "    Traverse binary parse tree representation of Boolean sentence\n",
    "        :params: boolean expression in form of '(<annotator_engine_name1><boolean operator><annotator_engine_name2>)'\n",
    "                 analysis_type (string value of: 'entity', 'cui', 'full') used to filter set of reference and system annotations \n",
    "        :return: dictionary with values needed for confusion matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence = Sentence(boolean_expression)   \n",
    "    pt = make_parse_tree(sentence.sentence)\n",
    "    \n",
    "    if filter_semtype:\n",
    "        r = process_sentence(pt, sentence, analysis_type, corpus, filter_semtype, semtype)\n",
    "    else:\n",
    "        r = process_sentence(pt, sentence, analysis_type, corpus)\n",
    "    \n",
    "    # vectorize merges using i-o labeling\n",
    "    if run_type == 'overlap':\n",
    "        \n",
    "        if filter_semtype:\n",
    "            TP, TN, FP, FN = vectorized_coocurences(r, analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "            TP, TN, FP, FN = vectorized_coocurences(r, analysis_type, corpus)\n",
    "\n",
    "        # TODO: validate against ann1/sys1 where val = 1\n",
    "        # total by number chars\n",
    "        system_n = TP + FP\n",
    "        reference_n = TP + FN\n",
    "\n",
    "        d = cm_dict(FN, FP, TP, system_n, reference_n)\n",
    "        \n",
    "        d['TN'] = TN\n",
    "        \n",
    "        print(d)\n",
    "            \n",
    "        # return full metrics\n",
    "        return d\n",
    "\n",
    "    elif run_type == 'exact':\n",
    "        # total by number spans\n",
    "        system_n = len(r.system_merges)\n",
    "        reference_n = get_ref_n(analysis_type, corpus, filter_semtype)\n",
    "\n",
    "        reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, r.results).get_ref_sys()\n",
    "        # get overall TP/TF and various other counts for running confusion matrix metric analysis\n",
    "        return cm_dict(reference_only, system_only, reference_system_match, system_n, reference_n)\n",
    "    \n",
    "def get_merge_data(boolean_expression: str, analysis_type: str, corpus: str, filter_semtype = None):\n",
    "    \"\"\"\n",
    "    Traverse binary parse tree representation of Boolean sentence\n",
    "        :params: boolean expression in form of '(<annotator_engine_name1><boolean operator><annotator_engine_name2>)'\n",
    "                 analysis_type (string value of: 'entity', 'cui', 'full') used to filter set of reference and system annotations \n",
    "        :return: dictionary with values needed for confusion matrix\n",
    "    \"\"\"\n",
    "    sentence = Sentence(boolean_expression)   \n",
    "\n",
    "    pt = make_parse_tree(sentence.sentence)\n",
    "    \n",
    "    if filter_semtype:\n",
    "        r = process_sentence(pt, sentence, analysis_type, corpus, filter_semtype, semtype)\n",
    "    else:\n",
    "        r = process_sentence(pt, sentence, analysis_type, corpus)\n",
    "    \n",
    "    system_n = len(r.system_merges, r.resu)\n",
    "    reference_n = get_ref_n(analysis_type, corpus, filter_semtype)\n",
    "\n",
    "    reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, r.results).get_ref_sys()\n",
    "\n",
    "    print(cm_dict(reference_only, system_only, reference_system_match, system_n, reference_n))\n",
    "    # get matched data from merge\n",
    "    return r.system_merges # merge_eval(reference_only, system_only, reference_system_match, system_n, reference_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all combinations of given list of annotators:\n",
    "def partly_unordered_permutations(lst, k):\n",
    "    elems = set(lst)\n",
    "    for c in combinations(lst, k):\n",
    "        for d in permutations(elems - set(c)):\n",
    "            yield c + d\n",
    "            \n",
    "def expressions(l, n):\n",
    "    for (operations, *operands), operators in product(\n",
    "            combinations(l, n), product(('&', '|'), repeat=n - 1)):\n",
    "        for operation in zip(operators, operands):\n",
    "            operations = [operations, *operation]\n",
    "        yield operations\n",
    "\n",
    "def flatten_list(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "# get list of systems with a semantic type in grouping\n",
    "def get_valid_systems(systems, semtype):\n",
    "    test = []\n",
    "    for sys in systems:\n",
    "        st = system_semtype_check(sys, semtype, corpus)\n",
    "        if st:\n",
    "            test.append(sys)\n",
    "\n",
    "    return test\n",
    "\n",
    "# permute system combinations and evaluate system merges for performance\n",
    "def run_ensemble(systems, analysis_type, corpus, filter_semtype = None, semtype = None):\n",
    "    metrics = pd.DataFrame()\n",
    "    for l in partly_unordered_permutations(systems, 2):\n",
    "        print('processing merge combo:', l)\n",
    "        for i in range(1, len(l)+1):\n",
    "            test = list(expressions(l, i))\n",
    "            for t in test:\n",
    "                if i > 1:\n",
    "                    # format Boolean sentence for parse tree \n",
    "                    t = '(' + \" \".join(str(x) for x in t).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "\n",
    "                if filter_semtype:\n",
    "                    d = get_metrics(t, analysis_type, corpus, run_type, filter_semtype, semtype)\n",
    "                else:\n",
    "                    d = get_metrics(t, analysis_type, corpus, run_type)\n",
    "                    \n",
    "                d['merge'] = t\n",
    "                d['n_terms'] = i\n",
    "\n",
    "                frames = [metrics, pd.DataFrame(d, index=[0])]\n",
    "                metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# write to file\n",
    "def generate_ensemble_metrics(metrics, analysis_type, corpus, filter_semtype = None, semtype = None):\n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    \n",
    "    file_name = corpus + '_all_merge_metrics_'\n",
    "   \n",
    "    # drop exact matches:\n",
    "    metrics = metrics.drop_duplicates()\n",
    "    metrics = metrics.sort_values(by=['n_terms', 'merge'])\n",
    "    #metrics = metrics.drop_duplicates(subset=['TP', 'FN', 'FP', 'n_sys', 'precision', 'recall', 'F', 'TM', 'TP/FN', 'TM', 'n_terms'])\n",
    "\n",
    "    file = file_name + analysis_type\n",
    "    \n",
    "    if run_type == 'overlap':\n",
    "        file += '_overlap_'\n",
    "    \n",
    "    elif run_type == 'exact':\n",
    "        file += '_exact_'\n",
    "    \n",
    "    if filter_semtype:\n",
    "        file += semtype\n",
    "    \n",
    "    geometric_mean(metrics).to_csv(analysisConf.data_dir + file + str(timestamp) + '.csv')\n",
    "    print(geometric_mean(metrics))\n",
    "    \n",
    "# control ensemble run\n",
    "def ensemble_control(systems, analysis_type, corpus, run_type, filter_semtype = None, semtypes = None):\n",
    "    \n",
    "    print(semtypes, systems)\n",
    "    if filter_semtype:\n",
    "        for semtype in semtypes:\n",
    "            test = get_valid_systems(systems, semtype)\n",
    "            print('SYSYEMS FOR SEMTYPE', semtype, 'ARE', test)\n",
    "            metrics = run_ensemble(test, analysis_type, corpus, filter_semtype, semtype)\n",
    "            generate_ensemble_metrics(metrics, analysis_type, corpus, filter_semtype, semtype)\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        metrics = run_ensemble(systems, analysis_type, corpus)\n",
    "        generate_ensemble_metrics(metrics, analysis_type, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTS -> ensemble:\n",
    "def test_match_consistency(matches, ref_only, ref_n, sys):\n",
    "    \"\"\"test for reference only/match set consistency:\n",
    "        params: match, system and reference only sets\"\"\"\n",
    "   \n",
    "    print('len', len(sys), len(matches), len(matches.union(sys)), len(matches.intersection(sys)))\n",
    "    assert len(matches.union(ref_only)) == ref_n, 'Reference annotation mismatch union'\n",
    "    assert len(matches.intersection(sys)) == len(matches), 'System annotation mismatch intersect'\n",
    "    assert len(matches.union(sys)) == len(sys), 'System annotation mismatch union'\n",
    "    assert len(matches.intersection(ref_only)) == 0, 'Reference annotation mismatch intersect'\n",
    "\n",
    "def test_systems(analysis_type, systems, corpus):\n",
    "    sys = df_to_set(get_sys_data(systems[0], analysis_type, corpus), analysis_type)\n",
    "    test_match_consistency(*get_system_matches(systems[0], analysis_type, corpus), get_ref_n(analysis_type), sys)\n",
    "    print('Match consistency:', len(sys),get_ref_n(analysis_type))\n",
    "\n",
    "def test_metrics(ref, sys_m, match_m):\n",
    "    test = True\n",
    "    reference_n = len(ref)\n",
    "    system_n = len(sys_m)\n",
    "\n",
    "    print('Test metrics:', type(reference_n), type(system_n), type(match_m))\n",
    "\n",
    "    reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, match_m).get_ref_sys()\n",
    "    F, recall, precision, _, _, _, _, _ = Metrics(system_only, reference_only, reference_system_match, system_n).get_confusion_metrics()\n",
    "    F_, recall_, precision_, _, _, _, _, _ = Metrics(system_only, reference_only, reference_system_match, system_n).get_confusion_metrics(test)\n",
    "\n",
    "    assert F[1] == F_, 'F1 issue'\n",
    "    assert recall[1] == recall_, 'recall issue'\n",
    "    assert precision[1] == precision_, 'precision issue'\n",
    "    print(F[1], F_)\n",
    "    print(recall[1], recall_)\n",
    "    print(precision[1], precision_)\n",
    "\n",
    "def test_count(analysis_type, corpus):\n",
    "    # test match counts:\n",
    "    ctakes, _ = get_system_matches('ctakes', analysis_type, corpus)\n",
    "    clamp, _ = get_system_matches('clamp', analysis_type, corpus)\n",
    "    b9, _ = get_system_matches('biomedicus', analysis_type, corpus)\n",
    "    mm, _ = get_system_matches('metamap', analysis_type, corpus)\n",
    "\n",
    "    print('count:', len(mm.intersection(b9.intersection(clamp.intersection(ctakes)))))\n",
    "    \n",
    "def test_ensemble(analysis_type, corpus):\n",
    "    \n",
    "    print('ensemble:')\n",
    "    # Get mixed system_n\n",
    "    ref_ann, data = get_metric_data(analysis_type, corpus)\n",
    "\n",
    "    names = ['ctakes', 'biomedicus', 'metamap', 'clamp']\n",
    "    if 'entity' in analysis_type: \n",
    "        cols_to_keep = ['begin', 'end', 'note_id']\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['cui', 'note_id']\n",
    "    elif 'full' in analysis_type:\n",
    "        cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "\n",
    "    biomedicus = data[data[\"system\"]=='biomedicus'][cols_to_keep].copy()\n",
    "    ctakes = data[data[\"system\"]=='ctakes'][cols_to_keep].copy()\n",
    "    clamp = data[data[\"system\"]=='clamp'][cols_to_keep].copy()\n",
    "    metamap = data[data[\"system\"]=='metamap'][cols_to_keep].copy()\n",
    "    quickumls = data[data[\"system\"]=='quick_umls'][cols_to_keep].copy()\n",
    "\n",
    "    print('systems:', len(biomedicus), len(clamp), len(ctakes), len(metamap), len(quickumls))\n",
    "\n",
    "    b9 = set()\n",
    "    cl = set()\n",
    "    ct = set()\n",
    "    mm = set()\n",
    "    qu = set()\n",
    "\n",
    "    b9 = df_to_set(get_sys_data('biomedicus', analysis_type, corpus), analysis_type)\n",
    "    print(len(b9))\n",
    "\n",
    "    ct = df_to_set(get_sys_data('ctakes', analysis_type, corpus), analysis_type)\n",
    "    print(len(ct))\n",
    "\n",
    "    cl = df_to_set(get_sys_data('clamp', analysis_type, corpus), analysis_type)\n",
    "    print(len(cl))\n",
    "\n",
    "    mm = df_to_set(get_sys_data('metamap', analysis_type, corpus), analysis_type)\n",
    "    print(len(mm))\n",
    "\n",
    "    qu = df_to_set(get_sys_data('quick_umls', analysis_type, corpus), analysis_type)\n",
    "    print(len(qu))\n",
    "    \n",
    "    print('various merges:')\n",
    "    print(len(b9), len(cl), len(ct), len(mm), len(qu))\n",
    "    print(len(mm.intersection(b9.intersection(cl.intersection(ct)))))\n",
    "    print(len(mm.union(b9.intersection(cl.intersection(ct)))))\n",
    "    print(len(mm.union(b9.union(cl.intersection(ct)))))\n",
    "    print(len(mm.union(b9.union(cl.union(ct)))))\n",
    "    print(len(b9.intersection(ct)))\n",
    "\n",
    "    sys_m = b9.intersection(ct.intersection(qu))\n",
    "    print('sys_m:', len(sys_m))\n",
    "\n",
    "    # Get match merges:\n",
    "    ct, _ = get_system_matches('ctakes', analysis_type, corpus)\n",
    "    cl, _ = get_system_matches('clamp', analysis_type, corpus)\n",
    "    b9, _ = get_system_matches('biomedicus', analysis_type, corpus)\n",
    "    mm, _ = get_system_matches('metamap', analysis_type, corpus)\n",
    "    qu, _ = get_system_matches('quick_umls', analysis_type, corpus)\n",
    "\n",
    "    match_m = b9.intersection(ct.intersection(qu))\n",
    "    print('match_m:', len(match_m))\n",
    "    # reference df to set\n",
    "    if 'entity' in analysis_type: \n",
    "        cols_to_keep = ['end', 'start','file']\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['value','file']\n",
    "    elif 'full' in analysis_type:\n",
    "        cols_to_keep = ['end', 'start', 'value','file']\n",
    "\n",
    "    ref = df_to_set(ref_ann[cols_to_keep], analysis_type, 'ref')\n",
    "\n",
    "    print('ref:', len(ref))\n",
    "\n",
    "    # test difference:\n",
    "    print('FP:', len(sys_m - match_m), len(sys_m - ref))\n",
    "    assert len(sys_m - match_m) == len(sys_m - ref), 'FP mismatch'\n",
    "    print('FN:', len(ref - match_m), len(ref - sys_m))\n",
    "    assert len(ref - match_m) == len(ref - sys_m), 'FN mismatch'\n",
    "    \n",
    "    test_metrics(ref, sys_m, match_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Incoming Boolean sentences are parsed into a binary tree.\n",
    "\n",
    "Test expressions to parse:\n",
    "\n",
    "sentence = '((((A&B)|C)|D)&E)'\n",
    "\n",
    "sentence = '(E&(D|(C|(A&B))))'\n",
    "\n",
    "sentence = '(((A|(B&C))|(D&(E&F)))|(H&I))'\n",
    "\n",
    "\"\"\"\n",
    "# build parse tree from passed sentence\n",
    "# using grammatical rules of Boolean logic\n",
    "def buildParseTree(fpexp):\n",
    "    \"\"\"\n",
    "       Iteratively build parse tree from passed sentence using grammatical rules of Boolean logic\n",
    "       :param fpexp: sentence to parse\n",
    "       :return eTree: parse tree representation\n",
    "       Incoming Boolean sentences are parsed into a binary tree.\n",
    "       Test expressions to parse:\n",
    "       sentence = '(A&B)'\n",
    "       sentence = '(A|B)'\n",
    "       sentence = '((A|B)&C)'\n",
    "       \n",
    "    \"\"\"\n",
    "    fplist = fpexp.split()\n",
    "    pStack = Stack()\n",
    "    eTree = BinaryTree('')\n",
    "    pStack.push(eTree)\n",
    "    currentTree = eTree\n",
    "\n",
    "    for i in fplist:\n",
    "\n",
    "        if i == '(':\n",
    "            currentTree.insertLeft('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getLeftChild()\n",
    "        elif i not in ['&', '|', ')']:\n",
    "            currentTree.setRootVal(i)\n",
    "            parent = pStack.pop()\n",
    "            currentTree = parent\n",
    "        elif i in ['&', '|']:\n",
    "            currentTree.setRootVal(i)\n",
    "            currentTree.insertRight('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getRightChild()\n",
    "        elif i == ')':\n",
    "            currentTree = pStack.pop()\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    return eTree\n",
    "\n",
    "def make_parse_tree(payload):\n",
    "    \"\"\"\n",
    "    Ensure data to create tree are in standard form\n",
    "    :param sentence: sentence to preprocess\n",
    "    :return pt, parse tree graph\n",
    "            sentence, processed sentence to build tree\n",
    "            a: order\n",
    "    \"\"\"\n",
    "    def preprocess_sentence(sentence):\n",
    "        # prepare statement for case when a boolean AND/OR is given\n",
    "        sentence = payload.replace('(', ' ( '). \\\n",
    "            replace(')', ' ) '). \\\n",
    "            replace('&', ' & '). \\\n",
    "            replace('|', ' | '). \\\n",
    "            replace('  ', ' ')\n",
    "        return sentence\n",
    "\n",
    "    sentence = preprocess_sentence(payload)\n",
    "    print(sentence)\n",
    "    \n",
    "    pt = buildParseTree(sentence)\n",
    "    #pt.postorder() \n",
    "    \n",
    "    return pt\n",
    "\n",
    "class Sentence(object):\n",
    "    def __init__(self, sentence):\n",
    "        self = self\n",
    "        self.n_and = sentence.count('&')\n",
    "        self.n_or = sentence.count('|')\n",
    "        self.sentence = sentence\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_docs(corpus):\n",
    "    sql = 'select distinct note_id, sofa from sofas where corpus = %(corpus)s order by note_id'\n",
    "    df = pd.read_sql(sql, params={\"corpus\":corpus}, con=engine)\n",
    "    df.drop_duplicates()\n",
    "    df['len_doc'] = df['sofa'].apply(len)\n",
    "    \n",
    "    subset = df[['note_id', 'len_doc']]\n",
    "    docs = [tuple(x) for x in subset.to_numpy()]\n",
    "    \n",
    "    return docs\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_ref_ann(analysis_type, corpus, filter_semtype = None, semtype = None):\n",
    "    \n",
    "    if filter_semtype:\n",
    "        if ',' in semtype:\n",
    "            semtype = semtype.split(',')\n",
    "        else:\n",
    "            semtype = [semtype]\n",
    "        \n",
    "    ann, _ = get_metric_data(analysis_type, corpus)\n",
    "    ann = ann.rename(index=str, columns={\"start\": \"begin\", \"file\": \"case\"})\n",
    "    \n",
    "    if filter_semtype:\n",
    "        ann = ann[ann['semtype'].isin(semtype)]\n",
    "        \n",
    "    ann[\"label\"] = 'concept'\n",
    "    cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "    ann = ann[cols_to_keep]\n",
    "    \n",
    "    return ann\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_sys_ann(r):\n",
    "    sys = r.system_merges    \n",
    "    sys = sys.rename(index=str, columns={\"note_id\": \"case\"})\n",
    "    sys[\"label\"] = 'concept'\n",
    "    cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "    sys = sys[cols_to_keep]\n",
    "\n",
    "    return sys\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_metrics(boolean_expression: str, analysis_type: str, corpus: str, run_type: str, filter_semtype = None, semtype = None):\n",
    "    \"\"\"\n",
    "    Traverse binary parse tree representation of Boolean sentence\n",
    "        :params: boolean expression in form of '(<annotator_engine_name1><boolean operator><annotator_engine_name2>)'\n",
    "                 analysis_type (string value of: 'entity', 'cui', 'full') used to filter set of reference and system annotations \n",
    "        :return: dictionary with values needed for confusion matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence = Sentence(boolean_expression)   \n",
    "    pt = make_parse_tree(sentence.sentence)\n",
    "    \n",
    "    if filter_semtype:\n",
    "        r = process_sentence(pt, sentence, analysis_type, corpus, filter_semtype, semtype)\n",
    "    else:\n",
    "        r = process_sentence(pt, sentence, analysis_type, corpus)\n",
    "    \n",
    "    # vectorize merges using i-o labeling\n",
    "    if run_type == 'overlap':\n",
    "        \n",
    "        if filter_semtype:\n",
    "            TP, TN, FP, FN = vectorized_coocurences(r, analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "            TP, TN, FP, FN = vectorized_coocurences(r, analysis_type, corpus)\n",
    "\n",
    "        # TODO: validate against ann1/sys1 where val = 1\n",
    "        # total by number chars\n",
    "        system_n = TP + FP\n",
    "        reference_n = TP + FN\n",
    "\n",
    "        d = cm_dict(FN, FP, TP, system_n, reference_n)\n",
    "            \n",
    "        # return full metrics\n",
    "        return d\n",
    "\n",
    "    elif run_type == 'exact':\n",
    "        # total by number spans\n",
    "        system_n = len(r.system_merges)\n",
    "        reference_n = get_ref_n(analysis_type, corpus, filter_semtype)\n",
    "\n",
    "        reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, r.results).get_ref_sys()\n",
    "        # get overall TP/TF and various other counts for running confusion matrix metric analysis\n",
    "        return cm_dict(reference_only, system_only, reference_system_match, system_n, reference_n)\n",
    "    \n",
    "def get_merge_data(boolean_expression: str, analysis_type: str, corpus: str, filter_semtype):\n",
    "    \"\"\"\n",
    "    Traverse binary parse tree representation of Boolean sentence\n",
    "        :params: boolean expression in form of '(<annotator_engine_name1><boolean operator><annotator_engine_name2>)'\n",
    "                 analysis_type (string value of: 'entity', 'cui', 'full') used to filter set of reference and system annotations \n",
    "        :return: dictionary with values needed for confusion matrix\n",
    "    \"\"\"\n",
    "    sentence = Sentence(boolean_expression)   \n",
    "\n",
    "    pt = make_parse_tree(sentence.sentence)\n",
    "\n",
    "    r = process_sentence(pt, sentence, analysis_type, corpus)\n",
    "    \n",
    "    system_n = len(r.system_merges, r.resu)\n",
    "    reference_n = get_ref_n(analysis_type, corpus, filter_semtype)\n",
    "\n",
    "    reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, r.results).get_ref_sys()\n",
    "\n",
    "    print(cm_dict(reference_only, system_only, reference_system_match, system_n, reference_n))\n",
    "    # get matched data from merge\n",
    "    return r.system_merges # merge_eval(reference_only, system_only, reference_system_match, system_n, reference_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clamp', 'quick_umls', 'biomedicus'] ('analytical_fairview.csv', 'concepts.fairview_all')\n",
      "run_type: overlap\n",
      "None ['clamp', 'quick_umls', 'biomedicus']\n",
      "processing merge combo: ('clamp', 'quick_umls', 'biomedicus')\n",
      "clamp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick_umls\n",
      "biomedicus\n",
      " ( clamp & quick_umls ) \n",
      " ( clamp | quick_umls ) \n",
      " ( clamp & biomedicus ) \n",
      " ( clamp | biomedicus ) \n",
      " ( quick_umls & biomedicus ) \n",
      " ( quick_umls | biomedicus ) \n",
      " ( ( clamp & quick_umls ) & biomedicus ) \n",
      " ( ( clamp & quick_umls ) | biomedicus ) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:80: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ( ( clamp | quick_umls ) & biomedicus ) \n",
      " ( ( clamp | quick_umls ) | biomedicus ) \n",
      "processing merge combo: ('clamp', 'biomedicus', 'quick_umls')\n",
      " ( biomedicus & quick_umls ) \n",
      " ( biomedicus | quick_umls ) \n",
      " ( ( clamp & biomedicus ) & quick_umls ) \n",
      " ( ( clamp & biomedicus ) | quick_umls ) \n",
      " ( ( clamp | biomedicus ) & quick_umls ) \n",
      " ( ( clamp | biomedicus ) | quick_umls ) \n",
      "processing merge combo: ('quick_umls', 'biomedicus', 'clamp')\n",
      " ( quick_umls & clamp ) \n",
      " ( quick_umls | clamp ) \n",
      " ( biomedicus & clamp ) \n",
      " ( biomedicus | clamp ) \n",
      " ( ( quick_umls & biomedicus ) & clamp ) \n",
      " ( ( quick_umls & biomedicus ) | clamp ) \n",
      " ( ( quick_umls | biomedicus ) & clamp ) \n",
      " ( ( quick_umls | biomedicus ) | clamp ) \n",
      "           F  precision    recall      TP      FN       FP     TP/FN  n_gold  \\\n",
      "2   0.381392   0.270930  0.643931  302402  167217   813759  1.808441  469619   \n",
      "0   0.431167   0.315762  0.679515  319113  150506   691498  2.120268  469619   \n",
      "1   0.322481   0.211455  0.678982  318863  150756  1189081  2.115093  469619   \n",
      "33  0.425509   0.484618  0.379252  178104  291515   189410  0.610960  469619   \n",
      "20  0.297913   0.255365  0.357475  167877  301742   489524  0.556359  469619   \n",
      "34  0.374057   0.241766  0.826076  387941   81678  1216673  4.749639  469619   \n",
      "21  0.324494   0.206424  0.758127  356031  113588  1368729  3.134407  469619   \n",
      "5   0.425509   0.484618  0.379252  178104  291515   189410  0.610960  469619   \n",
      "3   0.271852   0.398898  0.206184   96828  372791   145911  0.259738  469619   \n",
      "6   0.374057   0.241766  0.826076  387941   81678  1216673  4.749639  469619   \n",
      "4   0.321890   0.199281  0.836638  392901   76718  1578692  5.121367  469619   \n",
      "7   0.297913   0.255365  0.357475  167877  301742   489524  0.556359  469619   \n",
      "31  0.271852   0.398898  0.206184   96828  372791   145911  0.259738  469619   \n",
      "8   0.324494   0.206424  0.758127  356031  113588  1368729  3.134407  469619   \n",
      "32  0.321890   0.199281  0.836638  392901   76718  1578692  5.121367  469619   \n",
      "22  0.250254   0.422678  0.177746   83473  386146   114013  0.216170  469619   \n",
      "23  0.333133   0.216354  0.723825  339922  129697  1231218  2.620893  469619   \n",
      "9   0.250254   0.422678  0.177746   83473  386146   114013  0.216170  469619   \n",
      "10  0.386203   0.271530  0.668542  313960  155659   842301  2.016973  469619   \n",
      "24  0.309198   0.257925  0.385913  181232  288387   521422  0.628433  469619   \n",
      "25  0.313899   0.192111  0.857523  402709   66910  1693519  6.018667  469619   \n",
      "11  0.399467   0.315427  0.544552  255732  213887   555016  1.195641  469619   \n",
      "12  0.313899   0.192111  0.857523  402709   66910  1693519  6.018667  469619   \n",
      "35  0.250254   0.422678  0.177746   83473  386146   114013  0.216170  469619   \n",
      "36  0.399295   0.268146  0.781542  367027  102592  1001729  3.577540  469619   \n",
      "37  0.433582   0.464047  0.406870  191074  278545   220682  0.685972  469619   \n",
      "38  0.313899   0.192111  0.857523  402709   66910  1693519  6.018667  469619   \n",
      "\n",
      "      n_sys          TM                            merge  n_terms  F1 rank  \\\n",
      "2   1116161  286.234012                       biomedicus        1      8.0   \n",
      "0   1010611  317.433302                            clamp        1      2.0   \n",
      "1   1507944  259.663867                       quick_umls        1     14.0   \n",
      "33   367514  293.789811               (biomedicus&clamp)        2      3.5   \n",
      "20   657401  207.050374          (biomedicus&quick_umls)        2     21.5   \n",
      "34  1604614  306.253029               (biomedicus|clamp)        2      9.5   \n",
      "21  1724760  271.096230          (biomedicus|quick_umls)        2     12.5   \n",
      "5    367514  293.789811               (clamp&biomedicus)        2      3.5   \n",
      "3    242739  196.531054               (clamp&quick_umls)        2     23.5   \n",
      "6   1604614  306.253029               (clamp|biomedicus)        2      9.5   \n",
      "4   1971593  279.817260               (clamp|quick_umls)        2     15.5   \n",
      "7    657401  207.050374          (quick_umls&biomedicus)        2     21.5   \n",
      "31   242739  196.531054               (quick_umls&clamp)        2     23.5   \n",
      "8   1724760  271.096230          (quick_umls|biomedicus)        2     12.5   \n",
      "32  1971593  279.817260               (quick_umls|clamp)        2     15.5   \n",
      "22   197486  187.835582  ((clamp&biomedicus)&quick_umls)        3     26.0   \n",
      "23  1571140  271.188851  ((clamp&biomedicus)|quick_umls)        3     11.0   \n",
      "9    197486  187.835582  ((clamp&quick_umls)&biomedicus)        3     26.0   \n",
      "10  1156261  291.975480  ((clamp&quick_umls)|biomedicus)        3      7.0   \n",
      "24   702654  216.204198  ((clamp|biomedicus)&quick_umls)        3     20.0   \n",
      "25  2096228  278.145525  ((clamp|biomedicus)|quick_umls)        3     18.0   \n",
      "11   810748  284.015559  ((clamp|quick_umls)&biomedicus)        3      5.0   \n",
      "12  2096228  278.145525  ((clamp|quick_umls)|biomedicus)        3     18.0   \n",
      "35   197486  187.835582  ((quick_umls&biomedicus)&clamp)        3     26.0   \n",
      "36  1368756  313.714791  ((quick_umls&biomedicus)|clamp)        3      6.0   \n",
      "37   411756  297.770471  ((quick_umls|biomedicus)&clamp)        3      1.0   \n",
      "38  2096228  278.145525  ((quick_umls|biomedicus)|clamp)        3     18.0   \n",
      "\n",
      "    TP/FN rank  TM rank      Gmean  \n",
      "2         15.0      9.0  11.147009  \n",
      "0         12.0      1.0   3.258996  \n",
      "1         13.0     19.0  15.515595  \n",
      "33        19.5      6.5   9.887706  \n",
      "20        21.5     21.5  21.500000  \n",
      "34         6.5      3.5   5.149189  \n",
      "21         9.5     17.5  12.849436  \n",
      "5         19.5      6.5   9.887706  \n",
      "3         23.5     23.5  23.500000  \n",
      "6          6.5      3.5   5.149189  \n",
      "4          4.5     11.5   7.834243  \n",
      "7         21.5     21.5  21.500000  \n",
      "31        23.5     23.5  23.500000  \n",
      "8          9.5     17.5  12.849436  \n",
      "32         4.5     11.5   7.834243  \n",
      "22        26.0     26.0  26.000000  \n",
      "23        11.0     16.0  12.993194  \n",
      "9         26.0     26.0  26.000000  \n",
      "10        14.0      8.0  10.107956  \n",
      "24        18.0     20.0  19.085051  \n",
      "25         2.0     14.0   6.062560  \n",
      "11        16.0     10.0  11.409647  \n",
      "12         2.0     14.0   6.062560  \n",
      "35        26.0     26.0  26.000000  \n",
      "36         8.0      2.0   4.184328  \n",
      "37        17.0      5.0   7.203101  \n",
      "38         2.0     14.0   6.062560  \n",
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         36631333 function calls (36421148 primitive calls) in 63.933 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "     2221   11.531    0.005   11.531    0.005 {pandas._libs.ops.scalar_compare}\n",
       "  6131039    5.779    0.000    5.779    0.000 {built-in method numpy.arange}\n",
       "     2214    5.161    0.002    5.161    0.002 <ipython-input-41-ac764bb18648>:9(<listcomp>)\n",
       "       27    5.127    0.190   46.961    1.739 <ipython-input-41-ac764bb18648>:30(vectorized_coocurences)\n",
       "     2214    4.811    0.002   17.525    0.008 <ipython-input-41-ac764bb18648>:1(label_vector)\n",
       "  6127777    3.561    0.000    3.561    0.000 {built-in method __new__ of type object at 0x105bbe778}\n",
       "      914    2.429    0.003    2.429    0.003 {built-in method numpy.concatenate}\n",
       "        1    1.742    1.742    1.751    1.751 {method 'read' of 'pandas._libs.parsers.TextReader' objects}\n",
       "      286    1.702    0.006    3.166    0.011 blocks.py:3131(_merge_blocks)\n",
       "     2214    1.679    0.001    7.433    0.003 <ipython-input-41-ac764bb18648>:8(<listcomp>)\n",
       "  6124349    1.328    0.000    5.205    0.000 __init__.py:403(_make)\n",
       "      406    1.325    0.003    1.325    0.003 {method 'copy' of 'numpy.ndarray' objects}\n",
       "       80    1.177    0.015    1.177    0.015 managers.py:2004(<listcomp>)\n",
       "      108    0.883    0.008    0.884    0.008 {method 'factorize' of 'pandas._libs.hashtable.Int64Factorizer' objects}\n",
       "     1107    0.805    0.001    1.325    0.001 <ipython-input-41-ac764bb18648>:16(confused)\n",
       "       91    0.783    0.009    0.784    0.009 {method 'factorize' of 'pandas._libs.hashtable.Int64HashTable' objects}\n",
       "       60    0.703    0.012    0.703    0.012 {method 'factorize' of 'pandas._libs.hashtable.StringHashTable' objects}\n",
       "     5272    0.580    0.000    0.582    0.000 base.py:3918(__contains__)\n",
       "    18770    0.552    0.000    0.552    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
       "    12812    0.519    0.000    0.519    0.000 {built-in method numpy.empty}\n",
       "      294    0.476    0.002    0.476    0.002 {method 'recv_into' of '_socket.socket' objects}\n",
       "      916    0.438    0.000    0.443    0.000 {pandas._libs.lib.infer_dtype}\n",
       "6783231/6701866    0.398    0.000    0.426    0.000 {built-in method builtins.len}\n",
       "       42    0.370    0.009    0.370    0.009 {pandas._libs.hashtable.duplicated_int64}\n",
       "    25733    0.358    0.000    0.401    0.000 generic.py:5069(__setattr__)\n",
       "       36    0.350    0.010    0.350    0.010 {method 'factorize' of 'pandas._libs.hashtable.Factorizer' objects}\n",
       "   216/24    0.333    0.002    9.905    0.413 <ipython-input-48-407a16a06efa>:26(evaluate)\n",
       "      253    0.312    0.001    0.312    0.001 {pandas._libs.algos.take_2d_axis0_object_object}\n",
       "1572032/1572028    0.268    0.000    0.428    0.000 {built-in method builtins.isinstance}\n",
       "        5    0.255    0.051    0.255    0.051 {method 'get_labels_groupby' of 'pandas._libs.hashtable.Int64HashTable' objects}\n",
       "     2286    0.252    0.000    0.252    0.000 {pandas._libs.algos.take_2d_axis1_object_object}\n",
       "  1473715    0.233    0.000    0.304    0.000 strings.py:1519(<lambda>)\n",
       "    38625    0.203    0.000    0.203    0.000 {built-in method builtins.compile}\n",
       "    44571    0.164    0.000    0.515    0.000 connections.py:1195(_read_row_from_packet)\n",
       "       18    0.162    0.009    0.188    0.010 {pandas._libs.join.inner_join}\n",
       "      226    0.160    0.001    0.160    0.001 {built-in method pandas._libs.missing.isnaobj}\n",
       "      126    0.159    0.001    0.192    0.002 managers.py:1841(_stack_arrays)\n",
       "    89206    0.158    0.000    0.158    0.000 {method 'settimeout' of '_socket.socket' objects}\n",
       "       43    0.155    0.004    0.468    0.011 sorting.py:20(get_group_index)\n",
       "        6    0.134    0.022    0.358    0.060 {pandas._libs.lib.map_infer_mask}\n",
       "     6290    0.132    0.000    0.132    0.000 {method 'fill' of 'numpy.ndarray' objects}\n",
       "      436    0.126    0.000    0.138    0.000 cast.py:1193(construct_1d_object_array_from_listlike)\n",
       "     2402    0.121    0.000    0.121    0.000 {pandas._libs.algos.take_2d_axis1_int64_int64}\n",
       "     2216    0.120    0.000    0.120    0.000 {built-in method numpy.zeros}\n",
       "     2367    0.119    0.000    0.119    0.000 {method 'take' of 'numpy.ndarray' objects}\n",
       "      111    0.108    0.001    2.240    0.020 managers.py:2029(concatenate_block_managers)\n",
       "   2215/1    0.106    0.000   63.934   63.934 {built-in method builtins.exec}\n",
       "154500/38625    0.103    0.000    0.142    0.000 ast.py:64(_convert)\n",
       "    44596    0.097    0.000    0.906    0.000 connections.py:648(_read_packet)\n",
       "     2214    0.092    0.000    0.225    0.000 __init__.py:316(namedtuple)\n",
       "   360769    0.092    0.000    0.126    0.000 generic.py:7(_check)\n",
       "      160    0.092    0.001    1.982    0.012 algorithms.py:559(factorize)\n",
       "    89192    0.091    0.000    0.764    0.000 connections.py:687(_read_bytes)\n",
       "     2314    0.089    0.000    0.089    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
       "   222774    0.088    0.000    0.102    0.000 protocol.py:63(read)\n",
       "      241    0.084    0.000    0.084    0.000 {pandas._libs.algos.take_2d_axis0_int64_int64}\n",
       "   222774    0.083    0.000    0.290    0.000 protocol.py:168(read_length_coded_string)\n",
       "    52890    0.083    0.000    0.217    0.000 dtypes.py:68(find)\n",
       "      726    0.080    0.000    0.767    0.001 concat.py:165(get_reindexed_values)\n",
       "       18    0.080    0.004    0.081    0.005 merge.py:1701(_get_join_keys)\n",
       "   245619    0.078    0.000    0.158    0.000 strings.py:87(g)\n",
       "  1266721    0.076    0.000    0.076    0.000 {method 'strip' of 'str' objects}\n",
       "      675    0.074    0.000    0.074    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
       "    67444    0.067    0.000    0.153    0.000 common.py:1845(_is_dtype_type)\n",
       "   562862    0.067    0.000    0.082    0.000 {built-in method builtins.getattr}\n",
       "       18    0.064    0.004    1.619    0.090 merge.py:1104(_get_join_indexers)\n",
       "     5712    0.064    0.000    1.635    0.000 algorithms.py:1544(take_nd)\n",
       "       18    0.060    0.003    3.210    0.178 frame.py:6858(merge)\n",
       "    56649    0.059    0.000    0.095    0.000 {method 'format' of 'str' objects}\n",
       "     2329    0.059    0.000    0.082    0.000 indexing.py:2575(maybe_convert_indices)\n",
       "   222796    0.058    0.000    0.105    0.000 protocol.py:150(read_length_encoded_integer)\n",
       "       42    0.057    0.001    3.272    0.078 frame.py:4605(drop_duplicates)\n",
       "    93694    0.054    0.000    0.652    0.000 {built-in method builtins.hasattr}\n",
       "       18    0.053    0.003    1.672    0.093 merge.py:730(_get_join_indexers)\n",
       "31538/26975    0.052    0.000    0.118    0.000 {built-in method numpy.array}\n",
       "        2    0.051    0.025    1.024    0.512 connections.py:1182(_read_rowdata_packet)\n",
       "    73195    0.050    0.000    0.206    0.000 base.py:75(is_dtype)\n",
       "     8874    0.049    0.000    0.564    0.000 indexing.py:960(_getitem_lowerdim)\n",
       "        3    0.048    0.016    6.341    2.114 <ipython-input-46-5ca52ce91f6b>:27(get_sys_data)\n",
       "   222796    0.048    0.000    0.048    0.000 protocol.py:117(read_uint8)\n",
       "       72    0.045    0.001    1.286    0.018 merge.py:1617(_factorize_keys)\n",
       "      160    0.045    0.000    0.049    0.000 sorting.py:55(maybe_lift)\n",
       "      152    0.044    0.000    0.044    0.000 {pandas._libs.algos.take_2d_axis0_float64_float64}\n",
       "   222762    0.043    0.000    0.043    0.000 {method 'decode' of 'bytes' objects}\n",
       "     2885    0.043    0.000    0.098    0.000 managers.py:186(_rebuild_blknos_and_blklocs)\n",
       "    25136    0.041    0.000    0.102    0.000 _dtype.py:319(_name_get)\n",
       "    38625    0.040    0.000    0.399    0.000 ast.py:38(literal_eval)\n",
       "    50499    0.040    0.000    0.202    0.000 common.py:1702(is_extension_array_dtype)\n",
       "11448/11447    0.040    0.000    0.517    0.000 series.py:152(__init__)\n",
       "       27    0.039    0.001   16.742    0.620 <ipython-input-48-407a16a06efa>:1(process_sentence)\n",
       "      160    0.037    0.000    1.816    0.011 algorithms.py:434(_factorize_array)\n",
       "    43222    0.037    0.000    0.118    0.000 common.py:1981(pandas_dtype)\n",
       "     2222    0.037    0.000   12.215    0.005 ops.py:1660(wrapper)\n",
       "     2329    0.036    0.000    1.365    0.001 managers.py:1329(take)\n",
       "        2    0.035    0.018    0.071    0.035 managers.py:772(_interleave)\n",
       "   260339    0.035    0.000    0.035    0.000 {built-in method builtins.issubclass}\n",
       "        4    0.034    0.009    0.193    0.048 {pandas._libs.lib.map_infer}\n",
       "      146    0.034    0.000    0.034    0.000 {pandas._libs.algos.take_2d_axis1_float64_float64}\n",
       "    17338    0.033    0.000    0.076    0.000 blocks.py:78(__init__)\n",
       "     9113    0.031    0.000    0.119    0.000 managers.py:963(iget)\n",
       "    89192    0.031    0.000    0.508    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
       "       27    0.029    0.001    2.549    0.094 <ipython-input-52-a7c9da0d1405>:120(get_sys_ann)\n",
       "    11199    0.029    0.000    0.220    0.000 base.py:1117(__iter__)\n",
       "        1    0.028    0.028    4.563    4.563 <ipython-input-43-ee20452e93e4>:1(get_metric_data)\n",
       "    29013    0.028    0.000    0.136    0.000 common.py:93(is_bool_indexer)\n",
       "     8881    0.027    0.000    0.315    0.000 frame.py:2829(_ixs)\n",
       "    11088    0.027    0.000    2.534    0.000 indexing.py:1485(__getitem__)\n",
       "     5712    0.026    0.000    0.071    0.000 algorithms.py:1417(_get_take_nd_function)\n",
       "    63937    0.025    0.000    0.025    0.000 {built-in method _abc._abc_instancecheck}\n",
       "    57699    0.024    0.000    0.083    0.000 inference.py:253(is_list_like)\n",
       "    17338    0.024    0.000    0.192    0.000 blocks.py:3080(make_block)\n",
       "      230    0.024    0.000    0.024    0.000 {pandas._libs.algos.take_1d_int64_int64}\n",
       "    36547    0.023    0.000    0.033    0.000 generic.py:363(_get_axis_name)\n",
       "    50272    0.023    0.000    0.036    0.000 numerictypes.py:293(issubclass_)\n",
       "    11447    0.023    0.000    0.109    0.000 managers.py:1443(__init__)\n",
       "    25136    0.022    0.000    0.060    0.000 numerictypes.py:365(issubdtype)\n",
       "    16196    0.022    0.000    0.051    0.000 common.py:160(is_sparse)\n",
       "     5518    0.022    0.000    0.081    0.000 cast.py:255(maybe_promote)\n",
       "   273945    0.022    0.000    0.022    0.000 {method 'append' of 'list' objects}\n",
       "18415/18414    0.022    0.000    0.607    0.000 generic.py:5053(__getattr__)\n",
       "     2214    0.021    0.000    1.262    0.001 frame.py:849(itertuples)\n",
       "        1    0.021    0.021    2.031    2.031 parsers.py:403(_read)\n",
       "    74812    0.021    0.000    0.029    0.000 base.py:652(__len__)\n",
       "       56    0.021    0.000    1.700    0.030 managers.py:159(rename_axis)\n",
       "       18    0.021    0.001    0.021    0.001 {built-in method _operator.or_}\n",
       "     2453    0.020    0.000    4.075    0.002 frame.py:2893(__getitem__)\n",
       "       19    0.020    0.001    0.020    0.001 {built-in method _operator.and_}\n",
       "        9    0.020    0.002    0.020    0.002 {method 'factorize' of 'pandas._libs.hashtable.Float64HashTable' objects}\n",
       "    36437    0.019    0.000    0.060    0.000 generic.py:377(_get_axis)\n",
       "    14069    0.019    0.000    0.019    0.000 generic.py:127(__init__)\n",
       "    17338    0.019    0.000    0.024    0.000 blocks.py:199(mgr_locs)\n",
       "    11449    0.019    0.000    0.036    0.000 series.py:354(_set_axis)\n",
       "    42520    0.018    0.000    0.132    0.000 common.py:434(is_datetime64tz_dtype)\n",
       "    13748    0.018    0.000    0.029    0.000 series.py:392(name)\n",
       "     2811    0.018    0.000    0.291    0.000 construction.py:537(sanitize_array)\n",
       "    37827    0.018    0.000    0.029    0.000 managers.py:1549(internal_values)\n",
       "    20484    0.018    0.000    0.241    0.000 <ipython-input-46-5ca52ce91f6b>:85(<genexpr>)\n",
       "     2328    0.018    0.000    4.872    0.002 generic.py:3323(_take)\n",
       "     4730    0.018    0.000    1.034    0.000 blocks.py:1217(take_nd)\n",
       "    20425    0.017    0.000    0.083    0.000 common.py:403(is_datetime64_dtype)\n",
       "     8874    0.017    0.000    0.403    0.000 indexing.py:2205(_getitem_axis)\n",
       "    11080    0.017    0.000    0.042    0.000 managers.py:139(shape)\n",
       "    39384    0.017    0.000    0.046    0.000 integer.py:80(construct_from_string)\n",
       "     8874    0.016    0.000    0.142    0.000 indexing.py:217(_has_valid_tuple)\n",
       "    63937    0.016    0.000    0.040    0.000 abc.py:137(__instancecheck__)\n",
       "     5251    0.015    0.000    0.030    0.000 dtypes.py:786(construct_from_string)\n",
       "     3597    0.015    0.000    0.089    0.000 blocks.py:3034(get_block_type)\n",
       "     8874    0.015    0.000    0.028    0.000 indexing.py:2089(_is_scalar_access)\n",
       "        8    0.015    0.002    0.015    0.002 {pandas._libs.lib.maybe_convert_objects}\n",
       "    17748    0.015    0.000    0.115    0.000 indexing.py:2056(_validate_key)\n",
       "    10386    0.015    0.000    0.040    0.000 dtypes.py:973(is_dtype)\n",
       "    18299    0.015    0.000    0.022    0.000 {pandas._libs.lib.is_scalar}\n",
       "    18147    0.015    0.000    0.195    0.000 <ipython-input-46-5ca52ce91f6b>:88(<genexpr>)\n",
       "    60986    0.014    0.000    0.021    0.000 common.py:119(<lambda>)\n",
       "    17659    0.014    0.000    0.053    0.000 common.py:131(is_object_dtype)\n",
       " 1011/683    0.014    0.000    0.283    0.000 base.py:253(__new__)\n",
       "    24686    0.014    0.000    0.024    0.000 managers.py:1522(dtype)\n",
       "    44573    0.014    0.000    0.022    0.000 connections.py:1137(_check_packet_is_eof)\n",
       "    44598    0.014    0.000    0.014    0.000 {built-in method _struct.unpack}\n",
       "    36613    0.013    0.000    0.026    0.000 generic.py:450(ndim)\n",
       "     6343    0.013    0.000    0.036    0.000 _dtype.py:46(__str__)\n",
       "    17744    0.013    0.000    0.052    0.000 indexing.py:2116(_validate_integer)\n",
       "    11070    0.013    0.000    0.792    0.000 frame.py:919(<genexpr>)\n",
       "     2342    0.013    0.000    0.029    0.000 cast.py:832(maybe_castable)\n",
       "       42    0.013    0.000    0.013    0.000 {built-in method _operator.inv}\n",
       "    44596    0.013    0.000    0.022    0.000 protocol.py:214(check_error)\n",
       "        2    0.012    0.006    0.012    0.006 __init__.py:131(lmap)\n",
       "     9641    0.012    0.000    0.020    0.000 base.py:3940(__getitem__)\n",
       "     9113    0.012    0.000    0.122    0.000 frame.py:3349(_box_col_values)\n",
       "    60986    0.012    0.000    0.012    0.000 common.py:117(classes)\n",
       "    45552    0.012    0.000    0.016    0.000 managers.py:143(ndim)\n",
       "     2780    0.012    0.000    0.221    0.000 managers.py:97(__init__)\n",
       "    37827    0.012    0.000    0.040    0.000 series.py:476(_values)\n",
       "    18168    0.012    0.000    0.057    0.000 common.py:472(is_timedelta64_dtype)\n",
       "     7323    0.012    0.000    0.042    0.000 blocks.py:2626(__init__)\n",
       "     4544    0.012    0.000    0.028    0.000 numeric.py:2656(seterr)\n",
       "     3349    0.012    0.000    0.025    0.000 base.py:504(_simple_new)\n",
       "      702    0.012    0.000    0.016    0.000 concat.py:117(needs_filling)\n",
       "       42    0.012    0.000    2.866    0.068 frame.py:4639(duplicated)\n",
       "    32308    0.012    0.000    0.046    0.000 <frozen importlib._bootstrap>:1009(_handle_fromlist)\n",
       "     2214    0.012    0.000    1.745    0.001 indexing.py:1855(_getitem_axis)\n",
       "     2670    0.011    0.000    0.034    0.000 managers.py:306(_verify_integrity)\n",
       "     4574    0.011    0.000    0.511    0.000 fromnumeric.py:69(_wrapreduction)\n",
       "    26622    0.011    0.000    0.018    0.000 indexing.py:1487(<genexpr>)\n",
       "     2752    0.011    0.000    0.047    0.000 cast.py:953(maybe_cast_to_datetime)\n",
       "    26681    0.011    0.000    0.051    0.000 indexing.py:2689(is_list_like_indexer)\n",
       "     8746    0.011    0.000    0.024    0.000 dtypes.py:672(construct_from_string)\n",
       "    11211    0.011    0.000    0.130    0.000 common.py:702(is_datetimelike)\n",
       "    64974    0.011    0.000    0.011    0.000 managers.py:1488(_block)\n",
       "     2263    0.011    0.000    0.054    0.000 base.py:786(array)\n",
       "    14234    0.010    0.000    0.093    0.000 blocks.py:225(make_block_same_class)\n",
       "    51726    0.010    0.000    0.010    0.000 {method 'get' of 'dict' objects}\n",
       "     5101    0.010    0.000    0.066    0.000 base.py:4051(equals)\n",
       "       27    0.010    0.000   63.718    2.360 <ipython-input-52-a7c9da0d1405>:130(get_metrics)\n",
       "     2621    0.010    0.000    0.391    0.000 frame.py:378(__init__)\n",
       "     2811    0.010    0.000    0.131    0.000 construction.py:684(_try_cast)\n",
       "    44596    0.010    0.000    0.010    0.000 protocol.py:211(is_error_packet)\n",
       "    44596    0.010    0.000    0.010    0.000 protocol.py:56(__init__)\n",
       "    24686    0.010    0.000    0.034    0.000 series.py:406(dtype)\n",
       "     4509    0.010    0.000    0.521    0.000 fromnumeric.py:1966(sum)\n",
       "     2609    0.009    0.000    0.040    0.000 base.py:566(_shallow_copy)\n",
       "     4544    0.009    0.000    0.010    0.000 numeric.py:2758(geterr)\n",
       "     8874    0.009    0.000    0.027    0.000 indexing.py:229(_is_nested_tuple_indexer)\n",
       "        2    0.009    0.004    0.048    0.024 frame.py:1430(from_records)\n",
       "     2347    0.009    0.000    1.245    0.001 managers.py:1198(reindex_indexer)\n",
       "    37754    0.008    0.000    0.008    0.000 blocks.py:308(dtype)\n",
       "     2263    0.008    0.000    0.020    0.000 numpy_.py:35(__init__)\n",
       "    38625    0.008    0.000    0.211    0.000 ast.py:30(parse)\n",
       "    44577    0.008    0.000    0.008    0.000 protocol.py:190(is_eof_packet)\n",
       "    15686    0.008    0.000    0.063    0.000 common.py:572(is_categorical_dtype)\n",
       "    11507    0.008    0.000    0.015    0.000 series.py:399(name)\n",
       "     8219    0.008    0.000    0.008    0.000 dtypes.py:452(construct_from_string)\n",
       "        3    0.008    0.003    0.447    0.149 <ipython-input-46-5ca52ce91f6b>:69(get_system_matches)\n",
       "    27018    0.008    0.000    0.041    0.000 inference.py:304(is_array_like)\n",
       "       36    0.008    0.000    0.008    0.000 {method 'update' of 'set' objects}\n",
       "     2347    0.008    0.000    0.188    0.000 base.py:784(take)\n",
       "    13548    0.008    0.000    0.066    0.000 {method 'any' of 'numpy.ndarray' objects}\n",
       "    19901    0.008    0.000    0.016    0.000 common.py:1809(_get_dtype)\n",
       "    13053    0.008    0.000    0.008    0.000 {method 'view' of 'numpy.ndarray' objects}\n",
       "    33240    0.008    0.000    0.025    0.000 managers.py:141(<genexpr>)\n",
       "      432    0.007    0.000    0.055    0.000 concat.py:261(get_empty_dtype_and_na)\n",
       "     5124    0.007    0.000    0.007    0.000 {method 'match' of 're.Pattern' objects}\n",
       "    33674    0.007    0.000    0.007    0.000 {method 'startswith' of 'str' objects}\n",
       "20165/15605    0.007    0.000    0.109    0.000 numeric.py:469(asarray)\n",
       "    17748    0.007    0.000    0.022    0.000 indexing.py:2695(is_label_like)\n",
       "     6026    0.007    0.000    0.007    0.000 {pandas._libs.algos.ensure_int64}\n",
       "    22452    0.007    0.000    0.010    0.000 common.py:316(apply_if_callable)\n",
       "     5251    0.007    0.000    0.012    0.000 dtypes.py:929(construct_from_string)\n",
       "     8874    0.007    0.000    0.715    0.000 indexing.py:2141(_getitem_tuple)\n",
       "     2214    0.007    0.000    1.092    0.000 indexing.py:1511(_getbool_axis)\n",
       "    17768    0.007    0.000    0.048    0.000 base.py:5318(ensure_index)\n",
       "     4490    0.007    0.000    0.032    0.000 common.py:1578(is_bool_dtype)\n",
       "     6343    0.007    0.000    0.066    0.000 blocks.py:312(ftype)\n",
       "     9113    0.007    0.000    0.019    0.000 generic.py:3070(_set_as_cached)\n",
       "     2264    0.007    0.000    0.687    0.000 managers.py:1233(<listcomp>)\n",
       "    36612    0.007    0.000    0.007    0.000 blocks.py:195(mgr_locs)\n",
       "       18    0.007    0.000    0.007    0.000 {method 'union' of 'set' objects}\n",
       "    17338    0.006    0.000    0.006    0.000 blocks.py:89(_check_ndim)\n",
       "     2809    0.006    0.000    0.076    0.000 managers.py:599(_consolidate_check)\n",
       "     8872    0.006    0.000    0.320    0.000 indexing.py:143(_get_loc)\n",
       "    26622    0.006    0.000    0.008    0.000 indexing.py:230(<genexpr>)\n",
       "     9113    0.006    0.000    0.006    0.000 blocks.py:332(iget)\n",
       "    11331    0.006    0.000    0.046    0.000 common.py:536(is_interval_dtype)\n",
       "    10025    0.006    0.000    0.016    0.000 {built-in method builtins.any}\n",
       "    11449    0.006    0.000    0.006    0.000 series.py:382(_set_subtyp)\n",
       "     8876    0.006    0.000    0.006    0.000 common.py:279(is_null_slice)\n",
       "    14271    0.006    0.000    0.008    0.000 inference.py:438(is_hashable)\n",
       "    10767    0.006    0.000    0.025    0.000 base.py:3608(values)\n",
       "     2263    0.006    0.000    0.114    0.000 indexing.py:2475(check_bool_indexer)\n",
       "     5706    0.005    0.000    0.019    0.000 dtypes.py:827(is_dtype)\n",
       "     2245    0.005    0.000    0.005    0.000 {built-in method pandas._libs.lib.is_datetime_array}\n",
       "      185    0.005    0.000    0.009    0.000 concat.py:22(get_mgr_concatenation_plan)\n",
       "    13548    0.005    0.000    0.058    0.000 _methods.py:42(_any)\n",
       "     7570    0.005    0.000    0.012    0.000 {pandas._libs.lib.values_from_object}\n",
       "    37843    0.005    0.000    0.005    0.000 blocks.py:165(internal_values)\n",
       "     2221    0.005    0.000   11.539    0.005 ops.py:1591(_comp_method_OBJECT_ARRAY)\n",
       "     4388    0.005    0.000    0.011    0.000 common.py:1545(is_float_dtype)\n",
       "        2    0.005    0.002    0.005    0.002 result.py:1192(<listcomp>)\n",
       "     2263    0.005    0.000    0.025    0.000 numpy_.py:127(__init__)\n",
       "     2263    0.005    0.000    0.075    0.000 series.py:669(__array__)\n",
       "     2222    0.005    0.000   11.560    0.005 ops.py:1615(na_op)\n",
       "       18    0.004    0.000    0.004    0.000 {method 'intersection' of 'set' objects}\n",
       "     4544    0.004    0.000    0.004    0.000 {built-in method numpy.seterrobj}\n",
       "     9364    0.004    0.000    0.004    0.000 frame.py:474(axes)\n",
       "     9638    0.004    0.000    0.006    0.000 common.py:144(cast_scalar_indexer)\n",
       "     3493    0.004    0.000    0.182    0.000 missing.py:105(_isna_new)\n",
       "    30917    0.004    0.000    0.004    0.000 {pandas._libs.lib.is_integer}\n",
       "     2597    0.004    0.000    0.014    0.000 missing.py:360(array_equivalent)\n",
       "     8418    0.004    0.000    0.011    0.000 managers.py:291(__len__)\n",
       "     4923    0.004    0.000    0.007    0.000 sparse.py:196(construct_from_string)\n",
       "     6399    0.004    0.000    0.030    0.000 common.py:262(is_categorical)\n",
       "     2264    0.004    0.000    0.010    0.000 managers.py:1556(get_values)\n",
       "     2231    0.004    0.000    0.009    0.000 ops.py:43(get_op_result_name)\n",
       "     3958    0.004    0.000    0.011    0.000 common.py:868(is_integer_dtype)\n",
       "     2214    0.004    0.000    0.004    0.000 {built-in method builtins.repr}\n",
       "     8669    0.004    0.000    0.006    0.000 managers.py:308(<genexpr>)\n",
       "     2272    0.004    0.000    0.016    0.000 numeric.py:3063(__exit__)\n",
       "    19580    0.004    0.000    0.004    0.000 {built-in method builtins.hash}\n",
       "      535    0.004    0.000    0.173    0.000 missing.py:183(_isna_ndarraylike)\n",
       "        8    0.004    0.000    0.004    0.000 numeric.py:2551(array_equal)\n",
       "     2809    0.004    0.000    0.069    0.000 managers.py:600(<listcomp>)\n",
       "     5127    0.003    0.000    0.008    0.000 common.py:746(is_dtype_equal)\n",
       "      117    0.003    0.000    0.003    0.000 {built-in method posix.stat}\n",
       "     2567    0.003    0.000    3.411    0.001 generic.py:5122(_protect_consolidate)\n",
       "     5101    0.003    0.000    0.004    0.000 base.py:613(is_)\n",
       "     2363    0.003    0.000    0.795    0.000 {method 'extend' of 'list' objects}\n",
       "        2    0.003    0.002    0.003    0.002 {pandas._libs.lib.to_object_array_tuples}\n",
       "     3414    0.003    0.000    0.005    0.000 generic.py:349(_get_axis_number)\n",
       "     2298    0.003    0.000    0.005    0.000 generic.py:3175(_set_is_copy)\n",
       "     2947    0.003    0.000    0.006    0.000 {method 'join' of 'str' objects}\n",
       "     5706    0.003    0.000    0.023    0.000 common.py:503(is_period_dtype)\n",
       "     2272    0.003    0.000    0.004    0.000 numeric.py:3054(__init__)\n",
       "     2780    0.003    0.000    0.005    0.000 managers.py:98(<listcomp>)\n",
       "      850    0.003    0.000    0.146    0.000 concat.py:137(is_na)\n",
       "     2567    0.003    0.000    3.407    0.001 generic.py:5135(f)\n",
       "     2762    0.003    0.000    0.006    0.000 base.py:547(_get_attributes_dict)\n",
       "     2245    0.003    0.000    0.011    0.000 base.py:1736(is_all_dates)\n",
       "    19926    0.003    0.000    0.003    0.000 {method '__contains__' of 'frozenset' objects}\n",
       "     2800    0.003    0.000    0.031    0.000 common.py:1643(is_extension_type)\n",
       "     3414    0.003    0.000    0.005    0.000 __init__.py:221(iteritems)\n",
       "     2504    0.003    0.000    0.019    0.000 generic.py:3056(_get_item_cache)\n",
       "     2632    0.003    0.000    0.003    0.000 {method 'replace' of 'str' objects}\n",
       "       85    0.003    0.000    0.371    0.004 managers.py:1241(_slice_take_blocks_ax0)\n",
       "     2960    0.003    0.000    0.008    0.000 {built-in method builtins.sum}\n",
       "     4574    0.003    0.000    0.003    0.000 fromnumeric.py:70(<dictcomp>)\n",
       "      678    0.003    0.000    0.004    0.000 concat.py:425(combine_concat_plans)\n",
       "        5    0.003    0.001    0.262    0.052 sorting.py:366(compress_group_index)\n",
       "     7672    0.003    0.000    0.004    0.000 managers.py:591(is_consolidated)\n",
       "     2272    0.003    0.000    0.018    0.000 numeric.py:3058(__enter__)\n",
       "     2514    0.003    0.000    0.003    0.000 generic.py:144(_init_mgr)\n",
       "        7    0.003    0.000    0.003    0.000 {built-in method io.open}\n",
       "    11070    0.003    0.000    0.003    0.000 __init__.py:388(<genexpr>)\n",
       "     5108    0.003    0.000    3.185    0.001 managers.py:927(_consolidate_inplace)\n",
       "     6458    0.003    0.000    0.003    0.000 common.py:127(<lambda>)\n",
       "    15449    0.003    0.000    0.003    0.000 {pandas._libs.lib.is_float}\n",
       "        2    0.003    0.001    1.556    0.778 sql.py:317(read_sql)\n",
       "      985    0.002    0.000    0.144    0.000 common.py:222(asarray_tuplesafe)\n",
       "    22453    0.002    0.000    0.002    0.000 {built-in method builtins.callable}\n",
       "     2498    0.002    0.000    0.006    0.000 generic.py:381(_get_block_manager_axis)\n",
       "    11761    0.002    0.000    0.002    0.000 managers.py:206(items)\n",
       "     2215    0.002    0.000    0.583    0.000 base.py:3975(_can_hold_identifiers_and_holds_name)\n",
       "     2567    0.002    0.000    3.414    0.001 generic.py:5132(_consolidate_inplace)\n",
       "    19926    0.002    0.000    0.002    0.000 {method 'isidentifier' of 'str' objects}\n",
       "     9088    0.002    0.000    0.002    0.000 {built-in method numpy.geterrobj}\n",
       "     3220    0.002    0.000    0.007    0.000 common.py:1784(_is_dtype)\n",
       "     2263    0.002    0.000    0.004    0.000 common.py:1118(is_datetime64_ns_dtype)\n",
       "      348    0.002    0.000    0.015    0.000 concat.py:20(get_dtype_kinds)\n",
       "     2297    0.002    0.000    0.088    0.000 base.py:802(_assert_take_fillable)\n",
       "     2227    0.002    0.000    0.086    0.000 generic.py:178(_validate_dtype)\n",
       "     2616    0.002    0.000    0.003    0.000 generic.py:5036(__finalize__)\n",
       "      109    0.002    0.000    0.211    0.002 managers.py:1696(form_blocks)\n",
       "     2715    0.002    0.000    0.006    0.000 cast.py:1218(construct_1d_ndarray_preserving_na)\n",
       "     2762    0.002    0.000    0.003    0.000 base.py:551(<dictcomp>)\n",
       "    11467    0.002    0.000    0.002    0.000 {method 'items' of 'dict' objects}\n",
       "       93    0.002    0.000    0.038    0.000 concat.py:237(__init__)\n",
       "     1176    0.002    0.000    0.006    0.000 blocks.py:128(_consolidate_key)\n",
       "      108    0.002    0.000    0.022    0.000 {pandas._libs.lib.clean_index_list}\n",
       "     2743    0.002    0.000    0.599    0.000 inference.py:121(is_iterator)\n",
       "      432    0.002    0.000    1.764    0.004 concat.py:230(concatenate_join_units)\n",
       "     5194    0.002    0.000    0.007    0.000 base.py:3663(get_values)\n",
       "     3493    0.002    0.000    0.184    0.000 missing.py:25(isna)\n",
       "  433/109    0.002    0.000    0.018    0.000 <frozen importlib._bootstrap>:978(_find_and_load)\n",
       "    18711    0.002    0.000    0.002    0.000 {method 'add' of 'set' objects}\n",
       "     2811    0.002    0.000    0.010    0.000 arrays.py:7(extract_array)\n",
       "     4924    0.002    0.000    0.002    0.000 {method 'search' of 're.Pattern' objects}\n",
       "     2263    0.002    0.000    0.004    0.000 numpy_.py:170(__array__)\n",
       "       18    0.002    0.000    0.003    0.000 merge.py:647(_maybe_add_join_keys)\n",
       "     2567    0.002    0.000    3.197    0.001 managers.py:911(consolidate)\n",
       "       93    0.002    0.000    1.707    0.018 concat.py:383(get_result)\n",
       "     3411    0.002    0.000    0.002    0.000 base.py:633(_reset_identity)\n",
       "     2217    0.002    0.000    0.007    0.000 base.py:1681(is_object)\n",
       "     2264    0.002    0.000    0.011    0.000 series.py:490(get_values)\n",
       "     6006    0.002    0.000    0.002    0.000 blocks.py:304(shape)\n",
       "      330    0.002    0.000    0.003    0.000 numeric.py:676(require)\n",
       "      599    0.002    0.000    0.002    0.000 {method 'get_loc' of 'pandas._libs.index.IndexEngine' objects}\n",
       "     9807    0.002    0.000    0.002    0.000 base.py:3632(_values)\n",
       "      294    0.002    0.000    0.917    0.003 concat.py:101(_concat_compat)\n",
       "     2263    0.002    0.000    0.004    0.000 common.py:1167(is_timedelta64_ns_dtype)\n",
       "      168    0.002    0.000    0.003    0.000 base.py:1658(is_unique)\n",
       "      172    0.002    0.000    0.002    0.000 {method 'get_indexer' of 'pandas._libs.index.IndexEngine' objects}\n",
       "      542    0.002    0.000    0.003    0.000 blocks.py:3100(_extend_blocks)\n",
       "     1272    0.002    0.000    0.134    0.000 concat.py:379(<genexpr>)\n",
       "     5021    0.002    0.000    0.002    0.000 blocks.py:191(fill_value)\n",
       "     2264    0.001    0.000    0.004    0.000 blocks.py:184(to_dense)\n",
       "     2958    0.001    0.000    0.001    0.000 {built-in method pandas._libs.missing.checknull}\n",
       "     6458    0.001    0.000    0.001    0.000 common.py:122(classes_and_not_datetimelike)\n",
       "     4978    0.001    0.000    0.001    0.000 {pandas._libs.algos.ensure_platform_int}\n",
       "      117    0.001    0.000    0.007    0.000 <frozen importlib._bootstrap_external>:1356(find_spec)\n",
       "      112    0.001    0.000    0.001    0.000 socket.py:337(send)\n",
       "2301/2300    0.001    0.000    0.139    0.000 {built-in method builtins.all}\n",
       "        1    0.001    0.001   63.911   63.911 <ipython-input-50-c781f889ab45>:29(run_ensemble)\n",
       "      105    0.001    0.000    3.175    0.030 managers.py:1887(_consolidate)\n",
       "      172    0.001    0.000    0.032    0.000 base.py:2715(get_indexer)\n",
       "      115    0.001    0.000    1.215    0.011 managers.py:318(apply)\n",
       "      160    0.001    0.000    0.066    0.000 algorithms.py:132(_reconstruct_data)\n",
       "      109    0.001    0.000    0.010    0.000 <frozen importlib._bootstrap>:882(_find_spec)\n",
       "      494    0.001    0.000    0.137    0.000 concat.py:367(is_uniform_join_units)\n",
       "     1313    0.001    0.000    0.001    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
       "        1    0.001    0.001    0.001    0.001 parsers.py:1830(__init__)\n",
       "     4948    0.001    0.000    0.001    0.000 {method 'lower' of 'str' objects}\n",
       "     8880    0.001    0.000    0.001    0.000 base.py:704(ndim)\n",
       "     2326    0.001    0.000    0.002    0.000 generic.py:426(_info_axis)\n",
       "     2166    0.001    0.000    0.002    0.000 {built-in method builtins.max}\n",
       "     3197    0.001    0.000    0.008    0.000 numeric.py:541(asanyarray)\n",
       "       27    0.001    0.000    0.002    0.000 <ipython-input-42-996dcf9791ab>:1(cm_dict)\n",
       "      186    0.001    0.000    0.001    0.000 blocks.py:3145(<listcomp>)\n",
       "     1098    0.001    0.000    0.002    0.000 range.py:510(__len__)\n",
       "     2766    0.001    0.000    0.001    0.000 {method 'update' of 'dict' objects}\n",
       "     3527    0.001    0.000    0.001    0.000 {built-in method builtins.iter}\n",
       "      433    0.001    0.000    0.002    0.000 <frozen importlib._bootstrap>:157(_get_module_lock)\n",
       "     1229    0.001    0.000    0.002    0.000 <frozen importlib._bootstrap>:416(parent)\n",
       "      821    0.001    0.000    0.001    0.000 _internal.py:886(npy_ctypes_check)\n",
       "      468    0.001    0.000    0.007    0.000 cast.py:1151(construct_1d_arraylike_from_scalar)\n",
       "     1153    0.001    0.000    0.003    0.000 common.py:923(is_signed_integer_dtype)\n",
       "      232    0.001    0.000    0.008    0.000 managers.py:934(get)\n",
       "      191    0.001    0.000    0.001    0.000 {method 'argsort' of 'numpy.ndarray' objects}\n",
       "     2825    0.001    0.000    0.001    0.000 base.py:676(dtype)\n",
       "     2214    0.001    0.000    0.001    0.000 {built-in method sys.intern}\n",
       "       18    0.001    0.000    3.150    0.175 merge.py:37(merge)\n",
       "     1237    0.001    0.000    0.012    0.000 common.py:1078(is_datetime64_any_dtype)\n",
       "     2214    0.001    0.000    0.001    0.000 indexing.py:1831(_get_partial_string_timestamp_match_key)\n",
       "       28    0.001    0.000    1.876    0.067 generic.py:960(rename)\n",
       "     2214    0.001    0.000    0.001    0.000 {built-in method sys._getframe}\n",
       "  433/109    0.001    0.000    0.016    0.000 <frozen importlib._bootstrap>:948(_find_and_load_unlocked)\n",
       "      524    0.001    0.000    0.001    0.000 cast.py:341(infer_dtype_from_scalar)\n",
       "     2420    0.001    0.000    0.001    0.000 frame.py:361(_constructor)\n",
       "      326    0.001    0.000    0.007    0.000 algorithms.py:38(_ensure_data)\n",
       "      499    0.001    0.000    0.006    0.000 cast.py:846(maybe_infer_to_datetimelike)\n",
       "      107    0.001    0.000    0.373    0.003 construction.py:170(init_dict)\n",
       "      702    0.001    0.000    0.017    0.000 concat.py:126(dtype)\n",
       "     3174    0.001    0.000    0.001    0.000 {method 'pop' of 'dict' objects}\n",
       "      360    0.001    0.000    0.002    0.000 numerictypes.py:602(find_common_type)\n",
       "      256    0.001    0.000    1.200    0.005 blocks.py:749(copy)\n",
       "     2286    0.001    0.000    0.001    0.000 series.py:338(_constructor)\n",
       "       65    0.001    0.000    0.039    0.001 indexing.py:1271(_convert_to_indexer)\n",
       "     2781    0.001    0.000    0.001    0.000 {pandas._libs.algos.ensure_object}\n",
       "      158    0.001    0.000    1.984    0.013 frame.py:4666(f)\n",
       "      432    0.001    0.000    0.768    0.002 concat.py:240(<listcomp>)\n",
       "      360    0.001    0.000    0.001    0.000 numerictypes.py:654(<listcomp>)\n",
       "      525    0.001    0.000    0.001    0.000 shape_base.py:83(atleast_2d)\n",
       "     3292    0.001    0.000    0.001    0.000 format.py:301(len)\n",
       "      433    0.001    0.000    0.001    0.000 <frozen importlib._bootstrap>:78(acquire)\n",
       "      209    0.001    0.000    0.015    0.000 frame.py:742(iteritems)\n",
       "     1779    0.001    0.000    0.001    0.000 {method 'rpartition' of 'str' objects}\n",
       "      720    0.001    0.000    0.001    0.000 numerictypes.py:578(_can_coerce_all)\n",
       "      566    0.001    0.000    0.002    0.000 base.py:2650(get_loc)\n",
       "      243    0.001    0.000    0.001    0.000 base.py:643(_engine)\n",
       "      863    0.001    0.000    0.002    0.000 common.py:980(is_unsigned_integer_dtype)\n",
       "      232    0.001    0.000    0.007    0.000 frame.py:3342(_box_item_values)\n",
       "      109    0.001    0.000    0.121    0.001 construction.py:254(_homogenize)\n",
       "       33    0.001    0.000    0.001    0.000 index_tricks.py:316(__getitem__)\n",
       "      146    0.001    0.000    0.003    0.000 numeric.py:34(__new__)\n",
       "       18    0.001    0.000    2.234    0.124 merge.py:541(get_result)\n",
       "       65    0.001    0.000    0.035    0.001 indexing.py:1108(_get_listlike_indexer)\n",
       "     1344    0.001    0.000    0.001    0.000 concat.py:376(<genexpr>)\n",
       "       33    0.001    0.000    0.014    0.000 managers.py:1134(insert)\n",
       "        2    0.001    0.000    0.049    0.024 sql.py:136(_wrap_result)\n",
       "     1176    0.001    0.000    0.006    0.000 managers.py:1893(<lambda>)\n",
       "       80    0.001    0.000    1.391    0.017 managers.py:1988(_transform_index)\n",
       "      110    0.001    0.000    0.005    0.000 generic.py:1657(_get_label_or_level_values)\n",
       "     2492    0.001    0.000    0.001    0.000 base.py:1396(nlevels)\n",
       "      433    0.001    0.000    0.001    0.000 <frozen importlib._bootstrap>:103(release)\n",
       "       93    0.001    0.000    1.746    0.019 concat.py:24(concat)\n",
       "       93    0.001    0.000    0.016    0.000 concat.py:440(_get_new_axes)\n",
       "      457    0.000    0.000    0.003    0.000 base.py:963(_ndarray_values)\n",
       "      109    0.000    0.000    0.007    0.000 <frozen importlib._bootstrap_external>:1240(_get_spec)\n",
       "      433    0.000    0.000    0.001    0.000 <frozen importlib._bootstrap>:58(__init__)\n",
       "      124    0.000    0.000    0.004    0.000 {built-in method builtins.sorted}\n",
       "       65    0.000    0.000    0.001    0.000 indexing.py:1209(_validate_read_indexer)\n",
       "      864    0.000    0.000    0.001    0.000 concat.py:391(<genexpr>)\n",
       "      165    0.000    0.000    0.007    0.000 numeric.py:67(_shallow_copy)\n",
       "      330    0.000    0.000    0.001    0.000 numeric.py:748(<setcomp>)\n",
       "      585    0.000    0.000    0.001    0.000 <frozen importlib._bootstrap_external>:56(_path_join)\n",
       "      564    0.000    0.000    0.004    0.000 {method 'all' of 'numpy.ndarray' objects}\n",
       "      731    0.000    0.000    0.004    0.000 common.py:605(is_string_dtype)\n",
       "       95    0.000    0.000    0.010    0.000 base.py:3089(reindex)\n",
       "      186    0.000    0.000    1.448    0.008 shape_base.py:229(vstack)\n",
       "      164    0.000    0.000    0.001    0.000 generic.py:1546(_is_label_reference)\n",
       "     3292    0.000    0.000    0.001    0.000 __init__.py:291(strlen)\n",
       "      711    0.000    0.000    0.030    0.000 blocks.py:175(get_values)\n",
       "      731    0.000    0.000    0.002    0.000 common.py:634(condition)\n",
       "       18    0.000    0.000    0.001    0.000 function_base.py:4220(delete)\n",
       "      166    0.000    0.000    0.271    0.002 algorithms.py:217(_get_data_algo)\n",
       "     2263    0.000    0.000    0.000    0.000 common.py:1195(<lambda>)\n",
       "      294    0.000    0.000    0.477    0.002 socket.py:575(readinto)\n",
       "       37    0.000    0.000    0.001    0.000 base.py:3926(contains)\n",
       "       93    0.000    0.000    0.002    0.000 concat.py:84(_get_frame_result_type)\n",
       "      108    0.000    0.000    1.219    0.011 managers.py:710(copy)\n",
       "      172    0.000    0.000    0.001    0.000 base.py:4459(_maybe_promote)\n",
       "      109    0.000    0.000    0.218    0.002 managers.py:1663(create_block_manager_from_arrays)\n",
       "       83    0.000    0.000    0.023    0.000 managers.py:1810(_multi_blockify)\n",
       "      216    0.000    0.000    0.001    0.000 generic.py:1513(_is_level_reference)\n",
       "      484    0.000    0.000    0.001    0.000 common.py:1472(is_numeric_dtype)\n",
       "      200    0.000    0.000    0.016    0.000 frame.py:4685(<genexpr>)\n",
       "      585    0.000    0.000    0.001    0.000 <frozen importlib._bootstrap_external>:58(<listcomp>)\n",
       "       27    0.000    0.000    0.000    0.000 {built-in method builtins.__build_class__}\n",
       "      138    0.000    0.000    0.000    0.000 {built-in method pandas._libs.lib.is_bool_array}\n",
       "      102    0.000    0.000    0.009    0.000 base.py:584(_shallow_copy_with_infer)\n",
       "      433    0.000    0.000    0.003    0.000 <frozen importlib._bootstrap>:147(__enter__)\n",
       "      602    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
       "       37    0.000    0.000    0.073    0.002 frame.py:3565(_sanitize_column)\n",
       "       52    0.000    0.000    0.928    0.018 generic.py:5699(copy)\n",
       "      338    0.000    0.000    0.001    0.000 common.py:1198(is_datetime_or_timedelta_dtype)\n",
       "      160    0.000    0.000    1.982    0.012 _decorators.py:146(wrapper)\n",
       "      923    0.000    0.000    0.000    0.000 concat.py:104(__init__)\n",
       "       18    0.000    0.000    0.006    0.000 base.py:2357(intersection)\n",
       "       18    0.000    0.000    0.914    0.051 merge.py:777(_get_merge_keys)\n",
       "      151    0.000    0.000    0.001    0.000 indexing.py:2450(convert_to_index_sliceable)\n",
       "       37    0.000    0.000    0.015    0.000 managers.py:1019(set)\n",
       "      186    0.000    0.000    0.018    0.000 generic.py:5140(_consolidate)\n",
       "       18    0.000    0.000    0.915    0.051 merge.py:474(__init__)\n",
       "  324/108    0.000    0.000    0.015    0.000 {built-in method builtins.__import__}\n",
       "      432    0.000    0.000    0.001    0.000 concat.py:388(is_uniform_reindex)\n",
       "      866    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n",
       "      274    0.000    0.000    0.000    0.000 {pandas._libs.internals.get_blkno_placements}\n",
       "       49    0.000    0.000    0.436    0.009 frame.py:2952(_getitem_bool_array)\n",
       "      812    0.000    0.000    0.000    0.000 concat.py:450(_next_or_none)\n",
       "       36    0.000    0.000    0.128    0.004 frame.py:6558(append)\n",
       "       93    0.000    0.000    0.006    0.000 concat.py:464(_get_comb_axis)\n",
       "       27    0.000    0.000    0.001    0.000 <ipython-input-52-a7c9da0d1405>:15(buildParseTree)\n",
       "      109    0.000    0.000    0.358    0.003 construction.py:43(arrays_to_mgr)\n",
       "       93    0.000    0.000    0.010    0.000 concat.py:475(_get_concat_axis)\n",
       "        1    0.000    0.000    0.026    0.026 <ipython-input-52-a7c9da0d1405>:99(get_ref_ann)\n",
       "      336    0.000    0.000    0.003    0.000 common.py:1431(needs_i8_conversion)\n",
       "      216    0.000    0.000    0.004    0.000 base.py:700(view)\n",
       "      433    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:176(cb)\n",
       "      186    0.000    0.000    0.002    0.000 fromnumeric.py:942(argsort)\n",
       "      493    0.000    0.000    0.001    0.000 managers.py:1844(_asarray_compat)\n",
       "      112    0.000    0.000    0.002    0.000 iostream.py:195(schedule)\n",
       "      243    0.000    0.000    0.001    0.000 format.py:1019(base_formatter)\n",
       "      215    0.000    0.000    0.000    0.000 generic.py:3149(_clear_item_cache)\n",
       "       82    0.000    0.000    0.003    0.000 iostream.py:382(write)\n",
       "      110    0.000    0.000    0.001    0.000 generic.py:1607(_check_label_or_level_ambiguity)\n",
       "       60    0.000    0.000    0.001    0.000 range.py:69(__new__)\n",
       "       24    0.000    0.000    0.000    0.000 base.py:1587(is_monotonic_increasing)\n",
       "     1008    0.000    0.000    0.000    0.000 concat.py:381(<genexpr>)\n",
       "      180    0.000    0.000    0.000    0.000 managers.py:1546(external_values)\n",
       "       54    0.000    0.000    0.011    0.000 base.py:3988(append)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'writelines' of '_io._IOBase' objects}\n",
       "      172    0.000    0.000    0.002    0.000 base.py:1669(is_boolean)\n",
       "       65    0.000    0.000    0.001    0.000 indexing.py:256(_convert_scalar_indexer)\n",
       "      150    0.000    0.000    0.001    0.000 codecs.py:319(decode)\n",
       "      138    0.000    0.000    0.001    0.000 blocks.py:2633(is_bool)\n",
       "       33    0.000    0.000    0.002    0.000 managers.py:2008(_fast_count_smallints)\n",
       "       18    0.000    0.000    0.223    0.012 generic.py:3787(_drop_axis)\n",
       "       62    0.000    0.000    0.079    0.001 blocks.py:323(concat_same_type)\n",
       "      150    0.000    0.000    0.000    0.000 {built-in method _codecs.utf_8_decode}\n",
       "       57    0.000    0.000    0.004    0.000 api.py:128(_union_indexes)\n",
       "       42    0.000    0.000    0.018    0.000 generic.py:1439(__neg__)\n",
       "       85    0.000    0.000    0.000    0.000 managers.py:2015(_preprocess_slice_or_indexer)\n",
       "        2    0.000    0.000    1.024    0.512 connections.py:1149(_read_result_packet)\n",
       "      186    0.000    0.000    0.001    0.000 shape_base.py:283(<listcomp>)\n",
       "      294    0.000    0.000    0.001    0.000 concat.py:151(<listcomp>)\n",
       "      266    0.000    0.000    0.000    0.000 managers.py:174(_is_single_block)\n",
       "       14    0.000    0.000    0.000    0.000 {method 'sendall' of '_socket.socket' objects}\n",
       "      294    0.000    0.000    0.000    0.000 concat.py:126(<listcomp>)\n",
       "       18    0.000    0.000    0.005    0.000 base.py:4939(drop)\n",
       "      426    0.000    0.000    0.000    0.000 base.py:1237(_get_names)\n",
       "      252    0.000    0.000    0.002    0.000 base.py:646(<lambda>)\n",
       "      294    0.000    0.000    0.000    0.000 {method '_checkReadable' of '_io._IOBase' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'close' of '_io.TextIOWrapper' objects}\n",
       "      564    0.000    0.000    0.003    0.000 _methods.py:45(_all)\n",
       "       93    0.000    0.000    0.001    0.000 concat.py:309(<listcomp>)\n",
       "     1302    0.000    0.000    0.000    0.000 {built-in method _imp.acquire_lock}\n",
       "      433    0.000    0.000    0.001    0.000 <frozen importlib._bootstrap>:151(__exit__)\n",
       "      216    0.000    0.000    0.004    0.000 managers.py:729(<lambda>)\n",
       "      186    0.000    0.000    0.001    0.000 fromnumeric.py:54(_wrapfunc)\n",
       "       18    0.000    0.000    0.214    0.012 generic.py:4469(_reindex_with_indexers)\n",
       "      924    0.000    0.000    0.001    0.000 {built-in method builtins.next}\n",
       "      226    0.000    0.000    0.000    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
       "       93    0.000    0.000    0.000    0.000 api.py:73(_get_distinct_objs)\n",
       "       93    0.000    0.000    0.005    0.000 api.py:87(_get_combined_index)\n",
       "        1    0.000    0.000    0.203    0.203 {pandas._libs.reduction.reduce}\n",
       "       18    0.000    0.000    1.676    0.093 merge.py:737(_get_join_info)\n",
       "       93    0.000    0.000    0.001    0.000 generic.py:337(_from_axes)\n",
       "       47    0.000    0.000    0.000    0.000 sorting.py:47(_int64_cut_off)\n",
       "      918    0.000    0.000    0.001    0.000 format.py:1401(just)\n",
       "      585    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:222(_verbose_message)\n",
       "      186    0.000    0.000    0.000    0.000 shape_base.py:220(_warn_for_nonsequence)\n",
       "      588    0.000    0.000    0.000    0.000 concat.py:120(is_nonempty)\n",
       "      288    0.000    0.000    0.001    0.000 frame.py:937(__len__)\n",
       "     1728    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}\n",
       "       19    0.000    0.000    0.216    0.011 generic.py:4113(reindex)\n",
       "      261    0.000    0.000    0.004    0.000 base.py:1729(inferred_type)\n",
       "      111    0.000    0.000    0.010    0.000 managers.py:2041(<listcomp>)\n",
       "     1302    0.000    0.000    0.000    0.000 {built-in method _imp.release_lock}\n",
       "       42    0.000    0.000    0.004    0.000 series.py:730(__array_wrap__)\n",
       "        1    0.000    0.000    0.000    0.000 {pandas._libs.writers.write_csv_rows}\n",
       "      594    0.000    0.000    0.000    0.000 concat.py:136(<genexpr>)\n",
       "       28    0.000    0.000    1.877    0.067 frame.py:3942(rename)\n",
       "       37    0.000    0.000    0.090    0.002 frame.py:3356(__setitem__)\n",
       "       62    0.000    0.000    0.000    0.000 range.py:136(_simple_new)\n",
       "       59    0.000    0.000    0.001    0.000 base.py:5408(default_index)\n",
       "       54    0.000    0.000    0.011    0.000 base.py:4017(_concat)\n",
       "       58    0.000    0.000    0.000    0.000 managers.py:147(set_axis)\n",
       "       65    0.000    0.000    0.002    0.000 fromnumeric.py:2664(prod)\n",
       "        1    0.000    0.000    0.023    0.023 <ipython-input-50-c781f889ab45>:54(generate_ensemble_metrics)\n",
       "      180    0.000    0.000    0.000    0.000 {pandas._libs.lib.array_equivalent_object}\n",
       "      187    0.000    0.000    0.001    0.000 frame.py:491(shape)\n",
       "       37    0.000    0.000    0.089    0.002 frame.py:3433(_set_item)\n",
       "      408    0.000    0.000    0.000    0.000 blocks.py:255(__len__)\n",
       "      952    0.000    0.000    0.001    0.000 format.py:1392(<genexpr>)\n",
       "     1170    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}\n",
       "       47    0.000    0.000    0.000    0.000 generic.py:297(_construct_axes_from_arguments)\n",
       "       65    0.000    0.000    0.000    0.000 base.py:2849(_convert_scalar_indexer)\n",
       "       93    0.000    0.000    0.005    0.000 api.py:44(_get_objs_combined_axis)\n",
       "      251    0.000    0.000    0.000    0.000 _validators.py:221(validate_bool_kwarg)\n",
       "       18    0.000    0.000    0.001    0.000 merge.py:889(_maybe_coerce_merge_keys)\n",
       "      433    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:143(__init__)\n",
       "        6    0.000    0.000    0.000    0.000 {pandas._libs.algos.rank_1d_float64}\n",
       "      109    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:873(_find_spec_legacy)\n",
       "       18    0.000    0.000    0.000    0.000 function_base.py:1149(diff)\n",
       "       33    0.000    0.000    0.010    0.000 base.py:4919(insert)\n",
       "      436    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:859(__exit__)\n",
       "      226    0.000    0.000    0.000    0.000 common.py:1513(is_string_like_dtype)\n",
       "       46    0.000    0.000    0.000    0.000 _validators.py:230(validate_axis_style_args)\n",
       "      328    0.000    0.000    0.000    0.000 generic.py:1576(<genexpr>)\n",
       "     1053    0.000    0.000    0.000    0.000 format.py:1418(_is_number)\n",
       "      951    0.000    0.000    0.000    0.000 {pandas._libs.lib.is_bool}\n",
       "       42    0.000    0.000    0.002    0.000 base.py:2445(difference)\n",
       "       36    0.000    0.000    0.005    0.000 concat.py:481(_concat_index_same_dtype)\n",
       "       18    0.000    0.000    0.909    0.050 generic.py:1729(_drop_labels_or_levels)\n",
       "      180    0.000    0.000    0.001    0.000 series.py:434(values)\n",
       "       69    0.000    0.000    0.009    0.000 construction.py:284(extract_index)\n",
       "      106    0.000    0.000    0.000    0.000 common.py:212(dict_keys_to_ordered_list)\n",
       "      109    0.000    0.000    0.008    0.000 <frozen importlib._bootstrap_external>:1272(find_spec)\n",
       "      749    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
       "      191    0.000    0.000    0.000    0.000 missing.py:530(clean_reindex_fill_method)\n",
       "      143    0.000    0.000    0.000    0.000 config.py:78(_get_single_key)\n",
       "      594    0.000    0.000    0.000    0.000 concat.py:137(<genexpr>)\n",
       "  324/108    0.000    0.000    0.015    0.000 <frozen importlib._bootstrap>:211(_call_with_frames_removed)\n",
       "        9    0.000    0.000    0.002    0.000 format.py:1045(get_result_as_array)\n",
       "       54    0.000    0.000    0.000    0.000 twodim_base.py:216(diag)\n",
       "       34    0.000    0.000    0.012    0.000 {built-in method builtins.print}\n",
       "        9    0.000    0.000    0.002    0.000 format.py:1060(format_values_with)\n",
       "      112    0.000    0.000    0.000    0.000 threading.py:1080(is_alive)\n",
       "      109    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:369(__init__)\n",
       "      118    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1203(_path_importer_cache)\n",
       "       18    0.000    0.000    0.301    0.017 generic.py:3759(drop)\n",
       "      110    0.000    0.000    0.002    0.000 generic.py:3461(xs)\n",
       "       50    0.000    0.000    0.000    0.000 api.py:262(<setcomp>)\n",
       "      939    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}\n",
       "       65    0.000    0.000    0.001    0.000 base.py:2965(_convert_listlike_indexer)\n",
       "      252    0.000    0.000    0.001    0.000 generic.py:1895(<genexpr>)\n",
       "      132    0.000    0.000    0.000    0.000 {method 'add' of 'pandas._libs.internals.BlockPlacement' objects}\n",
       "       34    0.000    0.000    0.001    0.000 format.py:1407(<listcomp>)\n",
       "      117    0.000    0.000    0.004    0.000 <frozen importlib._bootstrap_external>:74(_path_stat)\n",
       "       36    0.000    0.000    0.006    0.000 numeric.py:110(_concat_same_dtype)\n",
       "      108    0.000    0.000    0.004    0.000 managers.py:730(<listcomp>)\n",
       "       46    0.000    0.000    2.093    0.046 _decorators.py:195(wrapper)\n",
       "      160    0.000    0.000    0.001    0.000 algorithms.py:163(_ensure_arraylike)\n",
       "       84    0.000    0.000    0.001    0.000 generic.py:1848(empty)\n",
       "      436    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:855(__enter__)\n",
       "      274    0.000    0.000    0.000    0.000 managers.py:706(nblocks)\n",
       "       36    0.000    0.000    0.001    0.000 base.py:1272(set_names)\n",
       "      330    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
       "       65    0.000    0.000    0.023    0.000 base.py:4447(get_indexer_for)\n",
       "       18    0.000    0.000    0.078    0.004 generic.py:3840(_update_inplace)\n",
       "       93    0.000    0.000    0.000    0.000 api.py:67(<listcomp>)\n",
       "       66    0.000    0.000    0.001    0.000 function_base.py:4641(append)\n",
       "       18    0.000    0.000    0.002    0.000 concat.py:486(_concat_index_asobject)\n",
       "      108    0.000    0.000    0.001    0.000 generic.py:1844(__contains__)\n",
       "       49    0.000    0.000    0.000    0.000 generic.py:1814(__hash__)\n",
       "       57    0.000    0.000    0.000    0.000 api.py:205(_sanitize_and_check)\n",
       "       23    0.000    0.000    0.002    0.000 range.py:272(_shallow_copy)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method posix.listdir}\n",
       "       33    0.000    0.000    0.006    0.000 base.py:3830(_coerce_scalar_to_index)\n",
       "      186    0.000    0.000    0.001    0.000 concat.py:92(<genexpr>)\n",
       "      192    0.000    0.000    0.000    0.000 _config.py:49(getter)\n",
       "      146    0.000    0.000    0.000    0.000 base.py:3806(_coerce_to_ndarray)\n",
       "      284    0.000    0.000    0.000    0.000 config.py:561(_get_deprecated_option)\n",
       "      294    0.000    0.000    0.000    0.000 socket.py:614(readable)\n",
       "       54    0.000    0.000    0.000    0.000 fromnumeric.py:1395(diagonal)\n",
       "       48    0.000    0.000    0.000    0.000 printing.py:59(<listcomp>)\n",
       "       50    0.000    0.000    0.000    0.000 api.py:243(_get_consensus_names)\n",
       "      186    0.000    0.000    0.000    0.000 blocks.py:3146(<listcomp>)\n",
       "        1    0.000    0.000    0.008    0.008 format.py:503(_to_str_columns)\n",
       "       37    0.000    0.000    0.015    0.000 generic.py:3171(_set_item)\n",
       "      112    0.000    0.000    0.000    0.000 threading.py:1038(_wait_for_tstate_lock)\n",
       "       18    0.000    0.000    0.215    0.012 frame.py:3729(_reindex_axes)\n",
       "      448    0.000    0.000    0.000    0.000 format.py:541(<genexpr>)\n",
       "       34    0.000    0.000    0.002    0.000 format.py:1384(_make_fixed_width)\n",
       "       15    0.000    0.000    0.000    0.000 format.py:1427(<listcomp>)\n",
       "       48    0.000    0.000    0.000    0.000 <ipython-input-50-c781f889ab45>:8(expressions)\n",
       "       82    0.000    0.000    0.000    0.000 iostream.py:307(_is_master_process)\n",
       "      232    0.000    0.000    0.000    0.000 base.py:475(<genexpr>)\n",
       "      169    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
       "      866    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}\n",
       "      279    0.000    0.000    0.000    0.000 common.py:164(<genexpr>)\n",
       "        6    0.000    0.000    0.013    0.002 strings.py:1854(_wrap_result)\n",
       "      120    0.000    0.000    0.000    0.000 range.py:89(ensure_int)\n",
       "       43    0.000    0.000    0.174    0.004 managers.py:1796(_simple_blockify)\n",
       "       24    0.000    0.000    0.000    0.000 format.py:1422(<listcomp>)\n",
       "       66    0.000    0.000    0.000    0.000 fromnumeric.py:1583(ravel)\n",
       "      141    0.000    0.000    0.000    0.000 config.py:546(_get_root)\n",
       "      215    0.000    0.000    0.000    0.000 {method 'clear' of 'dict' objects}\n",
       "       65    0.000    0.000    0.001    0.000 base.py:1672(is_integer)\n",
       "       18    0.000    0.000    0.216    0.012 frame.py:3794(reindex)\n",
       "       93    0.000    0.000    0.000    0.000 common.py:162(_not_none)\n",
       "      117    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:36(_relax_case)\n",
       "       18    0.000    0.000    0.011    0.001 managers.py:1959(items_overlap_with_suffix)\n",
       "      186    0.000    0.000    0.001    0.000 concat.py:93(<genexpr>)\n",
       "       17    0.000    0.000    0.005    0.000 format.py:848(format_array)\n",
       "      880    0.000    0.000    0.000    0.000 {method 'rjust' of 'str' objects}\n",
       "       27    0.000    0.000    0.003    0.000 <ipython-input-52-a7c9da0d1405>:55(make_parse_tree)\n",
       "      694    0.000    0.000    0.000    0.000 {method 'ljust' of 'str' objects}\n",
       "      294    0.000    0.000    0.000    0.000 {method '_checkClosed' of '_io._IOBase' objects}\n",
       "      109    0.000    0.000    0.000    0.000 six.py:184(find_module)\n",
       "       65    0.000    0.000    0.001    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
       "       56    0.000    0.000    0.000    0.000 common.py:457(_get_rename_function)\n",
       "       36    0.000    0.000    0.008    0.000 concat.py:531(_concat_indexes)\n",
       "      186    0.000    0.000    0.000    0.000 shape_base.py:209(_arrays_for_stack_dispatcher)\n",
       "       14    0.000    0.000    0.000    0.000 connections.py:744(_execute_command)\n",
       "       10    0.000    0.000    0.000    0.000 protocol.py:283(__init__)\n",
       "       27    0.000    0.000    0.010    0.000 <ipython-input-48-407a16a06efa>:11(__init__)\n",
       "      244    0.000    0.000    0.000    0.000 missing.py:259(notna)\n",
       "       18    0.000    0.000    0.215    0.012 frame.py:3754(_reindex_columns)\n",
       "      106    0.000    0.000    0.000    0.000 construction.py:210(<listcomp>)\n",
       "      109    0.000    0.000    0.000    0.000 {built-in method _imp.is_frozen}\n",
       "      141    0.000    0.000    0.001    0.000 config.py:96(_get_option)\n",
       "        4    0.000    0.000    0.001    0.000 printing.py:15(adjoin)\n",
       "       24    0.000    0.000    0.000    0.000 copy.py:66(copy)\n",
       "      440    0.000    0.000    0.000    0.000 managers.py:1814(<lambda>)\n",
       "       36    0.000    0.000    0.000    0.000 base.py:1240(_set_names)\n",
       "      448    0.000    0.000    0.000    0.000 format.py:1423(<genexpr>)\n",
       "       93    0.000    0.000    0.000    0.000 concat.py:434(_get_result_dim)\n",
       "        6    0.000    0.000    0.590    0.098 strings.py:57(_na_map)\n",
       "        2    0.000    0.000    0.001    0.000 connections.py:1213(_get_descriptions)\n",
       "      112    0.000    0.000    0.000    0.000 iostream.py:93(_event_pipe)\n",
       "      349    0.000    0.000    0.000    0.000 numeric.py:113(is_all_dates)\n",
       "      226    0.000    0.000    0.000    0.000 common.py:1542(<lambda>)\n",
       "       18    0.000    0.000    0.001    0.000 base.py:4909(delete)\n",
       "      108    0.000    0.000    0.000    0.000 managers.py:1974(lrenamer)\n",
       "       16    0.000    0.000    0.006    0.000 format.py:702(_format_col)\n",
       "       18    0.000    0.000    0.000    0.000 merge.py:1704(<lambda>)\n",
       "       10    0.000    0.000    0.603    0.060 <ipython-input-46-5ca52ce91f6b>:52(<lambda>)\n",
       "       20    0.000    0.000    0.000    0.000 {pandas._libs.lib.infer_datetimelike_array}\n",
       "      191    0.000    0.000    0.000    0.000 missing.py:71(clean_fill_method)\n",
       "       18    0.000    0.000    0.000    0.000 sorting.py:124(is_int64_overflow_possible)\n",
       "       18    0.000    0.000    0.301    0.017 frame.py:3819(drop)\n",
       "      109    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:792(find_spec)\n",
       "      316    0.000    0.000    0.000    0.000 numerictypes.py:587(<listcomp>)\n",
       "      7/6    0.000    0.000    0.590    0.098 strings.py:62(_map)\n",
       "       28    0.000    0.000    0.071    0.003 cast.py:1123(cast_scalar_to_array)\n",
       "       43    0.000    0.000    0.000    0.000 range.py:330(equals)\n",
       "      180    0.000    0.000    0.000    0.000 blocks.py:161(external_values)\n",
       "      192    0.000    0.000    0.000    0.000 _config.py:140(get)\n",
       "       68    0.000    0.000    0.000    0.000 base.py:2590(_assert_can_do_setop)\n",
       "       18    0.000    0.000    0.000    0.000 merge.py:614(_maybe_restore_index_levels)\n",
       "       54    0.000    0.000    0.000    0.000 {method 'diagonal' of 'numpy.ndarray' objects}\n",
       "       25    0.000    0.000    0.000    0.000 printing.py:55(<listcomp>)\n",
       "       46    0.000    0.000    0.000    0.000 common.py:199(count_not_none)\n",
       "      120    0.000    0.000    0.000    0.000 <ipython-input-50-c781f889ab45>:38(<genexpr>)\n",
       "       54    0.000    0.000    0.001    0.000 generic.py:1578(_is_label_or_level_reference)\n",
       "       12    0.000    0.000    0.000    0.000 concat.py:396(trim_join_unit)\n",
       "       19    0.000    0.000    0.000    0.000 protocol.py:180(read_struct)\n",
       "       95    0.000    0.000    0.000    0.000 base.py:5381(_ensure_has_len)\n",
       "      360    0.000    0.000    0.000    0.000 numerictypes.py:655(<listcomp>)\n",
       "       28    0.000    0.000    0.000    0.000 __init__.py:125(lrange)\n",
       "       21    0.000    0.000    0.000    0.000 range.py:180(_int64index)\n",
       "       93    0.000    0.000    0.000    0.000 concat.py:507(<listcomp>)\n",
       "        1    0.000    0.000    0.699    0.699 apply.py:228(apply_standard)\n",
       "       36    0.000    0.000    0.001    0.000 base.py:1346(rename)\n",
       "        7    0.000    0.000    0.015    0.002 construction.py:495(convert)\n",
       "        2    0.000    0.000    0.005    0.003 {_cython_magic_a8678e4eba7059b7e523b0769dde009b.geometric_mean}\n",
       "        6    0.000    0.000    0.000    0.000 {built-in method _warnings.warn}\n",
       "      112    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
       "        9    0.000    0.000    0.001    0.000 format.py:1078(<listcomp>)\n",
       "       99    0.000    0.000    0.000    0.000 stack.py:17(pop)\n",
       "       27    0.000    0.000    0.000    0.000 <ipython-input-52-a7c9da0d1405>:81(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'close' of 'pandas._libs.parsers.TextReader' objects}\n",
       "        1    0.000    0.000    0.001    0.001 csvs.py:290(_save_chunk)\n",
       "       18    0.000    0.000    0.000    0.000 {built-in method numpy.copyto}\n",
       "       54    0.000    0.000    0.000    0.000 base.py:4012(<setcomp>)\n",
       "        7    0.000    0.000    0.000    0.000 format.py:1140(<listcomp>)\n",
       "       38    0.000    0.000    0.001    0.000 generic.py:4341(<genexpr>)\n",
       "        7    0.000    0.000    0.000    0.000 protocol.py:237(_parse_field_descriptor)\n",
       "      136    0.000    0.000    0.000    0.000 common.py:183(_any_not_none)\n",
       "        1    0.000    0.000    0.404    0.404 apply.py:262(apply_series_generator)\n",
       "       70    0.000    0.000    0.000    0.000 {method 'transpose' of 'numpy.ndarray' objects}\n",
       "      141    0.000    0.000    0.001    0.000 config.py:226(__call__)\n",
       "       75    0.000    0.000    0.000    0.000 inference.py:470(is_sequence)\n",
       "       22    0.000    0.000    0.014    0.001 range.py:176(_data)\n",
       "       82    0.000    0.000    0.000    0.000 iostream.py:320(_schedule_flush)\n",
       "       43    0.000    0.000    0.000    0.000 printing.py:156(pprint_thing)\n",
       "      186    0.000    0.000    0.000    0.000 concat.py:97(<genexpr>)\n",
       "        6    0.000    0.000    0.000    0.000 common.py:120(_stringify_path)\n",
       "       99    0.000    0.000    0.000    0.000 binaryTree.py:17(__init__)\n",
       "       65    0.000    0.000    0.001    0.000 base.py:2999(_convert_arr_indexer)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:1343(_handle_dbapi_exception)\n",
       "       65    0.000    0.000    0.000    0.000 base.py:3035(_convert_list_indexer)\n",
       "       18    0.000    0.000    0.000    0.000 generic.py:4378(_needs_reindex_multi)\n",
       "      252    0.000    0.000    0.000    0.000 format.py:1107(<genexpr>)\n",
       "        4    0.000    0.000    1.498    0.374 base.py:1163(_execute_context)\n",
       "       42    0.000    0.000    0.000    0.000 base.py:978(empty)\n",
       "      126    0.000    0.000    0.000    0.000 managers.py:1850(_shape_compat)\n",
       "      112    0.000    0.000    0.000    0.000 threading.py:507(is_set)\n",
       "       27    0.000    0.000    0.000    0.000 <ipython-input-52-a7c9da0d1405>:63(preprocess_sentence)\n",
       "      141    0.000    0.000    0.000    0.000 config.py:602(_warn_if_deprecated)\n",
       "       43    0.000    0.000    0.000    0.000 printing.py:185(as_escaped_unicode)\n",
       "       65    0.000    0.000    0.000    0.000 indexing.py:2676(is_nested_tuple)\n",
       "      186    0.000    0.000    0.000    0.000 concat.py:383(<genexpr>)\n",
       "       33    0.000    0.000    0.000    0.000 format.py:356(_get_formatter)\n",
       "        2    0.000    0.000    0.019    0.009 construction.py:429(_list_to_arrays)\n",
       "      170    0.000    0.000    0.000    0.000 concat.py:510(<genexpr>)\n",
       "      118    0.000    0.000    0.000    0.000 {method 'pop' of 'list' objects}\n",
       "       47    0.000    0.000    0.000    0.000 generic.py:334(<dictcomp>)\n",
       "      420    0.000    0.000    0.000    0.000 format.py:1424(<genexpr>)\n",
       "      189    0.000    0.000    0.000    0.000 format.py:1139(<lambda>)\n",
       "       27    0.000    0.000    0.000    0.000 <ipython-input-48-407a16a06efa>:10(Results)\n",
       "       18    0.000    0.000    0.000    0.000 generic.py:3115(_maybe_update_cacher)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'writerow' of '_csv.writer' objects}\n",
       "       54    0.000    0.000    0.000    0.000 merge.py:1731(_should_fill)\n",
       "       99    0.000    0.000    0.000    0.000 stack.py:14(push)\n",
       "       42    0.000    0.000    0.000    0.000 base.py:759(size)\n",
       "       12    0.000    0.000    0.000    0.000 base.py:1096(tolist)\n",
       "       65    0.000    0.000    0.001    0.000 _methods.py:34(_sum)\n",
       "       35    0.000    0.000    0.000    0.000 base.py:658(__array__)\n",
       "        4    0.000    0.000    0.001    0.000 base.py:748(_checkout)\n",
       "       36    0.000    0.000    0.000    0.000 binaryTree.py:34(insertRight)\n",
       "        1    0.000    0.000    0.233    0.233 <ipython-input-52-a7c9da0d1405>:87(get_docs)\n",
       "      143    0.000    0.000    0.000    0.000 config.py:528(_select_options)\n",
       "       18    0.000    0.000    0.000    0.000 common.py:246(index_labels_to_array)\n",
       "      110    0.000    0.000    0.000    0.000 generic.py:1693(<listcomp>)\n",
       "        6    0.000    0.000    0.001    0.000 connections.py:422(rollback)\n",
       "        1    0.000    0.000   63.934   63.934 <ipython-input-50-c781f889ab45>:80(ensemble_control)\n",
       "       33    0.000    0.000    0.000    0.000 {built-in method numpy.bincount}\n",
       "       55    0.000    0.000    0.000    0.000 generic.py:3205(_check_setitem_copy)\n",
       "       57    0.000    0.000    0.000    0.000 api.py:226(<setcomp>)\n",
       "       72    0.000    0.000    0.000    0.000 managers.py:1979(rrenamer)\n",
       "        9    0.000    0.000    0.000    0.000 format.py:1430(<listcomp>)\n",
       "        2    0.000    0.000    0.001    0.000 series.py:3466(apply)\n",
       "       54    0.000    0.000    0.000    0.000 merge.py:799(<lambda>)\n",
       "       36    0.000    0.000    0.000    0.000 binaryTree.py:22(insertLeft)\n",
       "       18    0.000    0.000    0.001    0.000 generic.py:1765(<listcomp>)\n",
       "       27    0.000    0.000    0.000    0.000 format.py:945(_format)\n",
       "       10    0.000    0.000    0.001    0.000 connections.py:393(_read_ok_packet)\n",
       "      298    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
       "      104    0.000    0.000    0.000    0.000 common.py:464(f)\n",
       "      138    0.000    0.000    0.000    0.000 common.py:201(<genexpr>)\n",
       "       18    0.000    0.000    0.000    0.000 base.py:143(__setattr__)\n",
       "       18    0.000    0.000    0.002    0.000 base.py:4025(_concat_same_dtype)\n",
       "        5    0.000    0.000    0.000    0.000 blocks.py:536(_astype)\n",
       "       24    0.000    0.000    0.001    0.000 format.py:1421(_cond)\n",
       "        1    0.000    0.000   63.934   63.934 <ipython-input-53-a67a2a6b2272>:2(main)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'getvalue' of '_io.StringIO' objects}\n",
       "       18    0.000    0.000    0.000    0.000 numeric.py:175(ones)\n",
       "       36    0.000    0.000    0.000    0.000 function_base.py:258(iterable)\n",
       "       73    0.000    0.000    0.000    0.000 printing.py:50(justify)\n",
       "       36    0.000    0.000    0.000    0.000 concat.py:483(<listcomp>)\n",
       "        5    0.000    0.000    0.001    0.000 generic.py:5581(astype)\n",
       "       36    0.000    0.000    0.000    0.000 merge.py:1738(_any)\n",
       "       14    0.000    0.000    0.000    0.000 connections.py:710(_write_bytes)\n",
       "        2    0.000    0.000    0.000    0.000 schema.py:3753(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 <ipython-input-52-a7c9da0d1405>:95(<listcomp>)\n",
       "       36    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.normalize_axis_index}\n",
       "      164    0.000    0.000    0.000    0.000 generic.py:1567(<listcomp>)\n",
       "       62    0.000    0.000    0.000    0.000 managers.py:2058(<listcomp>)\n",
       "       61    0.000    0.000    0.000    0.000 series.py:591(__len__)\n",
       "       32    0.000    0.000    0.000    0.000 protocol.py:127(read_uint24)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:69(__init__)\n",
       "       17    0.000    0.000    0.004    0.000 format.py:927(get_result)\n",
       "       54    0.000    0.000    0.000    0.000 {method 'count' of 'str' objects}\n",
       "      143    0.000    0.000    0.000    0.000 config.py:589(_translate_key)\n",
       "       72    0.000    0.000    0.000    0.000 common.py:273(maybe_make_list)\n",
       "      132    0.000    0.000    0.000    0.000 binaryTree.py:47(getRightChild)\n",
       "      124    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
       "      109    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:719(find_spec)\n",
       "        6    0.000    0.000    0.001    0.000 api.py:154(_unique_indices)\n",
       "      115    0.000    0.000    0.000    0.000 managers.py:376(<dictcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:931(_format_strings)\n",
       "       18    0.000    0.000    0.000    0.000 merge.py:1011(_validate_specification)\n",
       "        1    0.000    0.000    1.751    1.751 parsers.py:1993(read)\n",
       "       43    0.000    0.000    0.000    0.000 {built-in method _struct.unpack_from}\n",
       "       96    0.000    0.000    0.000    0.000 timeout.py:56(stop)\n",
       "        1    0.000    0.000    0.001    0.001 sorting.py:189(lexsort_indexer)\n",
       "       73    0.000    0.000    0.000    0.000 format.py:304(justify)\n",
       "       18    0.000    0.000    0.000    0.000 format.py:338(_get_adjustment)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:481(checkout)\n",
       "        4    0.000    0.000    0.000    0.000 queue.py:135(get)\n",
       "       18    0.000    0.000    0.000    0.000 concat.py:503(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 generic.py:5948(fillna)\n",
       "        6    0.000    0.000    0.001    0.000 generic.py:8324(ranker)\n",
       "        6    0.000    0.000    0.001    0.000 base.py:2223(do_rollback)\n",
       "       18    0.000    0.000    0.000    0.000 {built-in method numpy.can_cast}\n",
       "        6    0.000    0.000    0.000    0.000 {pandas._libs.lib.fast_unique_multiple_list}\n",
       "       19    0.000    0.000    0.000    0.000 base.py:4071(identical)\n",
       "        6    0.000    0.000    0.000    0.000 strings.py:1795(__init__)\n",
       "       37    0.000    0.000    0.000    0.000 frame.py:3416(_ensure_valid_index)\n",
       "       18    0.000    0.000    0.000    0.000 generic.py:1778(<listcomp>)\n",
       "       82    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
       "      116    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
       "       96    0.000    0.000    0.000    0.000 timeout.py:260(_start_new_or_dummy)\n",
       "       33    0.000    0.000    0.000    0.000 base.py:4683(_maybe_cast_indexer)\n",
       "        2    0.000    0.000    0.031    0.016 construction.py:382(to_arrays)\n",
       "        4    0.000    0.000    0.000    0.000 default.py:862(_init_statement)\n",
       "       27    0.000    0.000    0.000    0.000 stack.py:8(__init__)\n",
       "        8    0.000    0.000    0.000    0.000 threading.py:335(notify)\n",
       "       60    0.000    0.000    0.000    0.000 common.py:175(_all_none)\n",
       "       18    0.000    0.000    0.000    0.000 concat.py:496(<listcomp>)\n",
       "       18    0.000    0.000    0.000    0.000 generic.py:1775(<listcomp>)\n",
       "       62    0.000    0.000    0.000    0.000 blocks.py:327(<listcomp>)\n",
       "        4    0.000    0.000    0.001    0.000 base.py:845(_reset)\n",
       "        4    0.000    0.000    0.000    0.000 queue.py:92(put)\n",
       "       96    0.000    0.000    0.000    0.000 binaryTree.py:56(getRootVal)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method posix.getcwd}\n",
       "       66    0.000    0.000    0.000    0.000 fromnumeric.py:2847(ndim)\n",
       "       54    0.000    0.000    0.000    0.000 {method 'get_count' of 'pandas._libs.hashtable.Int64Factorizer' objects}\n",
       "        2    0.000    0.000    0.001    0.000 categorical.py:317(__init__)\n",
       "       68    0.000    0.000    0.000    0.000 base.py:2249(_validate_sort_keyword)\n",
       "        6    0.000    0.000    0.000    0.000 strings.py:1805(_validate)\n",
       "        6    0.000    0.000    0.603    0.101 strings.py:2723(strip)\n",
       "        2    0.000    0.000    0.001    0.000 generic.py:3155(_slice)\n",
       "       12    0.000    0.000    0.000    0.000 api.py:168(conv)\n",
       "        9    0.000    0.000    0.001    0.000 format.py:1412(_trim_zeros)\n",
       "        4    0.000    0.000    1.497    0.374 connections.py:1073(read)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:507(checkin)\n",
       "       10    0.000    0.000    0.000    0.000 apply.py:325(<genexpr>)\n",
       "        6    0.000    0.000    0.000    0.000 {built-in method _locale.nl_langinfo}\n",
       "        6    0.000    0.000    0.000    0.000 _bootlocale.py:33(getpreferredencoding)\n",
       "        2    0.000    0.000    0.000    0.000 warnings.py:35(_formatwarnmsg_impl)\n",
       "        6    0.000    0.000    0.000    0.000 parse.py:409(urlsplit)\n",
       "        2    0.000    0.000    0.000    0.000 _methods.py:58(_mean)\n",
       "        4    0.000    0.000    0.000    0.000 blocks.py:335(set)\n",
       "        2    0.000    0.000    0.000    0.000 series.py:3600(_reduce)\n",
       "       54    0.000    0.000    0.000    0.000 merge.py:800(<lambda>)\n",
       "        1    0.000    0.000    2.031    2.031 parsers.py:536(parser_f)\n",
       "        7    0.000    0.000    0.000    0.000 protocol.py:233(__init__)\n",
       "        4    0.000    0.000    0.001    0.000 base.py:645(_finalize_fairy)\n",
       "        2    0.000    0.000    0.000    0.000 result.py:215(__init__)\n",
       "      132    0.000    0.000    0.000    0.000 binaryTree.py:50(getLeftChild)\n",
       "      115    0.000    0.000    0.000    0.000 printing.py:62(_join_unicode)\n",
       "       24    0.000    0.000    0.000    0.000 base.py:1580(is_monotonic)\n",
       "       36    0.000    0.000    0.000    0.000 concat.py:523(_maybe_check_integrity)\n",
       "        1    0.000    0.000    2.009    2.009 parsers.py:1137(read)\n",
       "        2    0.000    0.000    0.001    0.001 base.py:2312(has_table)\n",
       "        4    0.000    0.000    0.000    0.000 <ipython-input-50-c781f889ab45>:2(partly_unordered_permutations)\n",
       "        2    0.000    0.000    0.001    0.000 ops.py:1815(<lambda>)\n",
       "       18    0.000    0.000    0.000    0.000 base.py:100(_reset_cache)\n",
       "        8    0.000    0.000    0.000    0.000 base.py:2254(union)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:3752(_try_convert_to_int_index)\n",
       "       60    0.000    0.000    0.000    0.000 range.py:165(_validate_dtype)\n",
       "        1    0.000    0.000    0.001    0.001 format.py:642(_join_multiline)\n",
       "       17    0.000    0.000    0.000    0.000 format.py:912(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 cursors.py:116(_escape_args)\n",
       "        2    0.000    0.000    0.000    0.000 exc.py:390(instance)\n",
       "        6    0.000    0.000    0.000    0.000 accessor.py:167(__get__)\n",
       "        7    0.000    0.000    0.000    0.000 frame.py:3585(reindexer)\n",
       "      110    0.000    0.000    0.000    0.000 generic.py:1627(<listcomp>)\n",
       "        5    0.000    0.000    0.001    0.000 blocks.py:1982(to_native_types)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:739(_get_formatted_column_labels)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:900(_get_options_with_defaults)\n",
       "        2    0.000    0.000    0.000    0.000 sql.py:972(__init__)\n",
       "        4    0.000    0.000    1.497    0.374 connections.py:508(query)\n",
       "        2    0.000    0.000    0.000    0.000 stats.py:256(gmean)\n",
       "        1    0.000    0.000    0.699    0.699 apply.py:109(get_result)\n",
       "        1    0.000    0.000    0.000    0.000 csvs.py:30(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1404(_fill_cache)\n",
       "       19    0.000    0.000    0.000    0.000 {method 'unpack_from' of 'Struct' objects}\n",
       "       14    0.000    0.000    0.000    0.000 {built-in method _struct.pack}\n",
       "        2    0.000    0.000    0.000    0.000 {pandas._libs.algos.take_1d_object_object}\n",
       "        2    0.000    0.000    0.000    0.000 _validators.py:325(validate_fillna_kwargs)\n",
       "        1    0.000    0.000    0.010    0.010 frame.py:614(__unicode__)\n",
       "       65    0.000    0.000    0.000    0.000 indexing.py:937(_convert_for_reindex)\n",
       "        4    0.000    0.000    0.071    0.018 managers.py:736(as_array)\n",
       "        1    0.000    0.000    0.010    0.010 format.py:582(to_string)\n",
       "        9    0.000    0.000    0.002    0.000 format.py:1128(_format_strings)\n",
       "        2    0.000    0.000    1.551    0.776 sql.py:1055(read_query)\n",
       "        4    0.000    0.000    1.498    0.375 base.py:1138(_execute_text)\n",
       "        2    0.000    0.000    0.000    0.000 result.py:588(_key_fallback)\n",
       "       99    0.000    0.000    0.000    0.000 binaryTree.py:53(setRootVal)\n",
       "       12    0.000    0.000    0.000    0.000 apipkg.py:133(__makeattr)\n",
       "       12    0.000    0.000    0.000    0.000 {method 'tolist' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.000    0.000 nanops.py:203(_get_values)\n",
       "        4    0.000    0.000    0.001    0.000 format.py:307(adjoin)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:956(_clean_options)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:295(__get__)\n",
       "        4    0.000    0.000    0.001    0.000 base.py:869(close)\n",
       "        4    0.000    0.000    0.001    0.000 base.py:2223(_contextual_connect)\n",
       "        9    0.000    0.000    0.000    0.000 result.py:461(_colnames_from_description)\n",
       "       11    0.000    0.000    0.000    0.000 weakref.py:395(__getitem__)\n",
       "        4    0.000    0.000    0.000    0.000 dtypes.py:485(validate_categories)\n",
       "        1    0.000    0.000    0.003    0.003 generic.py:2882(to_csv)\n",
       "        6    0.000    0.000    0.001    0.000 generic.py:8282(rank)\n",
       "        5    0.000    0.000    0.001    0.000 managers.py:530(astype)\n",
       "       16    0.000    0.000    0.000    0.000 managers.py:1552(formatting_values)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:769(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:1553(_binify)\n",
       "        1    0.000    0.000    0.001    0.001 parsers.py:813(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 sql.py:508(pandasSQL_builder)\n",
       "        1    0.000    0.000    0.002    0.002 csvs.py:130(save)\n",
       "        6    0.000    0.000    0.000    0.000 parse.py:361(urlparse)\n",
       "        9    0.000    0.000    0.000    0.000 common.py:1320(is_datetimelike_v_numeric)\n",
       "        1    0.000    0.000    0.010    0.010 frame.py:678(to_string)\n",
       "        1    0.000    0.000    0.002    0.002 frame.py:4695(sort_values)\n",
       "        2    0.000    0.000    0.001    0.000 generic.py:5457(dtypes)\n",
       "       18    0.000    0.000    0.000    0.000 format.py:298(__init__)\n",
       "        9    0.000    0.000    0.000    0.000 format.py:992(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1352(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1502(_maybe_dedup_names)\n",
       "        2    0.000    0.000    0.001    0.000 sql.py:115(_parse_date_columns)\n",
       "        2    0.000    0.000    0.000    0.000 err.py:100(raise_mysql_exception)\n",
       "        8    0.000    0.000    0.000    0.000 cursors.py:89(_nextset)\n",
       "        4    0.000    0.000    1.497    0.374 cursors.py:151(execute)\n",
       "       14    0.000    0.000    0.000    0.000 protocol.py:264(get_column_length)\n",
       "        2    0.000    0.000    0.000    0.000 protocol.py:308(__init__)\n",
       "        5    0.000    0.000    0.000    0.000 langhelpers.py:852(__get__)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:77(__init__)\n",
       "       12    0.000    0.000    0.000    0.000 attr.py:267(__bool__)\n",
       "        2    0.000    0.000    0.005    0.003 result.py:1195(fetchall)\n",
       "        2    0.000    0.000    0.000    0.000 cast.py:1072(find_common_type)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:998(_format_with_header)\n",
       "        1    0.000    0.000    0.699    0.699 frame.py:6310(apply)\n",
       "        6    0.000    0.000    0.000    0.000 generic.py:277(_construct_axes_dict)\n",
       "        2    0.000    0.000    0.000    0.000 managers.py:225(get_dtypes)\n",
       "        9    0.000    0.000    0.000    0.000 common.py:94(_expand_user)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1926(close)\n",
       "        4    0.000    0.000    0.001    0.000 connections.py:532(ping)\n",
       "        4    0.000    0.000    1.497    0.374 connections.py:720(_read_query_result)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:729(_rollback_impl)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:2260(is_disconnect)\n",
       "       15    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}\n",
       "        6    0.000    0.000    0.000    0.000 dtypes.py:328(_finalize)\n",
       "        1    0.000    0.000    0.000    0.000 sorting.py:339(get_group_index_sorter)\n",
       "       18    0.000    0.000    0.000    0.000 base.py:3072(_can_reindex)\n",
       "        4    0.000    0.000    0.071    0.018 generic.py:5250(values)\n",
       "       36    0.000    0.000    0.000    0.000 merge.py:1742(validate_operand)\n",
       "        3    0.000    0.000    0.000    0.000 parsers.py:1180(_is_potential_multi_index)\n",
       "        2    0.000    0.000    0.002    0.001 sql.py:1197(has_table)\n",
       "        4    0.000    0.000    1.497    0.374 cursors.py:324(_query)\n",
       "       12    0.000    0.000    0.000    0.000 protocol.py:86(advance)\n",
       "        2    0.000    0.000    0.000    0.000 exc.py:462(__init__)\n",
       "        7    0.000    0.000    0.000    0.000 type_api.py:483(_cached_result_processor)\n",
       "        4    0.000    0.000    0.001    0.000 base.py:831(_checkin)\n",
       "        4    0.000    0.000    0.000    0.000 impl.py:111(_do_get)\n",
       "        2    0.000    0.000    0.000    0.000 result.py:740(_init_metadata)\n",
       "        2    0.000    0.000    0.001    0.000 result.py:869(_soft_close)\n",
       "        2    0.000    0.000    0.000    0.000 pymysql.py:64(is_disconnect)\n",
       "       12    0.000    0.000    0.000    0.000 inference.py:406(<genexpr>)\n",
       "        6    0.000    0.000    0.000    0.000 inference.py:381(is_dict_like)\n",
       "        1    0.000    0.000    0.001    0.001 ops.py:1817(wrapper)\n",
       "        6    0.000    0.000    0.590    0.098 strings.py:1504(str_strip)\n",
       "        2    0.000    0.000    0.001    0.000 managers.py:684(get_slice)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:381(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:179(get_filepath_or_buffer)\n",
       "        1    0.000    0.000    0.001    0.001 common.py:314(_get_handle)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:271(_init_dict)\n",
       "        2    0.000    0.000    0.000    0.000 series.py:3831(fillna)\n",
       "        2    0.000    0.000    0.000    0.000 sql.py:45(_is_sqlalchemy_connectable)\n",
       "        4    0.000    0.000    0.000    0.000 connections.py:1053(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 cursors.py:341(_do_get_result)\n",
       "       22    0.000    0.000    0.000    0.000 protocol.py:186(is_ok_packet)\n",
       "        7    0.000    0.000    0.000    0.000 protocol.py:253(description)\n",
       "        4    0.000    0.000    0.000    0.000 _collections.py:140(__new__)\n",
       "        2    0.000    0.000    0.000    0.000 compat.py:123(reraise)\n",
       "        2    0.000    0.000    0.000    0.000 <string>:1(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:183(execution_options)\n",
       "        2    0.000    0.000    0.002    0.001 base.py:2133(run_callable)\n",
       "        4    0.000    0.000    0.001    0.000 base.py:987(close)\n",
       "        9    0.000    0.000    0.000    0.000 result.py:560(_merge_cols_by_none)\n",
       "        2    0.000    0.000    0.000    0.000 result.py:714(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 default.py:947(should_autocommit)\n",
       "        4    0.000    0.000    0.000    0.000 default.py:1034(create_cursor)\n",
       "        7    0.000    0.000    0.000    0.000 posixpath.py:232(expanduser)\n",
       "        2    0.000    0.000    0.000    0.000 warnings.py:20(_showwarnmsg_impl)\n",
       "        8    0.000    0.000    0.000    0.000 __init__.py:1619(isEnabledFor)\n",
       "        6    0.000    0.000    0.000    0.000 algorithms.py:835(rank)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:1050(_format_native_types)\n",
       "        2    0.000    0.000    0.000    0.000 generic.py:11018(logical_func)\n",
       "        6    0.000    0.000    0.000    0.000 api.py:174(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 blocks.py:730(to_native_types)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:651(<listcomp>)\n",
       "        7    0.000    0.000    0.000    0.000 format.py:1138(_format_strings)\n",
       "        3    0.000    0.000    0.000    0.000 common.py:260(_infer_compression)\n",
       "        4    0.000    0.000    0.000    0.000 cursors.py:40(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 cursors.py:135(mogrify)\n",
       "       11    0.000    0.000    0.000    0.000 protocol.py:122(read_uint16)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:123(_join)\n",
       "        4    0.000    0.000    1.498    0.375 base.py:922(execute)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:1327(_safe_close_cursor)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:580(get_connection)\n",
       "        2    0.000    0.000    0.000    0.000 result.py:441(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:1174(should_autocommit_text)\n",
       "        1    0.000    0.000    0.000    0.000 apply.py:16(frame_apply)\n",
       "       21    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}\n",
       "        6    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
       "       12    0.000    0.000    0.000    0.000 parse.py:109(_coerce_args)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method now}\n",
       "       17    0.000    0.000    0.000    0.000 {method 'max' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method numpy.where}\n",
       "        8    0.000    0.000    0.000    0.000 common.py:1752(is_complex_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 {pandas._libs.algos.groupsort_indexer}\n",
       "        2    0.000    0.000    0.000    0.000 {pandas._libs.algos.ensure_int8}\n",
       "        5    0.000    0.000    0.000    0.000 blocks.py:532(astype)\n",
       "        2    0.000    0.000    0.000    0.000 managers.py:226(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:801(_get_formatted_index)\n",
       "       27    0.000    0.000    0.000    0.000 format.py:943(<lambda>)\n",
       "        2    0.000    0.000    0.015    0.008 construction.py:501(<listcomp>)\n",
       "        2    0.000    0.000    0.015    0.008 construction.py:484(_convert_object_array)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _csv.writer}\n",
       "        1    0.000    0.000    0.001    0.001 parsers.py:1120(_make_engine)\n",
       "       10    0.000    0.000    0.000    0.000 protocol.py:297(__getattr__)\n",
       "        2    0.000    0.000    0.000    0.000 deprecations.py:117(warned)\n",
       "        2    0.000    0.000    0.000    0.000 schema.py:4027(_bind_to)\n",
       "        2    0.000    0.000    1.497    0.749 base.py:2149(execute)\n",
       "        2    0.000    0.000    0.005    0.002 result.py:1178(process_rows)\n",
       "        4    0.000    0.000    0.001    0.000 mysqldb.py:120(do_ping)\n",
       "        1    0.000    0.000    0.000    0.000 apply.py:36(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 apply.py:89(columns)\n",
       "        1    0.000    0.000    0.021    0.021 apply.py:336(wrap_results_for_axis)\n",
       "        1    0.000    0.000    0.001    0.001 csvs.py:272(_save)\n",
       "        8    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}\n",
       "        2    0.000    0.000    0.000    0.000 warnings.py:419(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method numpy.putmask}\n",
       "        6    0.000    0.000    0.000    0.000 dtypes.py:465(validate_ordered)\n",
       "       24    0.000    0.000    0.000    0.000 {pandas._libs.algos.ensure_float64}\n",
       "        1    0.000    0.000    0.000    0.000 sorting.py:387(_reorder_by_uniques)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:1010(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:2897(_convert_slice_indexer)\n",
       "        2    0.000    0.000    0.001    0.000 indexing.py:2170(_get_slice_axis)\n",
       "       16    0.000    0.000    0.000    0.000 blocks.py:171(formatting_values)\n",
       "        2    0.000    0.000    0.000    0.000 managers.py:1868(_interleaved_dtype)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:76(_is_url)\n",
       "       16    0.000    0.000    0.000    0.000 series.py:483(_formatting_values)\n",
       "       18    0.000    0.000    0.000    0.000 cursors.py:71(_get_db)\n",
       "        2    0.000    0.000    0.000    0.000 exc.py:328(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:261(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:479(_still_open_and_connection_is_valid)\n",
       "        2    0.000    0.000    0.001    0.001 base.py:1591(run_callable)\n",
       "        2    0.000    0.000    0.000    0.000 compiler.py:3425(_escape_identifier)\n",
       "        4    0.000    0.000    0.000    0.000 log.py:56(_should_log_debug)\n",
       "        4    0.000    0.000    0.001    0.000 base.py:345(connect)\n",
       "        4    0.000    0.000    0.000    0.000 impl.py:102(_do_return_conn)\n",
       "        2    0.000    0.000    0.000    0.000 result.py:334(_merge_cursor_description)\n",
       "        2    0.000    0.000    0.000    0.000 default.py:1092(get_result_proxy)\n",
       "        1    0.000    0.000    0.000    0.000 apply.py:323(series_generator)\n",
       "        1    0.000    0.000    0.000    0.000 csvs.py:191(_save_header)\n",
       "        6    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
       "       17    0.000    0.000    0.000    0.000 _methods.py:26(_amax)\n",
       "        1    0.000    0.000    0.000    0.000 console.py:47(get_console_size)\n",
       "        2    0.000    0.000    0.000    0.000 dtypes.py:521(update_dtype)\n",
       "        9    0.000    0.000    0.000    0.000 ops.py:66(_maybe_match_name)\n",
       "        4    0.000    0.000    0.000    0.000 categorical.py:668(_get_codes)\n",
       "        1    0.000    0.000    0.000    0.000 sorting.py:177(indexer_from_factorized)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:983(format)\n",
       "        8    0.000    0.000    0.000    0.000 base.py:2238(_get_reconciled_name_object)\n",
       "       30    0.000    0.000    0.000    0.000 base.py:5398(<genexpr>)\n",
       "        2    0.000    0.000    0.000    0.000 managers.py:524(fillna)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:648(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 construction.py:509(sanitize_index)\n",
       "        2    0.000    0.000    1.497    0.749 sql.py:988(execute)\n",
       "        4    0.000    0.000    0.000    0.000 connections.py:481(cursor)\n",
       "        2    0.000    0.000    0.000    0.000 _collections.py:151(union)\n",
       "        2    0.000    0.000    0.000    0.000 exc.py:24(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 compat.py:378(raise_from_cause)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:169(_clone)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:949(cursor)\n",
       "        4    0.000    0.000    0.000    0.000 queue.py:195(_full)\n",
       "        4    0.000    0.000    0.000    0.000 queue.py:199(_put)\n",
       "        4    0.000    0.000    0.000    0.000 queue.py:203(_get)\n",
       "        4    0.000    0.000    0.000    0.000 default.py:1002(_use_server_side_cursor)\n",
       "        7    0.000    0.000    0.000    0.000 default.py:1051(get_result_processor)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:2057(<listcomp>)\n",
       "        1    0.000    0.000    0.021    0.021 apply.py:302(wrap_results)\n",
       "        3    0.000    0.000    0.000    0.000 <ipython-input-32-6e3aceb14808>:9(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1433(<setcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 linecache.py:37(getlines)\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method sqlalchemy.cutils._distill_params}\n",
       "        2    0.000    0.000    0.000    0.000 _methods.py:48(_count_reduce_items)\n",
       "        4    0.000    0.000    0.000    0.000 printing.py:36(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 console.py:149(in_ipython_frontend)\n",
       "        6    0.000    0.000    0.000    0.000 dtypes.py:225(__init__)\n",
       "        7    0.000    0.000    0.000    0.000 cast.py:1100(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 ops.py:1447(_align_method_SERIES)\n",
       "        1    0.000    0.000    0.010    0.010 base.py:48(__str__)\n",
       "        2    0.000    0.000    0.000    0.000 nanops.py:337(nanany)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:1024(to_native_types)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:5393(_trim_front)\n",
       "        6    0.000    0.000    0.000    0.000 generic.py:279(<dictcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:431(_chk_truncate)\n",
       "        9    0.000    0.000    0.000    0.000 format.py:1004(_value_formatter)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1936(_set_noconvert_columns)\n",
       "        4    0.000    0.000    0.000    0.000 cursors.py:51(close)\n",
       "        8    0.000    0.000    0.000    0.000 cursors.py:106(nextset)\n",
       "        2    0.000    0.000    0.000    0.000 cursors.py:299(fetchall)\n",
       "        4    0.000    0.000    0.000    0.000 cursors.py:332(_clear_result)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:116(_for_class)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:865(_autorollback)\n",
       "        4    0.000    0.000    0.001    0.000 base.py:2259(_wrap_pool_connect)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:366(_return_conn)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:715(__init__)\n",
       "        6    0.000    0.000    0.000    0.000 base.py:958(__getattr__)\n",
       "        4    0.000    0.000    0.000    0.000 queue.py:191(_empty)\n",
       "        4    0.000    0.000    1.497    0.374 default.py:551(do_execute)\n",
       "        1    0.000    0.000    0.071    0.071 apply.py:97(values)\n",
       "        1    0.000    0.000    0.000    0.000 apply.py:105(agg_axis)\n",
       "        4    0.000    0.000    0.000    0.000 {method 'discard' of 'set' objects}\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method sys.exc_info}\n",
       "        8    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}\n",
       "        2    0.000    0.000    0.000    0.000 warnings.py:96(_showwarnmsg)\n",
       "        5    0.000    0.000    0.000    0.000 inspect.py:72(isclass)\n",
       "       12    0.000    0.000    0.000    0.000 parse.py:98(_noop)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'mean' of 'numpy.ndarray' objects}\n",
       "       18    0.000    0.000    0.000    0.000 {method 'get_count' of 'pandas._libs.hashtable.Factorizer' objects}\n",
       "        7    0.000    0.000    0.000    0.000 cast.py:1112(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 ops.py:1787(na_op)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:1806(hasnans)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:637(_set_axis)\n",
       "        2    0.000    0.000    0.000    0.000 generic.py:3110(_is_view)\n",
       "        2    0.000    0.000    0.001    0.000 indexing.py:148(_slice)\n",
       "        2    0.000    0.000    0.000    0.000 indexing.py:264(_convert_slice_indexer)\n",
       "        2    0.000    0.000    0.000    0.000 blocks.py:364(fillna)\n",
       "       32    0.000    0.000    0.000    0.000 format.py:535(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:544(UnicodeWriter)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:3735(reindex)\n",
       "        2    0.000    0.000    0.000    0.000 parsers.py:347(_validate_integer)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:897(close)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1814(_do_date_conversions)\n",
       "        4    0.000    0.000    0.000    0.000 connections.py:1069(__del__)\n",
       "       10    0.000    0.000    0.000    0.000 protocol.py:77(read_all)\n",
       "        2    0.000    0.000    0.000    0.000 <string>:1(<lambda>)\n",
       "        4    0.000    0.000    0.000    0.000 attr.py:255(__call__)\n",
       "       10    0.000    0.000    0.000    0.000 base.py:155(_root)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:180(__exit__)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:364(invalidated)\n",
       "        2    0.000    0.000    0.000    0.000 compiler.py:3464(quote_identifier)\n",
       "        4    0.000    0.000    0.000    0.000 log.py:59(_should_log_info)\n",
       "        2    0.000    0.000    0.000    0.000 result.py:1161(_fetchall_impl)\n",
       "        2    0.000    0.000    0.000    0.000 default.py:1108(_setup_crud_result_proxy)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:2054(_quote_free_identifiers)\n",
       "        3    0.000    0.000    0.000    0.000 pymysql.py:76(_extract_error_code)\n",
       "        1    0.000    0.000    0.000    0.000 apply.py:101(dtypes)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'translate' of 'str' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _imp.is_builtin}\n",
       "        1    0.000    0.000    0.000    0.000 re.py:271(_compile)\n",
       "        2    0.000    0.000    0.000    0.000 linecache.py:15(getline)\n",
       "        6    0.000    0.000    0.000    0.000 <string>:1(__new__)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'put' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:227(itervalues)\n",
       "        2    0.000    0.000    0.000    0.000 config.py:170(get_default_val)\n",
       "        2    0.000    0.000    0.000    0.000 config.py:578(_get_registered_option)\n",
       "        1    0.000    0.000    0.000    0.000 console.py:90(in_interactive_session)\n",
       "        2    0.000    0.000    0.000    0.000 dtypes.py:244(_from_values_or_dtype)\n",
       "        2    0.000    0.000    0.000    0.000 cast.py:552(coerce_indexer_dtype)\n",
       "        4    0.000    0.000    0.000    0.000 cast.py:1097(<genexpr>)\n",
       "        4    0.000    0.000    0.000    0.000 cast.py:1105(<genexpr>)\n",
       "        2    0.000    0.000    0.000    0.000 categorical.py:441(dtype)\n",
       "        6    0.000    0.000    0.000    0.000 base.py:4698(_validate_indexer)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:5399(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 frame.py:606(_info_repr)\n",
       "        1    0.000    0.000    0.000    0.000 frame.py:1140(to_numpy)\n",
       "        1    0.000    0.000    0.000    0.000 frame.py:4710(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 generic.py:1904(__array__)\n",
       "        2    0.000    0.000    0.000    0.000 generic.py:5337(get_values)\n",
       "        4    0.000    0.000    0.000    0.000 blocks.py:2011(should_store)\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method pandas._libs.internals.slice_len}\n",
       "        2    0.000    0.000    0.000    0.000 managers.py:627(is_view)\n",
       "        7    0.000    0.000    0.000    0.000 managers.py:1513(index)\n",
       "        2    0.000    0.000    0.000    0.000 managers.py:1884(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 format.py:789(show_row_idx_names)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:816(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:163(is_s3_url)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:171(is_gcs_url)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:4209(isnull)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1329(_validate_parse_dates_arg)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1536(_make_index)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:3195(_process_date_conversion)\n",
       "        2    0.000    0.000    0.000    0.000 sql.py:74(_convert_params)\n",
       "        2    0.000    0.000    0.000    0.000 sql.py:85(_process_parse_dates_argument)\n",
       "        2    0.000    0.000    0.000    0.000 sql.py:490(_engine_builder)\n",
       "        1    0.000    0.000    0.000    0.000 converters.py:68(_escape_unicode)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:448(escape)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:462(literal)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:469(escape_string)\n",
       "        2    0.000    0.000    0.000    0.000 cursors.py:355(_show_warnings)\n",
       "        2    0.000    0.000    0.000    0.000 langhelpers.py:59(__enter__)\n",
       "        2    0.000    0.000    0.000    0.000 langhelpers.py:62(__exit__)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:119(_for_instance)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:177(__enter__)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:355(closed)\n",
       "        2    0.000    0.000    0.000    0.000 result.py:263(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 result.py:266(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 apply.py:332(result_columns)\n",
       "        2    0.000    0.000    0.000    0.000 <ipython-input-32-6e3aceb14808>:14(corpus_config)\n",
       "        8    0.000    0.000    0.000    0.000 csvs.py:112(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'find' of 'str' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method sys.getfilesystemencoding}\n",
       "        7    0.000    0.000    0.000    0.000 {built-in method posix.fspath}\n",
       "        4    0.000    0.000    0.000    0.000 {method 'popleft' of 'collections.deque' objects}\n",
       "        8    0.000    0.000    0.000    0.000 {method '_is_owned' of '_thread.RLock' objects}\n",
       "        1    0.000    0.000    0.000    0.000 codecs.py:186(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 warnings.py:117(_formatwarnmsg)\n",
       "        1    0.000    0.000    0.000    0.000 re.py:180(search)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'timestamp' of 'datetime.datetime' objects}\n",
       "        1    0.000    0.000    0.000    0.000 interactiveshell.py:698(get_ipython)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:457(is_platform_windows)\n",
       "        1    0.000    0.000    0.000    0.000 inference.py:160(is_file_like)\n",
       "       10    0.000    0.000    0.000    0.000 dtypes.py:555(categories)\n",
       "        2    0.000    0.000    0.000    0.000 function.py:40(__call__)\n",
       "        6    0.000    0.000    0.000    0.000 base.py:138(_freeze)\n",
       "        2    0.000    0.000    0.000    0.000 nanops.py:270(_na_ok_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 frame.py:7600(_get_agg_axis)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:7083(isnull)\n",
       "        2    0.000    0.000    0.000    0.000 range.py:184(_get_data_as_items)\n",
       "        2    0.000    0.000    0.000    0.000 blocks.py:136(is_view)\n",
       "        2    0.000    0.000    0.000    0.000 indexing.py:2700(need_slice)\n",
       "        5    0.000    0.000    0.000    0.000 blocks.py:146(is_categorical_astype)\n",
       "        2    0.000    0.000    0.000    0.000 managers.py:604(is_mixed_type)\n",
       "        4    0.000    0.000    0.000    0.000 managers.py:1045(value_getitem)\n",
       "       14    0.000    0.000    0.000    0.000 managers.py:1578(_consolidate_inplace)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:351(should_show_dimensions)\n",
       "        2    0.000    0.000    0.000    0.000 format.py:781(has_index_names)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:785(has_column_names)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:795(show_col_idx_names)\n",
       "        3    0.000    0.000    0.000    0.000 format.py:1434(_has_names)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:939(_check_file_or_buffer)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1162(_create_index)\n",
       "        6    0.000    0.000    0.000    0.000 parsers.py:1195(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1423(_has_complex_date_col)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:2064(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:2066(<dictcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:3157(_make_date_converter)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:3278(_clean_na_values)\n",
       "        2    0.000    0.000    0.000    0.000 cursors.py:76(_check_executed)\n",
       "        4    0.000    0.000    0.000    0.000 cursors.py:127(<dictcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 protocol.py:94(rewind)\n",
       "        2    0.000    0.000    0.000    0.000 protocol.py:208(is_load_local_packet)\n",
       "        2    0.000    0.000    0.000    0.000 langhelpers.py:56(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 elements.py:4139(__new__)\n",
       "        2    0.000    0.000    0.000    0.000 elements.py:4322(_string_or_unprintable)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:709(in_transaction)\n",
       "        2    0.000    0.000    0.000    0.000 result.py:760(keys)\n",
       "        2    0.000    0.000    0.000    0.000 default.py:477(set_connection_execution_options)\n",
       "        1    0.000    0.000    0.000    0.000 apply.py:93(index)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'with_traceback' of 'BaseException' objects}\n",
       "        4    0.000    0.000    0.000    0.000 {method 'insert' of 'list' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method builtins.globals}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.ord}\n",
       "        1    0.000    0.000    0.000    0.000 parse.py:394(_checknetloc)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:275(u)\n",
       "        8    0.000    0.000    0.000    0.000 dtypes.py:562(ordered)\n",
       "        4    0.000    0.000    0.000    0.000 cast.py:1107(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 ops.py:1832(<lambda>)\n",
       "        2    0.000    0.000    0.000    0.000 categorical.py:394(categories)\n",
       "        2    0.000    0.000    0.000    0.000 nanops.py:180(_get_fill_value)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:1785(_isnan)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:3867(_has_complex_internals)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:4077(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:1818(__iter__)\n",
       "        3    0.000    0.000    0.000    0.000 managers.py:1572(is_consolidated)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:112(_validate_header_arg)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:377(_validate_names)\n",
       "        3    0.000    0.000    0.000    0.000 parsers.py:1176(_is_index_col)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1274(_validate_usecols_arg)\n",
       "        2    0.000    0.000    0.000    0.000 parsers.py:1530(_maybe_make_multi_index_columns)\n",
       "        4    0.000    0.000    0.000    0.000 _collections.py:145(__init__)\n",
       "        6    0.000    0.000    0.000    0.000 base.py:370(connection)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:875(is_valid)\n",
       "        2    0.000    0.000    0.000    0.000 result.py:864(_cursor_description)\n",
       "        3    0.000    0.000    0.000    0.000 default.py:943(no_parameters)\n",
       "        2    0.000    0.000    0.000    0.000 default.py:1089(handle_dbapi_exception)\n",
       "        1    0.000    0.000    0.000    0.000 <string>:1(<module>)\n",
       "        1    0.000    0.000    0.000    0.000 apply.py:328(result_index)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%time\n",
    "def main():\n",
    "    '''\n",
    "        corpora: i2b2, mipacq, fv017\n",
    "        analyses: entity only (exact span), cui by document, full (aka (entity and cui on exaact span/exact cui)\n",
    "        systems: ctakes, biomedicus, clamp, metamap, quick_umls\n",
    "        \n",
    "        TODO -> Vectorization (entity only) -> done:\n",
    "                add switch for use of TN on single system performance evaluations -> done\n",
    "                add switch for overlap matching versus exact span -> done\n",
    "             -> Other tasks besides concept extraction\n",
    "        \n",
    "    ''' \n",
    "    analysisConf =  AnalysisConfig()\n",
    "    print(analysisConf.systems, analysisConf.corpus_config())\n",
    "    \n",
    "    if (rtype == 1):\n",
    "        generate_metrics(analysis_type, corpus, filter_semtype)\n",
    "    elif (rtype == 2):\n",
    "        print('run_type:', run_type)\n",
    "        if filter_semtype:\n",
    "            ensemble_control(analysisConf.systems, analysis_type, corpus, run_type, filter_semtype, semtypes)\n",
    "        else:\n",
    "            ensemble_control(analysisConf.systems, analysis_type, corpus, run_type)\n",
    "    elif (rtype == 3):\n",
    "        t = ['concept_jaccard_score_false']\n",
    "        test_systems(analysis_type, analysisConf.systems, corpus)  \n",
    "        test_count(analysis_type, corpus)\n",
    "        test_ensemble(analysis_type, corpus)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    %prun main()\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = get_sys_data('quick_umls', analysis_type, corpus, filter_semtype)\n",
    "# q = q.sort_values(by=['note_id', 'begin'])\n",
    "# print(q.head(20))\n",
    "\n",
    "# b = get_sys_data('biomedicus', analysis_type, corpus, filter_semtype)\n",
    "# b = b.sort_values(by=['note_id', 'begin'])\n",
    "# print(b.head(20))\n",
    "\n",
    "# df = pd.read_csv('/Users/gms/development/nlp/nlpie/data/ensembling-u01/output/analytical_fairview.csv')\n",
    "\n",
    "# ref_ann, _ = get_metric_data(analysis_type, corpus)\n",
    "# ref_ann = ref_ann[ref_ann['semtype'] == 'Drug']\n",
    "# #ref_ann.sort_values(by=['file', 'start']).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# merge = 'biomedicus' \n",
    "# sys_ann = get_contingency_table(analysis_type, corpus, semtypes[0], merge)  \n",
    "# ref_ann, _ = get_metric_data(analysis_type, corpus)\n",
    "# #print(len(ref_ann))\n",
    "# ref_ann = ref_ann[ref_ann['semtype'].isin(SemanticTypes(semtypes).get_system_type('reference'))]\n",
    "# #print(len(ref_ann))\n",
    "\n",
    "# def get_rand_idx(ref_ann, sys_ann):\n",
    "#     r_idx = ref_ann.index.values.tolist() \n",
    "#     s_idx = sys_ann.index.values.tolist()\n",
    "    \n",
    "#     n = int(len(r_idx)/(1.33))\n",
    "#     r = random.sample(r_idx, k=n)\n",
    "#     n = int(len(s_idx)/(1.33))\n",
    "#     s = random.sample(s_idx, k=n)\n",
    "    \n",
    "#     return r, s\n",
    "\n",
    "# metrics = pd.DataFrame()\n",
    "# for i in range(1, 5):\n",
    "#     r, s = get_rand_idx(ref_ann, sys_ann)\n",
    "#     ref = ref_ann.ix[r]\n",
    "#     sys = sys_ann.ix[s]\n",
    "    \n",
    "#     c = get_cooccurences(ref, sys, analysis_type, corpus, False)\n",
    "#     #print(c.ref_n, c.ref_only, c.system_n, c.system_only, c.ref_system_match)\n",
    "    \n",
    "#     d = cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n)\n",
    "    \n",
    "#     frames = [metrics, pd.DataFrame(d, index=[0])]\n",
    "#     metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "    \n",
    "# print(geometric_mean(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control filter_semtype in get_sys_data, get_ref_n and generate_metrics. TODO consolidate. \n",
    " \n",
    "# # run single statement\n",
    "# statement = '(ctakes&clamp)'\n",
    "# analysis_type = 'entity'\n",
    "# corpus = 'fairview'\n",
    "# #matches = get_merge_data(statement, analysis_type, corpus)\n",
    "# #print(matches)\n",
    "\n",
    "# import spacy\n",
    "# nlp = spacy.load('en')\n",
    "\n",
    "# sql = \"select distinct note_id, sofa from concepts.sofas where corpus = 'fairview'\"\n",
    "\n",
    "# docs = pd.read_sql(sql, con=engine)\n",
    "\n",
    "# d = {}\n",
    "\n",
    "# for row in docs.itertuples():\n",
    "#     d[row.note_id] = row.sofa\n",
    "    \n",
    "# print(len(d))\n",
    "\n",
    "# test = matches[matches['note_id'] == '0000200926']\n",
    "# print(len(test))\n",
    "\n",
    "# doc = nlp(d['0000200926'])\n",
    "\n",
    "# for row in test.itertuples():\n",
    "#     my_str = [token.text.strip('\\n').lower() for token in doc if token.idx >= (row.begin) and token.idx <= (row.end)]\n",
    "#     if 'diabetes' in my_str:\n",
    "#         print(my_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
