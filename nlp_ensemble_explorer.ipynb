{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  Copyright (c) 2019 Regents of the University of Minnesota.\n",
    " \n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    " \n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gevent\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pymysql\n",
    "import time \n",
    "import functools as ft\n",
    "import glob, os, sys   \n",
    "import operator as op\n",
    "import shelve\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from pandas.api.types import is_numeric_dtypen()\n",
    "from pathlib import Path\n",
    "from itertools import combinations, product, permutations\n",
    "from sqlalchemy.engine import create_engine\n",
    "from datetime import datetime\n",
    "from ast import literal_eval\n",
    "from scipy import stats  \n",
    "from scipy.stats.mstats import gmean\n",
    "from pythonds.basic.stack import Stack\n",
    "from pythonds.trees.binaryTree import BinaryTree\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "from typing import List, Set, Tuple \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The cell below contains the configurable parameters to ensure that our ensemble explorer runs properaly on your machine. \n",
    "Please read carfully through steps (1-11) before running the rest of the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-1: CHOOSE YOUR CORPUS\n",
    "# TODO: get working with list of corpora\n",
    "#corpora = ['mipacq','i2b2','fairview'] #options for concept extraction include 'fairview', 'mipacq' OR 'i2b2'\n",
    "\n",
    "# cross-system semantic union merge filter for cross system aggregations using custom system annotations file with corpus name and system name using 'ray_test': \n",
    "# need to add semantic type filrering when reading in sys_data\n",
    "#corpus = 'ray_test'\n",
    "#corpus = 'clinical_trial2'\n",
    "corpus = 'mipacq'\n",
    "#corpora = ['i2b2','fairview']\n",
    "\n",
    "# STEP-2: CHOOSE YOUR DATA DIRECTORY; this is where output data will be saved on your machine\n",
    "data_directory = '/Users/gms/development/nlp/nlpie/data/ensembling-u01/output/' \n",
    "\n",
    "# STEP-3: CHOOSE WHICH SYSTEMS YOU'D LIKE TO EVALUATE AGAINST THE CORPUS REFERENCE SET\n",
    "#systems = ['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls']\n",
    "#systems = ['biomedicus', 'clamp', 'metamap', 'quick_umls']\n",
    "#systems = ['biomedicus', 'quick_umls']\n",
    "#systems = ['biomedicus', 'ctakes', 'quick_umls']\n",
    "systems = ['biomedicus', 'clamp', 'ctakes', 'metamap']\n",
    "#systems = ['biomedicus', 'clamp']\n",
    "#systems = ['ctakes', 'quick_umls', 'biomedicus', 'metamap']\n",
    "systems = ['biomedicus', 'metamap']\n",
    "#systems = ['ray_test']\n",
    "#systems = ['metamap']\n",
    "\n",
    "# STEP-4: CHOOSE TYPE OF RUN\n",
    "rtype = 6      # OPTIONS INCLUDE: 1->Single systems; 2->Ensemble; 3->Tests; 4 -> majority vote \n",
    "               # The Ensemble can include the max system set ['ctakes','biomedicus','clamp','metamap','quick_umls']\n",
    "    \n",
    "# STEP-5: CHOOSE WHAT TYPE OF ANALYSIS YOU'D LIKE TO RUN ON THE CORPUS\n",
    "analysis_type = 'full' #options include 'entity', 'cui' OR 'full'\n",
    "\n",
    "# STEP-(6A): ENTER DETAILS FOR ACCESSING MANUAL ANNOTATION DATA\n",
    "database_type = 'mysql+pymysql' # We use mysql+pymql as default\n",
    "database_username = 'gms'\n",
    "database_password = 'nej123' \n",
    "database_url = 'localhost' # HINT: use localhost if you're running database on your local machine\n",
    "#database_name = 'clinical_trial' # Enter database name\n",
    "database_name = 'concepts' # Enter database name\n",
    "\n",
    "def ref_data(corpus):\n",
    "    return corpus + '_all' # Enter the table within the database where your reference data is stored\n",
    "\n",
    "table_name = ref_data(corpus)\n",
    "\n",
    "# STEP-(6B): ENTER DETAILS FOR ACCESSING SYSTEM ANNOTATION DATA\n",
    "\n",
    "def sys_data(corpus, analysis_type):\n",
    "    if analysis_type == 'entity':\n",
    "        return 'analytical_'+corpus+'.csv' # OPTIONS include 'analytical_cui_mipacq_concepts.csv' OR 'analytical_cui_i2b2_concepts.csv' \n",
    "    elif analysis_type in ('cui', 'full'):\n",
    "        return 'analytical_'+corpus+'_cui.csv' # OPTIONS include 'analytical_cui_mipacq_concepts.csv' OR 'analytical_cui_i2b2_concepts.csv' \n",
    "        \n",
    "system_annotation = sys_data(corpus, analysis_type)\n",
    "\n",
    "# STEP-7: CREATE A DB CONNECTION POOL\n",
    "engine_request = str(database_type)+'://'+database_username+':'+database_password+\"@\"+database_url+'/'+database_name\n",
    "engine = create_engine(engine_request, pool_pre_ping=True, pool_size=20, max_overflow=30)\n",
    "\n",
    "# STEP-(8A): FILTER BY SEMTYPE\n",
    "filter_semtype = False\n",
    "\n",
    "# STEP-(8B): IF STEP-(8A) == True -> GET REFERENCE SEMTYPES\n",
    "\n",
    "def ref_semtypes(filter_semtype, corpus):\n",
    "    if filter_semtype:\n",
    "        if corpus == 'fairview':\n",
    "            semtypes = ['Drug', 'Finding', 'Anatomy', 'Procedure']\n",
    "        elif corpus == 'i2b2':\n",
    "            semtypes = ['test,treatment', 'problem']\n",
    "        elif corpus == 'mipacq':\n",
    "            semtypes = ['Procedures', 'Disorders,Sign_Symptom', 'Anatomy', 'Chemicals_and_drugs']\n",
    "        elif corpus in ['clinical_trial', 'clinical_trial2']:\n",
    "            semtypes = ['drug,drug::drug_name,drug::drug_dose,dietary_supplement::dietary_supplement_name,dietary_supplement::dietary_supplement_dose',\n",
    "                        'temporal_measurement,qualifier,measurement',\n",
    "                        'device',\n",
    "                        'condition,observation', \n",
    "                        'demographics::age,demographics::sex,demographics::race_ethnicity,demographics::bmi,demographics::weight',\n",
    "                        'diet',\n",
    "                        'measurement,qualifier',\n",
    "                        'procedure,observation']\n",
    "        \n",
    "        return semtypes\n",
    "\n",
    "semtypes = ref_semtypes(filter_semtype, corpus)\n",
    "\n",
    "# STEP-9: Set data directory/table for source documents for vectorization\n",
    "src_table = 'sofa'\n",
    "\n",
    "# STEP-10: Specificy match type from {'exact', 'overlap', 'cui' -> kludge for majority}\n",
    "run_type = 'overlap'\n",
    "\n",
    "# for clinical trial, measurement/temoral are single system since no overlap for intersect\n",
    "# STEP-11: Specify expression type for run (TODO: run all at once; make less kludgey)\n",
    "expression_type = 'nested' #'nested_with_singleton' # type of merge expression: nested ((A&B)|C), paired ((A&B)|(C&D)), nested_with_singleton ((A&B)|((C&D)|E)) \n",
    "# -> NB: len(systems) for pair must be >= 4, and for nested_with_singleton == 5; single-> skip merges\n",
    "\n",
    "# STEP-12: Specify type of ensemble: merge or vote\n",
    "ensemble_type = 'merge'\n",
    "\n",
    "# STEP-13: run on negation modifier (TODO: negated entity)\n",
    "modification = None #'negation' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****** TODO \n",
    "-> add majority vote to union for analysis_type = 'full'\n",
    "-> case for multiple labels on same/overlapping span/same system; disambiguate (order by score if exists and select random for ties): done!\n",
    "-> port to command line \n",
    "----------------------->\n",
    "-> still need to validate that all semtypes in corpus!\n",
    "-> handle case where intersect merges are empty/any confusion matrix values are 0; specificallly on empty df in evaluate method: done!\n",
    "-> case when system annotations empty from semtype filter; print as 0: done!\n",
    "-> trim whitespace on CSV import -> done for semtypes\n",
    "-> eliminate rtype = 1 for expression_type = 'single'\n",
    "-> cross-system semantic union merge on aggregation\n",
    "-> negation: testing\n",
    "-> other modification, such as 'present'\n",
    "-> clean up configuration process\n",
    "-> allow iteration through all corpora and semtypes\n",
    "-> optimize vecorization (remove confusion?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config class for analysis\n",
    "class AnalysisConfig():\n",
    "    \"\"\"\n",
    "    Configuration object:\n",
    "    systems to use\n",
    "    notes by corpus\n",
    "    paths by output, gold and system location\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self = self    \n",
    "        self.systems = systems\n",
    "        self.data_dir = data_directory\n",
    "    \n",
    "    def corpus_config(self): \n",
    "        usys_data = system_annotation\n",
    "        ref_data = database_name+'.'+table_name\n",
    "        return usys_data, ref_data\n",
    "\n",
    "analysisConf =  AnalysisConfig()\n",
    "#usys, ref = analysisConf.corpus_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticTypes(object):\n",
    "    '''\n",
    "    Filter semantic types based on: https://metamap.nlm.nih.gov/SemanticTypesAndGroups.shtml\n",
    "    :params: semtypes list from corpus, system to query\n",
    "    :return: list of equivalent system semtypes \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, semtypes, corpus):\n",
    "        self = self\n",
    "        \n",
    "        if corpus == 'clinical_trial2':\n",
    "            corpus = 'clinical_trial' # kludge!!\n",
    "        sql = \"SELECT st.tui, abbreviation, clamp_name, ctakes_name, biomedicus_name FROM clinical_trial.semantic_groups sg join semantic_types st on sg.tui = st.tui where \" + corpus + \"_name in ({})\"\\\n",
    "            .format(', '.join(['%s' for _ in semtypes]))  \n",
    "#        sql = \"SELECT st.tui, abbreviation, clamp_name, ctakes_name FROM concepts.semantic_groups sg join semantic_types st on sg.tui = st.tui where \" + corpus + \"_name in ({})\"\\\n",
    "#           .format(', '.join(['%s' for _ in semtypes]))  \n",
    "        \n",
    "        stypes = pd.read_sql(sql, params=[semtypes], con=engine) \n",
    "       \n",
    "        if len(stypes['tui'].tolist()) > 0:\n",
    "            self.biomedicus_types = set(stypes['tui'].tolist())\n",
    "            self.qumls_types = set(stypes['tui'].tolist())\n",
    "        \n",
    "        else:\n",
    "            self.biomedicus_types = None\n",
    "            self.qumls_types = None\n",
    "        \n",
    "        if stypes['clamp_name'].dropna(inplace=True) or len(stypes['clamp_name']) == 0:\n",
    "            self.clamp_types = None\n",
    "        else:\n",
    "            self.clamp_types = set(stypes['clamp_name'].tolist()[0].split(','))\n",
    "         \n",
    "        if stypes['ctakes_name'].dropna(inplace=True) or len(stypes['ctakes_name']) == 0:\n",
    "            self.ctakes_types = None\n",
    "        else:\n",
    "            self.ctakes_types = set(stypes['ctakes_name'].tolist()[0].split(','))\n",
    " \n",
    "# # Kludge for b9 temporal\n",
    "        if stypes['biomedicus_name'].dropna(inplace=True) or len(stypes['biomedicus_name']) > 0:\n",
    "            self.biomedicus_types.update(set(stypes['biomedicus_name'].tolist()[0].split(',')))\n",
    "        #else:\n",
    "        #    self.biomedicus_type = None\n",
    "        \n",
    "        if len(stypes['abbreviation'].tolist()) > 0:\n",
    "            self.metamap_types = set(stypes['abbreviation'].tolist())\n",
    "        else:\n",
    "            self.metamap_types = None\n",
    "        \n",
    "        self.reference_types =  set(semtypes)\n",
    "    \n",
    "    def get_system_type(self, system):  \n",
    "        \n",
    "        if system == 'biomedicus':\n",
    "            semtypes = self.biomedicus_types\n",
    "        elif system == 'ctakes':\n",
    "            semtypes = self.ctakes_types\n",
    "        elif system == 'clamp':\n",
    "            semtypes = self.clamp_types\n",
    "        elif system == 'metamap':\n",
    "            semtypes = self.metamap_types\n",
    "        elif system == 'quick_umls':\n",
    "            semtypes = self.qumls_types\n",
    "        elif system == 'reference':\n",
    "            semtypes = self.reference_types\n",
    "            \n",
    "        return semtypes\n",
    "    \n",
    "# print(SemanticTypes(['Drug'], corpus).get_system_type('biomedicus'))\n",
    "#print(SemanticTypes(['Drug'], corpus).get_system_type('quick_umls'))\n",
    "#print(SemanticTypes(['drug'], corpus).get_system_type('clamp'))\n",
    "#print(SemanticTypes(['Anatomy'], 'mipacq').get_system_type('ctakes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semtypes = ['test,treatment']\n",
    "#semtypes = 'drug,drug::drug_name,drug::drug_dose,dietary_supplement::dietary_supplement_name,dietary_supplement::dietary_supplement_dose'\n",
    "#semtypes =  'demographics::age,demographics::sex,demographics::race_ethnicity,demographics::bmi,demographics::weight'\n",
    "#corpus = 'clinical_trial'\n",
    "#sys = 'quick_umls'\n",
    "\n",
    "# is semantic type in particular system\n",
    "def system_semtype_check(sys, semtype, corpus):\n",
    "    st = SemanticTypes([semtype], corpus).get_system_type(sys)\n",
    "    if st:\n",
    "        return sys\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#print(system_semtype_check(sys, semtypes, corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation class for systems\n",
    "class AnnotationSystems():\n",
    "    \"\"\"   \n",
    "    System annotations of interest for UMLS concept extraction\n",
    "    NB: ctakes combines all \"mentions\" annotation types\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\" \n",
    "        annotation base types\n",
    "        \"\"\"   \n",
    "        \n",
    "        self.biomedicus_types = [\"biomedicus.v2.UmlsConcept\"]\n",
    "        self.clamp_types = [\"edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA\"]\n",
    "        self.ctakes_types = [\"ctakes_mentions\"]\n",
    "        self.metamap_types = [\"org.metamap.uima.ts.Candidate\"]\n",
    "        self.qumls_types = [\"concept_jaccard_score_False\"]\n",
    "       \n",
    "    def get_system_type(self, system):\n",
    "        \n",
    "        \"\"\"\n",
    "        return system types\n",
    "        \"\"\"\n",
    "        \n",
    "        if system == \"biomedicus\":\n",
    "            view = \"Analysis\"\n",
    "        else:\n",
    "            view = \"_InitialView\"\n",
    "\n",
    "        if system == 'biomedicus':\n",
    "            types = self.biomedicus_types\n",
    "\n",
    "        elif system == 'clamp':\n",
    "            types = self.clamp_types\n",
    "\n",
    "        elif system == 'ctakes':\n",
    "            types = self.ctakes_types\n",
    "\n",
    "        elif system == 'metamap':\n",
    "            types = self.metamap_types\n",
    "        \n",
    "        elif system == \"quick_umls\":\n",
    "            types = self.qumls_types\n",
    "            \n",
    "        return types, view\n",
    "    \n",
    "annSys = AnnotationSystems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%cython\n",
    "\n",
    "#import numpy as np # access to Numpy from Python layer\n",
    "#import math\n",
    "\n",
    "class Metrics(object):\n",
    "    \"\"\"\n",
    "    metrics class:\n",
    "    returns an instance with confusion matrix metrics\n",
    "    \"\"\"\n",
    "    def __init__(self, system_only, gold_only, gold_system_match, system_n, neither = 0): # neither: no sys or manual annotation\n",
    "\n",
    "        self = self    \n",
    "        self.system_only = system_only\n",
    "        self.gold_only = gold_only\n",
    "        self.gold_system_match = gold_system_match\n",
    "        self.system_n = system_n\n",
    "        self.neither = neither\n",
    "        \n",
    "    def get_confusion_metrics(self, corpus = None, test = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        compute confusion matrix measures, as per  \n",
    "        https://stats.stackexchange.com/questions/51296/how-do-you-calculate-precision-and-recall-for-multiclass-classification-using-co\n",
    "        \"\"\"\n",
    "#         cdef:\n",
    "#             int TP, FP, FN\n",
    "#             double TM\n",
    "\n",
    "        TP = self.gold_system_match\n",
    "        FP = self.system_only\n",
    "        FN = self.gold_only\n",
    "        \n",
    "        TM = TP/math.sqrt(self.system_n) # TigMetric\n",
    "       \n",
    "        if not test:\n",
    "            \n",
    "            if corpus == 'casi':\n",
    "                recall = TP/(TP + FN)\n",
    "                precision = TP/(TP + FP)\n",
    "                F = 2*(precision*recall)/(precision + recall)\n",
    "            else:\n",
    "                if self.neither == 0:\n",
    "                    confusion = [[0, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "                else:\n",
    "                    confusion = [[self.neither, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "                c = np.asarray(confusion)\n",
    "                \n",
    "                if TP != 0 or FP != 0:\n",
    "                    precision = TP/(TP+FP)\n",
    "                else:\n",
    "                    precision = 0\n",
    "                \n",
    "                if TP != 0 or FN != 0:\n",
    "                    recall = TP/(TP+FN)\n",
    "                else:\n",
    "                    recall = 0\n",
    "                \n",
    "                if precision + recall != 0:\n",
    "                    F = 2*(precision*recall)/(precision + recall)\n",
    "                else:\n",
    "                    F = 0\n",
    "    \n",
    "#                 recall = np.diag(c) / np.sum(c, axis = 1)\n",
    "#                 precision = np.diag(c) / np.sum(c, axis = 0)\n",
    "#                 #print('Yo!', np.mean(precision), np.mean(recall))\n",
    "#                 if np.mean(precision) != 0 and np.mean(recall) != 0:\n",
    "#                     F = 2*(precision*recall)/(precision + recall)\n",
    "#                 else:\n",
    "#                     F = 0\n",
    "        else:\n",
    "            precision = TP/(TP+FP)\n",
    "            recall = TP/(TP+FN)\n",
    "            F = 2*(precision*recall)/(precision + recall)\n",
    "        \n",
    "        # Tignanelli Metric\n",
    "        if FN == 0:\n",
    "            TP_FN_R = TP\n",
    "        elif FN > 0:\n",
    "            TP_FN_R = TP/FN\n",
    " \n",
    "        return F, recall, precision, TP, FP, FN, TP_FN_R, TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_set(df, analysis_type = 'entity', df_type = 'sys', corpus = None):\n",
    "    \n",
    "    # get values for creation of series of type tuple\n",
    "    if 'entity' in analysis_type: \n",
    "        if corpus == 'casi':\n",
    "            arg = df.case, df.overlap\n",
    "        else:    \n",
    "            arg = df.begin, df.end, df.case\n",
    "            \n",
    "    elif 'cui' in analysis_type:\n",
    "        arg = df.value, df.case\n",
    "    elif 'full' in analysis_type:\n",
    "        arg = df.begin, df.end, df.value, df.case\n",
    "    \n",
    "    return set(list(zip(*arg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%cython \n",
    "\n",
    "from __main__ import df_to_set, engine\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "def get_cooccurences(ref, sys, analysis_type: str, corpus: str):\n",
    "    \"\"\"\n",
    "    get cooccurences between system and reference; exact match; TODO: add relaxed -> done in single system evals during ensemble run\n",
    "    \"\"\"\n",
    "    # cooccurences\n",
    "    class Cooccurences(object):\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.ref_system_match = 0\n",
    "            self.ref_only = 0\n",
    "            self.system_only = 0\n",
    "            self.system_n = 0\n",
    "            self.ref_n = 0\n",
    "            self.matches = set()\n",
    "            self.false_negatives = set()\n",
    "            self.corpus = corpus\n",
    "\n",
    "    c = Cooccurences()\n",
    "    \n",
    "    if c.corpus != 'casi':\n",
    "        if analysis_type in ['cui', 'full']:\n",
    "            sys = sys.rename(index=str, columns={\"note_id\": \"case\", \"cui\": \"value\"})\n",
    "            # do not overestimate FP\n",
    "            sys = sys[~sys['value'].isnull()] \n",
    "            ref = ref[~ref['value'].isnull()]\n",
    "        \n",
    "        if 'entity' in analysis_type: \n",
    "            sys = sys.rename(index=str, columns={\"note_id\": \"case\"})\n",
    "            cols_to_keep = ['begin', 'end', 'case']\n",
    "        elif 'cui' in analysis_type: \n",
    "            cols_to_keep = ['value', 'case']\n",
    "        elif 'full' in analysis_type: \n",
    "            cols_to_keep = ['begin', 'end', 'value', 'case']\n",
    "        \n",
    "        sys = sys[cols_to_keep].drop_duplicates()\n",
    "        ref = ref[cols_to_keep].drop_duplicates()\n",
    "        # matches via inner join\n",
    "        tp = pd.merge(sys, ref, how = 'inner', left_on=cols_to_keep, right_on = cols_to_keep) \n",
    "        # reference-only via left outer join\n",
    "        fn = pd.merge(ref, sys, how = 'left', left_on=cols_to_keep, right_on = cols_to_keep, indicator=True) \n",
    "        fn = fn[fn[\"_merge\"] == 'left_only']\n",
    "\n",
    "        tp = tp[cols_to_keep]\n",
    "        fn = fn[cols_to_keep]\n",
    "\n",
    "        # use for metrics \n",
    "        c.matches = c.matches.union(df_to_set(tp, analysis_type, 'ref'))\n",
    "        c.false_negatives = c.false_negatives.union(df_to_set(fn, analysis_type, 'ref'))\n",
    "        c.ref_system_match = len(c.matches)\n",
    "        c.system_only = len(sys) - len(c.matches) # fp\n",
    "        c.system_n = len(sys)\n",
    "        c.ref_n = len(ref)\n",
    "        c.ref_only = len(c.false_negatives)\n",
    "        \n",
    "    else:\n",
    "        sql = \"select `case` from test.amia_2019_analytical_v where overlap = 1 and `system` = %(sys.name)s\"  \n",
    "        tp = pd.read_sql(sql, params={\"sys.name\":sys.name}, con=engine)\n",
    "        \n",
    "        sql = \"select `case` from test.amia_2019_analytical_v where (overlap = 0 or overlap is null) and `system` = %(sys.name)s\"  \n",
    "        fn = pd.read_sql(sql, params={\"sys.name\":sys.name}, con=engine)\n",
    "        \n",
    "        c.matches = df_to_set(tp, 'entity', 'sys', 'casi')\n",
    "        c.fn = df_to_set(fn, 'entity', 'sys', 'casi')\n",
    "        c.ref_system_match = len(c.matches)\n",
    "        c.system_only = len(sys) - len(c.matches)\n",
    "        c.system_n = len(tp) + len(fn)\n",
    "        c.ref_n = len(tp) + len(fn)\n",
    "        c.ref_only = len(fn)\n",
    "        \n",
    "    # sanity check\n",
    "    if len(ref) - c.ref_system_match < 0:\n",
    "        print('Error: ref_system_match > len(ref)!')\n",
    "    if len(ref) != c.ref_system_match + c.ref_only:\n",
    "        print('Error: ref count mismatch!', len(ref), c.ref_system_match, c.ref_only)\n",
    "   \n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_vector(doc: str, ann: List[int], labels: List[str]) -> np.array:\n",
    "\n",
    "    v = np.zeros(doc)\n",
    "    labels = list(labels)\n",
    "    \n",
    "    for (i, lab) in enumerate(labels):\n",
    "        i += 1  # 0 is reserved for no label\n",
    "        idxs = [np.arange(a.begin, a.end) for a in ann if a.label == lab]\n",
    "        idxs = [j for mask in idxs for j in mask]\n",
    "        v[idxs] = i\n",
    "\n",
    "    return v\n",
    "\n",
    "# test confusion matrix elements for vectorized annotation set; includes TN\n",
    "# https://kawahara.ca/how-to-compute-truefalse-positives-and-truefalse-negatives-in-python-for-binary-classification-problems/\n",
    "# def confused(sys1, ann1):\n",
    "#     TP = np.sum(np.logical_and(ann1 == 1, sys1 == 1))\n",
    "\n",
    "#     # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "#     TN = np.sum(np.logical_and(ann1 == 0, sys1 == 0))\n",
    "\n",
    "#     # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "#     FP = np.sum(np.logical_and(ann1 == 0, sys1 == 1))\n",
    "\n",
    "#     # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "#     FN = np.sum(np.logical_and(ann1 == 1, sys1 == 0))\n",
    "    \n",
    "#     return TP, TN, FP, FN\n",
    "\n",
    "def confused(sys1, ann1):\n",
    "    TP = np.sum(np.logical_and(ann1 > 0, sys1 == ann1))\n",
    "\n",
    "    # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "    TN = np.sum(np.logical_and(ann1 == 0, sys1 == ann1))\n",
    "\n",
    "    # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "    FP = np.sum(np.logical_and(sys1 > 0, sys1 != ann1))\n",
    "\n",
    "    # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "    FN = np.sum(np.logical_and(ann1 > 0, sys1 == 0))\n",
    "    \n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def vectorized_cooccurences(r: object, analysis_type: str, corpus: str, filter_semtype, semtype = None) -> np.int64:\n",
    "    docs = get_docs(corpus)\n",
    "    \n",
    "    if filter_semtype:\n",
    "        ann = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "    else: \n",
    "        ann = get_ref_ann(analysis_type, corpus, filter_semtype)\n",
    "        \n",
    "    sys = get_sys_ann(analysis_type, r)\n",
    "    \n",
    "    #cvals = []\n",
    "    if analysis_type == 'entity':\n",
    "        labels = [\"concept\"]\n",
    "    elif analysis_type in ['cui', 'full']:\n",
    "        labels = list(set(ann[\"value\"].tolist()))\n",
    "    \n",
    "    sys2 = list()\n",
    "    ann2 = list()\n",
    "    s2 = list()\n",
    "    a2 = list()\n",
    "    \n",
    "    for n in range(len(docs)):\n",
    "        \n",
    "        if analysis_type != 'cui':\n",
    "            a1 = list(ann.loc[ann[\"case\"] == docs[n][0]].itertuples(index=False))\n",
    "            s1 = list(sys.loc[sys[\"case\"] == docs[n][0]].itertuples(index=False))\n",
    "            ann1 = label_vector(docs[n][1], a1, labels)\n",
    "            sys1 = label_vector(docs[n][1], s1, labels)\n",
    "\n",
    "            #TP, TN, FP, FN = confused(sys1, ann1)\n",
    "            #cvals.append([TP, TN, FP, FN])\n",
    "            sys2.append(list(sys1))\n",
    "            ann2.append(list(ann1))\n",
    "\n",
    "        else:\n",
    "            a = ann.loc[ann[\"case\"] == docs[n][0]]['label'].tolist()\n",
    "            s = sys.loc[sys[\"case\"] == docs[n][0]]['label'].tolist()\n",
    "            x = [1 if x in a else 0 for x in labels]\n",
    "            y = [1 if x in s else 0 for x in labels]\n",
    "#             x_sparse = sparse.csr_matrix(x)\n",
    "#             y_sparse = sparse.csr_matrix(y)\n",
    "            s2.append(y)\n",
    "            a2.append(x)\n",
    "           \n",
    "            \n",
    "            #a1 = list(ann.loc[ann[\"case\"] == docs[n][0]].itertuples(index=False))\n",
    "            #s1 = list(sys.loc[sys[\"case\"] == docs[n][0]].itertuples(index=False))\n",
    "   \n",
    "    if analysis_type != 'cui':\n",
    "        a2 = [item for sublist in ann2 for item in sublist]\n",
    "        s2 = [item for sublist in sys2 for item in sublist]\n",
    "        report = classification_report(a2, s2, output_dict=True)\n",
    "        #print('classification:', report)\n",
    "        macro_precision =  report['macro avg']['precision'] \n",
    "        macro_recall = report['macro avg']['recall']    \n",
    "        macro_f1 = report['macro avg']['f1-score']\n",
    "        TN, FP, FN, TP = confusion_matrix(a2, s2).ravel()\n",
    "        \n",
    "        #return (np.sum(cvals, axis=0), (macro_precision, macro_recall, macro_f1))\n",
    "        return ((TP, TN, FP, FN), (macro_precision, macro_recall, macro_f1))\n",
    "    else:\n",
    "        x_sparse = sparse.csr_matrix(a2)\n",
    "        y_sparse = sparse.csr_matrix(s2)\n",
    "        report = classification_report(x_sparse, y_sparse, output_dict=True)\n",
    "        macro_precision =  report['macro avg']['precision'] \n",
    "        macro_recall = report['macro avg']['recall']    \n",
    "        macro_f1 = report['macro avg']['f1-score']\n",
    "        #print((macro_precision, macro_recall, macro_f1))\n",
    "        return ((0, 0, 0, 0), (macro_precision, macro_recall, macro_f1))\n",
    "                                       \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm_dict(ref_only: int, system_only: int, ref_system_match: int, system_n: int, ref_n: int) -> dict:\n",
    "    \"\"\"\n",
    "    Generate dictionary of confusion matrix params and measures\n",
    "    :params: ref_only, system_only, reference_system_match -> sets\n",
    "    matches, system_n, reference_n -> counts\n",
    "    :return: dictionary object\n",
    "    \"\"\"\n",
    "\n",
    "    if ref_only + ref_system_match != ref_n:\n",
    "        print('ERROR!')\n",
    "        \n",
    "    # get evaluation metrics\n",
    "    F, recall, precision, TP, FP, FN, TP_FN_R, TM  = Metrics(system_only, ref_only, ref_system_match, system_n).get_confusion_metrics()\n",
    "\n",
    "    d = {\n",
    "#          'F1': F[1], \n",
    "#          'precision': precision[1], \n",
    "#          'recall': recall[1], \n",
    "         'F1': F, \n",
    "         'precision': precision, \n",
    "         'recall': recall, \n",
    "         'TP': TP, \n",
    "         'FN': FN, \n",
    "         'FP': FP, \n",
    "         'TP/FN': TP_FN_R,\n",
    "         'n_gold': ref_n, \n",
    "         'n_sys': system_n, \n",
    "         'TM': TM\n",
    "    }\n",
    "    \n",
    "    if system_n - FP != TP:\n",
    "        print('inconsistent system n!')\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ft.lru_cache(maxsize=None)\n",
    "def get_metric_data(analysis_type: str, corpus: str):\n",
    "   \n",
    "    usys_file, ref_table = AnalysisConfig().corpus_config()\n",
    "    systems = AnalysisConfig().systems\n",
    "    \n",
    "    sys_ann = pd.read_csv(analysisConf.data_dir + usys_file, dtype={'note_id': str})\n",
    "    \n",
    "    sql = \"SELECT * FROM \" + ref_table #+ \" where semtype in('Anatomy', 'Chemicals_and_drugs')\" \n",
    "    \n",
    "    ref_ann = pd.read_sql(sql, con=engine)\n",
    "    sys_ann = sys_ann.drop_duplicates()\n",
    "    \n",
    "    return ref_ann, sys_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%cython\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "def geometric_mean(metrics):\n",
    "    \"\"\"\n",
    "    1. Get rank average of F1, TP/FN, TM\n",
    "        http://www.datasciencemadesimple.com/rank-dataframe-python-pandas-min-max-dense-rank-group/\n",
    "        https://stackoverflow.com/questions/46686315/in-pandas-how-to-create-a-new-column-with-a-rank-according-to-the-mean-values-o?rq=1\n",
    "    2. Take geomean of rank averages\n",
    "        https://stackoverflow.com/questions/42436577/geometric-mean-applied-on-row\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.DataFrame() \n",
    "\n",
    "    metrics['F1 rank']=metrics['F1'].rank(ascending=0,method='average')\n",
    "    metrics['TP/FN rank']=metrics['TP/FN'].rank(ascending=0,method='average')\n",
    "    metrics['TM rank']=metrics['TM'].rank(ascending=0,method='average')\n",
    "    metrics['Gmean'] = gmean(metrics.iloc[:,-3:],axis=1)\n",
    "\n",
    "    return metrics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metrics(analysis_type: str, corpus: str, filter_semtype, semtype = None):\n",
    "    start = time.time()\n",
    "\n",
    "    systems = AnalysisConfig().systems\n",
    "    metrics = pd.DataFrame()\n",
    "\n",
    "    __, sys_ann = get_metric_data(analysis_type, corpus)\n",
    "    c = None\n",
    "    \n",
    "    for sys in systems:\n",
    "       \n",
    "        if filter_semtype and semtype:\n",
    "            ref_ann = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "            ref_ann = get_ref_ann(analysis_type, corpus, filter_semtype)\n",
    "            \n",
    "        system_annotations = sys_ann[sys_ann['system'] == sys].copy()\n",
    "\n",
    "        if filter_semtype:\n",
    "            st = SemanticTypes([semtype], corpus).get_system_type(sys)\n",
    "\n",
    "            if st: \n",
    "                system_annotations = sys_ann[sys_ann['semtypes'].isin(st)].copy()\n",
    "        else:\n",
    "            system_annotations = sys_ann.copy()\n",
    "\n",
    "        if (filter_semtype and st) or filter_semtype is False:\n",
    "            system = system_annotations.copy()\n",
    "\n",
    "            if sys == 'quick_umls':\n",
    "                system = system[system.score.astype(float) >= .8]\n",
    "\n",
    "            if sys == 'metamap' and modification == None:\n",
    "                system = system.fillna(0)\n",
    "                system = system[system.score.abs().astype(int) >= 800]\n",
    "\n",
    "            system = system.drop_duplicates()\n",
    "\n",
    "            ref_ann = ref_ann.rename(index=str, columns={\"start\": \"begin\", \"file\": \"case\"})\n",
    "            c = get_cooccurences(ref_ann, system, analysis_type, corpus) # get matches, FN, etc.\n",
    "\n",
    "            \n",
    "            if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "                # get dictionary of confusion matrix metrics\n",
    "                d = cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n)\n",
    "                d['system'] = sys\n",
    "\n",
    "                data = pd.DataFrame(d,  index=[0])\n",
    "                metrics = pd.concat([metrics, data], ignore_index=True)\n",
    "                metrics.drop_duplicates(keep='last', inplace=True)\n",
    "            else:\n",
    "                print(\"NO EXACT MATCHES FOR\", sys)\n",
    "            elapsed = (time.time() - start)\n",
    "            print(\"elapsed:\", sys, elapsed)\n",
    "   \n",
    "    if c:\n",
    "        elapsed = (time.time() - start)\n",
    "        print(geometric_mean(metrics))\n",
    "\n",
    "        now = datetime.now()\n",
    "        timestamp = datetime.timestamp(now)\n",
    "\n",
    "        file_name = 'metrics_'\n",
    "\n",
    "        metrics.to_csv(analysisConf.data_dir + corpus + '_' + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "\n",
    "        print(\"total elapsed time:\", elapsed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ft.lru_cache(maxsize=None)\n",
    "def get_ref_n(analysis_type: str, corpus: str, filter_semtype: str) -> int:\n",
    "    \n",
    "    ref_ann, _ = get_metric_data(analysis_type, corpus)\n",
    "    \n",
    "    if filter_semtype:\n",
    "        ref_ann = ref_ann[ref_ann['semtype'].isin(SemanticTypes(semtypes, corpus).get_system_type('reference'))]\n",
    "            \n",
    "    if corpus == 'casi':\n",
    "        return len(ref_ann)\n",
    "        \n",
    "    else:\n",
    "        # do not overestimate fn\n",
    "        if 'entity' in analysis_type:\n",
    "            ref_ann = ref_ann[['start', 'end', 'file']].drop_duplicates()\n",
    "        elif 'cui' in analysis_type:\n",
    "            ref_ann = ref_ann[['value', 'file']].drop_duplicates()\n",
    "        elif 'full' in analysis_type:\n",
    "            ref_ann = ref_ann[['start', 'end', 'value', 'file']].drop_duplicates()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        ref_n = len(ref_ann.drop_duplicates())\n",
    "\n",
    "        return ref_n\n",
    "    \n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_sys_data(system: str, analysis_type: str, corpus: str, filter_semtype, semtype = None) -> pd.DataFrame:\n",
    "   \n",
    "    _, data = get_metric_data(analysis_type, corpus)\n",
    "    \n",
    "    out = data[data['system'] == system].copy()\n",
    "    \n",
    "    if filter_semtype:\n",
    "        st = SemanticTypes([semtype], corpus).get_system_type(system)\n",
    "        print(system, 'st', st)\n",
    "    \n",
    "    if corpus == 'casi':\n",
    "        cols_to_keep = ['case', 'overlap'] \n",
    "        out = out[cols_to_keep].drop_duplicates()\n",
    "        return out\n",
    "        \n",
    "    else:\n",
    "        if filter_semtype:\n",
    "            out = out[out['semtypes'].isin(st)].copy()\n",
    "            \n",
    "        else:\n",
    "            out = out[out['system']== system].copy()\n",
    "            \n",
    "        if modification == 'negation':\n",
    "            out = out[out['modification'] == 'negation'].copy()\n",
    "        \n",
    "        if system == 'quick_umls':\n",
    "            out = out[(out.score.astype(float) >= 0.8) & (out[\"type\"] == 'concept_jaccard_score_False')]\n",
    "            # fix for leading space on semantic type field\n",
    "            out = out.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x) \n",
    "            out['semtypes'] = out['semtypes'].str.strip()\n",
    "        \n",
    "        if system == 'metamap' and modification == None:\n",
    "            out = out[out.score.abs().astype(int) >= 800]\n",
    "            \n",
    "        if 'entity' in analysis_type:\n",
    "            cols_to_keep = ['begin', 'end', 'note_id']\n",
    "        elif 'cui' in analysis_type:\n",
    "            cols_to_keep = ['cui', 'note_id']\n",
    "        elif 'full' in analysis_type:\n",
    "            cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "\n",
    "        out = out[cols_to_keep]\n",
    "        \n",
    "        return out.drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GENERATE merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetTotals(object):\n",
    "    \"\"\" \n",
    "    returns an instance with merged match set numbers using either union or intersection of elements in set \n",
    "    \"\"\"\n",
    "    def __init__(self, ref_n, sys_n, match_set):\n",
    "\n",
    "        self = self    \n",
    "        self.ref_ann = ref_n\n",
    "        self.sys_n = sys_n\n",
    "        self.match_set = match_set\n",
    "\n",
    "    def get_ref_sys(self):\n",
    "\n",
    "        ref_only = self.ref_ann - len(self.match_set)\n",
    "        sys_only = self.sys_n - len(self.match_set)\n",
    "\n",
    "        return ref_only, sys_only, len(self.match_set), self.match_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_vote(df):\n",
    "    mv = df.copy()\n",
    "    mv['length'] = (mv.end - mv.begin).abs()\n",
    "    mv['span'] = list(zip(mv.begin, mv.end))\n",
    "    \n",
    "    mv.sort_values(by=['note_id','begin'],inplace=True)\n",
    "    \n",
    "    i = 0\n",
    "    #out = pd.DataFrame()\n",
    "\n",
    "    data = []\n",
    "    for row in mv.itertuples():\n",
    "    \n",
    "        #if i < 5000:\n",
    "        #print('row:', i, row.note_id, row.begin, row.end)\n",
    "        test = mv[mv['note_id']==row.note_id].copy()\n",
    "\n",
    "        iix = pd.IntervalIndex.from_arrays(test.begin, test.end, closed='neither')\n",
    "        span_range = pd.Interval(row.begin, row.end)\n",
    "        fx = test[iix.overlaps(span_range)].copy()\n",
    "\n",
    "        #print('matches:', fx)\n",
    "\n",
    "        maxLength = fx['length'].max()\n",
    "        minLength = fx['length'].min()\n",
    "\n",
    "        if len(fx) > 1: \n",
    "            if i%5000 == 0:\n",
    "                print('iteration:', i)\n",
    "\n",
    "            # if longer span exists, use it as tie-breaker\n",
    "            if maxLength > minLength:\n",
    "                fx = fx[fx['length'] == fx['length'].max()]\n",
    "\n",
    "        i += 1\n",
    "        #frames = [ out, fx ]\n",
    "        #out = pd.concat(frames, ignore_index=True)\n",
    "        data.append(fx)\n",
    "#         else:\n",
    "#             break\n",
    "        \n",
    "    out = pd.concat(data, axis=0)\n",
    "    print(out.head())\n",
    "    \n",
    "    out = out.drop_duplicates(['begin', 'end', 'note_id', 'length', 'cui'])\n",
    "    # for those still with duplicate spans, different CUI select random row\n",
    "    idx = np.random.permutation(np.arange(len(out)))\n",
    "    #out.iloc[idx].drop_duplicates(subset=['begin', 'end', 'note_id', 'length']).to_csv('/Users/gms/development/nlp/nlpie/data/ensembling-u01/output/test_vote_dddd.csv')\n",
    "    return out.iloc[idx].drop_duplicates(subset=['begin', 'end', 'note_id', 'length']) # .to_csv('/Users/gms/development/nlp/nlpie/data/ensembling-u01/output/test_vote_dd.csv')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ft.lru_cache(maxsize=None)\n",
    "def process_sentence(pt, sentence, analysis_type, corpus, filter_semtype, semtype = None):\n",
    "    \"\"\"\n",
    "    Recursively evaluate parse tree, \n",
    "    with check for existence before build\n",
    "       :param sentence: to process\n",
    "       :return class of merged annotations, boolean operated system df \n",
    "    \"\"\"\n",
    "    \n",
    "    class Results(object):\n",
    "        def __init__(self):\n",
    "            self.results = set()\n",
    "            self.system_merges = pd.DataFrame()\n",
    "            \n",
    "    r = Results()\n",
    "    \n",
    "    if 'entity' in analysis_type and corpus != 'casi': \n",
    "        cols_to_keep = ['begin', 'end', 'note_id'] # entity only\n",
    "    elif 'full' in analysis_type: \n",
    "        cols_to_keep = ['cui', 'begin', 'end', 'note_id'] # entity only\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['cui', 'note_id'] # entity only\n",
    "    elif corpus == 'casi':\n",
    "        cols_to_keep = ['case', 'overlap']\n",
    "    \n",
    "    def evaluate(parseTree):\n",
    "        oper = {'&': op.and_, '|': op.or_}\n",
    "        \n",
    "        if parseTree:\n",
    "            leftC = gevent.spawn(evaluate, parseTree.getLeftChild())\n",
    "            rightC = gevent.spawn(evaluate, parseTree.getRightChild())\n",
    "            \n",
    "            if leftC.get() is not None and rightC.get() is not None:\n",
    "                system_query = pd.DataFrame()\n",
    "                fn = oper[parseTree.getRootVal()]\n",
    "                \n",
    "                if isinstance(leftC.get(), str):\n",
    "                    # get system as leaf node \n",
    "                    if filter_semtype:\n",
    "                        left_sys = get_sys_data(leftC.get(), analysis_type, corpus, filter_semtype, semtype)\n",
    "                    else:\n",
    "                        left_sys = get_sys_data(leftC.get(), analysis_type, corpus, filter_semtype)\n",
    "                \n",
    "                elif isinstance(leftC.get(), pd.DataFrame):\n",
    "                    l_sys = leftC.get()\n",
    "                \n",
    "                if isinstance(rightC.get(), str):\n",
    "                    # get system as leaf node\n",
    "                    if filter_semtype:\n",
    "                        right_sys = get_sys_data(rightC.get(), analysis_type, corpus, filter_semtype, semtype)\n",
    "                    else:\n",
    "                        right_sys = get_sys_data(rightC.get(), analysis_type, corpus, filter_semtype)\n",
    "                    \n",
    "                elif isinstance(rightC.get(), pd.DataFrame):\n",
    "                    r_sys = rightC.get()\n",
    "                    \n",
    "                if fn == op.or_:\n",
    "\n",
    "                    if isinstance(leftC.get(), str) and isinstance(rightC.get(), str):\n",
    "                        frames = [left_sys, right_sys]\n",
    "\n",
    "                    elif isinstance(leftC.get(), str) and isinstance(rightC.get(), pd.DataFrame):\n",
    "                        frames = [left_sys, r_sys]\n",
    "\n",
    "                    elif isinstance(leftC.get(), pd.DataFrame) and isinstance(rightC.get(), str):\n",
    "                        frames = [l_sys, right_sys]\n",
    "\n",
    "                    elif isinstance(leftC.get(), pd.DataFrame) and isinstance(rightC.get(), pd.DataFrame):\n",
    "                        frames = [l_sys, r_sys]\n",
    "                    \n",
    "                    # add in vote\n",
    "                        \n",
    "                    df = pd.concat(frames,  ignore_index=True)\n",
    "                    #df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "                    \n",
    "                    if analysis_type == 'full':\n",
    "                        df = union_vote(df)\n",
    "\n",
    "                if fn == op.and_:\n",
    "                    \n",
    "                    if isinstance(leftC.get(), str) and isinstance(rightC.get(), str):\n",
    "                        if not left_sys.empty and not right_sys.empty:\n",
    "                            df = left_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "                        else:\n",
    "                            df = pd.DataFrame(columns=cols_to_keep)\n",
    "\n",
    "                    elif isinstance(leftC.get(), str) and isinstance(rightC.get(), pd.DataFrame):\n",
    "                        if not left_sys.empty and not r_sys.empty:\n",
    "                            df = left_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "                        else:\n",
    "                            df = pd.DataFrame(columns=cols_to_keep)\n",
    "\n",
    "                    elif isinstance(leftC.get(), pd.DataFrame) and isinstance(rightC.get(), str):\n",
    "                        if not l_sys.empty and not right_sys.empty:\n",
    "                            df = l_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "                        else:\n",
    "                            df = pd.DataFrame(columns=cols_to_keep)\n",
    "\n",
    "                    elif isinstance(leftC.get(), pd.DataFrame) and isinstance(rightC.get(), pd.DataFrame):\n",
    "                        if not l_sys.empty and not r_sys.empty:\n",
    "                            df = l_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "                        else:\n",
    "                            df = pd.DataFrame(columns=cols_to_keep)\n",
    "                \n",
    "                # get combined system results\n",
    "                r.system_merges = df\n",
    "                \n",
    "                if len(df) > 0:\n",
    "                    system_query = system_query.append(df)\n",
    "                else:\n",
    "                    print('wtf!')\n",
    "                    \n",
    "                return system_query\n",
    "            else:\n",
    "                return parseTree.getRootVal()\n",
    "    \n",
    "    if sentence.n_or > 0 or sentence.n_and > 0:\n",
    "        evaluate(pt)  \n",
    "    \n",
    "    # trivial case\n",
    "    elif sentence.n_or == 0 and sentence.n_and == 0:\n",
    "        \n",
    "        if filter_semtype:\n",
    "            r.system_merges = get_sys_data(sentence.sentence, analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "            r.system_merges = get_sys_data(sentence.sentence, analysis_type, corpus, filter_semtype)\n",
    "        \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Incoming Boolean sentences are parsed into a binary tree.\n",
    "\n",
    "Test expressions to parse:\n",
    "\n",
    "sentence = '((((A&B)|C)|D)&E)'\n",
    "\n",
    "sentence = '(E&(D|(C|(A&B))))'\n",
    "\n",
    "sentence = '(((A|(B&C))|(D&(E&F)))|(H&I))'\n",
    "\n",
    "\"\"\"\n",
    "# build parse tree from passed sentence using grammatical rules of Boolean logic\n",
    "def buildParseTree(fpexp):\n",
    "    \"\"\"\n",
    "       Iteratively build parse tree from passed sentence using grammatical rules of Boolean logic\n",
    "       :param fpexp: sentence to parse\n",
    "       :return eTree: parse tree representation\n",
    "       Incoming Boolean sentences are parsed into a binary tree.\n",
    "       Test expressions to parse:\n",
    "       sentence = '(A&B)'\n",
    "       sentence = '(A|B)'\n",
    "       sentence = '((A|B)&C)'\n",
    "       \n",
    "    \"\"\"\n",
    "    fplist = fpexp.split()\n",
    "    pStack = Stack()\n",
    "    eTree = BinaryTree('')\n",
    "    pStack.push(eTree)\n",
    "    currentTree = eTree\n",
    "\n",
    "    for i in fplist:\n",
    "\n",
    "        if i == '(':\n",
    "            currentTree.insertLeft('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getLeftChild()\n",
    "        elif i not in ['&', '|', ')']:\n",
    "            currentTree.setRootVal(i)\n",
    "            parent = pStack.pop()\n",
    "            currentTree = parent\n",
    "        elif i in ['&', '|']:\n",
    "            currentTree.setRootVal(i)\n",
    "            currentTree.insertRight('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getRightChild()\n",
    "        elif i == ')':\n",
    "            currentTree = pStack.pop()\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    return eTree\n",
    "\n",
    "def make_parse_tree(payload):\n",
    "    \"\"\"\n",
    "    Ensure data to create tree are in correct form\n",
    "    :param sentence: sentence to preprocess\n",
    "    :return pt, parse tree graph\n",
    "            sentence, processed sentence to build tree\n",
    "            a: order\n",
    "    \"\"\"\n",
    "    def preprocess_sentence(sentence):\n",
    "        # prepare statement for case when a boolean AND/OR is given\n",
    "        sentence = payload.replace('(', ' ( '). \\\n",
    "            replace(')', ' ) '). \\\n",
    "            replace('&', ' & '). \\\n",
    "            replace('|', ' | '). \\\n",
    "            replace('  ', ' ')\n",
    "        return sentence\n",
    "\n",
    "    sentence = preprocess_sentence(payload)\n",
    "    print('Processing sentence:', sentence)\n",
    "    \n",
    "    pt = buildParseTree(sentence)\n",
    "    #pt.postorder() \n",
    "    \n",
    "    return pt\n",
    "\n",
    "class Sentence(object):\n",
    "    '''\n",
    "    Details about boolean expression -> number operators and expression\n",
    "    '''\n",
    "    def __init__(self, sentence):\n",
    "        self = self\n",
    "        self.n_and = sentence.count('&')\n",
    "        self.n_or = sentence.count('|')\n",
    "        self.sentence = sentence\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_docs(corpus):\n",
    "    \n",
    "    # KLUDGE!!!\n",
    "    if corpus == 'ray_test':\n",
    "        corpus = 'fairview'\n",
    "        \n",
    "    sql = 'select distinct note_id, sofa from sofas where corpus = %(corpus)s order by note_id'\n",
    "    df = pd.read_sql(sql, params={\"corpus\":corpus}, con=engine)\n",
    "    df.drop_duplicates()\n",
    "    df['len_doc'] = df['sofa'].apply(len)\n",
    "    \n",
    "    subset = df[['note_id', 'len_doc']]\n",
    "    docs = [tuple(x) for x in subset.to_numpy()]\n",
    "    \n",
    "    return docs\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_ref_ann(analysis_type, corpus, filter_semtype, semtype = None):\n",
    "    \n",
    "    if filter_semtype:\n",
    "        if ',' in semtype:\n",
    "            semtype = semtype.split(',')\n",
    "        else:\n",
    "            semtype = [semtype]\n",
    "        \n",
    "    ann, _ = get_metric_data(analysis_type, corpus)\n",
    "    ann = ann.rename(index=str, columns={\"start\": \"begin\", \"file\": \"case\"})\n",
    "    \n",
    "    if filter_semtype:\n",
    "        ann = ann[ann['semtype'].isin(semtype)]\n",
    "    if analysis_type == 'entity':   \n",
    "        ann[\"label\"] = 'concept'\n",
    "    elif analysis_type in ['cui','full']:\n",
    "        ann[\"label\"] = ann[\"value\"]\n",
    "        \n",
    "    if modification == 'negation':\n",
    "        ann = ann[ann['semtype'] == 'negation']\n",
    "    \n",
    "    \n",
    "    if analysis_type == 'entity':\n",
    "        cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "    elif analysis_type == 'cui':\n",
    "        cols_to_keep = ['value', 'case', 'label']\n",
    "    elif analysis_type == 'full':\n",
    "        cols_to_keep = ['begin', 'end', 'value', 'case', 'label']\n",
    "    ann = ann[cols_to_keep]\n",
    "    \n",
    "    return ann\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_sys_ann(analysis_type, r):\n",
    "    sys = r.system_merges   \n",
    "    \n",
    "    sys = sys.rename(index=str, columns={\"note_id\": \"case\"})\n",
    "    if analysis_type == 'entity':\n",
    "        sys[\"label\"] = 'concept'\n",
    "        cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "    elif analysis_type == 'full':\n",
    "        sys[\"label\"] = sys[\"cui\"]\n",
    "        cols_to_keep = ['begin', 'end', 'case', 'value', 'label']\n",
    "    elif analysis_type == 'cui':\n",
    "        sys[\"label\"] = sys[\"cui\"]\n",
    "        cols_to_keep = ['case', 'cui', 'label']\n",
    "    \n",
    "    sys = sys[cols_to_keep]\n",
    "    return sys\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_metrics(boolean_expression: str, analysis_type: str, corpus: str, run_type: str, filter_semtype, semtype = None):\n",
    "    \"\"\"\n",
    "    Traverse binary parse tree representation of Boolean sentence\n",
    "        :params: boolean expression in form of '(<annotator_engine_name1><boolean operator><annotator_engine_name2>)'\n",
    "                 analysis_type (string value of: 'entity', 'cui', 'full') used to filter set of reference and system annotations \n",
    "        :return: dictionary with values needed for confusion matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence = Sentence(boolean_expression)   \n",
    "    pt = make_parse_tree(sentence.sentence)\n",
    "    \n",
    "    if filter_semtype:\n",
    "        r = process_sentence(pt, sentence, analysis_type, corpus, filter_semtype, semtype)\n",
    "    else:\n",
    "        r = process_sentence(pt, sentence, analysis_type, corpus, filter_semtype)\n",
    "        \n",
    "    # vectorize merges using i-o labeling\n",
    "    if run_type == 'overlap':\n",
    "        if filter_semtype:\n",
    "             ((TP, TN, FP, FN),(p,r,f1)) = vectorized_cooccurences(r, analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "            ((TP, TN, FP, FN),(p,r,f1)) = vectorized_cooccurences(r, analysis_type, corpus, filter_semtype)\n",
    "        \n",
    "        print('results:',((TP, TN, FP, FN),(p,r,f1)))\n",
    "        # TODO: validate against ann1/sys1 where val = 1\n",
    "        # total by number chars\n",
    "        system_n = TP + FP\n",
    "        reference_n = TP + FN\n",
    "\n",
    "        if analysis_type != 'cui':\n",
    "            d = cm_dict(FN, FP, TP, system_n, reference_n)\n",
    "        else:\n",
    "            d = dict()\n",
    "            d['F1'] = 0\n",
    "            d['precision'] = 0 \n",
    "            d['recall'] = 0\n",
    "            d['TP/FN'] = 0\n",
    "            d['TM'] = 0\n",
    "            \n",
    "        d['TN'] = TN\n",
    "        d['macro_p'] = p\n",
    "        d['macro_r'] = r\n",
    "        d['macro_f1'] = f1\n",
    "        \n",
    "        \n",
    "        # return full metrics\n",
    "        return d\n",
    "\n",
    "    elif run_type == 'exact':\n",
    "        # total by number spans\n",
    "        \n",
    "        if filter_semtype:\n",
    "            ann = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "        else: \n",
    "            ann = get_ref_ann(analysis_type, corpus, filter_semtype)\n",
    "        \n",
    "        c = get_cooccurences(ann, r.system_merges, analysis_type, corpus) # get matches, FN, etc.\n",
    "\n",
    "        if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "            # get dictionary of confusion matrix metrics\n",
    "            d = cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n)\n",
    "        else:\n",
    "            d = None\n",
    "            \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_valid_systems(['biomedicus'], 'Anatomy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all combinations of given list of annotators:\n",
    "def partly_unordered_permutations(lst, k):\n",
    "    elems = set(lst)\n",
    "    for c in combinations(lst, k):\n",
    "        for d in permutations(elems - set(c)):\n",
    "            yield c + d\n",
    "            \n",
    "def expressions(l, n):\n",
    "    for (operations, *operands), operators in product(\n",
    "            combinations(l, n), product(('&', '|'), repeat=n - 1)):\n",
    "        for operation in zip(operators, operands):\n",
    "            operations = [operations, *operation]\n",
    "        yield operations\n",
    "\n",
    "# get list of systems with a semantic type in grouping\n",
    "def get_valid_systems(systems, semtype):\n",
    "    test = []\n",
    "    for sys in systems:\n",
    "        st = system_semtype_check(sys, semtype, corpus)\n",
    "        if st:\n",
    "            test.append(sys)\n",
    "\n",
    "    return test\n",
    "\n",
    "# permute system combinations and evaluate system merges for performance\n",
    "def run_ensemble(systems, analysis_type, corpus, filter_semtype, expression_type, semtype = None):\n",
    "    metrics = pd.DataFrame()\n",
    "    \n",
    "    # pass single system to evaluate\n",
    "    if expression_type == 'single':\n",
    "        for system in systems:\n",
    "            if filter_semtype:\n",
    "                d = get_metrics(system, analysis_type, corpus, run_type, filter_semtype, semtype)\n",
    "            else:\n",
    "                d = get_metrics(system, analysis_type, corpus, run_type, filter_semtype)\n",
    "            d['merge'] = system\n",
    "            d['n_terms'] = 1\n",
    "\n",
    "            frames = [metrics, pd.DataFrame(d, index=[0])]\n",
    "            metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "    \n",
    "    elif expression_type == 'nested':\n",
    "        for l in partly_unordered_permutations(systems, 2):\n",
    "            print('processing merge combo:', l)\n",
    "            for i in range(1, len(l)+1):\n",
    "                test = list(expressions(l, i))\n",
    "                for t in test:\n",
    "                    if i > 1:\n",
    "                        # format Boolean sentence for parse tree \n",
    "                        t = '(' + \" \".join(str(x) for x in t).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "\n",
    "                    if filter_semtype:\n",
    "                        d = get_metrics(t, analysis_type, corpus, run_type, filter_semtype, semtype)\n",
    "                    else:\n",
    "                        d = get_metrics(t, analysis_type, corpus, run_type, filter_semtype)\n",
    "\n",
    "                    d['merge'] = t\n",
    "                    d['n_terms'] = i\n",
    "\n",
    "                    frames = [metrics, pd.DataFrame(d, index=[0])]\n",
    "                    metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "                    \n",
    "    elif expression_type == 'nested_with_singleton' and len(systems) == 5:\n",
    "        # form (((a&b)|c)&(d|e))\n",
    "        \n",
    "        nested = list(expressions(systems, 3))\n",
    "        test = list(expressions(systems, 2))\n",
    "        to_do_terms = []\n",
    "    \n",
    "        for n in nested:\n",
    "            # format Boolean sentence for parse tree \n",
    "            n = '(' + \" \".join(str(x) for x in n).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "\n",
    "            for t in test:\n",
    "                t = '(' + \" \".join(str(x) for x in t).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "\n",
    "                new_and = '(' + n +'&'+ t + ')'\n",
    "                new_or = '(' + n +'|'+ t + ')'\n",
    "\n",
    "                if new_and.count('biomedicus') != 2 and new_and.count('clamp') != 2 and new_and.count('ctakes') != 2 and new_and.count('metamap') != 2 and new_and.count('quick_umls') != 2:\n",
    "\n",
    "                    if new_and.count('&') != 4 and new_or.count('|') != 4:\n",
    "                        #print(new_and)\n",
    "                        #print(new_or)\n",
    "                        to_do_terms.append(new_or)\n",
    "                        to_do_terms.append(new_and)\n",
    "        \n",
    "        print('nested_with_singleton', len(to_do_terms))\n",
    "        for term in to_do_terms:\n",
    "            if filter_semtype:\n",
    "                d = get_metrics(term, analysis_type, corpus, run_type, filter_semtype, semtype)\n",
    "            else:\n",
    "                d = get_metrics(term, analysis_type, corpus, run_type, filter_semtype)\n",
    "                \n",
    "            n = term.count('&')\n",
    "            m = term.count('|')\n",
    "            d['merge'] = term\n",
    "            d['n_terms'] = m + n + 1\n",
    "\n",
    "            frames = [metrics, pd.DataFrame(d, index=[0])]\n",
    "            metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "                        \n",
    "    elif expression_type == 'paired':\n",
    "        m = list(expressions(systems, 2))\n",
    "        test = list(expressions(m, 2))\n",
    "\n",
    "        to_do_terms = []\n",
    "        for t in test:\n",
    "            # format Boolean sentence for parse tree \n",
    "            t = '(' + \" \".join(str(x) for x in t).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "            if t.count('biomedicus') != 2 and t.count('clamp') != 2 and t.count('ctakes') != 2 and t.count('metamap') != 2 and t.count('quick_umls') != 2:\n",
    "                if t.count('&') != 3 and t.count('|') != 3:\n",
    "                    to_do_terms.append(t)\n",
    "                    if len(systems) == 5:\n",
    "                        for i in systems:\n",
    "                            if i not in t:\n",
    "                                #print('('+t+'&'+i+')')\n",
    "                                #print('('+t+'|'+i+')')\n",
    "                                new_and = '('+t+'&'+i+')'\n",
    "                                new_or = '('+t+'|'+i+')'\n",
    "                                to_do_terms.append(new_and)\n",
    "                                to_do_terms.append(new_or)\n",
    "                            \n",
    "        print('paired', len(to_do_terms))\n",
    "        for term in to_do_terms:\n",
    "            if filter_semtype:\n",
    "                d = get_metrics(term, analysis_type, corpus, run_type, filter_semtype, semtype)\n",
    "            else:\n",
    "                d = get_metrics(term, analysis_type, corpus, run_type, filter_semtype)\n",
    "                \n",
    "            n = term.count('&')\n",
    "            m = term.count('|')\n",
    "            d['merge'] = term\n",
    "            d['n_terms'] = m + n + 1\n",
    "\n",
    "            frames = [metrics, pd.DataFrame(d, index=[0])]\n",
    "            metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "        \n",
    "    return metrics\n",
    "\n",
    "# write to file\n",
    "def generate_ensemble_metrics(metrics, analysis_type, corpus, ensemble_type, filter_semtype, semtype = None):\n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    \n",
    "    file_name = corpus + '_all_'\n",
    "   \n",
    "    # drop exact matches:\n",
    "    metrics = metrics.drop_duplicates()\n",
    "    \n",
    "    if ensemble_type == 'merge':\n",
    "        metrics = metrics.sort_values(by=['n_terms', 'merge'])\n",
    "        file_name += 'merge_'\n",
    "    elif ensemble_type == 'vote':\n",
    "        file_name += 'vote_'\n",
    "    \n",
    "    #metrics = metrics.drop_duplicates(subset=['TP', 'FN', 'FP', 'n_sys', 'precision', 'recall', 'F', 'TM', 'TP/FN', 'TM', 'n_terms'])\n",
    "\n",
    "    file = file_name + analysis_type + '_' + run_type +'_'\n",
    "    \n",
    "    if filter_semtype:\n",
    "        file += semtype\n",
    "        \n",
    "    \n",
    "    geometric_mean(metrics).to_csv(analysisConf.data_dir + file + str(timestamp) + '.csv')\n",
    "    print(geometric_mean(metrics))\n",
    "    \n",
    "# control ensemble run\n",
    "def ensemble_control(systems, analysis_type, corpus, run_type, filter_semtype, semtypes = None):\n",
    "    if filter_semtype:\n",
    "        for semtype in semtypes:\n",
    "            test = get_valid_systems(systems, semtype)\n",
    "            print('SYSTEMS FOR SEMTYPE', semtype, 'ARE', test)\n",
    "            metrics = run_ensemble(test, analysis_type, corpus, filter_semtype, expression_type, semtype)\n",
    "            if (expression_type == 'nested_with_singleton' and len(test) == 5) or expression_type in ['nested', 'paired', 'single']:\n",
    "                generate_ensemble_metrics(metrics, analysis_type, corpus, ensemble_type, filter_semtype, semtype)\n",
    "    else:\n",
    "        metrics = run_ensemble(systems, analysis_type, corpus, filter_semtype, expression_type)\n",
    "        generate_ensemble_metrics(metrics, analysis_type, corpus, ensemble_type, filter_semtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ad hoc query for performance evaluation\n",
    "def get_merge_data(boolean_expression: str, analysis_type: str, corpus: str, run_type: str, filter_semtype, semtype = None):\n",
    "    \"\"\"\n",
    "    Traverse binary parse tree representation of Boolean sentence\n",
    "        :params: boolean expression in form of '(<annotator_engine_name1><boolean operator><annotator_engine_name2>)'\n",
    "                 analysis_type (string value of: 'entity', 'cui', 'full') used to filter set of reference and system annotations \n",
    "        :return: dictionary with values needed for confusion matrix\n",
    "    \"\"\"\n",
    "    if filter_semtype:\n",
    "        ann = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "    else: \n",
    "        ann = get_ref_ann(analysis_type, corpus, filter_semtype)\n",
    "    \n",
    "    sentence = Sentence(boolean_expression)   \n",
    "\n",
    "    pt = make_parse_tree(sentence.sentence)\n",
    "\n",
    "    r = process_sentence(pt, sentence, analysis_type, corpus, filter_semtype, semtype)\n",
    "\n",
    "    if run_type == 'overlap' and rtype != 6:\n",
    "        if filter_semtype:\n",
    "             ((TP, TN, FP, FN),(p,r,f1)) = vectorized_cooccurences(r, analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "             ((TP, TN, FP, FN),(p,r,f1)) = vectorized_cooccurences(r, analysis_type, corpus, filter_semtype)\n",
    "\n",
    "        # TODO: validate against ann1/sys1 where val = 1\n",
    "        # total by number chars\n",
    "        system_n = TP + FP\n",
    "        reference_n = TP + FN\n",
    "\n",
    "        d = cm_dict(FN, FP, TP, system_n, reference_n)\n",
    "        print(d)\n",
    "        \n",
    "    elif run_type == 'exact':\n",
    "        c = get_cooccurences(ann, r.system_merges, analysis_type, corpus) # get matches, FN, etc.\n",
    "\n",
    "        if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "            # get dictionary of confusion matrix metrics\n",
    "            d = cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n)\n",
    "\n",
    "            print('cm', d)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # get matched data from merge\n",
    "    return r.system_merges # merge_eval(reference_only, system_only, reference_system_match, system_n, reference_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# majority vote \n",
    "def vectorized_annotations(ann):\n",
    "    \n",
    "    docs = get_docs(corpus)\n",
    "    labels = [\"concept\"]\n",
    "    out= []\n",
    "    \n",
    "    for n in range(len(docs)):\n",
    "        a1 = list(ann.loc[ann[\"case\"] == docs[n][0]].itertuples(index=False))\n",
    "        a = label_vector(docs[n][1], a1, labels)\n",
    "        out.append(a)\n",
    "\n",
    "    return out\n",
    "\n",
    "def flatten_list(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def get_reference_vector(analysis_type, corpus, filter_semtype, semtype = None):\n",
    "    ref_ann = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "\n",
    "    df = ref_ann.copy()\n",
    "    df = df.drop_duplicates(subset=['begin','end','case'])\n",
    "    df['label'] = 'concept'\n",
    "\n",
    "    cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "    ref = df[cols_to_keep].copy()\n",
    "    test = vectorized_annotations(ref)\n",
    "    ref =  np.asarray(flatten_list(test), dtype=np.int32) \n",
    "\n",
    "    return ref\n",
    "\n",
    "def majority_overlap_sys(systems, analysis_type, corpus, filter_semtype, semtype = None):\n",
    "    \n",
    "    d = {}\n",
    "    cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "    sys_test = []\n",
    "    \n",
    "    for system in systems:\n",
    "        sys_ann = get_sys_data(system, analysis_type, corpus, filter_semtype, semtype)\n",
    "        df = sys_ann.copy()\n",
    "        df['label'] = 'concept'\n",
    "        df = df.rename(index=str, columns={\"note_id\": \"case\"})\n",
    "        sys = df[df['system']==system][cols_to_keep].copy()\n",
    "        test = vectorized_annotations(sys)\n",
    "        d[system] = flatten_list(test) \n",
    "        sys_test.append(d[system])\n",
    "\n",
    "    output = sum(np.array(sys_test))\n",
    "    \n",
    "    n = int(len(systems) / 2)\n",
    "    #print(n)\n",
    "    if ((len(systems) % 2) != 0):\n",
    "        vote = np.where(output > n, 1, 0)\n",
    "    else:\n",
    "        vote = np.where(output > n, 1, \n",
    "         (np.where(output == n, random.randint(0, 1), 0)))\n",
    "        \n",
    "    return vote\n",
    "\n",
    "def majority_overlap_vote_out(ref, vote, corpus):    \n",
    "    TP, TN, FP, FN = confused(ref, vote)\n",
    "    print(TP, TN, FP, FN)\n",
    "    system_n = TP + FP\n",
    "    reference_n = TP + FN\n",
    "\n",
    "    d = cm_dict(FN, FP, TP, system_n, reference_n)\n",
    "\n",
    "    d['TN'] = TN\n",
    "    d['corpus'] = corpus\n",
    "    print(d)\n",
    "    \n",
    "    metrics = pd.DataFrame(d, index=[0])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# control vote run\n",
    "def majority_vote(systems, analysis_type, corpus, run_type, filter_semtype, semtypes = None):\n",
    "    print(semtypes, systems)\n",
    "    if filter_semtype:\n",
    "        for semtype in semtypes:\n",
    "            test = get_valid_systems(systems, semtype)\n",
    "            print('SYSYEMS FOR SEMTYPE', semtype, 'ARE', test)\n",
    "            \n",
    "            if run_type == 'overlap':\n",
    "                ref = get_reference_vector(analysis_type, corpus, filter_semtype, semtype)\n",
    "                vote = majority_overlap_sys(test, analysis_type, corpus, filter_semtype, semtype)\n",
    "                metrics = majority_overlap_vote_out(ref, vote, corpus)\n",
    "                #generate_ensemble_metrics(metrics, analysis_type, corpus, ensemble_type, filter_semtype, semtype)\n",
    "            elif run_type == 'exact':\n",
    "                sys = majority_exact_sys(test, analysis_type, corpus, filter_semtype, semtype)\n",
    "                d = majority_exact_vote_out(sys, filter_semtype, semtype)\n",
    "                metrics = pd.DataFrame(d, index=[0])\n",
    "            elif run_type == 'cui':\n",
    "                sys = majority_cui_sys(test, analysis_type, corpus, filter_semtype, semtype)\n",
    "                d = majority_cui_vote_out(sys, filter_semtype, semtype)\n",
    "                metrics = pd.DataFrame(d, index=[0])\n",
    "           \n",
    "            metrics['systems'] = ','.join(test)\n",
    "            generate_ensemble_metrics(metrics, analysis_type, corpus, ensemble_type, filter_semtype, semtype)\n",
    "                \n",
    "    else:\n",
    "        if run_type == 'overlap':\n",
    "            ref = get_reference_vector(analysis_type, corpus, filter_semtype)\n",
    "            vote = majority_overlap_sys(systems, analysis_type, corpus, filter_semtype)\n",
    "            metrics = majority_overlap_vote_out(ref, vote, corpus)\n",
    "            \n",
    "        elif run_type == 'exact':\n",
    "            sys = majority_exact_sys(systems, analysis_type, corpus, filter_semtype)\n",
    "            d = majority_exact_vote_out(sys, filter_semtype)\n",
    "            metrics = pd.DataFrame(d, index=[0])\n",
    "            \n",
    "        elif run_type == 'cui':\n",
    "            sys = majority_cui_sys(systems, analysis_type, corpus, filter_semtype)\n",
    "            d = majority_cui_vote_out(sys, filter_semtype)\n",
    "            metrics = pd.DataFrame(d, index=[0])\n",
    "            \n",
    "        metrics['systems'] = ','.join(systems)\n",
    "        generate_ensemble_metrics(metrics, analysis_type, corpus, ensemble_type, filter_semtype)\n",
    "    \n",
    "    print(metrics)\n",
    "    \n",
    "def majority_cui_sys(systems, analysis_type, corpus, filter_semtype, semtype = None):\n",
    "   \n",
    "    cols_to_keep = ['cui', 'note_id', 'system']\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for system in systems:\n",
    "        if filter_semtype:\n",
    "            sys = get_sys_data(system, analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "            sys = get_sys_data(system, analysis_type, corpus, filter_semtype)\n",
    "            \n",
    "        sys = sys[sys['system'] == system][cols_to_keep].drop_duplicates()\n",
    "        \n",
    "        frames = [df, sys]\n",
    "        df = pd.concat(frames)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def majority_cui_vote_out(sys, filter_semtype, semtype = None):\n",
    "    \n",
    "    sys = sys.astype(str)\n",
    "    sys['value_cui'] = list(zip(sys.cui, sys.note_id.astype(str)))\n",
    "    sys['count'] = sys.groupby(['value_cui'])['value_cui'].transform('count')\n",
    "\n",
    "    n = int(len(systems) / 2)\n",
    "    if ((len(systems) % 2) != 0):\n",
    "        sys = sys[sys['count'] > n]\n",
    "    else:\n",
    "        # https://stackoverflow.com/questions/23330654/update-a-dataframe-in-pandas-while-iterating-row-by-row\n",
    "        for i in sys.index:\n",
    "            if sys.at[i, 'count'] == n:\n",
    "                sys.at[i, 'count'] = random.choice([1,len(systems)])\n",
    "        sys = sys[sys['count'] > n]\n",
    "\n",
    "    sys = sys.drop_duplicates(subset=['value_cui', 'cui', 'note_id'])\n",
    "    ref = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "\n",
    "    c = get_cooccurences(ref, sys, analysis_type, corpus) # get matches, FN, etc.\n",
    "\n",
    "    if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "        # get dictionary of confusion matrix metrics\n",
    "        print(cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n))\n",
    "        return cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n)\n",
    "    \n",
    "\n",
    "def majority_exact_sys(systems, analysis_type, corpus, filter_semtype, semtype = None):\n",
    "   \n",
    "    cols_to_keep = ['begin', 'end', 'note_id', 'system']\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for system in systems:\n",
    "        if filter_semtype:\n",
    "            sys = get_sys_data(system, analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "            sys = get_sys_data(system, analysis_type, corpus, filter_semtype)\n",
    "            \n",
    "        sys = sys[sys['system'] == system][cols_to_keep].drop_duplicates()\n",
    "        \n",
    "        frames = [df, sys]\n",
    "        df = pd.concat(frames)\n",
    "        \n",
    "    return df\n",
    "        \n",
    "def majority_exact_vote_out(sys, filter_semtype, semtype = None):\n",
    "    sys['span'] = list(zip(sys.begin, sys.end, sys.note_id.astype(str)))\n",
    "    sys['count'] = sys.groupby(['span'])['span'].transform('count')\n",
    "\n",
    "    n = int(len(systems) / 2)\n",
    "    if ((len(systems) % 2) != 0):\n",
    "        sys = sys[sys['count'] > n]\n",
    "    else:\n",
    "        # https://stackoverflow.com/questions/23330654/update-a-dataframe-in-pandas-while-iterating-row-by-row\n",
    "        for i in sys.index:\n",
    "            if sys.at[i, 'count'] == n:\n",
    "                sys.at[i, 'count'] = random.choice([1,len(systems)])\n",
    "        sys = sys[sys['count'] > n]\n",
    "\n",
    "    sys = sys.drop_duplicates(subset=['span', 'begin', 'end', 'note_id'])\n",
    "    ref = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "\n",
    "    c = get_cooccurences(ref, sys, analysis_type, corpus) # get matches, FN, etc.\n",
    "\n",
    "    if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "        # get dictionary of confusion matrix metrics\n",
    "        print(cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n))\n",
    "        return cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n)\n",
    "    \n",
    "#ensemble_type = 'vote'        \n",
    "#filter_semtype = False\n",
    "#majority_vote(systems, analysis_type, corpus, run_type, filter_semtype, semtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biomedicus', 'metamap'] ('analytical_mipacq_cui.csv', 'concepts.mipacq_all')\n",
      "Processing sentence:  ( ctakes | biomedicus ) \n",
      "iteration: 0\n",
      "iteration: 5000\n",
      "iteration: 10000\n",
      "iteration: 20000\n",
      "iteration: 35000\n",
      "iteration: 40000\n",
      "       begin  end       cui       note_id  length      span\n",
      "13186     39   52  C0430404  0054074073-0      13  (39, 52)\n",
      "32891     39   52  C0430404  0054074073-0      13  (39, 52)\n",
      "13186     39   52  C0430404  0054074073-0      13  (39, 52)\n",
      "32891     39   52  C0430404  0054074073-0      13  (39, 52)\n",
      "13186     39   52  C0430404  0054074073-0      13  (39, 52)\n",
      "       begin  end       cui          case  length        span\n",
      "13186     39   52  C0430404  0054074073-0      13    (39, 52)\n",
      "32893     57   73  C0012655  0054074073-0      16    (57, 73)\n",
      "32894    156  163  C0030705  0054074073-0       7  (156, 163)\n",
      "9734     179  184  C0701042  0054074073-0       5  (179, 184)\n",
      "17632    240  249  C0020517  0054074073-0       9  (240, 249)\n",
      "(ctakes|biomedicus)\n",
      "        begin   end       cui          case  length          span\n",
      "13186     39    52  C0430404  0054074073-0      13      (39, 52)\n",
      "32893     57    73  C0012655  0054074073-0      16      (57, 73)\n",
      "32894    156   163  C0030705  0054074073-0       7    (156, 163)\n",
      "9734     179   184  C0701042  0054074073-0       5    (179, 184)\n",
      "17632    240   249  C0020517  0054074073-0       9    (240, 249)\n",
      "9735     253   258  C0701042  0054074073-0       5    (253, 258)\n",
      "5914     287   290  C0042029  0054074073-0       3    (287, 290)\n",
      "5915     291   295  C0270724  0054074073-0       4    (291, 295)\n",
      "9736     307   312  C0701042  0054074073-0       5    (307, 312)\n",
      "9737     323   331  C0591750  0054074073-0       8    (323, 331)\n",
      "13188    384   391  C0033707  0054074073-0       7    (384, 391)\n",
      "5916     470   473  C0042029  0054074073-0       3    (470, 473)\n",
      "17633    637   644  C0006147  0054074073-0       7    (637, 644)\n",
      "27367     41    46  C1305231   006154269-0       5      (41, 46)\n",
      "5074      47    57  C0851162   006154269-0      10      (47, 57)\n",
      "27369     67    76  C0589120   006154269-0       9      (67, 76)\n",
      "728       82    85  C0150934   006154269-0       3      (82, 85)\n",
      "9067      92   100  C0058751   006154269-0       8     (92, 100)\n",
      "9068     144   152  C0058751   006154269-0       8    (144, 152)\n",
      "16414    167   173  C0424522   006154269-0       6    (167, 173)\n",
      "16415    243   253  C0013144   006154269-0      10    (243, 253)\n",
      "729      297   300  C0150934   006154269-0       3    (297, 300)\n",
      "5075     347   360  C0030469   006154269-0      13    (347, 360)\n",
      "16416    365   373  C0332148   006154269-0       8    (365, 373)\n",
      "5077     374   383  C1394494   006154269-0       9    (374, 383)\n",
      "27382    388   396  C1457887   006154269-0       8    (388, 396)\n",
      "16417    408   412  C1513302   006154269-0       4    (408, 412)\n",
      "27383    417   427  C0436331   006154269-0      10    (417, 427)\n",
      "16418    438   449  C0009806   006154269-0      11    (438, 449)\n",
      "731      450   456  C0021853   006154269-0       6    (450, 456)\n",
      "...      ...   ...       ...           ...     ...           ...\n",
      "27447   1776  1794  C0681827   006154269-0      18  (1776, 1794)\n",
      "9076    1851  1857  C0023779   006154269-0       6  (1851, 1857)\n",
      "739     1877  1882  C0009368   006154269-0       5  (1877, 1882)\n",
      "12607   1883  1892  C0199230   006154269-0       9  (1883, 1892)\n",
      "9077    1928  1935  C0004057   006154269-0       7  (1928, 1935)\n",
      "27456   2005  2010  C1305231   006154269-0       5  (2005, 2010)\n",
      "5094    2011  2021  C0851162   006154269-0      10  (2011, 2021)\n",
      "16432   2025  2033  C0332148   006154269-0       8  (2025, 2033)\n",
      "5095    2034  2043  C1394494   006154269-0       9  (2034, 2043)\n",
      "5096    2047  2061  C0020676   006154269-0      14  (2047, 2061)\n",
      "741     2065  2076  C0222601   006154269-0      11  (2065, 2076)\n",
      "5099    2084  2101  C0007099   006154269-0      17  (2084, 2101)\n",
      "27465   2102  2128  C0016034   006154269-0      26  (2102, 2128)\n",
      "5103    2132  2142  C0029453   006154269-0      10  (2132, 2142)\n",
      "5104    2153  2165  C0020538   006154269-0      12  (2153, 2165)\n",
      "27474   2169  2179  C0332119   006154269-0      10  (2169, 2179)\n",
      "5105    2180  2197  C0007117   006154269-0      17  (2180, 2197)\n",
      "27477   2201  2212  C0268800   006154269-0      11  (2201, 2212)\n",
      "5108    2225  2236  C0037315   006154269-0      11  (2225, 2236)\n",
      "27484   2241  2255  C4018875   006154269-0      14  (2241, 2255)\n",
      "12608   2411  2423  C0199176   006154269-0      12  (2411, 2423)\n",
      "9078    2424  2432  C0013227   006154269-0       8  (2424, 2432)\n",
      "27485   2469  2473  C1273517   006154269-0       4  (2469, 2473)\n",
      "27486   2493  2500  C0030705   006154269-0       7  (2493, 2500)\n",
      "27487   2502  2503  C0184661   006154269-0       1  (2502, 2503)\n",
      "12609   2529  2537  C0523807   006154269-0       8  (2529, 2537)\n",
      "746     2556  2559  C0150934   006154269-0       3  (2556, 2559)\n",
      "30554     31    41  C0262926   006884697-0      10      (31, 41)\n",
      "1163      52    56  C0007634   006884697-0       4      (52, 56)\n",
      "5553      57    68  C0242379   006884697-0      11      (57, 68)\n",
      "\n",
      "[100 rows x 6 columns]\n",
      " done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "         338371676 function calls (334456578 primitive calls) in 320.340 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "    54019  121.644    0.002  121.644    0.002 {pandas._libs.ops.scalar_compare}\n",
       "54445423/54445421   12.088    0.000   21.291    0.000 {built-in method builtins.isinstance}\n",
       " 26426782    6.726    0.000    9.138    0.000 generic.py:7(_check)\n",
       "  1717116    4.989    0.000    4.989    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
       "41916041/41916038    4.654    0.000    6.074    0.000 {built-in method builtins.getattr}\n",
       "  4515008    4.197    0.000    8.471    0.000 dtypes.py:68(find)\n",
       "  5069526    4.145    0.000    9.821    0.000 common.py:1845(_is_dtype_type)\n",
       "   483302    4.041    0.000   29.547    0.000 algorithms.py:1544(take_nd)\n",
       "  6541871    3.789    0.000   17.683    0.000 base.py:75(is_dtype)\n",
       "  6227993    3.555    0.000    4.069    0.000 {built-in method builtins.hasattr}\n",
       "349709/349695    3.518    0.000   27.554    0.000 base.py:253(__new__)\n",
       "  3866692    3.473    0.000    4.338    0.000 numerictypes.py:293(issubclass_)\n",
       "2064156/1634908    3.307    0.000   10.162    0.000 {built-in method numpy.array}\n",
       "  4435394    3.172    0.000   10.603    0.000 common.py:1702(is_extension_array_dtype)\n",
       "  1933346    2.836    0.000    8.968    0.000 _dtype.py:319(_name_get)\n",
       " 21238563    2.669    0.000    2.669    0.000 {built-in method builtins.issubclass}\n",
       "  1900598    2.592    0.000    6.152    0.000 {method 'format' of 'str' objects}\n",
       "    54020    2.527    0.000    3.025    0.000 concat.py:22(get_mgr_concatenation_plan)\n",
       "   241664    2.450    0.000    6.929    0.000 managers.py:186(_rebuild_blknos_and_blklocs)\n",
       "        1    2.205    2.205  318.639  318.639 <ipython-input-95-1853eff3a0a1>:1(union_vote)\n",
       "12179961/9285666    2.090    0.000    3.092    0.000 {built-in method builtins.len}\n",
       "   483302    1.959    0.000    6.551    0.000 algorithms.py:1417(_get_take_nd_function)\n",
       "   454807    1.827    0.000   67.867    0.000 frame.py:2893(__getitem__)\n",
       "  1074720    1.810    0.000    1.810    0.000 {built-in method numpy.empty}\n",
       "  1370743    1.689    0.000    2.676    0.000 {pandas._libs.lib.is_scalar}\n",
       "   483289    1.644    0.000    7.411    0.000 cast.py:255(maybe_promote)\n",
       "  1933346    1.624    0.000    6.132    0.000 numerictypes.py:365(issubdtype)\n",
       "   349682    1.613    0.000    7.270    0.000 missing.py:183(_isna_ndarraylike)\n",
       "  2007407    1.498    0.000    4.317    0.000 common.py:1981(pandas_dtype)\n",
       "  1262304    1.436    0.000    3.295    0.000 common.py:160(is_sparse)\n",
       "   801546    1.347    0.000    3.218    0.000 numeric.py:2656(seterr)\n",
       "   349682    1.325    0.000    1.325    0.000 {method 'copy' of 'numpy.ndarray' objects}\n",
       "  3456955    1.320    0.000   10.824    0.000 common.py:434(is_datetime64tz_dtype)\n",
       "   565795    1.311    0.000    3.228    0.000 base.py:504(_simple_new)\n",
       "   162045    1.260    0.000    8.298    0.000 base.py:67(cmp_method)\n",
       "   133590    1.260    0.000    7.614    0.000 nanops.py:203(_get_values)\n",
       "   725052    1.245    0.000    2.494    0.000 blocks.py:78(__init__)\n",
       "  2902743    1.221    0.000    1.728    0.000 common.py:1809(_get_dtype)\n",
       "   133604    1.211    0.000    2.232    0.000 indexing.py:2575(maybe_convert_indices)\n",
       "   887154    1.147    0.000    2.002    0.000 generic.py:5069(__setattr__)\n",
       "   801546    1.109    0.000    1.207    0.000 numeric.py:2758(geterr)\n",
       "   912654    1.099    0.000    3.428    0.000 dtypes.py:973(is_dtype)\n",
       "   804572    1.083    0.000    2.771    0.000 managers.py:139(shape)\n",
       "   164784    1.066    0.000    1.066    0.000 {pandas._libs.algos.take_2d_axis1_int64_int64}\n",
       "   375372    1.038    0.000    1.378    0.000 dtypes.py:672(construct_from_string)\n",
       "  3116154    1.034    0.000    3.588    0.000 <frozen importlib._bootstrap>:1009(_handle_fromlist)\n",
       "   133604    1.023    0.000   44.120    0.000 managers.py:1329(take)\n",
       "  2171754    1.021    0.000    1.742    0.000 managers.py:1522(dtype)\n",
       "   164784    0.995    0.000    0.995    0.000 {pandas._libs.algos.take_2d_axis1_object_object}\n",
       "  4398268    0.990    0.000    1.496    0.000 common.py:119(<lambda>)\n",
       "241713/241712    0.984    0.000   13.968    0.000 series.py:152(__init__)\n",
       "    79578    0.954    0.000  142.754    0.002 ops.py:1660(wrapper)\n",
       "   483344    0.953    0.000    3.560    0.000 _dtype.py:46(__str__)\n",
       "  1262399    0.949    0.000    3.479    0.000 common.py:131(is_object_dtype)\n",
       "  1233841    0.924    0.000    4.157    0.000 common.py:403(is_datetime64_dtype)\n",
       "   855801    0.890    0.000    3.059    0.000 dtypes.py:827(is_dtype)\n",
       "   295645    0.853    0.000    5.761    0.000 base.py:786(array)\n",
       "  2681049    0.849    0.000    1.142    0.000 base.py:652(__len__)\n",
       "   725052    0.842    0.000    5.600    0.000 blocks.py:3080(make_block)\n",
       "   295645    0.837    0.000    2.102    0.000 numpy_.py:35(__init__)\n",
       "  4398268    0.836    0.000    0.836    0.000 common.py:117(classes)\n",
       "   267213    0.819    0.000   19.802    0.000 blocks.py:1217(take_nd)\n",
       "   133602    0.816    0.000   49.929    0.000 generic.py:3323(_take)\n",
       "   270205    0.795    0.000    8.832    0.000 base.py:3940(__getitem__)\n",
       "   349665    0.781    0.000   10.134    0.000 numeric.py:34(__new__)\n",
       "       12    0.754    0.063    4.265    0.355 concat.py:261(get_empty_dtype_and_na)\n",
       "724985/616951    0.754    0.000    7.043    0.000 generic.py:5053(__getattr__)\n",
       "   241660    0.749    0.000   16.293    0.000 managers.py:97(__init__)\n",
       "   133590    0.741    0.000   15.618    0.000 series.py:3600(_reduce)\n",
       "  2171754    0.740    0.000    2.482    0.000 series.py:406(dtype)\n",
       "   591493    0.724    0.000    9.694    0.000 missing.py:105(_isna_new)\n",
       "   565961    0.701    0.000    2.950    0.000 common.py:1578(is_bool_dtype)\n",
       "   562905    0.692    0.000    1.469    0.000 base.py:3918(__contains__)\n",
       "   133603    0.687    0.000    0.687    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
       "   241653    0.673    0.000    1.418    0.000 frame.py:378(__init__)\n",
       "   216072    0.650    0.000   14.205    0.000 concat.py:165(get_reindexed_values)\n",
       "   483366    0.643    0.000    0.643    0.000 generic.py:127(__init__)\n",
       "  1666191    0.627    0.000    4.543    0.000 common.py:572(is_categorical_dtype)\n",
       "  1020669    0.622    0.000    2.792    0.000 common.py:472(is_timedelta64_dtype)\n",
       "   162117    0.619    0.000    2.088    0.000 managers.py:963(iget)\n",
       "  1233828    0.615    0.000    4.152    0.000 {method 'any' of 'numpy.ndarray' objects}\n",
       "   483357    0.607    0.000    0.607    0.000 {built-in method numpy.arange}\n",
       "  1802856    0.601    0.000    0.601    0.000 {built-in method _abc._abc_instancecheck}\n",
       "   108041    0.597    0.000   11.241    0.000 managers.py:318(apply)\n",
       "   241635    0.576    0.000   14.579    0.000 base.py:584(_shallow_copy_with_infer)\n",
       "       11    0.565    0.051    4.702    0.427 concat.py:20(get_dtype_kinds)\n",
       "  3138423    0.554    0.000    0.554    0.000 blocks.py:308(dtype)\n",
       "  1074678    0.548    0.000    0.548    0.000 {method 'view' of 'numpy.ndarray' objects}\n",
       "   216096    0.547    0.000    2.524    0.000 base.py:566(_shallow_copy)\n",
       "   349669    0.538    0.000   16.206    0.000 numeric.py:67(_shallow_copy)\n",
       "    54015    0.536    0.000   16.719    0.000 interval.py:434(_validate)\n",
       "   645430    0.531    0.000    3.784    0.000 blocks.py:225(make_block_same_class)\n",
       "1691671/1262426    0.531    0.000   10.038    0.000 numeric.py:469(asarray)\n",
       "  2413716    0.530    0.000    1.688    0.000 managers.py:141(<genexpr>)\n",
       "   133589    0.526    0.000   12.302    0.000 nanops.py:102(f)\n",
       "   295645    0.519    0.000    2.679    0.000 numpy_.py:127(__init__)\n",
       "   725052    0.506    0.000    0.595    0.000 blocks.py:199(mgr_locs)\n",
       "   483344    0.503    0.000    5.798    0.000 blocks.py:312(ftype)\n",
       "   241712    0.498    0.000    3.131    0.000 managers.py:1443(__init__)\n",
       "   995087    0.486    0.000    0.987    0.000 base.py:3608(values)\n",
       "  2680676    0.484    0.000    0.484    0.000 managers.py:1488(_block)\n",
       "   321241    0.479    0.000    6.966    0.000 generic.py:3056(_get_item_cache)\n",
       "   699379    0.478    0.000    0.675    0.000 <frozen importlib._bootstrap>:416(parent)\n",
       "  1262357    0.477    0.000    3.905    0.000 common.py:536(is_interval_dtype)\n",
       "   801546    0.475    0.000    0.475    0.000 {built-in method numpy.seterrobj}\n",
       "   241711    0.475    0.000    2.583    0.000 common.py:93(is_bool_indexer)\n",
       "   585505    0.469    0.000    1.494    0.000 common.py:1198(is_datetime_or_timedelta_dtype)\n",
       "   133598    0.465    0.000   55.560    0.000 frame.py:2952(_getitem_bool_array)\n",
       "   400815    0.463    0.000    2.559    0.000 base.py:4051(equals)\n",
       "   833096    0.459    0.000   14.664    0.000 base.py:5318(ensure_index)\n",
       "  1802856    0.451    0.000    1.052    0.000 abc.py:137(__instancecheck__)\n",
       "   162045    0.451    0.000    1.520    0.000 warnings.py:130(filterwarnings)\n",
       "   807427    0.446    0.000    1.443    0.000 common.py:1784(_is_dtype)\n",
       "   241662    0.444    0.000    6.542    0.000 managers.py:599(_consolidate_check)\n",
       "   133604    0.443    0.000   30.072    0.000 managers.py:1198(reindex_indexer)\n",
       "  1069520    0.439    0.000    1.190    0.000 integer.py:80(construct_from_string)\n",
       "        4    0.423    0.106   30.937    7.734 concat.py:383(get_result)\n",
       "   133614    0.421    0.000    1.452    0.000 managers.py:306(_verify_integrity)\n",
       "   855801    0.416    0.000    3.475    0.000 common.py:503(is_period_dtype)\n",
       "    79610    0.416    0.000    4.823    0.000 construction.py:537(sanitize_array)\n",
       "   588431    0.414    0.000    0.755    0.000 {pandas._libs.lib.values_from_object}\n",
       "  1233828    0.405    0.000    3.537    0.000 _methods.py:42(_any)\n",
       "    76856    0.403    0.000    0.403    0.000 {pandas._libs.algos.take_2d_axis0_object_object}\n",
       "    76856    0.401    0.000    0.401    0.000 {pandas._libs.algos.take_2d_axis0_int64_int64}\n",
       "   671258    0.400    0.000    0.544    0.000 common.py:127(<lambda>)\n",
       "   565804    0.394    0.000    0.633    0.000 __init__.py:221(iteritems)\n",
       "   187614    0.390    0.000    7.294    0.000 series.py:669(__array__)\n",
       "   616886    0.389    0.000    1.145    0.000 common.py:746(is_dtype_equal)\n",
       "   216076    0.388    0.000    3.965    0.000 concat.py:137(is_na)\n",
       "   216072    0.385    0.000    0.771    0.000 concat.py:117(needs_filling)\n",
       "   324195    0.383    0.000    0.383    0.000 {method 'get_loc' of 'pandas._libs.index.IndexEngine' objects}\n",
       "   400773    0.381    0.000    1.867    0.000 numeric.py:3063(__exit__)\n",
       "   108060    0.379    0.000    0.379    0.000 {built-in method pandas._libs.missing.isnaobj}\n",
       "      7/1    0.374    0.053  319.120  319.120 <ipython-input-96-3fde1df72328>:26(evaluate)\n",
       "   241708    0.371    0.000    1.096    0.000 blocks.py:2626(__init__)\n",
       "   133704    0.359    0.000    0.684    0.000 dtypes.py:786(construct_from_string)\n",
       "   216094    0.356    0.000    0.726    0.000 blocks.py:3100(_extend_blocks)\n",
       "   457735    0.354    0.000    0.687    0.000 base.py:547(_get_attributes_dict)\n",
       "   133589    0.352    0.000   16.005    0.000 generic.py:10938(stat_func)\n",
       "  1393427    0.348    0.000    0.348    0.000 {method 'get' of 'dict' objects}\n",
       "   133601    0.347    0.000   20.207    0.000 managers.py:1233(<listcomp>)\n",
       "   162045    0.342    0.000    0.364    0.000 warnings.py:474(__enter__)\n",
       "   162093    0.342    0.000    3.105    0.000 managers.py:934(get)\n",
       "  2175043    0.341    0.000    0.341    0.000 blocks.py:195(mgr_locs)\n",
       "   537364    0.339    0.000    0.577    0.000 series.py:392(name)\n",
       "   400773    0.326    0.000    2.058    0.000 numeric.py:3058(__enter__)\n",
       "   108050    0.326    0.000    0.797    0.000 base.py:1658(is_unique)\n",
       "    79609    0.317    0.000    0.768    0.000 cast.py:832(maybe_castable)\n",
       "   108037    0.315    0.000   15.706    0.000 generic.py:5699(copy)\n",
       "    79637    0.315    0.000    1.858    0.000 blocks.py:3034(get_block_type)\n",
       "        1    0.315    0.315    0.316    0.316 {method 'read' of 'pandas._libs.parsers.TextReader' objects}\n",
       "   400773    0.310    0.000    0.398    0.000 numeric.py:3054(__init__)\n",
       "   133589    0.307    0.000   10.635    0.000 nanops.py:727(reduction)\n",
       "   725052    0.300    0.000    0.300    0.000 blocks.py:89(_check_ndim)\n",
       "    79578    0.295    0.000  123.660    0.002 ops.py:1615(na_op)\n",
       "   133604    0.292    0.000    9.432    0.000 base.py:784(take)\n",
       "   349665    0.291    0.000    6.482    0.000 base.py:3806(_coerce_to_ndarray)\n",
       "  1603092    0.287    0.000    0.287    0.000 {built-in method numpy.geterrobj}\n",
       "   241713    0.286    0.000    0.450    0.000 series.py:354(_set_axis)\n",
       "   429291    0.284    0.000    0.457    0.000 managers.py:1549(internal_values)\n",
       "    54015    0.281    0.000   32.235    0.001 interval.py:163(_simple_new)\n",
       "   267232    0.280    0.000    0.377    0.000 generic.py:349(_get_axis_number)\n",
       "   349789    0.277    0.000    0.919    0.000 common.py:923(is_signed_integer_dtype)\n",
       "   108041    0.276    0.000   14.718    0.000 managers.py:710(copy)\n",
       "   165406    0.268    0.000    0.410    0.000 re.py:271(_compile)\n",
       "    79604    0.264    0.000    1.278    0.000 cast.py:953(maybe_cast_to_datetime)\n",
       "   162093    0.263    0.000    2.875    0.000 frame.py:3342(_box_item_values)\n",
       "   216090    0.255    0.000    4.220    0.000 concat.py:379(<genexpr>)\n",
       "   457735    0.251    0.000    0.333    0.000 base.py:551(<dictcomp>)\n",
       "   318326    0.251    0.000    3.115    0.000 common.py:1431(needs_i8_conversion)\n",
       "   133589    0.251    0.000    0.954    0.000 nanops.py:1052(_maybe_null_out)\n",
       "   619816    0.248    0.000    0.248    0.000 base.py:633(_reset_identity)\n",
       "   241662    0.247    0.000    6.046    0.000 managers.py:600(<listcomp>)\n",
       "   162045    0.247    0.000    0.520    0.000 warnings.py:181(_add_filter)\n",
       "   483328    0.245    0.000    0.245    0.000 {method 'fill' of 'numpy.ndarray' objects}\n",
       "    54015    0.245    0.000   34.037    0.001 interval.py:183(from_arrays)\n",
       "   727865    0.245    0.000    0.245    0.000 {built-in method __new__ of type object at 0x10d7f2778}\n",
       "    54015    0.243    0.000    5.217    0.000 interval.py:1062(overlaps)\n",
       "    79610    0.236    0.000    3.574    0.000 construction.py:684(_try_cast)\n",
       "   162117    0.235    0.000    1.999    0.000 frame.py:3349(_box_col_values)\n",
       "        4    0.235    0.059   30.475    7.619 managers.py:2029(concatenate_block_managers)\n",
       "   216082    0.234    0.000    2.969    0.000 base.py:700(view)\n",
       "   295645    0.232    0.000    0.432    0.000 common.py:1118(is_datetime64_ns_dtype)\n",
       "   241660    0.226    0.000    0.441    0.000 managers.py:98(<listcomp>)\n",
       "   295845    0.226    0.000    0.915    0.000 common.py:1545(is_float_dtype)\n",
       "   133598    0.220    0.000    3.999    0.000 indexing.py:2475(check_bool_indexer)\n",
       "   241650    0.216    0.000    0.254    0.000 generic.py:144(_init_mgr)\n",
       "   349746    0.216    0.000    3.017    0.000 common.py:1078(is_datetime64_any_dtype)\n",
       "   725134    0.216    0.000    0.285    0.000 managers.py:143(ndim)\n",
       "   321344    0.213    0.000    0.302    0.000 generic.py:363(_get_axis_name)\n",
       "   884227    0.208    0.000    0.385    0.000 {built-in method builtins.hash}\n",
       "   591493    0.205    0.000    9.898    0.000 missing.py:25(isna)\n",
       "   133614    0.200    0.000    0.200    0.000 {method 'take' of 'numpy.ndarray' objects}\n",
       "   400815    0.198    0.000    0.266    0.000 base.py:613(is_)\n",
       "   699417    0.197    0.000    0.197    0.000 {method 'rpartition' of 'str' objects}\n",
       "   241721    0.197    0.000    0.331    0.000 series.py:399(name)\n",
       "   162045    0.196    0.000    0.196    0.000 {method 'remove' of 'list' objects}\n",
       "   429291    0.192    0.000    0.649    0.000 series.py:476(_values)\n",
       "   187626    0.189    0.000    0.671    0.000 generic.py:5122(_protect_consolidate)\n",
       "162045/108030    0.188    0.000    4.858    0.000 {built-in method _operator.lt}\n",
       "   216086    0.188    0.000    2.669    0.000 blocks.py:749(copy)\n",
       "   324160    0.187    0.000    0.187    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
       "   324192    0.187    0.000    0.570    0.000 base.py:2650(get_loc)\n",
       "   133730    0.183    0.000    0.183    0.000 {method 'match' of 're.Pattern' objects}\n",
       "   483301    0.182    0.000    0.339    0.000 managers.py:927(_consolidate_inplace)\n",
       "   321340    0.182    0.000    0.583    0.000 generic.py:377(_get_axis)\n",
       "   213313    0.179    0.000    0.525    0.000 common.py:868(is_integer_dtype)\n",
       "   133614    0.179    0.000    0.339    0.000 missing.py:360(array_equivalent)\n",
       "   108039    0.178    0.000    3.528    0.000 missing.py:259(notna)\n",
       "   403722    0.178    0.000    1.199    0.000 common.py:605(is_string_dtype)\n",
       "   162045    0.178    0.000    0.196    0.000 warnings.py:493(__exit__)\n",
       "   670926    0.176    0.000    0.176    0.000 managers.py:591(is_consolidated)\n",
       "    54015    0.172    0.000    0.172    0.000 {built-in method _operator.le}\n",
       "   403722    0.169    0.000    0.621    0.000 common.py:634(condition)\n",
       "   187723    0.167    0.000    0.167    0.000 dtypes.py:452(construct_from_string)\n",
       "       18    0.167    0.009    0.274    0.015 concat.py:425(combine_concat_plans)\n",
       "   267179    0.163    0.000    0.194    0.000 nanops.py:270(_na_ok_dtype)\n",
       "   133704    0.163    0.000    0.257    0.000 dtypes.py:929(construct_from_string)\n",
       "  1077574    0.162    0.000    0.162    0.000 {method 'items' of 'dict' objects}\n",
       "   454866    0.162    0.000    0.223    0.000 common.py:316(apply_if_callable)\n",
       "   241658    0.160    0.000    0.190    0.000 generic.py:5036(__finalize__)\n",
       "   108050    0.158    0.000    0.236    0.000 base.py:643(_engine)\n",
       "   483337    0.157    0.000    0.157    0.000 {pandas._libs.algos.ensure_int64}\n",
       "   400853    0.150    0.000    0.223    0.000 managers.py:308(<genexpr>)\n",
       "   187626    0.149    0.000    0.444    0.000 generic.py:5135(f)\n",
       "   566319    0.149    0.000    0.149    0.000 {built-in method builtins.iter}\n",
       "   133589    0.148    0.000    1.122    0.000 nanops.py:146(_bn_ok_dtype)\n",
       "   187626    0.144    0.000    0.814    0.000 generic.py:5132(_consolidate_inplace)\n",
       "   295645    0.143    0.000    0.449    0.000 common.py:1167(is_timedelta64_ns_dtype)\n",
       "   108037    0.142    0.000    0.451    0.000 base.py:1681(is_object)\n",
       "      114    0.141    0.001    0.141    0.001 {method 'recv_into' of '_socket.socket' objects}\n",
       "   133599    0.141    0.000    0.247    0.000 generic.py:3175(_set_is_copy)\n",
       "       12    0.136    0.011   14.341    1.195 concat.py:240(<listcomp>)\n",
       "   216082    0.135    0.000    3.104    0.000 managers.py:729(<lambda>)\n",
       "   321319    0.133    0.000    0.184    0.000 inference.py:438(is_hashable)\n",
       "   162117    0.129    0.000    0.366    0.000 generic.py:3070(_set_as_cached)\n",
       "   133617    0.129    0.000    0.334    0.000 generic.py:381(_get_block_manager_axis)\n",
       "   241713    0.129    0.000    0.129    0.000 series.py:382(_set_subtyp)\n",
       "   162045    0.126    0.000    0.126    0.000 warnings.py:453(__init__)\n",
       "   108030    0.125    0.000    1.018    0.000 interval.py:1080(maybe_convert_platform_interval)\n",
       "   671258    0.125    0.000    0.125    0.000 common.py:122(classes_and_not_datetimelike)\n",
       "   432169    0.122    0.000    0.122    0.000 base.py:676(dtype)\n",
       "   216072    0.122    0.000    0.943    0.000 concat.py:126(dtype)\n",
       "   216090    0.122    0.000    0.122    0.000 concat.py:376(<genexpr>)\n",
       "   378266    0.121    0.000    0.144    0.000 {built-in method builtins.next}\n",
       "   457749    0.120    0.000    0.120    0.000 {method 'update' of 'dict' objects}\n",
       "   133589    0.120    0.000    0.974    0.000 nanops.py:276(_wrap_results)\n",
       "   187614    0.119    0.000    0.270    0.000 numpy_.py:170(__array__)\n",
       "   133604    0.119    0.000    0.164    0.000 indexing.py:2450(convert_to_index_sliceable)\n",
       "   216077    0.117    0.000    0.857    0.000 {method 'all' of 'numpy.ndarray' objects}\n",
       "   216076    0.117    0.000    0.519    0.000 blocks.py:175(get_values)\n",
       "   537422    0.114    0.000    0.114    0.000 managers.py:206(items)\n",
       "   108036    0.111    0.000    0.111    0.000 {built-in method pandas._libs.lib.is_bool_array}\n",
       "   187639    0.111    0.000    0.333    0.000 {built-in method builtins.sum}\n",
       "   162117    0.111    0.000    0.111    0.000 blocks.py:332(iget)\n",
       "   401180    0.108    0.000    0.108    0.000 {method 'startswith' of 'str' objects}\n",
       "   108031    0.106    0.000    0.286    0.000 cast.py:35(maybe_convert_platform)\n",
       "       39    0.105    0.003    0.105    0.003 {built-in method numpy.concatenate}\n",
       "   645463    0.102    0.000    0.102    0.000 {pandas._libs.lib.is_float}\n",
       "   108036    0.102    0.000    0.264    0.000 blocks.py:2633(is_bool)\n",
       "    79889    0.099    0.000    0.229    0.000 inference.py:253(is_list_like)\n",
       "   108034    0.098    0.000    1.396    0.000 base.py:3975(_can_hold_identifiers_and_holds_name)\n",
       "   108041    0.097    0.000    3.202    0.000 managers.py:730(<listcomp>)\n",
       "   108050    0.097    0.000    0.678    0.000 base.py:646(<lambda>)\n",
       "    54015    0.096    0.000    0.096    0.000 {built-in method _operator.gt}\n",
       "   108061    0.096    0.000    0.581    0.000 base.py:963(_ndarray_values)\n",
       "   165405    0.096    0.000    0.131    0.000 types.py:164(__get__)\n",
       "    79583    0.096    0.000    0.237    0.000 managers.py:1556(get_values)\n",
       "    79581    0.094    0.000    0.201    0.000 ops.py:43(get_op_result_name)\n",
       "   915671    0.094    0.000    0.094    0.000 {method 'append' of 'list' objects}\n",
       "   133690    0.092    0.000    0.168    0.000 sparse.py:196(construct_from_string)\n",
       "   267228    0.091    0.000    0.341    0.000 base.py:3663(get_values)\n",
       "   400876    0.089    0.000    0.089    0.000 {method 'pop' of 'dict' objects}\n",
       "    54015    0.087    0.000   33.339    0.001 interval.py:320(from_arrays)\n",
       "    79583    0.085    0.000    0.177    0.000 generic.py:1814(__hash__)\n",
       "    54015    0.085    0.000    0.101    0.000 contextlib.py:81(__init__)\n",
       "   159244    0.084    0.000    0.664    0.000 common.py:262(is_categorical)\n",
       "   133590    0.084    0.000    0.161    0.000 nanops.py:180(_get_fill_value)\n",
       "   375249    0.084    0.000    0.084    0.000 blocks.py:191(fill_value)\n",
       "   241811    0.083    0.000    0.083    0.000 {built-in method pandas._libs.missing.checknull}\n",
       "   133592    0.082    0.000    0.703    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
       "    54019    0.081    0.000  121.810    0.002 ops.py:1591(_comp_method_OBJECT_ARRAY)\n",
       "   162168    0.076    0.000    0.104    0.000 common.py:144(cast_scalar_indexer)\n",
       "   187626    0.075    0.000    0.147    0.000 managers.py:911(consolidate)\n",
       "   216077    0.075    0.000    0.740    0.000 _methods.py:45(_all)\n",
       "    79603    0.073    0.000    0.825    0.000 common.py:1643(is_extension_type)\n",
       "   162046    0.073    0.000    0.480    0.000 re.py:232(compile)\n",
       "   429309    0.072    0.000    0.072    0.000 blocks.py:165(internal_values)\n",
       "    54015    0.071    0.000    0.122    0.000 contextlib.py:116(__exit__)\n",
       "   133589    0.070    0.000    0.615    0.000 cast.py:170(maybe_upcast_putmask)\n",
       "    79607    0.069    0.000    0.149    0.000 managers.py:291(__len__)\n",
       "   108060    0.067    0.000    0.215    0.000 common.py:1472(is_numeric_dtype)\n",
       "    54015    0.067    0.000    0.106    0.000 interval.py:155(_simple_new)\n",
       "   267241    0.065    0.000    0.065    0.000 blocks.py:304(shape)\n",
       "    79583    0.065    0.000    2.896    0.000 generic.py:178(_validate_dtype)\n",
       "    79597    0.064    0.000    0.164    0.000 cast.py:1218(construct_1d_ndarray_preserving_na)\n",
       "   454874    0.061    0.000    0.061    0.000 {built-in method builtins.callable}\n",
       "   162059    0.061    0.000    0.061    0.000 {method 'insert' of 'list' objects}\n",
       "   216076    0.058    0.000    0.058    0.000 concat.py:104(__init__)\n",
       "   108040    0.056    0.000    0.096    0.000 generic.py:426(_info_axis)\n",
       "   486135    0.056    0.000    0.056    0.000 {built-in method _warnings._filters_mutated}\n",
       "   137050    0.056    0.000    0.056    0.000 {method 'search' of 're.Pattern' objects}\n",
       "    17466    0.056    0.000    0.183    0.000 connections.py:1195(_read_row_from_packet)\n",
       "    54023    0.055    0.000    0.055    0.000 {pandas._libs.internals.get_blkno_placements}\n",
       "        2    0.055    0.027    0.055    0.027 {pandas._libs.writers.write_csv_rows}\n",
       "        4    0.054    0.014    1.995    0.499 concat.py:237(__init__)\n",
       "    79595    0.053    0.000    0.503    0.000 {method 'max' of 'numpy.ndarray' objects}\n",
       "   241645    0.052    0.000    0.052    0.000 frame.py:361(_constructor)\n",
       "    79610    0.051    0.000    0.247    0.000 arrays.py:7(extract_array)\n",
       "   133636    0.051    0.000    0.392    0.000 numeric.py:541(asanyarray)\n",
       "       14    0.050    0.004    0.051    0.004 {method 'factorize' of 'pandas._libs.hashtable.StringHashTable' objects}\n",
       "    54015    0.049    0.000    5.265    0.000 interval.py:1092(overlaps)\n",
       "   133592    0.048    0.000    0.621    0.000 _methods.py:34(_sum)\n",
       "    54015    0.048    0.000    0.079    0.000 contextlib.py:107(__enter__)\n",
       "       70    0.046    0.001    0.046    0.001 {pandas._libs.lib.infer_dtype}\n",
       "    79583    0.046    0.000    0.283    0.000 series.py:490(get_values)\n",
       "    35081    0.046    0.000    0.046    0.000 {method 'settimeout' of '_socket.socket' objects}\n",
       "    54015    0.045    0.000    0.146    0.000 contextlib.py:237(helper)\n",
       "   321215    0.045    0.000    0.045    0.000 base.py:1396(nlevels)\n",
       "   295645    0.044    0.000    0.044    0.000 common.py:1195(<lambda>)\n",
       "   108060    0.043    0.000    0.043    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
       "   267373    0.042    0.000    0.042    0.000 {pandas._libs.lib.is_integer}\n",
       "   216060    0.041    0.000    0.041    0.000 interval.py:861(right)\n",
       "   270091    0.041    0.000    0.102    0.000 concat.py:450(_next_or_none)\n",
       "    79583    0.040    0.000    0.087    0.000 blocks.py:184(to_dense)\n",
       "    86/85    0.039    0.000    4.380    0.052 {built-in method builtins.all}\n",
       "   108060    0.038    0.000    0.153    0.000 common.py:1513(is_string_like_dtype)\n",
       "    54015    0.037    0.000    0.075    0.000 __init__.py:403(_make)\n",
       "    54021    0.037    0.000    0.109    0.000 frame.py:491(shape)\n",
       "    54057    0.037    0.000    0.087    0.000 frame.py:937(__len__)\n",
       "   216060    0.037    0.000    0.037    0.000 interval.py:853(left)\n",
       "   165405    0.035    0.000    0.035    0.000 enum.py:628(value)\n",
       "   216068    0.035    0.000    0.035    0.000 concat.py:120(is_nonempty)\n",
       "        2    0.035    0.018    0.067    0.034 api.py:262(<setcomp>)\n",
       "    54021    0.035    0.000    0.069    0.000 base.py:658(__array__)\n",
       "   133590    0.035    0.000    0.035    0.000 function.py:40(__call__)\n",
       "    54015    0.034    0.000    0.259    0.000 {method 'min' of 'numpy.ndarray' objects}\n",
       "   241694    0.034    0.000    0.034    0.000 numeric.py:113(is_all_dates)\n",
       "    17529    0.034    0.000    0.283    0.000 connections.py:648(_read_packet)\n",
       "    79595    0.034    0.000    0.450    0.000 _methods.py:26(_amax)\n",
       "       81    0.034    0.000    0.034    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
       "        8    0.033    0.004    0.323    0.040 concat.py:151(<listcomp>)\n",
       "        4    0.033    0.008    0.050    0.012 api.py:73(_get_distinct_objs)\n",
       "   108270    0.033    0.000    0.034    0.000 {built-in method builtins.max}\n",
       "    87434    0.032    0.000    0.037    0.000 protocol.py:63(read)\n",
       "    35058    0.032    0.000    0.233    0.000 connections.py:687(_read_bytes)\n",
       "    25568    0.032    0.000    0.580    0.000 common.py:1320(is_datetimelike_v_numeric)\n",
       "    79583    0.030    0.000    0.215    0.000 series.py:591(__len__)\n",
       "   133651    0.030    0.000    0.030    0.000 {pandas._libs.algos.ensure_platform_int}\n",
       "    54023    0.030    0.000    0.049    0.000 managers.py:174(_is_single_block)\n",
       "   378145    0.030    0.000    0.030    0.000 {method 'add' of 'set' objects}\n",
       "    87434    0.029    0.000    0.105    0.000 protocol.py:168(read_length_coded_string)\n",
       "   133717    0.029    0.000    0.029    0.000 {method 'lower' of 'str' objects}\n",
       "        8    0.029    0.004    0.064    0.008 concat.py:126(<listcomp>)\n",
       "       15    0.028    0.002    0.028    0.002 {method 'factorize' of 'pandas._libs.hashtable.Int64HashTable' objects}\n",
       "   216083    0.028    0.000    0.028    0.000 {method 'copy' of 'dict' objects}\n",
       "    54021    0.027    0.000    0.166    0.000 generic.py:5140(_consolidate)\n",
       "        5    0.027    0.005    0.032    0.006 managers.py:1841(_stack_arrays)\n",
       "    54021    0.025    0.000    0.025    0.000 base.py:1237(_get_names)\n",
       "   108030    0.024    0.000    0.024    0.000 interval.py:869(closed)\n",
       "        4    0.023    0.006    3.048    0.762 managers.py:2041(<listcomp>)\n",
       "   108060    0.023    0.000    0.023    0.000 common.py:1542(<lambda>)\n",
       "   108030    0.022    0.000    0.022    0.000 _exceptions.py:4(rewrite_exception)\n",
       "    54015    0.022    0.000    0.225    0.000 _methods.py:30(_amin)\n",
       "   108041    0.022    0.000    0.022    0.000 managers.py:376(<dictcomp>)\n",
       "    87458    0.021    0.000    0.039    0.000 protocol.py:150(read_length_encoded_integer)\n",
       "    79585    0.021    0.000    0.021    0.000 series.py:338(_constructor)\n",
       "        4    0.019    0.005    0.096    0.024 api.py:67(<listcomp>)\n",
       "        3    0.019    0.006    0.019    0.006 {method 'get_labels_groupby' of 'pandas._libs.hashtable.Int64HashTable' objects}\n",
       "   216076    0.018    0.000    0.018    0.000 {method 'values' of 'dict' objects}\n",
       "        4    0.018    0.004    0.018    0.004 concat.py:507(<listcomp>)\n",
       "    87459    0.018    0.000    0.018    0.000 protocol.py:117(read_uint8)\n",
       "        4    0.017    0.004    0.133    0.033 concat.py:309(<listcomp>)\n",
       "       10    0.017    0.002    0.350    0.035 connections.py:1182(_read_rowdata_packet)\n",
       "    54159    0.015    0.000    0.032    0.000 generic.py:450(ndim)\n",
       "    87394    0.015    0.000    0.015    0.000 {method 'decode' of 'bytes' objects}\n",
       "        5    0.015    0.003    0.015    0.003 {pandas._libs.hashtable.duplicated_int64}\n",
       "        8    0.015    0.002    0.029    0.004 blocks.py:3131(_merge_blocks)\n",
       "        2    0.014    0.007    0.127    0.064 api.py:128(_union_indexes)\n",
       "    54112    0.013    0.000    0.013    0.000 base.py:3632(_values)\n",
       "        1    0.013    0.013    0.013    0.013 {method 'factorize' of 'pandas._libs.hashtable.PyObjectHashTable' objects}\n",
       "    35058    0.012    0.000    0.153    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
       "    54023    0.011    0.000    0.014    0.000 managers.py:706(nblocks)\n",
       "        4    0.010    0.003    0.010    0.003 managers.py:2004(<listcomp>)\n",
       "    54028    0.010    0.000    0.014    0.000 _validators.py:221(validate_bool_kwarg)\n",
       "    54039    0.009    0.000    0.009    0.000 base.py:704(ndim)\n",
       "        3    0.009    0.003    1.301    0.434 base.py:3988(append)\n",
       "        3    0.009    0.003    0.009    0.003 base.py:4012(<setcomp>)\n",
       "        2    0.009    0.004    0.022    0.011 concat.py:483(<listcomp>)\n",
       "        4    0.008    0.002   32.939    8.235 concat.py:24(concat)\n",
       "    54017    0.008    0.000    0.008    0.000 common.py:183(_any_not_none)\n",
       "   108042    0.008    0.000    0.008    0.000 {built-in method builtins.id}\n",
       "        7    0.006    0.001    0.029    0.004 sorting.py:20(get_group_index)\n",
       "    54025    0.006    0.000    0.006    0.000 common.py:164(<genexpr>)\n",
       "        2    0.006    0.003    0.101    0.051 <ipython-input-93-ca3a4ead7b00>:27(get_sys_data)\n",
       "        2    0.006    0.003    0.006    0.003 api.py:226(<setcomp>)\n",
       "       21    0.005    0.000    0.005    0.000 cast.py:1193(construct_1d_object_array_from_listlike)\n",
       "        1    0.005    0.005    1.054    1.054 <ipython-input-90-ae69ae0eb560>:1(get_metric_data)\n",
       "       11    0.005    0.000    0.005    0.000 {pandas._libs.lib.maybe_convert_objects}\n",
       "        1    0.005    0.005    0.005    0.005 __init__.py:131(lmap)\n",
       "    17476    0.005    0.000    0.008    0.000 connections.py:1137(_check_packet_is_eof)\n",
       "    17534    0.005    0.000    0.005    0.000 {built-in method _struct.unpack}\n",
       "    17530    0.004    0.000    0.008    0.000 protocol.py:214(check_error)\n",
       "       19    0.004    0.000    0.004    0.000 {method 'argsort' of 'numpy.ndarray' objects}\n",
       "        1    0.004    0.004    0.374    0.374 parsers.py:403(_read)\n",
       "    54051    0.004    0.000    0.004    0.000 {pandas._libs.lib.is_bool}\n",
       "        5    0.004    0.001    0.222    0.044 frame.py:4605(drop_duplicates)\n",
       "    17530    0.004    0.000    0.004    0.000 protocol.py:211(is_error_packet)\n",
       "    17529    0.004    0.000    0.004    0.000 protocol.py:56(__init__)\n",
       "       12    0.004    0.000   22.535    1.878 concat.py:230(concatenate_join_units)\n",
       "       31    0.004    0.000    0.004    0.000 sorting.py:55(maybe_lift)\n",
       "    17496    0.003    0.000    0.003    0.000 protocol.py:190(is_eof_packet)\n",
       "      141    0.003    0.000    0.003    0.000 {built-in method builtins.min}\n",
       "        5    0.002    0.000    0.002    0.000 {pandas._libs.algos.take_2d_axis0_float64_float64}\n",
       "        1    0.002    0.002    0.002    0.002 parsers.py:1830(__init__)\n",
       "        3    0.002    0.001    1.303    0.434 concat.py:531(_concat_indexes)\n",
       "        1    0.002    0.002    0.002    0.002 {built-in method posix.listdir}\n",
       "       31    0.002    0.000    0.131    0.004 algorithms.py:559(factorize)\n",
       "        1    0.002    0.002    0.518    0.518 sql.py:317(read_sql)\n",
       "        1    0.002    0.002    0.002    0.002 {method 'factorize' of 'pandas._libs.hashtable.Float64HashTable' objects}\n",
       "        1    0.001    0.001    0.001    0.001 {pandas._libs.lib.to_object_array_tuples}\n",
       "        5    0.001    0.000    0.001    0.000 result.py:1192(<listcomp>)\n",
       "        5    0.001    0.000    0.174    0.035 frame.py:4639(duplicated)\n",
       "       70    0.001    0.000    0.007    0.000 config.py:543(<listcomp>)\n",
       "       65    0.001    0.000    0.001    0.000 {built-in method builtins.any}\n",
       "     3360    0.001    0.000    0.006    0.000 re.py:180(search)\n",
       "        1    0.001    0.001  320.340  320.340 <string>:1(<module>)\n",
       "        1    0.001    0.001    0.001    0.001 {built-in method _socket.getaddrinfo}\n",
       "        1    0.001    0.001    0.017    0.017 frame.py:1430(from_records)\n",
       "     3237    0.001    0.000    0.001    0.000 format.py:301(len)\n",
       "  433/293    0.001    0.000    0.011    0.000 printing.py:156(pprint_thing)\n",
       "        4    0.001    0.000    0.178    0.045 api.py:87(_get_combined_index)\n",
       "       31    0.001    0.000    0.119    0.004 algorithms.py:434(_factorize_array)\n",
       "        2    0.001    0.000    0.001    0.000 {method 'put' of 'numpy.ndarray' objects}\n",
       "       10    0.001    0.000    0.001    0.000 {method 'clear' of 'dict' objects}\n",
       "        1    0.001    0.001    0.001    0.001 {method 'permutation' of 'mtrand.RandomState' objects}\n",
       "        2    0.001    0.000    0.047    0.024 concat.py:481(_concat_index_same_dtype)\n",
       "       49    0.001    0.000    0.001    0.000 socket.py:337(send)\n",
       "        4    0.001    0.000    0.274    0.069 api.py:44(_get_objs_combined_axis)\n",
       "       14    0.001    0.000    0.001    0.000 {pandas._libs.algos.take_1d_int64_int64}\n",
       "        1    0.001    0.001  320.339  320.339 <ipython-input-102-3812c86cd0f5>:2(main)\n",
       "      363    0.001    0.000    0.001    0.000 printing.py:185(as_escaped_unicode)\n",
       "        4    0.000    0.000    1.595    0.399 concat.py:440(_get_new_axes)\n",
       "        1    0.000    0.000    0.018    0.018 sql.py:136(_wrap_result)\n",
       "     3237    0.000    0.000    0.001    0.000 __init__.py:291(strlen)\n",
       "        9    0.000    0.000    0.000    0.000 numeric.py:2551(array_equal)\n",
       "        4    0.000    0.000    0.275    0.069 concat.py:464(_get_comb_axis)\n",
       "      332    0.000    0.000    0.008    0.000 config.py:78(_get_single_key)\n",
       "        2    0.000    0.000    0.000    0.000 {pandas._libs.algos.groupsort_indexer}\n",
       "        5    0.000    0.000    0.000    0.000 {built-in method _operator.inv}\n",
       "        9    0.000    0.000    0.012    0.001 format.py:931(_format_strings)\n",
       "       76    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
       "       70    0.000    0.000    0.009    0.000 printing.py:95(_pprint_seq)\n",
       "        2    0.000    0.000    0.094    0.047 csvs.py:290(_save_chunk)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'connect' of '_socket.socket' objects}\n",
       "      436    0.000    0.000    0.000    0.000 inference.py:470(is_sequence)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'writelines' of '_io._IOBase' objects}\n",
       "       20    0.000    0.000    0.000    0.000 {method 'sendall' of '_socket.socket' objects}\n",
       "      210    0.000    0.000    0.011    0.000 format.py:945(_format)\n",
       "      326    0.000    0.000    0.009    0.000 config.py:96(_get_option)\n",
       "      658    0.000    0.000    0.000    0.000 config.py:561(_get_deprecated_option)\n",
       "        1    0.000    0.000    1.074    1.074 <ipython-input-97-c9fd36f58387>:106(get_ref_ann)\n",
       "      915    0.000    0.000    0.001    0.000 format.py:1401(just)\n",
       "        1    0.000    0.000  319.121  319.121 <ipython-input-96-3fde1df72328>:1(process_sentence)\n",
       "      326    0.000    0.000    0.000    0.000 config.py:546(_get_root)\n",
       "        9    0.000    0.000    0.000    0.000 {pandas._libs.lib.map_infer}\n",
       "      955    0.000    0.000    0.001    0.000 format.py:1392(<genexpr>)\n",
       "       14    0.000    0.000    0.000    0.000 {built-in method posix.stat}\n",
       "       26    0.000    0.000    0.002    0.000 indexing.py:960(_getitem_lowerdim)\n",
       "      2/1    0.000    0.000  320.340  320.340 {built-in method builtins.exec}\n",
       "       62    0.000    0.000    0.003    0.000 algorithms.py:38(_ensure_data)\n",
       "      114    0.000    0.000    0.141    0.001 socket.py:575(readinto)\n",
       "      332    0.000    0.000    0.007    0.000 config.py:528(_select_options)\n",
       "       31    0.000    0.000    0.005    0.000 algorithms.py:132(_reconstruct_data)\n",
       "        3    0.000    0.000    0.021    0.007 format.py:503(_to_str_columns)\n",
       "        1    0.000    0.000    0.000    0.000 necompiler.py:765(evaluate)\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method numpy.where}\n",
       "     1105    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'close' of '_io.TextIOWrapper' objects}\n",
       "       70    0.000    0.000    0.001    0.000 printing.py:114(<listcomp>)\n",
       "       30    0.000    0.000    0.001    0.000 base.py:1117(__iter__)\n",
       "       40    0.000    0.000    0.001    0.000 format.py:1407(<listcomp>)\n",
       "        6    0.000    0.000    0.001    0.000 {pandas._libs.lib.clean_index_list}\n",
       "       40    0.000    0.000    0.002    0.000 format.py:1384(_make_fixed_width)\n",
       "       40    0.000    0.000    0.001    0.000 iostream.py:382(write)\n",
       "       49    0.000    0.000    0.001    0.000 iostream.py:195(schedule)\n",
       "        1    0.000    0.000    0.094    0.094 csvs.py:272(_save)\n",
       "       14    0.000    0.000    0.002    0.000 <frozen importlib._bootstrap_external>:1356(find_spec)\n",
       "       31    0.000    0.000    0.131    0.004 _decorators.py:146(wrapper)\n",
       "       19    0.000    0.000    0.015    0.001 format.py:848(format_array)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method io.open}\n",
       "      326    0.000    0.000    0.009    0.000 config.py:226(__call__)\n",
       "       60    0.000    0.000    0.000    0.000 printing.py:59(<listcomp>)\n",
       "        8    0.000    0.000    3.925    0.491 concat.py:101(_concat_compat)\n",
       "        4    0.000    0.000    0.000    0.000 {pandas._libs.algos.ensure_int16}\n",
       "        2    0.000    0.000    0.014    0.007 sorting.py:189(lexsort_indexer)\n",
       "      210    0.000    0.000    0.010    0.000 format.py:943(<lambda>)\n",
       "        7    0.000    0.000    0.003    0.000 <frozen importlib._bootstrap>:882(_find_spec)\n",
       "       27    0.000    0.000    0.125    0.005 frame.py:4666(f)\n",
       "        4    0.000    0.000    0.021    0.005 managers.py:159(rename_axis)\n",
       "       20    0.000    0.000    0.000    0.000 numeric.py:676(require)\n",
       "      334    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
       "        6    0.000    0.000    0.001    0.000 printing.py:15(adjoin)\n",
       "      438    0.000    0.000    0.000    0.000 format.py:541(<genexpr>)\n",
       "       10    0.000    0.000    0.001    0.000 connections.py:1213(_get_descriptions)\n",
       "       11    0.000    0.000    0.026    0.002 {built-in method builtins.print}\n",
       "        4    0.000    0.000    0.033    0.008 managers.py:1696(form_blocks)\n",
       "       49    0.000    0.000    0.002    0.000 common.py:222(asarray_tuplesafe)\n",
       "     25/7    0.000    0.000    0.003    0.000 <frozen importlib._bootstrap>:978(_find_and_load)\n",
       "       41    0.000    0.000    0.002    0.000 frame.py:742(iteritems)\n",
       "      864    0.000    0.000    0.000    0.000 {method 'rjust' of 'str' objects}\n",
       "       76    0.000    0.000    0.000    0.000 range.py:510(__len__)\n",
       "        2    0.000    0.000    0.000    0.000 socket.py:139(__init__)\n",
       "      326    0.000    0.000    0.000    0.000 config.py:602(_warn_if_deprecated)\n",
       "       29    0.000    0.000    0.000    0.000 printing.py:55(<listcomp>)\n",
       "       26    0.000    0.000    0.000    0.000 cast.py:846(maybe_infer_to_datetimelike)\n",
       "       69    0.000    0.000    0.000    0.000 _internal.py:886(npy_ctypes_check)\n",
       "       18    0.000    0.000    0.017    0.001 format.py:702(_format_col)\n",
       "       22    0.000    0.000    0.000    0.000 protocol.py:237(_parse_field_descriptor)\n",
       "       30    0.000    0.000    0.006    0.000 indexing.py:1485(__getitem__)\n",
       "        2    0.000    0.000    0.003    0.002 sorting.py:387(_reorder_by_uniques)\n",
       "        3    0.000    0.000    0.002    0.001 managers.py:1241(_slice_take_blocks_ax0)\n",
       "       49    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
       "      664    0.000    0.000    0.000    0.000 {method 'ljust' of 'str' objects}\n",
       "      332    0.000    0.000    0.000    0.000 config.py:589(_translate_key)\n",
       "       18    0.000    0.000    0.000    0.000 connections.py:744(_execute_command)\n",
       "        3    0.000    0.000    0.023    0.008 sorting.py:366(compress_group_index)\n",
       "       24    0.000    0.000    0.001    0.000 frame.py:2829(_ixs)\n",
       "       10    0.000    0.000    0.351    0.035 connections.py:1149(_read_result_packet)\n",
       "       14    0.000    0.000    4.380    0.313 concat.py:367(is_uniform_join_units)\n",
       "        8    0.000    0.000    0.000    0.000 common.py:120(_stringify_path)\n",
       "        2    0.000    0.000    0.027    0.014 generic.py:960(rename)\n",
       "       39    0.000    0.000    0.000    0.000 format.py:356(_get_formatter)\n",
       "       10    0.000    0.000    0.000    0.000 format.py:1140(<listcomp>)\n",
       "        8    0.000    0.000    0.000    0.000 {method 'get_indexer' of 'pandas._libs.index.IndexEngine' objects}\n",
       "       31    0.000    0.000    0.025    0.001 algorithms.py:217(_get_data_algo)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:316(namedtuple)\n",
       "      301    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}\n",
       "      113    0.000    0.000    0.000    0.000 inference.py:304(is_array_like)\n",
       "       49    0.000    0.000    0.000    0.000 threading.py:1080(is_alive)\n",
       "      114    0.000    0.000    0.000    0.000 {method '_checkReadable' of '_io._IOBase' objects}\n",
       "       96    0.000    0.000    0.000    0.000 common.py:980(is_unsigned_integer_dtype)\n",
       "       25    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:157(_get_module_lock)\n",
       "       20    0.000    0.000    0.000    0.000 numerictypes.py:578(_can_coerce_all)\n",
       "       32    0.000    0.000    0.002    0.000 frame.py:4685(<genexpr>)\n",
       "        2    0.000    0.000    0.029    0.014 frame.py:4695(sort_values)\n",
       "       39    0.000    0.000    0.000    0.000 protocol.py:180(read_struct)\n",
       "        3    0.000    0.000    0.000    0.000 index_tricks.py:316(__getitem__)\n",
       "        8    0.000    0.000    0.001    0.000 base.py:2715(get_indexer)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:9415(abs)\n",
       "       30    0.000    0.000    0.004    0.000 indexing.py:2205(_getitem_axis)\n",
       "     25/7    0.000    0.000    0.003    0.000 <frozen importlib._bootstrap>:948(_find_and_load_unlocked)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'close' of 'pandas._libs.parsers.TextReader' objects}\n",
       "        4    0.000    0.000    0.037    0.009 blocks.py:730(to_native_types)\n",
       "        5    0.000    0.000    0.001    0.000 base.py:2445(difference)\n",
       "       30    0.000    0.000    0.000    0.000 blocks.py:128(_consolidate_key)\n",
       "      115    0.000    0.000    0.000    0.000 socket.py:614(readable)\n",
       "      215    0.000    0.000    0.000    0.000 format.py:1139(<lambda>)\n",
       "        4    0.000    0.000    0.008    0.002 categorical.py:317(__init__)\n",
       "       52    0.000    0.000    0.000    0.000 indexing.py:2056(_validate_key)\n",
       "        3    0.000    0.000    0.001    0.000 managers.py:1134(insert)\n",
       "       89    0.000    0.000    0.000    0.000 printing.py:50(justify)\n",
       "       26    0.000    0.000    0.000    0.000 indexing.py:2089(_is_scalar_access)\n",
       "        4    0.000    0.000    0.029    0.007 managers.py:1887(_consolidate)\n",
       "       26    0.000    0.000    0.001    0.000 indexing.py:217(_has_valid_tuple)\n",
       "       12    0.000    0.000    0.000    0.000 blocks.py:284(getitem_block)\n",
       "        9    0.000    0.000    0.000    0.000 fromnumeric.py:69(_wrapreduction)\n",
       "        7    0.000    0.000    0.003    0.000 <frozen importlib._bootstrap_external>:1240(_get_spec)\n",
       "       30    0.000    0.000    0.000    0.000 common.py:702(is_datetimelike)\n",
       "        4    0.000    0.000    0.000    0.000 concat.py:84(_get_frame_result_type)\n",
       "        2    0.000    0.000    0.000    0.000 {function socket.close at 0x10e5f7510}\n",
       "       40    0.000    0.000    0.000    0.000 iostream.py:320(_schedule_flush)\n",
       "        6    0.000    0.000    0.485    0.081 base.py:1163(_execute_context)\n",
       "        2    0.000    0.000    0.000    0.000 arraysetops.py:484(in1d)\n",
       "        7    0.000    0.000    0.000    0.000 protocol.py:283(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method numpy.putmask}\n",
       "        6    0.000    0.000    0.001    0.000 base.py:998(_format_with_header)\n",
       "       19    0.000    0.000    0.014    0.001 format.py:927(get_result)\n",
       "        1    0.000    0.000    0.007    0.007 connections.py:564(connect)\n",
       "       11    0.000    0.000    0.486    0.044 connections.py:1073(read)\n",
       "       32    0.000    0.000    0.000    0.000 langhelpers.py:852(__get__)\n",
       "        5    0.000    0.000    0.000    0.000 base.py:1010(<listcomp>)\n",
       "        3    0.000    0.000    0.025    0.008 frame.py:614(__unicode__)\n",
       "        1    0.000    0.000    0.097    0.097 generic.py:2882(to_csv)\n",
       "       22    0.000    0.000    0.000    0.000 protocol.py:233(__init__)\n",
       "     14/4    0.000    0.000    0.000    0.000 langhelpers.py:273(get_cls_kwargs)\n",
       "       25    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:78(acquire)\n",
       "       22    0.000    0.000    0.000    0.000 format.py:338(_get_adjustment)\n",
       "       76    0.000    0.000    0.000    0.000 indexing.py:2689(is_list_like_indexer)\n",
       "        7    0.000    0.000    0.000    0.000 blocks.py:3145(<listcomp>)\n",
       "       49    0.000    0.000    0.000    0.000 threading.py:1038(_wait_for_tstate_lock)\n",
       "       40    0.000    0.000    0.000    0.000 iostream.py:307(_is_master_process)\n",
       "       26    0.000    0.000    0.003    0.000 indexing.py:2141(_getitem_tuple)\n",
       "        8    0.000    0.000    0.001    0.000 dtypes.py:485(validate_categories)\n",
       "       48    0.000    0.000    0.000    0.000 indexing.py:2116(_validate_integer)\n",
       "        4    0.000    0.000    0.014    0.003 managers.py:1988(_transform_index)\n",
       "       89    0.000    0.000    0.000    0.000 format.py:304(justify)\n",
       "        3    0.000    0.000    0.003    0.001 format.py:381(__init__)\n",
       "       20    0.000    0.000    0.000    0.000 numeric.py:748(<setcomp>)\n",
       "        5    0.000    0.000    0.001    0.000 generic.py:3155(_slice)\n",
       "       26    0.000    0.000    0.000    0.000 indexing.py:229(_is_nested_tuple_indexer)\n",
       "       78    0.000    0.000    0.000    0.000 indexing.py:1487(<genexpr>)\n",
       "        3    0.000    0.000    0.022    0.007 format.py:582(to_string)\n",
       "        3    0.000    0.000    0.001    0.000 format.py:739(_get_formatted_column_labels)\n",
       "       70    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:56(_path_join)\n",
       "        4    0.000    0.000    0.000    0.000 generic.py:1546(_is_label_reference)\n",
       "        4    0.000    0.000    0.001    0.000 generic.py:1657(_get_label_or_level_values)\n",
       "        5    0.000    0.000    0.001    0.000 managers.py:684(get_slice)\n",
       "       11    0.000    0.000    0.486    0.044 connections.py:720(_read_query_result)\n",
       "       15    0.000    0.000    0.000    0.000 {pandas._libs.lib.infer_datetimelike_array}\n",
       "        3    0.000    0.000    0.002    0.001 indexing.py:1271(_convert_to_indexer)\n",
       "        3    0.000    0.000    0.003    0.001 format.py:431(_chk_truncate)\n",
       "        3    0.000    0.000    0.001    0.000 format.py:642(_join_multiline)\n",
       "     20/3    0.000    0.000    0.001    0.000 visitors.py:85(_compiler_dispatch)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:1761(_label_select_column)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method posix.getcwd}\n",
       "       10    0.000    0.000    0.000    0.000 numerictypes.py:602(find_common_type)\n",
       "        2    0.000    0.000    0.005    0.003 sorting.py:177(indexer_from_factorized)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'getvalue' of '_io.StringIO' objects}\n",
       "       25    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:103(release)\n",
       "       17    0.000    0.000    0.000    0.000 shape_base.py:83(atleast_2d)\n",
       "        8    0.000    0.000    0.000    0.000 sorting.py:47(_int64_cut_off)\n",
       "        5    0.000    0.000    0.001    0.000 generic.py:1439(__neg__)\n",
       "       11    0.000    0.000    0.487    0.044 connections.py:508(query)\n",
       "       25    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:58(__init__)\n",
       "       10    0.000    0.000    0.000    0.000 numerictypes.py:654(<listcomp>)\n",
       "        5    0.000    0.000    0.000    0.000 base.py:2897(_convert_slice_indexer)\n",
       "        3    0.000    0.000    0.005    0.002 frame.py:3565(_sanitize_column)\n",
       "        3    0.000    0.000    0.002    0.001 indexing.py:1108(_get_listlike_indexer)\n",
       "       30    0.000    0.000    0.000    0.000 managers.py:1546(external_values)\n",
       "        7    0.000    0.000    0.002    0.000 connections.py:393(_read_ok_packet)\n",
       "        1    0.000    0.000    0.001    0.001 connections.py:786(_request_authentication)\n",
       "       70    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:58(<listcomp>)\n",
       "       21    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:873(_find_spec_legacy)\n",
       "      150    0.000    0.000    0.000    0.000 printing.py:62(_join_unicode)\n",
       "        2    0.000    0.000    0.000    0.000 sorting.py:339(get_group_index_sorter)\n",
       "        3    0.000    0.000    0.001    0.000 format.py:801(_get_formatted_index)\n",
       "       20    0.000    0.000    0.000    0.000 connections.py:710(_write_bytes)\n",
       "      140    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}\n",
       "        3    0.000    0.000    0.001    0.000 managers.py:1019(set)\n",
       "        4    0.000    0.000    1.320    0.330 concat.py:475(_get_concat_axis)\n",
       "        7    0.000    0.000    0.001    0.000 fromnumeric.py:54(_wrapfunc)\n",
       "        9    0.000    0.000    0.000    0.000 cursors.py:116(_escape_args)\n",
       "        5    0.000    0.000    0.000    0.000 result.py:215(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 managers.py:2008(_fast_count_smallints)\n",
       "        3    0.000    0.000    0.001    0.000 compiler.py:1999(visit_select)\n",
       "      114    0.000    0.000    0.000    0.000 {method '_checkClosed' of '_io._IOBase' objects}\n",
       "       19    0.000    0.000    0.000    0.000 format.py:912(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:371(_force_close)\n",
       "       11    0.000    0.000    0.487    0.044 cursors.py:324(_query)\n",
       "       10    0.000    0.000    0.000    0.000 protocol.py:308(__init__)\n",
       "       17    0.000    0.000    0.000    0.000 type_api.py:483(_cached_result_processor)\n",
       "       70    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:222(_verbose_message)\n",
       "        1    0.000    0.000    0.000    0.000 socket.py:221(makefile)\n",
       "       49    0.000    0.000    0.000    0.000 iostream.py:93(_event_pipe)\n",
       "        9    0.000    0.000    0.000    0.000 fromnumeric.py:2664(prod)\n",
       "        3    0.000    0.000    0.025    0.008 frame.py:678(to_string)\n",
       "        6    0.000    0.000    0.000    0.000 range.py:136(_simple_new)\n",
       "        3    0.000    0.000    0.000    0.000 indexing.py:1209(_validate_read_indexer)\n",
       "       24    0.000    0.000    0.000    0.000 concat.py:391(<genexpr>)\n",
       "       44    0.000    0.000    0.000    0.000 protocol.py:264(get_column_length)\n",
       "       10    0.000    0.000    0.000    0.000 elements.py:717(__getattr__)\n",
       "       25    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:176(cb)\n",
       "        5    0.000    0.000    0.000    0.000 range.py:272(_shallow_copy)\n",
       "       15    0.000    0.000    0.000    0.000 managers.py:1844(_asarray_compat)\n",
       "        3    0.000    0.000    0.052    0.017 construction.py:170(init_dict)\n",
       "        4    0.000    0.000    0.021    0.005 construction.py:254(_homogenize)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:69(__init__)\n",
       "       16    0.000    0.000    0.000    0.000 apipkg.py:133(__makeattr)\n",
       "        7    0.000    0.000    0.013    0.002 shape_base.py:229(vstack)\n",
       "        3    0.000    0.000    0.000    0.000 console.py:47(get_console_size)\n",
       "       28    0.000    0.000    0.000    0.000 inference.py:121(is_iterator)\n",
       "        6    0.000    0.000    0.001    0.000 format.py:307(adjoin)\n",
       "        6    0.000    0.000    0.001    0.000 series.py:730(__array_wrap__)\n",
       "     19/7    0.000    0.000    0.001    0.000 {built-in method builtins.__import__}\n",
       "       51    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n",
       "       31    0.000    0.000    0.000    0.000 algorithms.py:163(_ensure_arraylike)\n",
       "        3    0.000    0.000    0.001    0.000 base.py:4919(insert)\n",
       "        3    0.000    0.000    0.001    0.000 generic.py:5457(dtypes)\n",
       "       24    0.000    0.000    0.001    0.000 indexing.py:143(_get_loc)\n",
       "        3    0.000    0.000    0.000    0.000 format.py:648(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 format.py:1553(_binify)\n",
       "        1    0.000    0.000    0.007    0.007 connections.py:183(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:1343(_handle_dbapi_exception)\n",
       "       30    0.000    0.000    0.000    0.000 weakref.py:395(__getitem__)\n",
       "        7    0.000    0.000    0.000    0.000 {built-in method pandas._libs.lib.is_datetime_array}\n",
       "       12    0.000    0.000    0.001    0.000 dtypes.py:328(_finalize)\n",
       "        7    0.000    0.000    0.000    0.000 base.py:1736(is_all_dates)\n",
       "       52    0.000    0.000    0.000    0.000 indexing.py:2695(is_label_like)\n",
       "       30    0.000    0.000    0.000    0.000 managers.py:1893(<lambda>)\n",
       "        3    0.000    0.000    0.000    0.000 format.py:651(<listcomp>)\n",
       "        1    0.000    0.000    0.007    0.007 construction.py:429(_list_to_arrays)\n",
       "       22    0.000    0.000    0.000    0.000 protocol.py:253(description)\n",
       "       13    0.000    0.000    0.000    0.000 deprecations.py:117(warned)\n",
       "        3    0.000    0.000    0.002    0.001 default.py:342(check_unicode)\n",
       "       22    0.000    0.000    0.000    0.000 result.py:461(_colnames_from_description)\n",
       "        6    0.000    0.000    0.000    0.000 default.py:862(_init_statement)\n",
       "        1    0.000    0.000    0.002    0.002 <frozen importlib._bootstrap_external>:1404(_fill_cache)\n",
       "        2    0.000    0.000    0.000    0.000 _internal.py:272(_get_void_ptr)\n",
       "        3    0.000    0.000    0.000    0.000 {pandas._libs.algos.take_1d_object_object}\n",
       "        1    0.000    0.000    0.001    0.001 frame.py:849(itertuples)\n",
       "        5    0.000    0.000    0.000    0.000 indexing.py:264(_convert_slice_indexer)\n",
       "        3    0.000    0.000    0.005    0.002 managers.py:1810(_multi_blockify)\n",
       "        1    0.000    0.000    0.316    0.316 parsers.py:1993(read)\n",
       "        3    0.000    0.000    0.001    0.000 compiler.py:500(__init__)\n",
       "        1    0.000    0.000    0.022    0.022 base.py:622(__connect)\n",
       "        1    0.000    0.000    0.097    0.097 csvs.py:130(save)\n",
       "        1    0.000    0.000    0.000    0.000 csvs.py:30(__init__)\n",
       "        6    0.000    0.000    0.000    0.000 parse.py:361(urlparse)\n",
       "        8    0.000    0.000    0.000    0.000 categorical.py:668(_get_codes)\n",
       "        3    0.000    0.000    0.006    0.002 frame.py:3433(_set_item)\n",
       "        3    0.000    0.000    0.001    0.000 generic.py:4585(head)\n",
       "       78    0.000    0.000    0.000    0.000 indexing.py:230(<genexpr>)\n",
       "        5    0.000    0.000    0.001    0.000 indexing.py:2170(_get_slice_axis)\n",
       "        4    0.000    0.000    0.033    0.008 managers.py:1663(create_block_manager_from_arrays)\n",
       "        5    0.000    0.000    0.005    0.001 construction.py:495(convert)\n",
       "       30    0.000    0.000    0.000    0.000 series.py:434(values)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1423(_has_complex_date_col)\n",
       "       42    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:859(__exit__)\n",
       "        7    0.000    0.000    0.001    0.000 fromnumeric.py:942(argsort)\n",
       "        3    0.000    0.000    0.000    0.000 console.py:149(in_ipython_frontend)\n",
       "       12    0.000    0.000    0.000    0.000 dtypes.py:465(validate_ordered)\n",
       "       28    0.000    0.000    0.000    0.000 common.py:279(is_null_slice)\n",
       "        8    0.000    0.000    0.000    0.000 base.py:4459(_maybe_promote)\n",
       "       34    0.000    0.000    0.000    0.000 frame.py:474(axes)\n",
       "        2    0.000    0.000    0.027    0.014 frame.py:3942(rename)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'writerow' of '_csv.writer' objects}\n",
       "        1    0.000    0.000    0.000    0.000 _auth.py:186(scramble_caching_sha2)\n",
       "       10    0.000    0.000    0.000    0.000 protocol.py:208(is_load_local_packet)\n",
       "        6    0.000    0.000    0.485    0.081 base.py:1138(_execute_text)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:780(visit_label)\n",
       "        1    0.000    0.000  320.195  320.195 <ipython-input-100-6c9647983064>:2(get_merge_data)\n",
       "        3    0.000    0.000    0.002    0.001 base.py:1050(_format_native_types)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:3089(reindex)\n",
       "        3    0.000    0.000    1.279    0.426 base.py:4017(_concat)\n",
       "        3    0.000    0.000    0.000    0.000 range.py:69(__new__)\n",
       "        1    0.000    0.000    0.004    0.004 connections.py:973(_get_server_information)\n",
       "       11    0.000    0.000    0.000    0.000 cursors.py:135(mogrify)\n",
       "        3    0.000    0.000    0.000    0.000 selectable.py:2687(__init__)\n",
       "        5    0.000    0.000    0.000    0.000 types.py:69(__init__)\n",
       "       49    0.000    0.000    0.000    0.000 threading.py:507(is_set)\n",
       "        2    0.000    0.000    0.000    0.000 _internal.py:260(__array_interface__)\n",
       "       10    0.000    0.000    0.000    0.000 generic.py:1848(empty)\n",
       "       10    0.000    0.000    0.001    0.000 generic.py:3149(_clear_item_cache)\n",
       "       22    0.000    0.000    0.000    0.000 format.py:298(__init__)\n",
       "        5    0.000    0.000    0.000    0.000 result.py:441(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.__build_class__}\n",
       "       92    0.000    0.000    0.000    0.000 {built-in method _imp.acquire_lock}\n",
       "       25    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:147(__enter__)\n",
       "       42    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:855(__enter__)\n",
       "       15    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1203(_path_importer_cache)\n",
       "       39    0.000    0.000    0.000    0.000 {method 'unpack_from' of 'Struct' objects}\n",
       "        1    0.000    0.000    0.001    0.001 socket.py:691(create_connection)\n",
       "        1    0.000    0.000    0.000    0.000 <ipython-input-97-c9fd36f58387>:14(buildParseTree)\n",
       "        4    0.000    0.000    0.055    0.014 construction.py:43(arrays_to_mgr)\n",
       "       12    0.000    0.000    0.000    0.000 blocks.py:266(_slice)\n",
       "        3    0.000    0.000    0.000    0.000 construction.py:284(extract_index)\n",
       "        1    0.000    0.000    0.368    0.368 parsers.py:1137(read)\n",
       "       22    0.000    0.000    0.000    0.000 cursors.py:89(_nextset)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:2157(_setup_select_stack)\n",
       "       41    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
       "       31    0.000    0.000    0.000    0.000 {built-in method _struct.pack}\n",
       "        6    0.000    0.000    0.001    0.000 base.py:983(format)\n",
       "        4    0.000    0.000    0.000    0.000 generic.py:1607(_check_label_or_level_ambiguity)\n",
       "       30    0.000    0.000    0.000    0.000 generic.py:1895(<genexpr>)\n",
       "        3    0.000    0.000    0.000    0.000 format.py:769(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:956(_clean_options)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1352(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 langhelpers.py:1136(constructor_copy)\n",
       "        2    0.000    0.000    0.022    0.011 base.py:748(_checkout)\n",
       "       22    0.000    0.000    0.000    0.000 result.py:560(_merge_cols_by_none)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:1278(visit_typeclause)\n",
       "       15    0.000    0.000    0.000    0.000 base.py:1729(inferred_type)\n",
       "        2    0.000    0.000    0.067    0.034 api.py:243(_get_consensus_names)\n",
       "        3    0.000    0.000    0.000    0.000 indexing.py:256(_convert_scalar_indexer)\n",
       "       12    0.000    0.000    0.000    0.000 blocks.py:255(__len__)\n",
       "        4    0.000    0.000    0.000    0.000 managers.py:147(set_axis)\n",
       "       36    0.000    0.000    0.000    0.000 format.py:535(<genexpr>)\n",
       "       10    0.000    0.000    0.000    0.000 format.py:1138(_format_strings)\n",
       "        1    0.000    0.000    0.002    0.002 common.py:314(_get_handle)\n",
       "        1    0.000    0.000    0.374    0.374 parsers.py:536(parser_f)\n",
       "       11    0.000    0.000    0.000    0.000 connections.py:1053(__init__)\n",
       "       54    0.000    0.000    0.000    0.000 cursors.py:71(_get_db)\n",
       "       11    0.000    0.000    0.487    0.044 cursors.py:151(execute)\n",
       "       10    0.000    0.000    0.000    0.000 cursors.py:341(_do_get_result)\n",
       "        8    0.000    0.000    0.000    0.000 type_api.py:505(_dialect_info)\n",
       "        1    0.000    0.000    0.000    0.000 schema.py:3753(__init__)\n",
       "        1    0.000    0.000    0.008    0.008 default.py:286(initialize)\n",
       "        5    0.000    0.000    0.000    0.000 default.py:947(should_autocommit)\n",
       "        1    0.000    0.000    0.000    0.000 csvs.py:191(_save_header)\n",
       "        1    0.000    0.000    0.000    0.000 expressions.py:71(_can_use_numexpr)\n",
       "       72    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
       "       29    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}\n",
       "       92    0.000    0.000    0.000    0.000 {built-in method _imp.release_lock}\n",
       "       14    0.000    0.000    0.000    0.000 six.py:184(find_module)\n",
       "        3    0.000    0.000    0.001    0.000 base.py:3830(_coerce_scalar_to_index)\n",
       "        1    0.000    0.000    0.003    0.003 frame.py:6558(append)\n",
       "       18    0.000    0.000    0.000    0.000 managers.py:1552(formatting_values)\n",
       "       30    0.000    0.000    0.000    0.000 concat.py:381(<genexpr>)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:179(get_filepath_or_buffer)\n",
       "        9    0.000    0.000    0.000    0.000 cursors.py:280(fetchone)\n",
       "        4    0.000    0.000    0.000    0.000 type_api.py:1440(adapt_type)\n",
       "        9    0.000    0.000    0.000    0.000 <string>:1(__init__)\n",
       "      9/3    0.000    0.000    0.001    0.000 compiler.py:349(process)\n",
       "        2    0.000    0.000    0.022    0.011 base.py:481(checkout)\n",
       "        1    0.000    0.000    0.002    0.002 default.py:331(_check_unicode_returns)\n",
       "        2    0.000    0.000    0.000    0.000 queue.py:92(put)\n",
       "        5    0.000    0.000    0.000    0.000 result.py:714(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2184(_get_server_version_info)\n",
       "        1    0.000    0.000    0.007    0.007 mysqldb.py:136(_check_unicode_returns)\n",
       "       51    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
       "       50    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}\n",
       "       14    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:36(_relax_case)\n",
       "       25    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:143(__init__)\n",
       "       14    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:74(_path_stat)\n",
       "        1    0.000    0.000    0.000    0.000 socket.py:652(close)\n",
       "        9    0.000    0.000    0.000    0.000 fromnumeric.py:70(<dictcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 dtypes.py:521(update_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 ops.py:1536(wrapper)\n",
       "        3    0.000    0.000    0.002    0.001 base.py:1024(to_native_types)\n",
       "        7    0.000    0.000    0.001    0.000 frame.py:919(<genexpr>)\n",
       "        5    0.000    0.000    0.000    0.000 managers.py:694(<listcomp>)\n",
       "       13    0.000    0.000    0.000    0.000 common.py:94(_expand_user)\n",
       "       18    0.000    0.000    0.000    0.000 series.py:483(_formatting_values)\n",
       "        7    0.000    0.000    0.000    0.000 langhelpers.py:253(_inspect_func_args)\n",
       "       21    0.000    0.000    0.000    0.000 langhelpers.py:1145(<genexpr>)\n",
       "        6    0.000    0.000    0.000    0.000 type_api.py:440(dialect_impl)\n",
       "        3    0.000    0.000    0.000    0.000 selectable.py:2959(_froms)\n",
       "        1    0.000    0.000    0.000    0.000 registry.py:53(_collection_gced)\n",
       "        5    0.000    0.000    0.000    0.000 result.py:334(_merge_cursor_description)\n",
       "        5    0.000    0.000    0.000    0.000 result.py:740(_init_metadata)\n",
       "        9    0.000    0.000    0.486    0.054 default.py:551(do_execute)\n",
       "        1    0.000    0.000    0.014    0.014 base.py:2347(initialize)\n",
       "       25    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:151(__exit__)\n",
       "        7    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:369(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 __init__.py:489(cast)\n",
       "        7    0.000    0.000    0.000    0.000 shape_base.py:283(<listcomp>)\n",
       "        6    0.000    0.000    0.000    0.000 printing.py:36(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 dtypes.py:244(_from_values_or_dtype)\n",
       "        2    0.000    0.000    0.000    0.000 _validators.py:230(validate_axis_style_args)\n",
       "        1    0.000    0.000    0.000    0.000 ops.py:1502(na_op)\n",
       "        2    0.000    0.000    0.000    0.000 sorting.py:407(safe_sort)\n",
       "        4    0.000    0.000    0.001    0.000 base.py:802(_assert_take_fillable)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:1272(set_names)\n",
       "        8    0.000    0.000    0.000    0.000 base.py:1669(is_boolean)\n",
       "        3    0.000    0.000    0.006    0.002 frame.py:3356(__setitem__)\n",
       "        3    0.000    0.000    0.000    0.000 generic.py:297(_construct_axes_from_arguments)\n",
       "        2    0.000    0.000    0.006    0.003 api.py:205(_sanitize_and_check)\n",
       "       18    0.000    0.000    0.000    0.000 blocks.py:171(formatting_values)\n",
       "        3    0.000    0.000    0.000    0.000 managers.py:225(get_dtypes)\n",
       "       12    0.000    0.000    0.000    0.000 concat.py:388(is_uniform_reindex)\n",
       "        3    0.000    0.000    0.000    0.000 common.py:260(_infer_compression)\n",
       "        1    0.000    0.000    0.012    0.012 construction.py:382(to_arrays)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _csv.writer}\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1502(_maybe_dedup_names)\n",
       "        1    0.000    0.000    0.493    0.493 sql.py:1055(read_query)\n",
       "       11    0.000    0.000    0.000    0.000 connections.py:481(cursor)\n",
       "       11    0.000    0.000    0.000    0.000 cursors.py:40(__init__)\n",
       "       24    0.000    0.000    0.000    0.000 protocol.py:186(is_ok_packet)\n",
       "        3    0.000    0.000    0.000    0.000 <string>:1(select)\n",
       "        9    0.000    0.000    0.000    0.000 sqltypes.py:140(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:2186(_compose_select_body)\n",
       "        4    0.000    0.000    0.000    0.000 compiler.py:3477(_requires_quotes)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:507(checkin)\n",
       "        2    0.000    0.000    0.001    0.000 base.py:845(_reset)\n",
       "        2    0.000    0.000    0.000    0.000 queue.py:135(get)\n",
       "        6    0.000    0.000    0.000    0.000 default.py:1034(create_cursor)\n",
       "       17    0.000    0.000    0.000    0.000 default.py:1051(get_result_processor)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:1316(visit_cast)\n",
       "        4    0.000    0.000    0.001    0.000 base.py:2223(do_rollback)\n",
       "        3    0.000    0.000    0.000    0.000 types.py:689(_adapt_string_for_cast)\n",
       "        3    0.000    0.000    0.000    0.000 threading.py:335(notify)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:2849(_convert_scalar_indexer)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:5408(default_index)\n",
       "        1    0.000    0.000    0.000    0.000 concat.py:486(_concat_index_asobject)\n",
       "        4    0.000    0.000    0.000    0.000 generic.py:337(_from_axes)\n",
       "        3    0.000    0.000    0.001    0.000 generic.py:3171(_set_item)\n",
       "        3    0.000    0.000    0.000    0.000 format.py:816(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 parsers.py:1180(_is_potential_multi_index)\n",
       "        3    0.000    0.000    0.000    0.000 selectable.py:2988(_get_display_froms)\n",
       "        6    0.000    0.000    0.486    0.081 base.py:922(execute)\n",
       "        6    0.000    0.000    0.000    0.000 base.py:1327(_safe_close_cursor)\n",
       "        3    0.000    0.000    0.001    0.000 compiler.py:274(__init__)\n",
       "        6    0.000    0.000    0.000    0.000 compiler.py:3529(quote)\n",
       "        4    0.000    0.000    0.000    0.000 result.py:1279(first)\n",
       "        6    0.000    0.000    0.000    0.000 default.py:1002(_use_server_side_cursor)\n",
       "        5    0.000    0.000    0.000    0.000 base.py:1174(should_autocommit_text)\n",
       "        1    0.000    0.000  320.209  320.209 <ipython-input-102-3812c86cd0f5>:94(add_hoc)\n",
       "       30    0.000    0.000    0.001    0.000 {method 'extend' of 'list' objects}\n",
       "        7    0.000    0.000    0.003    0.000 <frozen importlib._bootstrap_external>:1272(find_spec)\n",
       "        6    0.000    0.000    0.000    0.000 parse.py:409(urlsplit)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'setsockopt' of '_socket.socket' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method numpy.copyto}\n",
       "        7    0.000    0.000    0.000    0.000 shape_base.py:220(_warn_for_nonsequence)\n",
       "        6    0.000    0.000    0.000    0.000 function_base.py:4641(append)\n",
       "        2    0.000    0.000    0.000    0.000 arraysetops.py:745(setdiff1d)\n",
       "       12    0.000    0.000    0.001    0.000 dtypes.py:225(__init__)\n",
       "       66    0.000    0.000    0.000    0.000 {pandas._libs.algos.ensure_object}\n",
       "        9    0.000    0.000    0.000    0.000 missing.py:530(clean_reindex_fill_method)\n",
       "       15    0.000    0.000    0.000    0.000 base.py:4698(_validate_indexer)\n",
       "        3    0.000    0.000    0.000    0.000 frame.py:606(_info_repr)\n",
       "        2    0.000    0.000    0.000    0.000 frame.py:3585(reindexer)\n",
       "        2    0.000    0.000    0.047    0.024 numeric.py:110(_concat_same_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:4113(reindex)\n",
       "        3    0.000    0.000    0.000    0.000 managers.py:2015(_preprocess_slice_or_indexer)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:271(_init_dict)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:900(_get_options_with_defaults)\n",
       "        1    0.000    0.000    0.002    0.002 parsers.py:1120(_make_engine)\n",
       "        1    0.000    0.000    0.000    0.000 sql.py:115(_parse_date_columns)\n",
       "        1    0.000    0.000    0.007    0.007 __init__.py:88(Connect)\n",
       "        4    0.000    0.000    0.001    0.000 connections.py:422(rollback)\n",
       "       10    0.000    0.000    0.000    0.000 cursors.py:355(_show_warnings)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:864(anon_label)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:4299(apply_map)\n",
       "        3    0.000    0.000    0.000    0.000 selectable.py:3641(_columns_plus_names)\n",
       "        7    0.000    0.000    0.000    0.000 attr.py:267(__bool__)\n",
       "        1    0.000    0.000    0.015    0.015 strategies.py:194(first_connect)\n",
       "        2    0.000    0.000    0.022    0.011 base.py:2223(_contextual_connect)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:1542(_truncated_identifier)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:1564(_process_anon)\n",
       "        2    0.000    0.000    0.001    0.000 base.py:645(_finalize_fairy)\n",
       "        5    0.000    0.000    0.000    0.000 result.py:869(_soft_close)\n",
       "        7    0.000    0.000    0.000    0.000 __init__.py:23(find_module)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:1747(_extend_string)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:1954(visit_CHAR)\n",
       "       10    0.000    0.000    0.000    0.000 {method 'pop' of 'list' objects}\n",
       "       21    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
       "     18/6    0.000    0.000    0.001    0.000 <frozen importlib._bootstrap>:211(_call_with_frames_removed)\n",
       "        7    0.000    0.000    0.000    0.000 posixpath.py:232(expanduser)\n",
       "        8    0.000    0.000    0.000    0.000 weakref.py:435(__contains__)\n",
       "        6    0.000    0.000    0.000    0.000 {method 'digest' of '_hashlib.HASH' objects}\n",
       "       12    0.000    0.000    0.000    0.000 parse.py:109(_coerce_args)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'from_buffer' of '_ctypes.PyCArrayType' objects}\n",
       "        6    0.000    0.000    0.000    0.000 fromnumeric.py:1583(ravel)\n",
       "       20    0.000    0.000    0.000    0.000 dtypes.py:555(categories)\n",
       "        4    0.000    0.000    0.000    0.000 common.py:457(_get_rename_function)\n",
       "        3    0.000    0.000    0.025    0.008 base.py:48(__str__)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:1806(hasnans)\n",
       "        2    0.000    0.000    0.001    0.000 frame.py:4710(<listcomp>)\n",
       "        8    0.000    0.000    0.000    0.000 generic.py:1576(<genexpr>)\n",
       "        4    0.000    0.000    0.000    0.000 generic.py:3461(xs)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:3840(_update_inplace)\n",
       "        5    0.000    0.000    0.001    0.000 indexing.py:148(_slice)\n",
       "       30    0.000    0.000    0.000    0.000 blocks.py:161(external_values)\n",
       "        6    0.000    0.000    0.000    0.000 format.py:781(has_index_names)\n",
       "        1    0.000    0.000    0.002    0.002 parsers.py:813(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:304(<dictcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 _auth.py:34(scramble_native_password)\n",
       "       11    0.000    0.000    0.000    0.000 cursors.py:51(close)\n",
       "        8    0.000    0.000    0.000    0.000 protocol.py:86(advance)\n",
       "        7    0.000    0.000    0.000    0.000 elements.py:705(comparator)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:2395(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 elements.py:4139(__new__)\n",
       "        2    0.000    0.000    0.022    0.011 impl.py:111(_do_get)\n",
       "        5    0.000    0.000    0.000    0.000 result.py:266(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 result.py:588(_key_fallback)\n",
       "        5    0.000    0.000    0.001    0.000 result.py:1178(process_rows)\n",
       "        5    0.000    0.000    0.000    0.000 default.py:1108(_setup_crud_result_proxy)\n",
       "        1    0.000    0.000    0.002    0.002 base.py:2312(has_table)\n",
       "        2    0.000    0.000    0.000    0.000 types.py:510(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 pymysql.py:64(is_disconnect)\n",
       "        1    0.000    0.000    0.000    0.000 expressions.py:96(_evaluate_numexpr)\n",
       "        7    0.000    0.000    0.000    0.000 {method 'partition' of 'str' objects}\n",
       "        7    0.000    0.000    0.000    0.000 {built-in method _imp.is_frozen}\n",
       "        3    0.000    0.000    0.000    0.000 weakref.py:358(remove)\n",
       "        4    0.000    0.000    0.000    0.000 weakref.py:408(__setitem__)\n",
       "        4    0.000    0.000    0.000    0.000 enum.py:284(__call__)\n",
       "        3    0.000    0.000    0.000    0.000 hashlib.py:139(__hash_new)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method _hashlib.new}\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method _hashlib.openssl_sha256}\n",
       "        2    0.000    0.000    0.000    0.000 socket.py:412(_real_close)\n",
       "        3    0.000    0.000    0.000    0.000 socket.py:416(close)\n",
       "        1    0.000    0.000    0.001    0.001 socket.py:731(getaddrinfo)\n",
       "       73    0.000    0.000    0.000    0.000 __init__.py:275(u)\n",
       "        6    0.000    0.000    0.000    0.000 config.py:170(get_default_val)\n",
       "        7    0.000    0.000    0.000    0.000 {pandas._libs.lib.array_equivalent_object}\n",
       "        2    0.000    0.000    0.027    0.014 _decorators.py:195(wrapper)\n",
       "        4    0.000    0.000    0.000    0.000 cast.py:552(coerce_indexer_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 ops.py:1512(safe_na_op)\n",
       "       11    0.000    0.000    0.000    0.000 common.py:464(f)\n",
       "        7    0.000    0.000    0.000    0.000 base.py:759(size)\n",
       "        5    0.000    0.000    0.000    0.000 base.py:978(empty)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:2965(_convert_listlike_indexer)\n",
       "        3    0.000    0.000    0.001    0.000 base.py:4447(get_indexer_for)\n",
       "        8    0.000    0.000    0.000    0.000 concat.py:93(<genexpr>)\n",
       "       16    0.000    0.000    0.000    0.000 concat.py:136(<genexpr>)\n",
       "        6    0.000    0.000    0.000    0.000 range.py:89(ensure_int)\n",
       "        4    0.000    0.000    0.000    0.000 range.py:330(equals)\n",
       "        2    0.000    0.000    0.027    0.014 managers.py:1796(_simple_blockify)\n",
       "        6    0.000    0.000    0.000    0.000 format.py:789(show_row_idx_names)\n",
       "        1    0.000    0.000    0.000    0.000 sql.py:972(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 connections.py:637(write_packet)\n",
       "        1    0.000    0.000    0.000    0.000 _auth.py:48(_my_crypt)\n",
       "       22    0.000    0.000    0.000    0.000 cursors.py:106(nextset)\n",
       "       11    0.000    0.000    0.000    0.000 cursors.py:332(_clear_result)\n",
       "        3    0.000    0.000    0.000    0.000 _collections.py:755(unique_list)\n",
       "        6    0.000    0.000    0.000    0.000 base.py:38(_from_objects)\n",
       "        1    0.000    0.000    0.000    0.000 exc.py:390(instance)\n",
       "        3    0.000    0.000    0.001    0.000 elements.py:464(_compiler)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:2340(literal_column)\n",
       "        1    0.000    0.000    0.000    0.000 elements.py:3066(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:3944(_set_table)\n",
       "        4    0.000    0.000    0.000    0.000 type_api.py:524(adapt)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:295(__get__)\n",
       "        1    0.000    0.000    0.015    0.015 attr.py:279(exec_once)\n",
       "        2    0.000    0.000    0.001    0.000 base.py:869(close)\n",
       "        3    0.000    0.000    0.001    0.000 base.py:1287(_cursor_execute)\n",
       "        1    0.000    0.000    0.024    0.024 base.py:2133(run_callable)\n",
       "        2    0.000    0.000    0.001    0.000 base.py:831(_checkin)\n",
       "        5    0.000    0.000    0.000    0.000 default.py:1092(get_result_proxy)\n",
       "        1    0.000    0.000    0.001    0.001 base.py:2172(get_isolation_level)\n",
       "        3    0.000    0.000    0.000    0.000 types.py:674(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 necompiler.py:724(getArguments)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'split' of 're.Pattern' objects}\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1433(<setcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 weakref.py:356(__init__)\n",
       "        6    0.000    0.000    0.000    0.000 <string>:1(__new__)\n",
       "        4    0.000    0.000    0.000    0.000 socket.py:97(_intenum_converter)\n",
       "        1    0.000    0.000    0.000    0.000 socket.py:563(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 numeric.py:175(ones)\n",
       "        2    0.000    0.000    0.000    0.000 _internal.py:289(__init__)\n",
       "        7    0.000    0.000    0.000    0.000 shape_base.py:209(_arrays_for_stack_dispatcher)\n",
       "        2    0.000    0.000    0.000    0.000 __init__.py:125(lrange)\n",
       "        3    0.000    0.000    0.000    0.000 ops.py:66(_maybe_match_name)\n",
       "        4    0.000    0.000    0.000    0.000 common.py:162(_not_none)\n",
       "        4    0.000    0.000    0.000    0.000 categorical.py:394(categories)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:1240(_set_names)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:3926(contains)\n",
       "       16    0.000    0.000    0.000    0.000 concat.py:137(<genexpr>)\n",
       "        3    0.000    0.000    0.000    0.000 generic.py:1818(__iter__)\n",
       "        3    0.000    0.000    0.000    0.000 range.py:176(_data)\n",
       "        2    0.000    0.000    0.000    0.000 range.py:180(_int64index)\n",
       "        2    0.000    0.000    0.002    0.001 blocks.py:323(concat_same_type)\n",
       "        9    0.000    0.000    0.000    0.000 format.py:1434(_has_names)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1926(close)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1936(_set_noconvert_columns)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:305(<dictcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 connections.py:532(ping)\n",
       "        1    0.000    0.000    0.000    0.000 compat.py:123(reraise)\n",
       "        3    0.000    0.000    0.000    0.000 langhelpers.py:897(expire_instance)\n",
       "        6    0.000    0.000    0.000    0.000 base.py:39(<listcomp>)\n",
       "        3    0.000    0.000    0.001    0.000 elements.py:399(compile)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:3831(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 sqltypes.py:411(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:94(__getattr__)\n",
       "        1    0.000    0.000    0.015    0.015 attr.py:291(__call__)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:2082(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 log.py:59(_should_log_info)\n",
       "        2    0.000    0.000    0.022    0.011 base.py:345(connect)\n",
       "        4    0.000    0.000    0.000    0.000 default.py:409(type_descriptor)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:1350(get_select_precolumns)\n",
       "       17    0.000    0.000    0.000    0.000 base.py:1753(attr)\n",
       "        1    0.000    0.000    0.002    0.002 base.py:2750(_detect_casing)\n",
       "        1    0.000    0.000    0.004    0.004 base.py:2794(_detect_sql_mode)\n",
       "        1    0.000    0.000    0.000    0.000 <ipython-input-97-c9fd36f58387>:83(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 idna.py:147(encode)\n",
       "        7    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method _ctypes.pointer}\n",
       "       38    0.000    0.000    0.000    0.000 numerictypes.py:587(<listcomp>)\n",
       "        6    0.000    0.000    0.000    0.000 {method 'transpose' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:227(itervalues)\n",
       "        9    0.000    0.000    0.000    0.000 common.py:1752(is_complex_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 ops.py:1468(_construct_result)\n",
       "        9    0.000    0.000    0.000    0.000 missing.py:71(clean_fill_method)\n",
       "       12    0.000    0.000    0.000    0.000 base.py:475(<genexpr>)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:1672(is_integer)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:5393(_trim_front)\n",
       "        8    0.000    0.000    0.000    0.000 concat.py:92(<genexpr>)\n",
       "        3    0.000    0.000    0.000    0.000 generic.py:334(<dictcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:11018(logical_func)\n",
       "        3    0.000    0.000    0.000    0.000 range.py:184(_get_data_as_items)\n",
       "        5    0.000    0.000    0.000    0.000 indexing.py:2700(need_slice)\n",
       "        7    0.000    0.000    0.000    0.000 blocks.py:3146(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 format.py:795(show_col_idx_names)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:76(_is_url)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:544(UnicodeWriter)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:3735(reindex)\n",
       "        3    0.000    0.000    0.000    0.000 concat.py:523(_maybe_check_integrity)\n",
       "        1    0.000    0.000    0.000    0.000 sql.py:508(pandasSQL_builder)\n",
       "        1    0.000    0.000    0.024    0.024 sql.py:1197(has_table)\n",
       "        1    0.000    0.000    0.000    0.000 err.py:100(raise_mysql_exception)\n",
       "        1    0.000    0.000    0.001    0.001 connections.py:401(_send_autocommit_mode)\n",
       "        1    0.000    0.000    0.001    0.001 connections.py:890(_process_auth)\n",
       "        8    0.000    0.000    0.000    0.000 protocol.py:77(read_all)\n",
       "        7    0.000    0.000    0.000    0.000 protocol.py:297(__getattr__)\n",
       "        3    0.000    0.000    0.000    0.000 _collections.py:140(__new__)\n",
       "        3    0.000    0.000    0.000    0.000 _collections.py:361(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 _collections.py:759(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 langhelpers.py:925(__getattr__)\n",
       "        1    0.000    0.000    0.000    0.000 exc.py:328(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 exc.py:462(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 elements.py:54(collate)\n",
       "        2    0.000    0.000    0.000    0.000 elements.py:681(self_group)\n",
       "        4    0.000    0.000    0.000    0.000 elements.py:4544(_literal_as_binds)\n",
       "        7    0.000    0.000    0.000    0.000 type_api.py:60(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 type_api.py:521(_gen_dialect_impl)\n",
       "        3    0.000    0.000    0.000    0.000 selectable.py:2120(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:116(_for_class)\n",
       "        1    0.000    0.000    0.007    0.007 strategies.py:106(connect)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:183(execution_options)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:729(_rollback_impl)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:865(_autorollback)\n",
       "        1    0.000    0.000    0.000    0.000 compiler.py:1261(visit_binary)\n",
       "        1    0.000    0.000    0.000    0.000 compiler.py:1318(_generate_generic_binary)\n",
       "        1    0.000    0.000    0.022    0.022 base.py:425(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:2296(_compat_first)\n",
       "        1    0.000    0.000    0.000    0.000 <ipython-input-97-c9fd36f58387>:54(make_parse_tree)\n",
       "        1    0.000    0.000    0.000    0.000 <ipython-input-96-3fde1df72328>:11(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 necompiler.py:535(getContext)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'find' of 'bytes' objects}\n",
       "       14    0.000    0.000    0.000    0.000 {method 'get' of 'mappingproxy' objects}\n",
       "       11    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}\n",
       "        4    0.000    0.000    0.000    0.000 {method 'difference' of 'set' objects}\n",
       "        7    0.000    0.000    0.000    0.000 {method 'update' of 'set' objects}\n",
       "        8    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.repr}\n",
       "        7    0.000    0.000    0.000    0.000 {built-in method posix.fspath}\n",
       "        4    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}\n",
       "        7    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:719(find_spec)\n",
       "        8    0.000    0.000    0.000    0.000 __init__.py:388(<genexpr>)\n",
       "        6    0.000    0.000    0.000    0.000 _config.py:49(getter)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method numpy.bincount}\n",
       "        2    0.000    0.000    0.000    0.000 common.py:199(count_not_none)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:212(dict_keys_to_ordered_list)\n",
       "        1    0.000    0.000    0.000    0.000 nanops.py:337(nanany)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:1346(rename)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:1785(_isnan)\n",
       "        5    0.000    0.000    0.000    0.000 base.py:2249(_validate_sort_keyword)\n",
       "        5    0.000    0.000    0.000    0.000 base.py:2590(_assert_can_do_setop)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:2595(_convert_can_do_setop)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:2999(_convert_arr_indexer)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:4071(identical)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:5381(_ensure_has_len)\n",
       "        8    0.000    0.000    0.000    0.000 base.py:5398(<genexpr>)\n",
       "        4    0.000    0.000    0.000    0.000 generic.py:1627(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:3115(_maybe_update_cacher)\n",
       "        2    0.000    0.000    0.000    0.000 generic.py:4341(<genexpr>)\n",
       "        5    0.000    0.000    0.000    0.000 managers.py:1850(_shape_compat)\n",
       "        3    0.000    0.000    0.000    0.000 format.py:785(has_column_names)\n",
       "        1    0.000    0.000    0.005    0.005 construction.py:484(_convert_object_array)\n",
       "        1    0.000    0.000    0.005    0.005 construction.py:501(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:4209(isnull)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1329(_validate_parse_dates_arg)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1536(_make_index)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1814(_do_date_conversions)\n",
       "        1    0.000    0.000    0.000    0.000 sql.py:45(_is_sqlalchemy_connectable)\n",
       "        2    0.000    0.000    0.000    0.000 charset.py:43(by_name)\n",
       "        1    0.000    0.000    0.001    0.001 connections.py:383(autocommit)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:448(escape)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:946(_get_auth_plugin_handler)\n",
       "        3    0.000    0.000    0.000    0.000 util.py:11(int2byte)\n",
       "        1    0.000    0.000    0.000    0.000 protocol.py:122(read_uint16)\n",
       "        1    0.000    0.000    0.015    0.015 langhelpers.py:1440(go)\n",
       "        6    0.000    0.000    0.000    0.000 elements.py:3941(_get_table)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:4345(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 type_api.py:307(_has_column_expression)\n",
       "       10    0.000    0.000    0.000    0.000 type_api.py:1430(to_instance)\n",
       "        3    0.000    0.000    0.000    0.000 <string>:1(cast)\n",
       "        1    0.000    0.000    0.000    0.000 schema.py:4027(_bind_to)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:261(__init__)\n",
       "       10    0.000    0.000    0.000    0.000 base.py:370(connection)\n",
       "        1    0.000    0.000    0.005    0.005 base.py:914(scalar)\n",
       "        1    0.000    0.000    0.473    0.473 base.py:2149(execute)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:410(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:890(escape_literal_column)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:3425(_escape_identifier)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:3464(quote_identifier)\n",
       "        2    0.000    0.000    0.000    0.000 log.py:56(_should_log_debug)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:580(get_connection)\n",
       "        1    0.000    0.000    0.002    0.002 default.py:379(<setcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 impl.py:102(_do_return_conn)\n",
       "        1    0.000    0.000    0.000    0.000 impl.py:142(_inc_overflow)\n",
       "        1    0.000    0.000    0.000    0.000 queue.py:203(_get)\n",
       "        4    0.000    0.000    0.000    0.000 result.py:1146(_fetchone_impl)\n",
       "        4    0.000    0.000    0.000    0.000 binaryTree.py:47(getRightChild)\n",
       "        3    0.000    0.000    0.000    0.000 stack.py:14(push)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2309(_get_default_schema_name)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2809(_detect_ansiquotes)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:2902(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 mysqldb.py:120(do_ping)\n",
       "        1    0.000    0.000    0.000    0.000 pymysql.py:51(supports_server_side_cursors)\n",
       "        2    0.000    0.000    0.000    0.000 pymysql.py:76(_extract_error_code)\n",
       "       15    0.000    0.000    0.000    0.000 {method '__contains__' of 'frozenset' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'count' of 'str' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method sys.exc_info}\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method sys._getframe}\n",
       "        1    0.000    0.000    0.000    0.000 {function SocketIO.close at 0x10e5f7e18}\n",
       "        7    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:792(find_spec)\n",
       "        1    0.000    0.000    0.000    0.000 codecs.py:186(__init__)\n",
       "        6    0.000    0.000    0.000    0.000 __init__.py:1619(isEnabledFor)\n",
       "        6    0.000    0.000    0.000    0.000 _config.py:140(get)\n",
       "        6    0.000    0.000    0.000    0.000 config.py:578(_get_registered_option)\n",
       "        3    0.000    0.000    0.000    0.000 console.py:90(in_interactive_session)\n",
       "       16    0.000    0.000    0.000    0.000 dtypes.py:562(ordered)\n",
       "        1    0.000    0.000    0.000    0.000 ops.py:101(maybe_upcast_for_op)\n",
       "        3    0.000    0.000    0.000    0.000 common.py:175(_all_none)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:2168(_get_unique_index)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:4025(_concat_same_dtype)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:4683(_maybe_cast_indexer)\n",
       "        8    0.000    0.000    0.000    0.000 concat.py:97(<genexpr>)\n",
       "        3    0.000    0.000    0.000    0.000 frame.py:3416(_ensure_valid_index)\n",
       "        4    0.000    0.000    0.000    0.000 generic.py:1567(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 generic.py:1693(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 generic.py:3205(_check_setitem_copy)\n",
       "        3    0.000    0.000    0.000    0.000 range.py:165(_validate_dtype)\n",
       "        1    0.000    0.000    0.002    0.002 indexing.py:2184(_get_list_axis)\n",
       "        3    0.000    0.000    0.000    0.000 indexing.py:2676(is_nested_tuple)\n",
       "        3    0.000    0.000    0.000    0.000 managers.py:226(<listcomp>)\n",
       "        6    0.000    0.000    0.000    0.000 concat.py:383(<genexpr>)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:163(is_s3_url)\n",
       "        1    0.000    0.000    0.000    0.000 construction.py:509(sanitize_index)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:737(__array_prepare__)\n",
       "        4    0.000    0.000    0.000    0.000 concat.py:434(_get_result_dim)\n",
       "        2    0.000    0.000    0.000    0.000 parsers.py:347(_validate_integer)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:3195(_process_date_conversion)\n",
       "        1    0.000    0.000    0.473    0.473 sql.py:988(execute)\n",
       "        1    0.000    0.000    0.000    0.000 converters.py:12(escape_item)\n",
       "        1    0.000    0.000    0.000    0.000 charset.py:40(by_id)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:74(_makefile)\n",
       "       10    0.000    0.000    0.000    0.000 cursors.py:76(_check_executed)\n",
       "        9    0.000    0.000    0.000    0.000 cursors.py:127(<dictcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 _collections.py:151(union)\n",
       "        3    0.000    0.000    0.000    0.000 _collections.py:396(__iter__)\n",
       "        3    0.000    0.000    0.000    0.000 _collections.py:730(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:2455(_from_objects)\n",
       "        1    0.000    0.000    0.000    0.000 elements.py:3098(_from_objects)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:3950(_from_objects)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:4184(__new__)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:4340(_select_iterables)\n",
       "        1    0.000    0.000    0.000    0.000 operators.py:1376(is_comparison)\n",
       "        1    0.000    0.000    0.000    0.000 operators.py:1399(is_boolean)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:77(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 attr.py:255(__call__)\n",
       "        1    0.000    0.000    0.000    0.000 attr.py:276(_memoized_attr__exec_once_mutex)\n",
       "        1    0.000    0.000    0.000    0.000 default.py:270(_type_memos)\n",
       "        1    0.000    0.000    0.007    0.007 default.py:452(connect)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:169(_clone)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:180(__exit__)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:355(closed)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:399(process)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:834(visit_column)\n",
       "        1    0.000    0.000    0.000    0.000 compiler.py:1168(_get_operator_dispatch)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:1758(_add_to_result_map)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:3598(format_label)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:366(_return_conn)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:715(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:958(__getattr__)\n",
       "        2    0.000    0.000    0.001    0.000 base.py:987(close)\n",
       "        5    0.000    0.000    0.000    0.000 result.py:263(<listcomp>)\n",
       "        5    0.000    0.000    0.000    0.000 result.py:864(_cursor_description)\n",
       "        4    0.000    0.000    0.000    0.000 result.py:901(close)\n",
       "        1    0.000    0.000    0.002    0.002 result.py:1195(fetchall)\n",
       "        2    0.000    0.000    0.000    0.000 result.py:1306(scalar)\n",
       "        1    0.000    0.000    0.000    0.000 binaryTree.py:22(insertLeft)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2057(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2054(_quote_free_identifiers)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2260(is_disconnect)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:2381(_is_mariadb)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:2407(_supports_cast)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:2906(__getitem__)\n",
       "        1    0.000    0.000    0.000    0.000 mysqldb.py:214(_detect_charset)\n",
       "        3    0.000    0.000    0.000    0.000 csvs.py:112(<genexpr>)\n",
       "        2    0.000    0.000    0.000    0.000 <ipython-input-80-01a4bf55ac48>:14(corpus_config)\n",
       "        2    0.000    0.000    0.000    0.000 necompiler.py:685(getType)\n",
       "        1    0.000    0.000    0.000    0.000 necompiler.py:821(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 <ipython-input-97-c9fd36f58387>:62(preprocess_sentence)\n",
       "        3    0.000    0.000    0.000    0.000 <ipython-input-80-01a4bf55ac48>:9(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 expressions.py:163(_has_bool_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 expressions.py:173(_bool_arith_check)\n",
       "        1    0.000    0.000    0.000    0.000 expressions.py:192(evaluate)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'split' of 'bytes' objects}\n",
       "       12    0.000    0.000    0.000    0.000 {method 'discard' of 'set' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'issuperset' of 'set' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.globals}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _imp.is_builtin}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
       "        4    0.000    0.000    0.000    0.000 enum.py:526(__new__)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:1361(debug)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _struct.unpack_from}\n",
       "        3    0.000    0.000    0.000    0.000 interactiveshell.py:698(get_ipython)\n",
       "        1    0.000    0.000    0.000    0.000 socket.py:406(_decref_socketios)\n",
       "        6    0.000    0.000    0.000    0.000 {built-in method sqlalchemy.cutils._distill_params}\n",
       "        2    0.000    0.000    0.000    0.000 _internal.py:257(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 _internal.py:338(data)\n",
       "        2    0.000    0.000    0.000    0.000 function_base.py:258(iterable)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:457(is_platform_windows)\n",
       "        1    0.000    0.000    0.000    0.000 ops.py:1447(_align_method_SERIES)\n",
       "        6    0.000    0.000    0.000    0.000 common.py:201(<genexpr>)\n",
       "        4    0.000    0.000    0.000    0.000 categorical.py:441(dtype)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:3035(_convert_list_indexer)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:5399(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:7083(isnull)\n",
       "        2    0.000    0.000    0.000    0.000 blocks.py:327(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:1572(is_consolidated)\n",
       "        6    0.000    0.000    0.000    0.000 managers.py:1814(<lambda>)\n",
       "        2    0.000    0.000    0.000    0.000 construction.py:210(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 format.py:351(should_show_dimensions)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:112(_validate_header_arg)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:171(is_gcs_url)\n",
       "        3    0.000    0.000    0.000    0.000 concat.py:510(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:377(_validate_names)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:897(close)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:939(_check_file_or_buffer)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1162(_create_index)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:2064(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:2066(<dictcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:3157(_make_date_converter)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:3278(_clean_na_values)\n",
       "        1    0.000    0.000    0.000    0.000 sql.py:490(_engine_builder)\n",
       "        1    0.000    0.000    0.000    0.000 converters.py:47(escape_bool)\n",
       "        1    0.000    0.000    0.000    0.000 charset.py:18(encoding)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:101(lenenc_int)\n",
       "        2    0.000    0.000    0.000    0.000 connections.py:96(pack_int24)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:389(get_autocommit)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:964(character_set_name)\n",
       "       11    0.000    0.000    0.000    0.000 connections.py:1069(__del__)\n",
       "        1    0.000    0.000    0.000    0.000 util.py:4(byte2int)\n",
       "        1    0.000    0.000    0.000    0.000 cursors.py:299(fetchall)\n",
       "        1    0.000    0.000    0.000    0.000 protocol.py:60(get_all_data)\n",
       "        1    0.000    0.000    0.000    0.000 protocol.py:94(rewind)\n",
       "        1    0.000    0.000    0.000    0.000 protocol.py:142(read_string)\n",
       "        1    0.000    0.000    0.000    0.000 protocol.py:196(is_auth_switch_request)\n",
       "        1    0.000    0.000    0.000    0.000 exc.py:24(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 langhelpers.py:56(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 langhelpers.py:62(__exit__)\n",
       "        1    0.000    0.000    0.000    0.000 compat.py:378(raise_from_cause)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:751(_select_iterable)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:1271(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 elements.py:4062(__init__)\n",
       "        5    0.000    0.000    0.000    0.000 type_api.py:269(result_processor)\n",
       "        1    0.000    0.000    0.000    0.000 <string>:1(<lambda>)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:119(_for_instance)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:123(_join)\n",
       "        5    0.000    0.000    0.000    0.000 base.py:155(_root)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:479(_still_open_and_connection_is_valid)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:709(in_transaction)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:352(__str__)\n",
       "        1    0.000    0.000    0.000    0.000 compiler.py:872(visit_collation)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:2080(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 compiler.py:3579(format_collation)\n",
       "        1    0.000    0.000    0.022    0.022 base.py:296(_create_connection)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:875(is_valid)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:949(cursor)\n",
       "        2    0.000    0.000    0.000    0.000 queue.py:191(_empty)\n",
       "        2    0.000    0.000    0.000    0.000 queue.py:195(_full)\n",
       "        2    0.000    0.000    0.000    0.000 queue.py:199(_put)\n",
       "        1    0.000    0.000    0.000    0.000 result.py:1161(_fetchall_impl)\n",
       "        1    0.000    0.000    0.000    0.000 default.py:477(set_connection_execution_options)\n",
       "        6    0.000    0.000    0.000    0.000 default.py:943(no_parameters)\n",
       "        3    0.000    0.000    0.000    0.000 binaryTree.py:17(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 binaryTree.py:34(insertRight)\n",
       "        1    0.000    0.000    0.000    0.000 stack.py:8(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 binaryTree.py:50(getLeftChild)\n",
       "        3    0.000    0.000    0.000    0.000 binaryTree.py:53(setRootVal)\n",
       "        3    0.000    0.000    0.000    0.000 stack.py:17(pop)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:1778(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2385(_is_mysql)\n",
       "        1    0.000    0.000    0.000    0.000 <ipython-input-96-3fde1df72328>:10(Results)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'with_traceback' of 'BaseException' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {method 'union' of 'set' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'isdigit' of 'str' objects}\n",
       "       15    0.000    0.000    0.000    0.000 {method 'isidentifier' of 'str' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.ord}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method sys.getfilesystemencoding}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method sys.intern}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'popleft' of 'collections.deque' objects}\n",
       "        4    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {method '_is_owned' of '_thread.RLock' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'write' of '_io.StringIO' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'update' of '_hashlib.HASH' objects}\n",
       "       12    0.000    0.000    0.000    0.000 parse.py:98(_noop)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        3    0.000    0.000    0.000    0.000 timeout.py:56(stop)\n",
       "        3    0.000    0.000    0.000    0.000 timeout.py:260(_start_new_or_dummy)\n",
       "       10    0.000    0.000    0.000    0.000 numerictypes.py:655(<listcomp>)\n",
       "        6    0.000    0.000    0.000    0.000 fromnumeric.py:2847(ndim)\n",
       "        1    0.000    0.000    0.000    0.000 inference.py:160(is_file_like)\n",
       "        2    0.000    0.000    0.000    0.000 {pandas._libs.algos.ensure_float64}\n",
       "        1    0.000    0.000    0.000    0.000 missing.py:534(fill_zeros)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:100(_reset_cache)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:4077(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 concat.py:496(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 concat.py:503(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 indexing.py:937(_convert_for_reindex)\n",
       "        2    0.000    0.000    0.000    0.000 managers.py:2058(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 parsers.py:1176(_is_index_col)\n",
       "        6    0.000    0.000    0.000    0.000 parsers.py:1195(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1274(_validate_usecols_arg)\n",
       "        2    0.000    0.000    0.000    0.000 parsers.py:1530(_maybe_make_multi_index_columns)\n",
       "        1    0.000    0.000    0.000    0.000 sql.py:74(_convert_params)\n",
       "        1    0.000    0.000    0.000    0.000 sql.py:85(_process_parse_dates_argument)\n",
       "        3    0.000    0.000    0.000    0.000 _collections.py:145(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 _collections.py:733(__missing__)\n",
       "        1    0.000    0.000    0.000    0.000 langhelpers.py:59(__enter__)\n",
       "        1    0.000    0.000    0.000    0.000 elements.py:4322(_string_or_unprintable)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:4559(_interpret_as_column_or_from)\n",
       "        3    0.000    0.000    0.000    0.000 selectable.py:3002(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 selectable.py:3663(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 attr.py:340(for_modify)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:177(__enter__)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:364(invalidated)\n",
       "        1    0.000    0.000    0.002    0.002 base.py:1591(run_callable)\n",
       "        2    0.000    0.000    0.022    0.011 base.py:2259(_wrap_pool_connect)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:419(type)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:708(default_from)\n",
       "        1    0.000    0.000    0.000    0.000 result.py:760(keys)\n",
       "        1    0.000    0.000    0.000    0.000 default.py:1089(handle_dbapi_exception)\n",
       "        3    0.000    0.000    0.000    0.000 binaryTree.py:56(getRootVal)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2367(_warn_for_known_db_issues)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%time\n",
    "def main():\n",
    "    '''\n",
    "        corpora: i2b2, mipacq, fv017\n",
    "        analyses: entity only (exact span), cui by document, full (aka (entity and cui on exaact span/exact cui)\n",
    "        systems: ctakes, biomedicus, clamp, metamap, quick_umls\n",
    "        \n",
    "        TODO -> Vectorization (entity only) -> done:\n",
    "                add switch for use of TN on single system performance evaluations -> done\n",
    "                add switch for overlap matching versus exact span -> done\n",
    "             -> Other tasks besides concept extraction\n",
    "        \n",
    "    ''' \n",
    "    analysisConf =  AnalysisConfig()\n",
    "    print(analysisConf.systems, analysisConf.corpus_config())\n",
    "    \n",
    "    if (rtype == 1):\n",
    "        print(semtypes, systems)\n",
    "        if filter_semtype:\n",
    "            for semtype in semtypes:\n",
    "                test = get_valid_systems(systems, semtype)\n",
    "                print('SYSYEMS FOR SEMTYPE', semtype, 'ARE', test)\n",
    "                generate_metrics(analysis_type, corpus, filter_semtype, semtype)\n",
    "            \n",
    "        else:\n",
    "            generate_metrics(analysis_type, corpus, filter_semtype)\n",
    "        \n",
    "    elif (rtype == 2):\n",
    "        print('run_type:', run_type)\n",
    "        if filter_semtype:\n",
    "            print(semtypes)\n",
    "            ensemble_control(analysisConf.systems, analysis_type, corpus, run_type, filter_semtype, semtypes)\n",
    "        else:\n",
    "            ensemble_control(analysisConf.systems, analysis_type, corpus, run_type, filter_semtype)\n",
    "    elif (rtype == 3):\n",
    "        t = ['concept_jaccard_score_false']\n",
    "        test_systems(analysis_type, analysisConf.systems, corpus)  \n",
    "        test_count(analysis_type, corpus)\n",
    "        test_ensemble(analysis_type, corpus)\n",
    "    elif (rtype == 4):\n",
    "        if filter_semtype:\n",
    "            majority_vote(systems, analysis_type, corpus, run_type, filter_semtype, semtypes)\n",
    "        else:\n",
    "            majority_vote(systems, analysis_type, corpus, run_type, filter_semtype)\n",
    "    elif (rtype == 5):\n",
    "        \n",
    "        # control filter_semtype in get_sys_data, get_ref_n and generate_metrics. TODO consolidate. \n",
    "        # # run single ad hoc statement\n",
    "        statement = '((ctakes&biomedicus)|metamap)'\n",
    "\n",
    "        def add_hoc(analysis_type, corpus, statement):\n",
    "            sys = get_merge_data(statement, analysis_type, corpus, run_type, filter_semtype)\n",
    "            sys = sys.rename(index=str, columns={\"note_id\": \"case\"})\n",
    "            sys['label'] = 'concept'\n",
    "\n",
    "            ref = get_reference_vector(analysis_type, corpus, filter_semtype)\n",
    "            sys = vectorized_annotations(sys)\n",
    "            sys = np.asarray(flatten_list(list(sys)), dtype=np.int32)\n",
    "\n",
    "            return ref, sys\n",
    "\n",
    "        ref, sys = add_hoc(analysis_type, corpus, statement)\n",
    "        \n",
    "        # query by term:\n",
    "        \n",
    "        # import spacy\n",
    "        # nlp = spacy.load('en')\n",
    "\n",
    "        # sql = \"select distinct note_id, sofa from concepts.sofas where corpus = 'fairview'\"\n",
    "\n",
    "        # docs = pd.read_sql(sql, con=engine)\n",
    "\n",
    "        # d = {}\n",
    "\n",
    "        # for row in docs.itertuples():\n",
    "        #     d[row.note_id] = row.sofa\n",
    "\n",
    "        # print(len(d))\n",
    "\n",
    "        # test = matches[matches['note_id'] == '0000200926']\n",
    "        # print(len(test))\n",
    "\n",
    "        # doc = nlp(d['0000200926'])\n",
    "\n",
    "        # for row in test.itertuples():\n",
    "        #     my_str = [token.text.strip('\\n').lower() for token in doc if token.idx >= (row.begin) and token.idx <= (row.end)]\n",
    "        #     if 'diabetes' in my_str:\n",
    "        #         print(my_str)\n",
    "\n",
    "    elif (rtype == 6): # 5 w/o evaluation\n",
    "        \n",
    "        statement = '(ctakes|biomedicus)'\n",
    "\n",
    "        def add_hoc(analysis_type, corpus, statement):\n",
    "            sys = get_merge_data(statement, analysis_type, corpus, run_type, filter_semtype)\n",
    "            sys = sys.rename(index=str, columns={\"note_id\": \"case\"})\n",
    "\n",
    "            return sys\n",
    "\n",
    "        sys = add_hoc(analysis_type, corpus, statement).sort_values(by=['case', 'begin'])\n",
    "        print(sys.head())\n",
    "        \n",
    "        print(statement + '\\n', sys.head(100))\n",
    "        sys.to_csv('test.csv')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    %prun main()\n",
    "    print('done!')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tests to implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TESTS -> ensemble:\n",
    "# def test_match_consistency(matches, ref_only, ref_n, sys):\n",
    "#     \"\"\"test for reference only/match set consistency:\n",
    "#         params: match, system and reference only sets\"\"\"\n",
    "   \n",
    "#     print('len', len(sys), len(matches), len(matches.union(sys)), len(matches.intersection(sys)))\n",
    "#     assert len(matches.union(ref_only)) == ref_n, 'Reference annotation mismatch union'\n",
    "#     assert len(matches.intersection(sys)) == len(matches), 'System annotation mismatch intersect'\n",
    "#     assert len(matches.union(sys)) == len(sys), 'System annotation mismatch union'\n",
    "#     assert len(matches.intersection(ref_only)) == 0, 'Reference annotation mismatch intersect'\n",
    "\n",
    "# def test_systems(analysis_type, systems, corpus):\n",
    "#     sys = df_to_set(get_sys_data(systems[0], analysis_type, corpus), analysis_type)\n",
    "#     test_match_consistency(*get_system_matches(systems[0], analysis_type, corpus), get_ref_n(analysis_type), sys)\n",
    "#     print('Match consistency:', len(sys),get_ref_n(analysis_type))\n",
    "\n",
    "# def test_metrics(ref, sys_m, match_m):\n",
    "#     test = True\n",
    "#     reference_n = len(ref)\n",
    "#     system_n = len(sys_m)\n",
    "\n",
    "#     print('Test metrics:', type(reference_n), type(system_n), type(match_m))\n",
    "\n",
    "#     reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, match_m).get_ref_sys()\n",
    "#     F, recall, precision, _, _, _, _, _ = Metrics(system_only, reference_only, reference_system_match, system_n).get_confusion_metrics()\n",
    "#     F_, recall_, precision_, _, _, _, _, _ = Metrics(system_only, reference_only, reference_system_match, system_n).get_confusion_metrics(test)\n",
    "\n",
    "#     assert F[1] == F_, 'F1 issue'\n",
    "#     assert recall[1] == recall_, 'recall issue'\n",
    "#     assert precision[1] == precision_, 'precision issue'\n",
    "#     print(F[1], F_)\n",
    "#     print(recall[1], recall_)\n",
    "#     print(precision[1], precision_)\n",
    "\n",
    "# def test_count(analysis_type, corpus):\n",
    "#     # test match counts:\n",
    "#     ctakes, _ = get_system_matches('ctakes', analysis_type, corpus)\n",
    "#     clamp, _ = get_system_matches('clamp', analysis_type, corpus)\n",
    "#     b9, _ = get_system_matches('biomedicus', analysis_type, corpus)\n",
    "#     mm, _ = get_system_matches('metamap', analysis_type, corpus)\n",
    "\n",
    "#     print('count:', len(mm.intersection(b9.intersection(clamp.intersection(ctakes)))))\n",
    "    \n",
    "# def test_ensemble(analysis_type, corpus):\n",
    "    \n",
    "#     print('ensemble:')\n",
    "#     # Get mixed system_n\n",
    "#     ref_ann, data = get_metric_data(analysis_type, corpus)\n",
    "\n",
    "#     names = ['ctakes', 'biomedicus', 'metamap', 'clamp']\n",
    "#     if 'entity' in analysis_type: \n",
    "#         cols_to_keep = ['begin', 'end', 'note_id']\n",
    "#     elif 'cui' in analysis_type:\n",
    "#         cols_to_keep = ['cui', 'note_id']\n",
    "#     elif 'full' in analysis_type:\n",
    "#         cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "\n",
    "#     biomedicus = data[data[\"system\"]=='biomedicus'][cols_to_keep].copy()\n",
    "#     ctakes = data[data[\"system\"]=='ctakes'][cols_to_keep].copy()\n",
    "#     clamp = data[data[\"system\"]=='clamp'][cols_to_keep].copy()\n",
    "#     metamap = data[data[\"system\"]=='metamap'][cols_to_keep].copy()\n",
    "#     quickumls = data[data[\"system\"]=='quick_umls'][cols_to_keep].copy()\n",
    "\n",
    "#     print('systems:', len(biomedicus), len(clamp), len(ctakes), len(metamap), len(quickumls))\n",
    "\n",
    "#     b9 = set()\n",
    "#     cl = set()\n",
    "#     ct = set()\n",
    "#     mm = set()\n",
    "#     qu = set()\n",
    "\n",
    "#     b9 = df_to_set(get_sys_data('biomedicus', analysis_type, corpus), analysis_type)\n",
    "#     print(len(b9))\n",
    "\n",
    "#     ct = df_to_set(get_sys_data('ctakes', analysis_type, corpus), analysis_type)\n",
    "#     print(len(ct))\n",
    "\n",
    "#     cl = df_to_set(get_sys_data('clamp', analysis_type, corpus), analysis_type)\n",
    "#     print(len(cl))\n",
    "\n",
    "#     mm = df_to_set(get_sys_data('metamap', analysis_type, corpus), analysis_type)\n",
    "#     print(len(mm))\n",
    "\n",
    "#     qu = df_to_set(get_sys_data('quick_umls', analysis_type, corpus), analysis_type)\n",
    "#     print(len(qu))\n",
    "    \n",
    "#     print('various merges:')\n",
    "#     print(len(b9), len(cl), len(ct), len(mm), len(qu))\n",
    "#     print(len(mm.intersection(b9.intersection(cl.intersection(ct)))))\n",
    "#     print(len(mm.union(b9.intersection(cl.intersection(ct)))))\n",
    "#     print(len(mm.union(b9.union(cl.intersection(ct)))))\n",
    "#     print(len(mm.union(b9.union(cl.union(ct)))))\n",
    "#     print(len(b9.intersection(ct)))\n",
    "\n",
    "#     sys_m = b9.intersection(ct.intersection(qu))\n",
    "#     print('sys_m:', len(sys_m))\n",
    "\n",
    "#     # Get match merges:\n",
    "#     ct, _ = get_system_matches('ctakes', analysis_type, corpus)\n",
    "#     cl, _ = get_system_matches('clamp', analysis_type, corpus)\n",
    "#     b9, _ = get_system_matches('biomedicus', analysis_type, corpus)\n",
    "#     mm, _ = get_system_matches('metamap', analysis_type, corpus)\n",
    "#     qu, _ = get_system_matches('quick_umls', analysis_type, corpus)\n",
    "\n",
    "#     match_m = b9.intersection(ct.intersection(qu))\n",
    "#     print('match_m:', len(match_m))\n",
    "#     # reference df to set\n",
    "#     if 'entity' in analysis_type: \n",
    "#         cols_to_keep = ['end', 'start','file']\n",
    "#     elif 'cui' in analysis_type:\n",
    "#         cols_to_keep = ['value','file']\n",
    "#     elif 'full' in analysis_type:\n",
    "#         cols_to_keep = ['end', 'start', 'value','file']\n",
    "\n",
    "#     ref = df_to_set(ref_ann[cols_to_keep], analysis_type, 'ref')\n",
    "\n",
    "#     print('ref:', len(ref))\n",
    "\n",
    "#     # test difference:\n",
    "#     print('FP:', len(sys_m - match_m), len(sys_m - ref))\n",
    "#     assert len(sys_m - match_m) == len(sys_m - ref), 'FP mismatch'\n",
    "#     print('FN:', len(ref - match_m), len(ref - sys_m))\n",
    "#     assert len(ref - match_m) == len(ref - sys_m), 'FN mismatch'\n",
    "    \n",
    "#     test_metrics(ref, sys_m, match_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_offset(case, begin, end):\n",
    "#     directory_to_parse = \"/Users/gms/development/nlp/nlpie/data/clinical_trial2/data_in/\"\n",
    "#     os.chdir(directory_to_parse)\n",
    "#     with open(case + '.txt') as f:\n",
    "#         line = f.readline()\n",
    "#         cnt = 0 # count number lines\n",
    "#         l = 0 # get cummulative length\n",
    "#         while line:\n",
    "#             if begin <= l + len(line):\n",
    "#                 x=begin-l-cnt*(2+1) # handle position in file by line number and deal with extra windows chars\n",
    "#                 y=end-l-cnt*(2+1)\n",
    "#                 print('line off:', cnt, line[x-1:y])\n",
    "                \n",
    "\n",
    "#                 return line[x-1:y]\n",
    "#                 break\n",
    "#             else:\n",
    "#                 l += len(line)\n",
    "#                 line = f.readline()\n",
    "#                 cnt += 1\n",
    "\n",
    "# sql = \"select substring(sofa, 145, 160-145) from clinical_trial.sofas where corpus = 'clinical_trial2' and note_id = 'NCT00006180_criteria'\"  \n",
    "# #test = pd.read_sql(sql, params={\"sys.name\":sys.name}, con=engine) \n",
    "# test = pd.read_sql(sql, con=engine) \n",
    "# print(test)\n",
    "\n",
    "import pandas as pd\n",
    "# use to generate FN/FP\n",
    "def span_to_text(gold, sys, type='FN'):\n",
    "    \n",
    "    #gold = gold[gold['note_id']=='NCT00042380_criteria'].drop_duplicates()\n",
    "    #sys = sys[sys['note_id']=='NCT00042380_criteria']\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    if type == 'FN':\n",
    "        #iterate through gold as tuples\n",
    "        for g in gold.itertuples():\n",
    "            g_begin = int(g.begin)\n",
    "            g_end = int(g.end)\n",
    "            g_case = g.note_id\n",
    "            mMatch = False # flag when there is a match\n",
    "            for s in sys.itertuples():\n",
    "                s_begin = int(s.begin)\n",
    "                s_end = int(s.end)\n",
    "                s_case = s.note_id\n",
    "                if (((g_begin <= s_end and g_end >= s_begin) or \n",
    "                     (g_end >= s_begin and s_end >= g_begin)) and g_case == s_case):\n",
    "                        mMatch = True\n",
    "                        # create a df of TP here if wanted\n",
    "                        # break once a match is found\n",
    "                        break\n",
    "                        \n",
    "            # no match found, write list of tuple elements to df\n",
    "            if mMatch == False:\n",
    "                l = [g.begin,g.end,g.semtype,g.note_id,g.text,'FN']\n",
    "                fn = pd.DataFrame(columns=['begin','end','semtypes','note_id','text','value'], data=None)\n",
    "                fn.loc[fn.shape[0]] = l \n",
    "                frames = [df,fn]\n",
    "                #print(fn)\n",
    "                df = pd.concat(frames)\n",
    "                \n",
    "    else:    \n",
    "        for s in sys.itertuples(index=False):\n",
    "            s_begin = int(s.begin)\n",
    "            s_end = int(s.end)\n",
    "            s_case = s.note_id\n",
    "            mMatch = False\n",
    "            for g in gold.itertuples():\n",
    "                g_begin = int(g.begin)\n",
    "                g_end = int(g.end)\n",
    "                g_case = g.note_id\n",
    "                if (((g_begin <= s_end and g_end >= s_begin) or \n",
    "                     (g_end >= s_begin and s_end >= g_begin)) and g_case == s_case):\n",
    "                        mMatch = True\n",
    "                        break\n",
    "\n",
    "            if mMatch == False:\n",
    "                l = [s.begin,s.end,s.semtypes,s.note_id,s.text,s.type,s.system, 'FP']\n",
    "                fp = pd.DataFrame(columns=['begin','end','semtypes','note_id','text','type','system','value'], data=None)\n",
    "                fp.loc[fp.shape[0]] = l \n",
    "                frames = [df,fp]\n",
    "                df = pd.concat(frames)\n",
    "                #print(fp)\n",
    "       \n",
    "    return df\n",
    "\n",
    "# path/file\n",
    "gold='~/Desktop/clinical_trial2_reference.csv' \n",
    "system='~/development/nlp/nlpie/data/ensembling-u01/output/analytical_clinical_trial2.csv'\n",
    "analysis_type = 'FP'  # default for method is 'FN'\n",
    "\n",
    "#df = span_to_text(pd.read_csv(gold).drop_duplicates(),pd.read_csv(system).drop_duplicates(), analysis_type)\n",
    "#print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
