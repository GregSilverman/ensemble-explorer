{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n  Copyright (c) 2019 Regents of the University of Minnesota.\\n \\n  Licensed under the Apache License, Version 2.0 (the \"License\");\\n  you may not use this file except in compliance with the License.\\n  You may obtain a copy of the License at\\n \\n      http://www.apache.org/licenses/LICENSE-2.0\\n \\n  Unless required by applicable law or agreed to in writing, software\\n  distributed under the License is distributed on an \"AS IS\" BASIS,\\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n  See the License for the specific language governing permissions and\\n  limitations under the License.\\n '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "  Copyright (c) 2019 Regents of the University of Minnesota.\n",
    " \n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    " \n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License.\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pymysql\n",
    "import time \n",
    "import functools as ft\n",
    "import glob   \n",
    "import operator as op\n",
    "import shelve\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from pathlib import Path\n",
    "from itertools import combinations, product\n",
    "from sqlalchemy.engine import create_engine\n",
    "from datetime import datetime\n",
    "from ast import literal_eval\n",
    "from scipy import stats  \n",
    "from scipy.stats.mstats import gmean\n",
    "from pythonds.basic.stack import Stack\n",
    "from pythonds.trees.binaryTree import BinaryTree\n",
    "from collections import defaultdict\n",
    "from typing import List, Set, Tuple "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The cell below contains the configurable parameters to ensure that our ensemble explorer runs properaly on your machine. Please read carfully through steps (1-7) before running the rest of the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-1: CHOOSE YOUR CORPUS\n",
    "corpus = 'i2b2' #options include 'mipacq' OR 'i2b2'\n",
    "\n",
    "# STEP-2: CHOOSE YOUR DATA DIRECTORY; this is where output data will be saved on your machine\n",
    "data_directory = '/Users/mvh24011/Documents/Projects/Ensemble_Explorer/output/' \n",
    "\n",
    "# STEP-4: CHOOSE WHAT SYSTEM YOU'D LIKE TO RUN ON THE CORPUS\n",
    "rtype = 2      # OPTIONS INCLUDE: 1->Single systems; 2->Ensemble; 3->Tests; 4-> MM Test\n",
    "               # The Ensemble includes ['ctakes','biomedicus','clamp','metamap','quick_umls']\n",
    "    \n",
    "# STEP-5: CHOOSE WHAT TYPE OF ANALYSIS YOU'D LIKE TO RUN ON THE CORPUS\n",
    "analysis_type = 'entity' #options include 'entity' OR 'full'\n",
    "\n",
    "# STEP-(6A): ENTER DETAILS FOR ACCESSING MANUAL ANNOTATION DATA\n",
    "database_type = 'mysql+pymysql' # We use mysql+pymql as default\n",
    "database_username = 'gms'\n",
    "database_password = 'nej123' \n",
    "database_url = 'localhost' # HINT: use localhost if you're running database on your local machine\n",
    "database_name = 'test' # Enter database name\n",
    "table_name = 'i2b2_all' # Enter the table within the database where your reference data is stored\n",
    "\n",
    "# STEP-(6B): ENTER DIRECTORY FOR ACCESSING SYSTEM ANNOTATION DATA\n",
    "system_annotation = 'analytical_cui_i2b2_concepts.csv' # OPTIONS include 'analytical_cui_mipacq_concepts.csv' OR 'analytical_cui_i2b2_concepts.csv' \n",
    "\n",
    "# STEP-7: WE'LL CREATE AN 'SYSTEM OUTPUT'DIRECTORY FOR YOU INSIDE THE DIRECTORY YOU SPECIFIED IN (STEP 2)\n",
    "single_sys_dir = Path(data_directory + \"single_system_out\")\n",
    "single_sys_dir.mkdir(parents=True, exist_ok=True)\n",
    "dir_out = Path(data_directory + 'single_system_out/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config class for analysis\n",
    "class AnalysisConfig(object):\n",
    "    \"\"\"\n",
    "    Configuration object:\n",
    "    notes by test, full per corpus\n",
    "    paths by output, gold and system location\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self = self    \n",
    "       \n",
    "        self.systems = ['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls']\n",
    "        #self.systems = ['biomedicus', 'clamp', 'ctakes', 'metamap']\n",
    "        #self.systems = ['metamap']\n",
    "        #self.systems = ['biomedicus']\n",
    "        #self.systems = ['quick_umls']\n",
    "        self.data_dir = data_directory\n",
    "    \n",
    "    def corpus_config(self): \n",
    "        usys_data = system_annotation\n",
    "        ref_data = database_name+'.'+table_name\n",
    "        return usys_data, ref_data\n",
    "        \n",
    "\n",
    "analysisConf =  AnalysisConfig()\n",
    "usys, ref = analysisConf.corpus_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation class for UIMA systems\n",
    "class AnnotationSystems(object):\n",
    "    \"\"\"   \n",
    "    CAS XMI Annotations of interest\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\" \n",
    "        annotation base types\n",
    "        \"\"\"   \n",
    "        \n",
    "        self.biomedicus_dir = \"biomedicus_out/\"\n",
    "        self.biomedicus_types = [\"biomedicus.v2.UmlsConcept\"]\n",
    "                                  #\"biomedicus.v2.Negated\"\n",
    "                                 #\"biomedicus.v2.Acronym\",\n",
    "                                 #\"biomedicus.v2.DictionaryTerm\",\n",
    "                                 #\"biomedicus.v2.Historical\"]\n",
    "        \n",
    "        self.clamp_dir = \"clamp_out/\"\n",
    "        self.clamp_types = [\"edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA\"]\n",
    "                             #\"org.apache.ctakes.typesystem.type.syntax.ConllDependencyNode\",\n",
    "                             #\"edu.uth.clamp.nlp.typesystem.ClampRelationUIMA\"]    \n",
    "        \n",
    "        self.ctakes_dir = \"ctakes_out/\"\n",
    "        self.ctakes_types = ['ctakes_mentions_all']#\"org.apache.ctakes.typesystem.type.textspan.Sentence\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.DiseaseDisorderMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.MedicationMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.ProcedureMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.refsem.UmlsConcept\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.SignSymptomMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.AnatomicalSiteMention\"]\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.MeasurementAnnotation\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.EventMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.EntityMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.Predicate\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.syntax.WordToken\"]\n",
    "        \n",
    "        self.metamap_dir = \"metamap_out/\"\n",
    "        self.metamap_types = [#\"org.metamap.uima.ts.Utterance\",\n",
    "                              #\"org.metamap.uima.ts.Span\",\n",
    "                              #\"org.metamap.uima.ts.Phrase\"]\n",
    "                              \"org.metamap.uima.ts.Candidate\"]\n",
    "                              #\"org.metamap.uima.ts.CuiConcept\",\n",
    "                              #\"org.metamap.uima.ts.Negation\"]\n",
    "                \n",
    "        self.quick_umls_types = [#'concept']#,\n",
    "                                #'concept_cosine_length_false',\n",
    "                                #'concept_cosine_length_true',\n",
    "                                #'concept_cosine_score_false',\n",
    "                                #'concept_cosine_score_true',\n",
    "                                #'concept_dice_length_false',\n",
    "                                #'concept_dice_length_true',\n",
    "                                #'concept_dice_score_false',\n",
    "                                #'concept_dice_score_true',\n",
    "                                #'concept_jaccard_length_false',\n",
    "                                #'concept_jaccard_length_true',\n",
    "                                'concept_jaccard_score_False']\n",
    "                                #'concept_jaccard_score_true']\n",
    "                \n",
    "        \n",
    "        '''\n",
    "\n",
    "        self.biomedicus_dir = \"biomedicus_out/\"\n",
    "        self.biomedicus_types = [#\"biomedicus.v2.UmlsConcept\"]\n",
    "                                  #\"biomedicus.v2.Negated\"\n",
    "                                 \"biomedicus.v2.Acronym\",\n",
    "                                 \"biomedicus.v2.DictionaryTerm\",\n",
    "                                 \"biomedicus.v2.Historical\"]\n",
    "        \n",
    "        \n",
    "        self.clamp_dir = \"clamp_out/\"\n",
    "        #self.clamp_types = [#\"edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA\"]\n",
    "                             #\"org.apache.ctakes.typesystem.type.syntax.ConllDependencyNode\",\n",
    "                             #\"edu.uth.clamp.nlp.typesystem.ClampRelationUIMA\"]\n",
    "        \n",
    "        \n",
    "        self.ctakes_dir = \"ctakes_out/\"\n",
    "        self.ctakes_types = [\"org.apache.ctakes.typesystem.type.textspan.Sentence\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.DiseaseDisorderMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.MedicationMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.ProcedureMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.refsem.UmlsConcept\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.SignSymptomMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.AnatomicalSiteMention\"]\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.MeasurementAnnotation\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.EventMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.EntityMention\",\n",
    "                             \"org.apache.ctakes.typesystem.type.textsem.Predicate\",\n",
    "                             \"org.apache.ctakes.typesystem.type.syntax.WordToken\"]\n",
    "        \n",
    "        self.metamap_dir = \"metamap_out/\"\n",
    "        self.metamap_types = [\"org.metamap.uima.ts.Utterance\",\n",
    "                              \"org.metamap.uima.ts.Span\",\n",
    "                              \"org.metamap.uima.ts.Phrase\"]\n",
    "                              #\"org.metamap.uima.ts.Candidate\"]\n",
    "                              #\"org.metamap.uima.ts.CuiConcept\",\n",
    "                              #\"org.metamap.uima.ts.Negation\"]\n",
    "                              \n",
    "        '''\n",
    "       \n",
    "    def get_system_type(self, system):\n",
    "        \n",
    "        \"\"\"\n",
    "        return system types\n",
    "        \"\"\"\n",
    "        \n",
    "        if system == \"biomedicus\":\n",
    "            view = \"Analysis\"\n",
    "        else:\n",
    "            view = \"_InitialView\"\n",
    "\n",
    "        if system == 'biomedicus':\n",
    "            types = self.biomedicus_types\n",
    "            output = self.biomedicus_dir\n",
    "\n",
    "        elif system == 'clamp':\n",
    "            types = self.clamp_types\n",
    "            output = self.clamp_dir\n",
    "\n",
    "        elif system == 'ctakes':\n",
    "            types = self.ctakes_types\n",
    "            output = self.ctakes_dir\n",
    "\n",
    "        elif system == 'metamap':\n",
    "            types = self.metamap_types\n",
    "            output = self.metamap_dir\n",
    "        \n",
    "        elif system == \"quick_umls\":\n",
    "            types = self.quick_umls_types\n",
    "            output = None\n",
    "            \n",
    "        return types, view, output\n",
    "    \n",
    "annSys = AnnotationSystems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qumls = pd.read_csv('/Users/gms/development/nlp/nlpie/data/amicus-u01/output/qumls_similarity.csv')\n",
    "\n",
    "# #print(len(qumls))\n",
    "# #print(len(qumls.drop_duplicates()))\n",
    "\n",
    "# cols_to_keep = ['begin', 'end', 'note_id', 'cui', 'similarity', 'overlap', 'best_match', 'system', 'type'] \n",
    "\n",
    "# qumls = qumls[cols_to_keep].drop_duplicates()\n",
    "\n",
    "# #print(len(qumls))\n",
    "\n",
    "# #print(len(qumls[['overlap']=='score']))\n",
    "# print(len(qumls[(qumls.similarity=='jaccard') & (qumls.overlap=='length') & (qumls.best_match=='true')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='jaccard') & (qumls.overlap=='length') & (qumls.best_match=='false')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='jaccard') & (qumls.overlap=='score') & (qumls.best_match=='true')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='jaccard') & (qumls.overlap=='score') & (qumls.best_match=='false')].drop_duplicates()))\n",
    "\n",
    "# print(len(qumls[(qumls.similarity=='cosine') & (qumls.overlap=='length') & (qumls.best_match=='true')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='cosine') & (qumls.overlap=='length') & (qumls.best_match=='false')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='cosine') & (qumls.overlap=='score') & (qumls.best_match=='true')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='cosine') & (qumls.overlap=='score') & (qumls.best_match=='false')].drop_duplicates()))\n",
    "\n",
    "# print(len(qumls[(qumls.similarity=='dice') & (qumls.overlap=='length') & (qumls.best_match=='true')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='dice') & (qumls.overlap=='length') & (qumls.best_match=='false')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='dice') & (qumls.overlap=='score') & (qumls.best_match=='true')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='dice') & (qumls.overlap=='score') & (qumls.best_match=='false')].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0081', '0401']\n"
     ]
    }
   ],
   "source": [
    "def get_notes(analysis_type: str, corpus: str) -> List[str]:\n",
    "    \n",
    "    if 'test' in analysis_type:\n",
    "        # test set of notes\n",
    "        if corpus == 'mipacq':\n",
    "            notes = ['522412787',\n",
    "             '617637585',\n",
    "             '3307880735-8',\n",
    "             '9080688558',\n",
    "             '618370565',\n",
    "             '573718188',\n",
    "             '534584',\n",
    "             '60891',\n",
    "             '62620',\n",
    "             '616172834']\n",
    "            \n",
    "        elif corpus == 'i2b2':\n",
    "            print('TODO')\n",
    "        \n",
    "        print('TEST NOTES!')\n",
    "        #,\n",
    "#          '4130154172-4',\n",
    "#          '3580478614',\n",
    "#          '5024581165-5',\n",
    "#          '4486835700-9',\n",
    "#          '534828617',\n",
    "#          '8154986253',\n",
    "#          '533855209',\n",
    "#          '60118',\n",
    "#          '3537704982-3',\n",
    "#          '617637585',\n",
    "#          '60118',\n",
    "#          '9045889026',\n",
    "#          '8819868493-8',\n",
    "#          '533698',\n",
    "#          '535978760']\n",
    "     \n",
    "    else:\n",
    "        \n",
    "        if corpus == 'mipacq':\n",
    "        # these did not meet the minimal criteria for parsing\n",
    "            notes = [\"0595040941-0\",\n",
    "                    \"0778429553-0\",\n",
    "                    \"1014681675\",\n",
    "                    \"2889522952-2\",\n",
    "                    \"3080383448-5\",\n",
    "                    \"3300000926-3\",\n",
    "                    \"3360037185-3\",\n",
    "                    \"3580973392\",\n",
    "                    \"3627629462-3\",\n",
    "                    \"4323116051-4\",\n",
    "                    \"477704053-4\",\n",
    "                    \"528317073\",\n",
    "                    \"531702602\",\n",
    "                    \"534061073\",\n",
    "                    \"54832076\",\n",
    "                    \"5643725437-6\",\n",
    "                    \"5944412090-5\",\n",
    "                    \"6613169476-6\",\n",
    "                    \"7261075903-7\",\n",
    "                    \"7504944368-7\",\n",
    "                    \"7999462393-7\",\n",
    "                    \"8131081430\",\n",
    "                    \"8171084310\",\n",
    "                    \"8193787896\",\n",
    "                    \"8295055184-8\",\n",
    "                    \"8823185307-8\"]\n",
    "            \n",
    "        elif corpus == 'i2b2':\n",
    "            # these notes were not processed \n",
    "            notes = ['0081', \n",
    "                     '0401']\n",
    "\n",
    "        else:\n",
    "            notes = None\n",
    "            \n",
    "    return notes# training_notes\n",
    "print(get_notes('entity', corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np # access to Numpy from Python layer\n",
    "import math\n",
    "\n",
    "class Metrics(object):\n",
    "    \"\"\"\n",
    "    metrics class:\n",
    "    returns an instance with confusion matrix metrics\n",
    "    \"\"\"\n",
    "    def __init__(self, system_only, gold_only, gold_system_match, system_n, neither = 0):\n",
    "\n",
    "        self = self    \n",
    "        self.system_only = system_only\n",
    "        self.gold_only = gold_only\n",
    "        self.gold_system_match = gold_system_match\n",
    "        self.system_n = system_n\n",
    "        self.neither = neither\n",
    "        \n",
    "    def get_confusion_metrics(self, corpus = None, test = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        compute confusion matrix measures, as per  \n",
    "        https://stats.stackexchange.com/questions/51296/how-do-you-calculate-precision-and-recall-for-multiclass-classification-using-co\n",
    "        \"\"\"\n",
    "        cdef:\n",
    "            int TP, FP, FN\n",
    "            double TM\n",
    "\n",
    "        TP = self.gold_system_match\n",
    "        FP = self.system_only\n",
    "        FN = self.gold_only\n",
    "        \n",
    "        TM = TP/math.sqrt(self.system_n) # TigMetric\n",
    "       \n",
    "        if not test:\n",
    "            \n",
    "            if corpus == 'casi':\n",
    "                recall = TP/(TP + FN)\n",
    "                precision = TP/(TP + FP)\n",
    "                F = 2*(precision*recall)/(precision + recall)\n",
    "            else:\n",
    "                if self.neither == 0:\n",
    "                    confusion = [[0, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "                else:\n",
    "                    confusion = [[self.neither, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "                c = np.asarray(confusion)\n",
    "                recall = np.diag(c) / np.sum(c, axis = 1)\n",
    "                precision = np.diag(c) / np.sum(c, axis = 0)\n",
    "                F = 2*(precision*recall)/(precision + recall)\n",
    "        else:\n",
    "            precision = TP/(TP+FP)\n",
    "            recall = TP/(TP+FN)\n",
    "            F = 2*(precision*recall)/(precision + recall)\n",
    "        \n",
    "        # Tignanelli Metric\n",
    "        if FN == 0:\n",
    "            TP_FN_R = TP\n",
    "        elif FN > 0:\n",
    "            TP_FN_R = TP/FN\n",
    " \n",
    "        return F, recall, precision, TP, FP, FN, TP_FN_R, TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out(name: str, analysis_type: str, c: object):\n",
    "   \n",
    "    \"\"\"\n",
    "    write matching and reference-only sets to file for ease in merging combinations\n",
    "    \"\"\"\n",
    "    \n",
    "    # write output to file\n",
    "    dir_out = analysisConf.data_dir + 'single_system_out/'\n",
    "    with open(dir_out + name + '_' + analysis_type + '_' + c.corpus + '_matches.txt', 'w') as f:\n",
    "        for item in list(c.matches):\n",
    "            f.write(\"%s\\n\" % str(item))\n",
    "\n",
    "    # write to file\n",
    "    with open(dir_out + name + '_' + analysis_type + '_' + c.corpus + '_ref_only.txt', 'w') as f:\n",
    "        for item in list(c.false_negatives):\n",
    "            f.write(\"%s\\n\" % str(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%cython \n",
    "\n",
    "#from __main__ import write_out\n",
    "\n",
    "#import numpy as np # access to Numpy from Python layer\n",
    "def label_vector(doc: str, ann: List[int], labels: List[str]) -> np.array:\n",
    "\n",
    "    #print(ann, doc, labels)\n",
    "\n",
    "    v = np.zeros(doc)\n",
    "    labels = list(labels)\n",
    "    \n",
    "    for (i, lab) in enumerate(labels):\n",
    "        i += 1  # 0 is reserved for no label\n",
    "        idxs = [np.arange(a.begin, a.end) for a in ann if a.label == lab]\n",
    "            \n",
    "        idxs = [j for mask in idxs for j in mask]\n",
    "        v[idxs] = i\n",
    "\n",
    "    return v\n",
    "\n",
    "# test confusion matrix elements for vectorized annotation set; includes TN\n",
    "def confused(sys1, ann1):\n",
    "    TP = np.sum(np.logical_and(sys1 >= 1, ann1 == sys1 ))\n",
    "\n",
    "    # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "    TN = np.sum(np.logical_and(sys1 == 0, ann1 == 0))\n",
    "\n",
    "    # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "    FP = np.sum(np.logical_and(sys1 >= 1, ann1 == 0))\n",
    "\n",
    "    # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "    FN = np.sum(np.logical_and(sys1 == 0, ann1 >= 1))\n",
    "    \n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "\n",
    "def get_cooccurences(ref, sys, analysis_type: str, corpus: str, single_sys = True, name = None):\n",
    "    \"\"\"\n",
    "    get coocurences between system and reference; exact match; TODO: add relaxed\n",
    "    \"\"\"\n",
    "    # test cooccurences\n",
    "    class Coocurences(object):\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.ref_system_match = 0\n",
    "            self.ref_only = 0\n",
    "            self.system_only = 0\n",
    "            self.system_n = 0\n",
    "            self.ref_n = 0\n",
    "            self.matches = set()\n",
    "            self.false_negatives = set()\n",
    "            self.corpus = corpus\n",
    "            #self.cases = set(ref[\"file\"].tolist()) # cases to label \n",
    "\n",
    "    c = Coocurences()\n",
    "    \n",
    "    # test for converting to vectorization and i-o labeling\n",
    "    def test_io():\n",
    "        test = c.cases\n",
    "        if analysis_type == 'entity':\n",
    "            docs = [(x, len(open(\"/Users/gms/development/nlp/nlpie/data/amicus-u01/i2b2/source_data/test_data/\" + x + \".txt\", 'r').read())) for x in test]\n",
    "        elif analysis_type == 'full':\n",
    "            docs = [(x, len(open(\"/Users/gms/development/nlp/nlpie/data/amicus-u01/mipacq/source_data/source/\" + x + \".source\", 'r').read())) for x in test]\n",
    "\n",
    "        ann = ref.copy()\n",
    "        ann = ann.rename(index=str, columns={\"start\": \"begin\", \"file\": \"case\"}).copy()\n",
    "        cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "        if analysis_type == 'entity':\n",
    "            labels = [\"concept\"]\n",
    "            ann[\"label\"] = 'concept'\n",
    "            ann = ann[cols_to_keep].copy()\n",
    "        elif analysis_type == 'full':  \n",
    "            ann[\"label\"] = ann[\"value\"]\n",
    "            sys[\"label\"] = sys[\"cui\"]\n",
    "            labels = set(ref['value'].tolist())\n",
    "            print('labels', len(set(labels)))\n",
    "\n",
    "        sys_ = sys.rename(index=str, columns={\"note_id\": \"case\"}).copy()\n",
    "        \n",
    "        # need for enttity-only\n",
    "        if analysis_type == 'entity':\n",
    "            sys_[\"label\"] = 'concept'\n",
    "        \n",
    "        sys_ = sys_[cols_to_keep]\n",
    "       \n",
    "        tp = []\n",
    "        tn = []\n",
    "        fp = []\n",
    "        fn = []\n",
    "        cvals = []\n",
    "        out = []\n",
    "        t = []\n",
    "        d = defaultdict(list)\n",
    "        \n",
    "        for n in range(len(docs)):\n",
    "            a1 = [i for i in ann[ann[\"case\"] == docs[n][0]].copy().itertuples(index=False)]\n",
    "            s1 = [i for i in sys_[sys_[\"case\"] == docs[n][0]].copy().itertuples(index=False)]\n",
    "\n",
    "            ann1 = label_vector(docs[n][1], a1, labels)\n",
    "            sys1 = label_vector(docs[n][1], s1, labels)\n",
    "            \n",
    "            TP, TN, FP, FN = confused(sys1, ann1)\n",
    "            cvals.append([TP, TN, FP, FN])\n",
    "            \n",
    "                 \n",
    "            d['sys'].append(list([int(i) for i in sys1]))\n",
    "            d['oracle'].append(list([int(i) for i in ann1]))\n",
    "            d['case'].append(docs[n][0])\n",
    "            \n",
    "            '''\n",
    "            print(\"tn:\", np.intersect1d(np.where(ann1 == 0)[0], np.where(sys1 == 0)[0]),  \n",
    "                  \"tp:\", np.intersect1d(np.where(ann1 == 1)[0], np.where(sys1 == 1)[0]), \n",
    "                  \"fn:\", np.intersect1d(np.where(ann1 == 1)[0], np.where(sys1 == 0)[0]), \n",
    "                  \"fp:\", np.intersect1d(np.where(ann1 == 0)[0], np.where(sys1 == 1)[0]))\n",
    "            '''\n",
    "        d['labels'] = labels\n",
    "        \n",
    "        corp = shelve.open('/Users/gms/Desktop/' + sys.name + '.dat')\n",
    "        \n",
    "        for k in d:\n",
    "            corp[k] = d[k]\n",
    "        \n",
    "        corp.close()\n",
    "       \n",
    "        return cvals\n",
    "    \n",
    "    '''\n",
    "    TP, TN, FP, FN = np.sum(test_io(), axis=0)\n",
    "    F, recall, precision, TP, FP, FN, TP_FN_R, TM = Metrics(FP, FN, TP, len(sys), TN).get_confusion_metrics() #no TN\n",
    "    print('test_io():', TP, TN, FP, FN, F, recall, precision)\n",
    "    \n",
    "    '''\n",
    "    # non-vectorized:\n",
    "    \n",
    "    if corpus != 'casi':\n",
    "        if 'entity' in analysis_type and single_sys: # mipacq n -> 16793\n",
    "            cols_to_keep = ['begin', 'end', 'note_id']\n",
    "            sys = sys[cols_to_keep].drop_duplicates()\n",
    "            ref = ref[['start', 'end', 'file']].drop_duplicates()\n",
    "            sys.name = name\n",
    "        elif 'cui' in analysis_type and single_sys: # mipacq n -> 10799\n",
    "            cols_to_keep = ['cui', 'note_id']\n",
    "            sys = sys[cols_to_keep].drop_duplicates()\n",
    "            # do not overestimate FP\n",
    "            sys = sys[~sys['cui'].isnull()] \n",
    "            ref = ref[['value', 'file']].drop_duplicates()\n",
    "            ref = ref[~ref['value'].isnull()]\n",
    "            sys.name = name\n",
    "        elif 'full' in analysis_type and single_sys: # mipacq n -> 17393\n",
    "            cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "            sys = sys[cols_to_keep].drop_duplicates()\n",
    "            sys = sys[~sys['cui'].isnull()]\n",
    "            ref = ref[['start', 'end', 'value', 'file']].drop_duplicates()\n",
    "            ref = ref[~ref['value'].isnull()]\n",
    "            sys.name = name\n",
    "\n",
    "        # matches via inner join\n",
    "        matches = pd.merge(sys, ref, how = 'inner', left_on=['begin','end','note_id'], right_on = ['start','end','file']) \n",
    "        # reference-only via left outer join\n",
    "        fn = pd.merge(ref, sys, how = 'left', left_on=['start','end','file'], right_on = ['begin','end','note_id']) \n",
    "\n",
    "        fn = fn[fn['begin'].isnull()] # get as outer join with no match\n",
    "\n",
    "        if 'entity' in analysis_type and single_sys:\n",
    "            cols_to_keep = ['start', 'end', 'file']\n",
    "        else:\n",
    "            cols_to_keep = ['start', 'end', 'value', 'file']\n",
    "\n",
    "\n",
    "        matches = matches[cols_to_keep]\n",
    "        fn = fn[cols_to_keep]\n",
    "\n",
    "        # use for metrics \n",
    "        c.matches = c.matches.union(df_to_set(matches, analysis_type, 'ref'))\n",
    "        c.false_negatives = c.false_negatives.union(df_to_set(fn, analysis_type, 'ref'))\n",
    "        c.ref_system_match = len(c.matches)\n",
    "        c.system_only = len(sys) - len(c.matches)\n",
    "        c.system_n = len(sys)\n",
    "        c.ref_n = len(ref)\n",
    "        c.ref_only = len(c.false_negatives)\n",
    "        \n",
    "    else:\n",
    "        #matches = df_to_set(pd.read_sql(\"select `case` from test.amia_2019_analytical_v where overlap = 1;\", con=engine), 'entity', 'sys', 'casi')\n",
    "        \n",
    "        \n",
    "        sql = \"select `case` from test.amia_2019_analytical_v where overlap = 1 and `system` = %(sys.name)s\"  \n",
    "        #ref_ann = pd.read_sql(sql, params={\"training_notes\":training_notes}, con=engine)\n",
    "        \n",
    "        matches = pd.read_sql(sql, params={\"sys.name\":sys.name}, con=engine)\n",
    "        \n",
    "        sql = \"select `case` from test.amia_2019_analytical_v where (overlap = 0 or overlap is null) and `system` = %(sys.name)s\"  \n",
    "        #ref_ann = pd.read_sql(sql, params={\"training_notes\":training_notes}, con=engine)\n",
    "        \n",
    "        fn = pd.read_sql(sql, params={\"sys.name\":sys.name}, con=engine)\n",
    "        \n",
    "        c.matches = df_to_set(matches, 'entity', 'sys', 'casi')\n",
    "        c.fn = df_to_set(fn, 'entity', 'sys', 'casi')\n",
    "        c.ref_system_match = len(c.matches)\n",
    "        c.system_only = len(sys) - len(c.matches)\n",
    "        c.system_n = len(matches) + len(fn)\n",
    "        c.ref_n = len(matches) + len(fn)\n",
    "        c.ref_only = len(fn)\n",
    "        \n",
    "        print('cooc', c.ref_system_match, c.system_only, c.ref_n, c.ref_n, c.ref_only)\n",
    "        \n",
    "    # sanity check\n",
    "    if len(ref) - c.ref_system_match < 0:\n",
    "        print('Error: ref_system_match > len(ref)!')\n",
    "    if len(ref) != c.ref_system_match + c.ref_only:\n",
    "        print('Error: ref count mismatch!')\n",
    "   \n",
    "    # save TP/FN\n",
    "    if single_sys and corpus != 'casi':\n",
    "        print(analysis_type)\n",
    "        write_out(sys.name, analysis_type, c)\n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging test for i-o labeled data\n",
    "import numpy as np\n",
    "import shelve\n",
    "# load shelve\n",
    "def read_shelve():\n",
    "        corp = shelve.open('/Users/gms/Desktop/test.dat')\n",
    "        #print(corp['case'])\n",
    "        \n",
    "        return corp\n",
    "        \n",
    "#test = read_shelve()\n",
    "\n",
    "#l0 = list(t0)\n",
    "#l1 = list(t1)\n",
    "\n",
    "def test_merge_vector(test):\n",
    "    # get sample for testing\n",
    "    for case in test['case'][3:5]:\n",
    "        for i in range(len(test['case'][3:5])):\n",
    "            if i == 3:\n",
    "                t0 = test['oracle'][3][0:750]\n",
    "            else:\n",
    "                t1 = test['oracle'][4][0:750]\n",
    "\n",
    "            #print('case:', case, test['sys'][i], test['oracle'][i], confused(np.array(test['sys'][i]), np.array(test['oracle'][i])))\n",
    "        #print(t0, t1)\n",
    "\n",
    "    t0 = np.array(test['oracle'][3][0:750])\n",
    "    t1 = np.array(test['oracle'][5][0:750])\n",
    "\n",
    "    l0 = list(t0)\n",
    "    l1 = list(t1)\n",
    "    \n",
    "    #l0 = [0, 4, 1, 4, 4, 0, 0, 0, 8, 0, 0] \n",
    "    #l1 = [0, 1, 4, 4, 0, 0, 0, 0, 8, 8, 8]\n",
    "\n",
    "    def intersection(lst1, lst2): \n",
    "        out = list()\n",
    "        if isinstance(lst1, set) and isinstance(lst2, set):\n",
    "            out = (set(lst1) & set(lst2))\n",
    "        elif isinstance(lst1, set) and isinstance(lst2, np.int64):\n",
    "            out = (set(lst1) & set([lst2]))\n",
    "        elif isinstance(lst1, np.int64) and isinstance(lst2, set):\n",
    "            out = (set([lst1]) & set(lst2))\n",
    "        elif isinstance(lst1, np.int64) and isinstance(lst2, np.int64):\n",
    "            out = (set([lst1]) & set([lst2]))\n",
    "        #if len(out) > 1:\n",
    "        return out\n",
    "        #elif len(out) == 1:\n",
    "        #    return out[0]\n",
    "        #else:\n",
    "        #    return 0\n",
    "\n",
    "    def union(lst1, lst2): \n",
    "        out = list()\n",
    "        if isinstance(lst1, set) and isinstance(lst2, set):\n",
    "            out = set(lst1) | set(lst2)\n",
    "        elif isinstance(lst1, set) and isinstance(lst2, np.int64):\n",
    "            out = set(lst1) | set([lst2])\n",
    "        elif isinstance(lst1, np.int64) and isinstance(lst2, set):\n",
    "            out = set([lst1]) | set(lst2)\n",
    "        elif isinstance(lst1, np.int64) and isinstance(lst2, np.int64):\n",
    "            out = set([lst1]) | set([lst2])\n",
    "        #if len(out) == 1:\n",
    "        #    #out = out[0]\n",
    "        return out\n",
    "\n",
    "    # union and intersect\n",
    "    def umerges(l0, l1):\n",
    "        #un = [0]*len(l0)\n",
    "        #for i in range(len(l0)):\n",
    "        #    un[i] = union(l0[i], l1[i])\n",
    "\n",
    "        return [union(l0[i], l1[i]) for i in range(len(l0))]\n",
    "\n",
    "    %timeit un = umerges(l0, l1)\n",
    "    \n",
    "    x = umerges(l0, l1)\n",
    "\n",
    "    #l2 = [1, {1, 4}, {3}, {2, 4}, {1}, 0, 2, 3, {0, 8}, {1, 8}]\n",
    "    \n",
    "    #print(umerges(x, l2))\n",
    "    \n",
    "    def imerges(l0, l1):\n",
    "        #inter = [0]*len(l0)\n",
    "        #for i in range(len(l0)):\n",
    "        \n",
    "\n",
    "        return [intersection(l0[i], l1[i]) for i in range(len(l0))]\n",
    "    \n",
    "    %timeit inter = imerges(l0, l1)\n",
    "    \n",
    "    '''\n",
    "    union = [\n",
    "        ( [set(x) | set(y)] if isinstance(x, list) and isinstance(y, list)\n",
    "          else [set(x) | set([y])] if isinstance(x, list) and isinstance(y, int)\n",
    "          else [set([x]) | set(y)] if isinstance(x, int) and isinstance(y, list)\n",
    "          else [set([x]) | set([y])])\n",
    "\n",
    "         for x, y in zip(l0, l1)\n",
    "    ]\n",
    "\n",
    "    # unpack map object\n",
    "    #*y, = list(map(list, zip(*union)))\n",
    "    #%timeit list(map(list, zip(*union)))\n",
    "\n",
    "    intersection = [\n",
    "        ( [set(x) & set(y)] if isinstance(x, list) and isinstance(y, list)\n",
    "          else [set(x) & set([y])] if isinstance(x, list) and isinstance(y, int)\n",
    "          else [set([x]) & set(y)] if isinstance(x, int) and isinstance(y, list)\n",
    "          else [set([x]) & set([y])])\n",
    "          for x, y in zip(l0, l1)\n",
    "\n",
    "    ]\n",
    "\n",
    "    #*x, = list(map(list, zip(*intersection)))\n",
    "    #%timeit list(map(list, zip(*intersection)))\n",
    "    '''\n",
    "# test_merge_vector(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nstart = time.perf_counter()\\nz = *map(list, zip(*umerge(l0, l1))),\\n#print(*map(list, zip(*imerge(l0, l1))),)\\nelapsed = (time.perf_counter() - start)\\nprint('time 1:', elapsed)\\n%timeit  *map(list, zip(*umerge(l0, l1))),\\n\\n*map(list, zip(*umerge(z[0], l1))),\\n\\n\\nstart = time.perf_counter()\\nz = *map(list, zip(*imerge(l0, l1))),\\nelapsed = (time.perf_counter() - start)\\nprint('time 2:', elapsed)\\n%timeit *map(list, zip(*imerge(l0, l1))),\\n\\n%timeit *map(list, zip(*imerge(z[0], l1))),\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%cython\n",
    "import numpy as np # access to Numpy from Python layer\n",
    "import time\n",
    "from __main__ import read_shelve\n",
    "#test = read_shelve()\n",
    "# t0 = np.array(test['oracle'][3][0:750])\n",
    "# t1 = np.array(test['oracle'][5][0:750])\n",
    "\n",
    "# l0 = list(t0)\n",
    "# l1 = list(t1)\n",
    "\n",
    "#union = [\n",
    "#    [list((set(x) | set(y)))]\n",
    "#      for x, y in zip(t0, t1)\n",
    "#]\n",
    "\n",
    "def imerge(l0, l1):\n",
    "\n",
    "    return [\n",
    "        ( [\n",
    "            set(x) & set(y)] if isinstance(x, list) and  isinstance(y, list)\n",
    "            else [set(x) & set([y])] if isinstance(x, list) and  isinstance(y, np.int64)\n",
    "            else [set(x) & y] if isinstance(x, list) and isinstance(y, set)\n",
    "            else [x & y] if isinstance(x, set) and isinstance(y, set)\n",
    "            else [x & set(y)] if isinstance(x, set) and isinstance(y, list)\n",
    "            else [x & set([y])] if isinstance(x, set) and isinstance(y, np.int64)\n",
    "            else [set([x]) & set(y)] if isinstance(x, np.int64) and  isinstance(y, list)\n",
    "            else [set([x]) & y] if isinstance(x, np.int64) and isinstance(y, set)\n",
    "            else [set([x]) & set([y])])\n",
    "        for x, y in zip(l0, l1)\n",
    "    ]\n",
    "\n",
    "def umerge(l0, l1):\n",
    "\n",
    "    return [\n",
    "        ( [\n",
    "            set(x) | set(y)] if isinstance(x, list) and  isinstance(y, list)\n",
    "            else [set(x) | set([y])] if isinstance(x, list) and  isinstance(y, np.int64)\n",
    "            else [set(x) | y] if isinstance(x, list) and isinstance(y, set)\n",
    "            else [x | y] if isinstance(x, set) and isinstance(y, set)\n",
    "            else [x | set(y)] if isinstance(x, set) and isinstance(y, list)\n",
    "            else [x | set([y])] if isinstance(x, set) and isinstance(y, np.int64)\n",
    "            else [set([x]) | y] if isinstance(x, np.int64) and isinstance(y, set)\n",
    "            else [set([x]) | set(y)] if isinstance(x, np.int64) and  isinstance(y, list)\n",
    "            else [set([x]) | set([y])])\n",
    "        for x, y in zip(l0, l1)\n",
    "    ]\n",
    "\n",
    "'''\n",
    "start = time.perf_counter()\n",
    "z = *map(list, zip(*umerge(l0, l1))),\n",
    "#print(*map(list, zip(*imerge(l0, l1))),)\n",
    "elapsed = (time.perf_counter() - start)\n",
    "print('time 1:', elapsed)\n",
    "%timeit  *map(list, zip(*umerge(l0, l1))),\n",
    "\n",
    "*map(list, zip(*umerge(z[0], l1))),\n",
    "\n",
    "\n",
    "start = time.perf_counter()\n",
    "z = *map(list, zip(*imerge(l0, l1))),\n",
    "elapsed = (time.perf_counter() - start)\n",
    "print('time 2:', elapsed)\n",
    "%timeit *map(list, zip(*imerge(l0, l1))),\n",
    "\n",
    "%timeit *map(list, zip(*imerge(z[0], l1))),\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_data(training_notes: List[str], analysis_type: str, corpus: str):\n",
    "    engine_request = str(database_type)+'://'+database_username+':'+database_password+\"@\"+database_url+'/'+database_name\n",
    "    engine = create_engine(engine_request, pool_pre_ping=True, pool_size=20, max_overflow=30)\n",
    "   \n",
    "    usys_file, ref_table = AnalysisConfig().corpus_config()\n",
    "    systems = AnalysisConfig().systems\n",
    "    \n",
    "    sys_ann = pd.read_csv(analysisConf.data_dir + usys_file, dtype={'note_id': str})\n",
    "    \n",
    "    if 'test' not in analysis_type:\n",
    "        sql = \"SELECT * FROM \" + ref_table + \" where file not in %(training_notes)s\"  \n",
    "        sys_ann = sys_ann[~sys_ann['note_id'].isin(training_notes)]\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        sql = \"SELECT * FROM \" + ref_table + \" where file in %(training_notes)s\"  \n",
    "        sys_ann = sys_ann[sys_ann['note_id'].isin(training_notes)]\n",
    "    \n",
    "    ref_ann = pd.read_sql(sql, params={\"training_notes\":training_notes}, con=engine)\n",
    "    sys_ann = sys_ann.drop_duplicates()\n",
    "    \n",
    "    return ref_ann, sys_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "def geometric_mean(metrics):\n",
    "    \"\"\"\n",
    "    1. Get rank average of F1, TP/FN, TM\n",
    "        http://www.datasciencemadesimple.com/rank-dataframe-python-pandas-min-max-dense-rank-group/\n",
    "        https://stackoverflow.com/questions/46686315/in-pandas-how-to-create-a-new-column-with-a-rank-according-to-the-mean-values-o?rq=1\n",
    "    2. Take geomean of 2.\n",
    "        https://stackoverflow.com/questions/42436577/geometric-mean-applied-on-row\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.DataFrame() \n",
    "\n",
    "    metrics['F1 rank']=metrics['F'].rank(ascending=0,method='average')\n",
    "    metrics['TP/FN rank']=metrics['TP/FN'].rank(ascending=0,method='average')\n",
    "    metrics['TM rank']=metrics['TM'].rank(ascending=0,method='average')\n",
    "    metrics['Gmean'] = gmean(metrics.iloc[:,-3:],axis=1)\n",
    "\n",
    "    return metrics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metrics(analysis_type: str, corpus: str, single_sys = None):\n",
    "    start = time.time()\n",
    "\n",
    "    systems = AnalysisConfig().systems\n",
    "    metrics = pd.DataFrame()\n",
    "\n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    ref_ann, sys_ann = get_metric_data(training_notes, analysis_type, corpus)\n",
    "    \n",
    "    for sys in systems:\n",
    "            types, _, _ = AnnotationSystems().get_system_type(sys) # system types for iterable\n",
    "            for t in types:\n",
    "                print(t)\n",
    "                system = pd.DataFrame()\n",
    "                \n",
    "                system_annotations = sys_ann.copy()\n",
    "                \n",
    "                system = system_annotations[system_annotations['type'] == str(t)]\n",
    "            \n",
    "                if sys == 'quick_umls':\n",
    "                    system = system[system.similarity.astype(float) >= 0.75]\n",
    "            \n",
    "                system = system.drop_duplicates()\n",
    "                system.name = sys\n",
    "                \n",
    "                c = get_cooccurences(ref_ann, system, analysis_type, corpus, True, system.name) # get matches, FN, etc.\n",
    "                \n",
    "                print(c.ref_n, c.ref_only, c.system_n, c.system_only, c.ref_system_match)\n",
    "                \n",
    "            if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "                F, recall, precision, TP, FP, FN, TP_FN_R, TM = Metrics(c.system_only, c.ref_only, c.ref_system_match, c.system_n).get_confusion_metrics(corpus)\n",
    "                \n",
    "                if corpus == 'casi':\n",
    "                    if sys == 'biomedicus':\n",
    "                        t = 'biomedicus.v2.Acronym'\n",
    "                        \n",
    "                    d = {'system': sys, \n",
    "                         'type': t, \n",
    "                         'F': F, \n",
    "                         'precision': precision, \n",
    "                         'recall': recall, \n",
    "                         'FN': FN, \n",
    "                         'TP/FN': TP_FN_R,\n",
    "                         'n_gold': c.ref_n, \n",
    "                         'n_sys': c.system_n, \n",
    "                         'TM': TM}\n",
    "                else:\n",
    "                    d = {'system': sys, \n",
    "                         'type': t, \n",
    "                         'F': F[1], \n",
    "                         'precision': precision[1], \n",
    "                         'recall': recall[1], \n",
    "                         'TP': TP, \n",
    "                         'FN': FN, \n",
    "                         'FP': FP, \n",
    "                         'TP/FN': TP_FN_R,\n",
    "                         'n_gold': c.ref_n, \n",
    "                         'n_sys': c.system_n, \n",
    "                         'TM': TM}\n",
    "\n",
    "                data = pd.DataFrame(d,  index=[0])\n",
    "                metrics = pd.concat([metrics, data], ignore_index=True)\n",
    "                metrics.drop_duplicates(keep='last', inplace=True)\n",
    "            else:\n",
    "                print(\"NO EXACT MATCHES FOR\", t)\n",
    "            elapsed = (time.time() - start)\n",
    "            print(\"elapsed:\", sys, elapsed)\n",
    "     \n",
    "    elapsed = (time.time() - start)\n",
    "    print(geometric_mean(metrics))\n",
    "    \n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    \n",
    "    if single_sys is None:\n",
    "        file_name = 'metrics_'\n",
    "    \n",
    "    metrics.to_csv(analysisConf.data_dir + corpus + '_' + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "    \n",
    "    print(\"total elapsed time:\", elapsed) \n",
    "\n",
    "# use to iterate through mm scores\n",
    "def generate_metrics_mm(analysis_type: str, corpus: str, single_sys = None):\n",
    "    start = time.time()\n",
    "    from pandas.api.types import is_numeric_dtype\n",
    "    #systems = [\"biomedicus\",\"ctakes\",\"metamap\",\"clamp\",\"quick_umls\"]\n",
    "    systems = AnalysisConfig().systems\n",
    "    #systems = [\"quick_umls\"]\n",
    "    metrics = pd.DataFrame()\n",
    "\n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    ref_ann, sys_ann = get_metric_data(training_notes, analysis_type, corpus)\n",
    "    \n",
    "    sys_ann = sys_ann[(sys_ann.score.notnull()) & (sys_ann['system'] == 'metamap')]\n",
    "    sys_ann = sys_ann[['begin', 'end', 'note_id', 'system', 'score']].drop_duplicates()\n",
    "    sys_ann.score = sys_ann.score.astype(int)\n",
    "    \n",
    "    for sys in systems:\n",
    "        types, _, _ = AnnotationSystems().get_system_type(sys) # system types for iterable\n",
    "        for t in types:\n",
    "            print(t)\n",
    "\n",
    "            for i in range(500, 1050, 50): \n",
    "\n",
    "                sys_ann = sys_ann[(sys_ann[\"score\"] >= i)].copy()\n",
    "\n",
    "                sys_ann.name = sys + str(i)\n",
    "\n",
    "                c = get_cooccurences(ref_ann, sys_ann, analysis_type, corpus, True, sys_ann.name) # get matches, FN, etc.\n",
    "\n",
    "                print(c.ref_n, c.ref_only, c.system_n, c.system_only, c.ref_system_match)\n",
    "\n",
    "                #print(i, len(system))\n",
    "\n",
    "                if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "                    F, recall, precision, TP, FP, FN, TP_FN_R, TM = Metrics(c.system_only, c.ref_only, c.ref_system_match, c.system_n).get_confusion_metrics()\n",
    "                    d = {'system': sys + '_score_' + str(i), \n",
    "                         'type': t, \n",
    "                         'F': F[1], \n",
    "                         'precision': precision[1], \n",
    "                         'recall': recall[1], \n",
    "                         'TP': TP, \n",
    "                         'FN': FN, \n",
    "                         'FP': FP, \n",
    "                         'TP/FN': TP_FN_R,\n",
    "                         'n_gold': c.ref_n, \n",
    "                         'n_sys': c.system_n, \n",
    "                         'TM': TM}\n",
    "\n",
    "                    data = pd.DataFrame(d,  index=[0])\n",
    "                    metrics = pd.concat([metrics, data], ignore_index=True)\n",
    "                    metrics.drop_duplicates(keep='last', inplace=True)\n",
    "                else:\n",
    "                    print(\"NO EXACT MATCHES FOR\", t)\n",
    "                elapsed = (time.time() - start)\n",
    "                print(\"elapsed:\", sys, elapsed)\n",
    "     \n",
    "    elapsed = (time.time() - start)\n",
    "    print(geometric_mean(metrics))\n",
    "    \n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    # UIMA or QuickUMLS\n",
    "    if single_sys is None:\n",
    "        file_name = 'mm_metrics_'\n",
    "    metrics.to_csv(analysisConf.data_dir + corpus + '_' + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "    \n",
    "    print(\"total elapsed time:\", elapsed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in system matches from file\n",
    "\n",
    "def get_ref_n(analysis_type: str, corpus) -> int:\n",
    "    \n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    ref_ann, _ = get_metric_data(training_notes, analysis_type, corpus)\n",
    "    \n",
    "    if corpus == 'casi':\n",
    "        return len(ref_ann)\n",
    "        \n",
    "    else:\n",
    "        # do not overestimate fn\n",
    "        if 'entity' in analysis_type:\n",
    "            ref_ann = ref_ann[['start', 'end', 'file']].drop_duplicates()\n",
    "        elif 'cui' in analysis_type:\n",
    "            ref_ann = ref_ann[['value', 'file']].drop_duplicates()\n",
    "        elif 'full' in analysis_type:\n",
    "            ref_ann = ref_ann[['start', 'end', 'value', 'file']].drop_duplicates()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        ref_n = len(ref_ann.drop_duplicates())\n",
    "\n",
    "        return ref_n\n",
    "\n",
    "def get_sys_data(system: str, analysis_type: str, corpus: str) -> int: \n",
    "   \n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    _, data = get_metric_data(training_notes, analysis_type, corpus)\n",
    "    \n",
    "    out = data[data['system'] == system].copy()\n",
    "    \n",
    "    if corpus == 'casi':\n",
    "        cols_to_keep = ['case', 'overlap'] \n",
    "        #cols_to_keep = ['case', 'begin', 'end'] \n",
    "        out = out[cols_to_keep].drop_duplicates()\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    else:\n",
    "        out = data[data['system']== system].copy()\n",
    "\n",
    "        if system == 'quick_umls':\n",
    "            out = out[(out.similarity.astype(float) >= 0.75) & (out[\"type\"] == 'concept_jaccard_score_False')]\n",
    "\n",
    "        if 'entity' in analysis_type:\n",
    "            cols_to_keep = ['begin', 'end', 'note_id']\n",
    "        elif 'cui' in analysis_type:\n",
    "            cols_to_keep = ['cui', 'note_id']\n",
    "        elif 'full' in analysis_type:\n",
    "            cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "\n",
    "        out = out[cols_to_keep]\n",
    "\n",
    "        return out.drop_duplicates()\n",
    "\n",
    "def get_system_matches(system: str, analysis_type: str, corpus: str):\n",
    "   \n",
    "    if corpus == 'casi':\n",
    "        \n",
    "        sql = \"select `case`, overlap from test.amia_2019_cases where overlap = 1 and `system` = %(system)s\"  \n",
    "        #ref_ann = pd.read_sql(sql, params={\"training_notes\":training_notes}, con=engine)\n",
    "        \n",
    "        data_matches = df_to_set(pd.read_sql(sql, params={\"system\":system}, con=engine), 'entity', 'sys', 'casi')\n",
    "        \n",
    "        sql = \"select `case`, overlap from test.amia_2019_cases where (overlap = 0 or overlap is null) and `system` = %(system)s\"  \n",
    "        #ref_ann = pd.read_sql(sql, params={\"training_notes\":training_notes}, con=engine)\n",
    "        \n",
    "        data_fn = df_to_set(pd.read_sql(sql, params={\"system\":system}, con=engine), 'entity', 'sys', 'casi')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        dir_test = analysisConf.data_dir + 'single_system_out/'\n",
    "\n",
    "        file = dir_test + system + '_' + analysis_type + '_' + corpus + '_matches.txt'\n",
    "        data_matches = set(literal_eval(line.strip()) for line in open(file))\n",
    "\n",
    "        file = dir_test + system + '_' + analysis_type + '_' + corpus + '_ref_only.txt'\n",
    "        data_fn = set(literal_eval(line.strip()) for line in open(file)) #{ f for f in file.readlines() }\n",
    "\n",
    "    return data_matches, data_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to generate QuickUMLS system annotations (must run from shell):\n",
    "\n",
    "import os, glob\n",
    "from client import get_quickumls_client\n",
    "from quickumls import QuickUMLS\n",
    "import pandas as pd\n",
    "\n",
    "directory_to_parse = '/Users/gms/development/nlp/nlpie/data/amicus-u01/mipacq/data_in/'\n",
    "quickumls_fp = '/Users/gms/development/nlp/engines_misc_tools/QuickUMLS/data/'\n",
    "os.chdir(directory_to_parse)\n",
    "\n",
    "#similarity = ['dice', 'cosine', 'jaccard', 'overlap']\n",
    "similarity = ['jaccard']\n",
    "overlapping_criteria = ['score', 'length']\n",
    "\n",
    "for s in similarity:\n",
    "    for o in overlapping_criteria:\n",
    "        #matcher = get_quickumls_client(similarity_name)\n",
    "        matcher = QuickUMLS(quickumls_fp=quickumls_fp, overlapping_criteria, threshold=0.7, window=5, similarity_name=s)\n",
    "        test = pd.DataFrame()\n",
    "        for fname in glob.glob(directory_to_parse + '*.txt'):\n",
    "            t = os.path.basename(fname)\n",
    "            u = t.split('.')[0]\n",
    "            with open(directory_to_parse + u + '.txt') as f:\n",
    "                f1 = f.read()\n",
    "                out = matcher.match(f1, best_match=True, ignore_syntax=False)\n",
    "                for i in out:\n",
    "                    i[0]['note_id'] = u\n",
    "                    frames = [ test, pd.DataFrame(i[0], index = [0]) ]\n",
    "                    test = pd.concat(frames, ignore_index=True)\n",
    "        test['system'] = 'quick_umls'\n",
    "        test['similarity'] = s\n",
    "        test['overlap'] = o\n",
    "        test['type'] = 'concept'\n",
    "        test['note_id'] = u\n",
    "        testt['best_match'] = 'true'\n",
    "        temp = test.rename(columns={'start': 'begin'}).copy()\n",
    "        print(temp.tail())\n",
    "\n",
    "        temp.to_csv('/Users/gms/development/nlp/nlpie/data/amicus-u01/output/qumls.csv', mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERATE merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetTotals(object):\n",
    "    \"\"\" \n",
    "    returns an instance with merged match set numbers using either union or intersection of elements in set \n",
    "    \"\"\"\n",
    "    def __init__(self, ref_n, sys_n, match_set):\n",
    "\n",
    "        self = self    \n",
    "        self.ref_ann = ref_n\n",
    "        self.sys_n = sys_n\n",
    "        self.match_set = match_set\n",
    "\n",
    "    def get_ref_sys(self):\n",
    "\n",
    "        ref_only = self.ref_ann - len(self.match_set)\n",
    "        sys_only = self.sys_n - len(self.match_set)\n",
    "\n",
    "        return ref_only, sys_only, len(self.match_set), self.match_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def merge_eval(ref_only: int, system_only: int, ref_system_match: int, matches, system_n: int, ref_n: int):\n",
    "def merge_eval(ref_only: int, system_only: int, ref_system_match: int, system_n: int, ref_n: int) -> dict:\n",
    "    \"\"\"\n",
    "    Generate confusion matrix params\n",
    "    :params: ref_only, system_only, reference_system_match -> sets\n",
    "    matches, system_n, reference_n -> counts\n",
    "    :return: dictionary object\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if ref_only + ref_system_match != ref_n:\n",
    "        print('ERROR!')\n",
    "\n",
    "    # get evaluation metrics\n",
    "    d = {}\n",
    "    \n",
    "    F, recall, precision, TP, FP, FN, TP_FN_R, TM  = Metrics(system_only, ref_only, ref_system_match, system_n).get_confusion_metrics()\n",
    "\n",
    "    d = {\n",
    "         'F': F[1], \n",
    "         'precision': precision[1], \n",
    "         'recall': recall[1], \n",
    "         'TP': TP, \n",
    "         'FN': FN, \n",
    "         'FP': FP, \n",
    "         'TP/FN': TP_FN_R,\n",
    "         'n_gold': ref_n, \n",
    "         'n_sys': system_n, \n",
    "         'TM': TM\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if system_n - FP != TP:\n",
    "        print('inconsistent system n!')\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUERY TO VALIDATE qumls system counts\n",
    "select count(*), type from (select cui, begin, end, note_id, type from\n",
    "(SELECT distinct *\n",
    " FROM test.qumls_cui\n",
    "where note_id not in (\"0595040941-0\",\n",
    "                            \"0778429553-0\",\n",
    "                            \"1014681675\",\n",
    "                            \"2889522952-2\",\n",
    "                            \"3080383448-5\",\n",
    "                            \"3300000926-3\",\n",
    "                            \"3360037185-3\",\n",
    "                            \"3580973392\",\n",
    "                            \"3627629462-3\",\n",
    "                            \"4323116051-4\",\n",
    "                            \"477704053-4\",\n",
    "                            \"528317073\",\n",
    "                            \"531702602\",\n",
    "                            \"534061073\",\n",
    "                            \"54832076\",\n",
    "                            \"5643725437-6\",\n",
    "                            \"5944412090-5\",\n",
    "                            \"6613169476-6\",\n",
    "                            \"7261075903-7\",\n",
    "                            \"7504944368-7\",\n",
    "                            \"7999462393-7\",\n",
    "                            \"8131081430\",\n",
    "                            \"8171084310\",\n",
    "                            \"8193787896\",\n",
    "                            \"8295055184-8\",\n",
    "                            \"8823185307-8\") \n",
    "                            and similarity >= 0.8 ) t\n",
    "group by cui, begin, end, note_id, type) t\n",
    "group by type;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import operator as op\n",
    "import pandas as pd\n",
    "from __main__ import get_system_matches, get_sys_data\n",
    "\n",
    "def process_sentence(pt, sentence, analysis_type, corpus):\n",
    "    \"\"\"\n",
    "    Recursively evaluate parse tree, \n",
    "    with check for existence before build\n",
    "       :param sentence: to process\n",
    "       :return class of merged annotations, boolean operated system df \n",
    "    \"\"\"\n",
    "    \n",
    "    class Results(object):\n",
    "        def __init__(self):\n",
    "            self.results = set()\n",
    "            #self.operations = []\n",
    "            self.system_merges = pd.DataFrame()\n",
    "            \n",
    "    r = Results()\n",
    "    \n",
    "    if 'entity' in analysis_type and corpus != 'casi': \n",
    "        cols_to_keep = ['begin', 'end', 'note_id'] # entity only\n",
    "    elif 'full' in analysis_type: \n",
    "        cols_to_keep = ['cui', 'begin', 'end', 'note_id'] # entity only\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['cui', 'note_id'] # entity only\n",
    "    elif corpus == 'casi':\n",
    "        cols_to_keep = ['case', 'overlap']\n",
    "        #cols_to_keep = ['case', 'begin', 'end']\n",
    "    \n",
    "    def evaluate(parseTree):\n",
    "        oper = {'&': op.and_, '|': op.or_}\n",
    "        \n",
    "        if parseTree:\n",
    "            leftC = evaluate(parseTree.getLeftChild())\n",
    "            rightC = evaluate(parseTree.getRightChild())\n",
    "            \n",
    "            if leftC and rightC:\n",
    "                query = set()\n",
    "                system_query = pd.DataFrame()\n",
    "                fn = oper[parseTree.getRootVal()]\n",
    "                \n",
    "                if isinstance(leftC, str):\n",
    "                    \n",
    "                    # get system as leaf node \n",
    "                    left, _ = get_system_matches(leftC, analysis_type, corpus)\n",
    "                    left_sys = get_sys_data(leftC, analysis_type, corpus)\n",
    "                \n",
    "                elif isinstance(leftC, tuple):\n",
    "                    left = leftC[0]\n",
    "                    l_sys = leftC[1]\n",
    "                \n",
    "                if isinstance(rightC, str):\n",
    "                    \n",
    "                    # get system as leaf node\n",
    "                    right, _ = get_system_matches(rightC, analysis_type, corpus)\n",
    "                    right_sys = get_sys_data(rightC, analysis_type, corpus)\n",
    "                    \n",
    "                elif isinstance(rightC, tuple):\n",
    "                    right = rightC[0]\n",
    "                    r_sys = rightC[1]\n",
    "                    \n",
    "                # create match set based on boolean operation\n",
    "                match_set = fn(left, right)\n",
    "               \n",
    "                if corpus != 'casi':\n",
    "                    if fn == op.or_:\n",
    "                        r.results = r.results.union(match_set)\n",
    "\n",
    "                        if isinstance(leftC, str) and isinstance(rightC, str):\n",
    "                            df = left_sys.append(right_sys)\n",
    "                            df = df[cols_to_keep].drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, str) and isinstance(rightC, tuple):\n",
    "                            df = left_sys.append(r_sys)\n",
    "                            df = df[cols_to_keep].drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, tuple) and isinstance(rightC, str):\n",
    "                            df = right_sys.append(l_sys)\n",
    "                            df = df[cols_to_keep].drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, tuple) and isinstance(rightC, tuple):\n",
    "                            df = l_sys.append(r_sys)\n",
    "                            df = df[cols_to_keep].drop_duplicates()\n",
    "\n",
    "                    if fn == op.and_:\n",
    "                        if len(r.results) == 0:\n",
    "                            r.results = match_set\n",
    "                        r.results = r.results.intersection(match_set)\n",
    "\n",
    "                        if isinstance(leftC, str) and isinstance(rightC, str):\n",
    "                            df = left_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, str) and isinstance(rightC, tuple):\n",
    "                            df = left_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, tuple) and isinstance(rightC, str):\n",
    "                            df = l_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, tuple) and isinstance(rightC, tuple):\n",
    "                            df = l_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates()\n",
    "                else:\n",
    "                    if fn == op.or_:\n",
    "                        r.results = r.results.union(match_set)\n",
    "\n",
    "                        if isinstance(leftC, str) and isinstance(rightC, str):\n",
    "                            df = left_sys.append(right_sys)\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, str) and isinstance(rightC, tuple):\n",
    "                            df = left_sys.append(r_sys)\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, tuple) and isinstance(rightC, str):\n",
    "                            df = right_sys.append(l_sys)\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, tuple) and isinstance(rightC, tuple):\n",
    "                            df = l_sys.append(r_sys)\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                    if fn == op.and_:\n",
    "                        if len(r.results) == 0:\n",
    "                            r.results = match_set\n",
    "                        r.results = r.results.intersection(match_set)\n",
    "\n",
    "                        if isinstance(leftC, str) and isinstance(rightC, str):\n",
    "                            df = left_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, str) and isinstance(rightC, tuple):\n",
    "                            df = left_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, tuple) and isinstance(rightC, str):\n",
    "                            df = l_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, tuple) and isinstance(rightC, tuple):\n",
    "                            df = l_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df.drop_duplicates()\n",
    "                \n",
    "                # get matched results\n",
    "                query.update(r.results)\n",
    "                \n",
    "                # get combined system results\n",
    "                r.system_merges = df\n",
    "                \n",
    "                if len(df) > 0:\n",
    "                    system_query = system_query.append(df)\n",
    "                else:\n",
    "                    print('wtf!')\n",
    "                    \n",
    "                return query, system_query\n",
    "            else:\n",
    "                return parseTree.getRootVal()\n",
    "    \n",
    "    if sentence.n_or > 0 or sentence.n_and > 0:\n",
    "        evaluate(pt)  \n",
    "    \n",
    "    # trivial case\n",
    "    elif sentence.n_or == 0 and sentence.n_and == 0:\n",
    "        r.results, _ = get_system_matches(sentence.sentence, analysis_type, corpus)\n",
    "        r.system_merges = get_sys_data(sentence.sentence, analysis_type, corpus)\n",
    "        print('trivial:', sentence.sentence, len(r.results), len(r.system_merges))\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Incoming Boolean sentences are parsed into a binary tree.\n",
    "\n",
    "Test expressions to parse:\n",
    "\n",
    "sentence = '((((A&B)|C)|D)&E)'\n",
    "\n",
    "sentence = '(E&(D|(C|(A&B))))'\n",
    "\n",
    "sentence = '(((A|(B&C))|(D&(E&F)))|(H&I))'\n",
    "\n",
    "\"\"\"\n",
    "# build parse tree from passed sentence\n",
    "# using grammatical rules of Boolean logic\n",
    "def buildParseTree(fpexp):\n",
    "    \"\"\"\n",
    "       Iteratively build parse tree from passed sentence using grammatical rules of Boolean logic\n",
    "       :param fpexp: sentence to parse\n",
    "       :return eTree: parse tree representation\n",
    "       Incoming Boolean sentences are parsed into a binary tree.\n",
    "       Test expressions to parse:\n",
    "       sentence = '(A&B)'\n",
    "       sentence = '(A|B)'\n",
    "       sentence = '((A|B)&C)'\n",
    "       \n",
    "    \"\"\"\n",
    "\n",
    "    fplist = fpexp.split()\n",
    "    pStack = Stack()\n",
    "    eTree = BinaryTree('')\n",
    "    pStack.push(eTree)\n",
    "    currentTree = eTree\n",
    "\n",
    "    for i in fplist:\n",
    "\n",
    "        if i == '(':\n",
    "            currentTree.insertLeft('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getLeftChild()\n",
    "        elif i not in ['&', '|', ')']:\n",
    "            currentTree.setRootVal(i)\n",
    "            parent = pStack.pop()\n",
    "            currentTree = parent\n",
    "        elif i in ['&', '|']:\n",
    "            currentTree.setRootVal(i)\n",
    "            currentTree.insertRight('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getRightChild()\n",
    "        elif i == ')':\n",
    "            currentTree = pStack.pop()\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    return eTree\n",
    "\n",
    "def make_parse_tree(payload):\n",
    "    \"\"\"\n",
    "    Ensure data to create tree are in standard form\n",
    "    :param sentence: sentence to preprocess\n",
    "    :return pt, parse tree graph\n",
    "            sentence, processed sentence to build tree\n",
    "            a: order\n",
    "    \"\"\"\n",
    "    def preprocess_sentence(sentence):\n",
    "        # prepare statement for case when a boolean AND/OR is given\n",
    "        sentence = payload.replace('(', ' ( '). \\\n",
    "            replace(')', ' ) '). \\\n",
    "            replace('&', ' & '). \\\n",
    "            replace('|', ' | '). \\\n",
    "            replace('  ', ' ')\n",
    "        return sentence\n",
    "\n",
    "    sentence = preprocess_sentence(payload)\n",
    "    print(sentence)\n",
    "    \n",
    "    pt = buildParseTree(sentence)\n",
    "    #pt.postorder() \n",
    "    \n",
    "    return pt\n",
    "\n",
    "class Sentence(object):\n",
    "\n",
    "    def __init__(self, sentence):\n",
    "        self = self\n",
    "        self.n_and = sentence.count('&')\n",
    "        self.n_or = sentence.count('|')\n",
    "        self.sentence = sentence\n",
    "    \n",
    "def get_metrics(boolean_expression: str, analysis_type: str, corpus: str):\n",
    "    \"\"\"\n",
    "    Traverse binary parse tree representation of Boolean sentence\n",
    "        :params: boolean expression in form of '(<annotator_engine_name1><boolean operator><annotator_engine_name2>)'\n",
    "                 analysis_type (string value of: 'test', 'entity', 'cui', 'full') used to filter set of reference and system annotations \n",
    "        :return: dictionary with values needed for confusion matrix\n",
    "    \"\"\"\n",
    "    sentence = Sentence(boolean_expression)   \n",
    "\n",
    "    pt = make_parse_tree(sentence.sentence)\n",
    "\n",
    "    r = process_sentence(pt, sentence, analysis_type, corpus)\n",
    "    \n",
    "    print('len sys merges:', len(r.system_merges))\n",
    "    system_n = len(r.system_merges)\n",
    "    reference_n = get_ref_n(analysis_type, corpus)\n",
    "\n",
    "    reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, r.results).get_ref_sys()\n",
    "\n",
    "    # get overall TP/TF and various other counts for running confusion matrix metric analysis\n",
    "    return merge_eval(reference_only, system_only, reference_system_match, system_n, reference_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all combinations of given list of annotators:\n",
    "def expressions(l, n):\n",
    "    for (operations, *operands), operators in product(\n",
    "            combinations(l, n), product(('&', '|'), repeat=n - 1)):\n",
    "        for operation in zip(operators, operands):\n",
    "            operations = [operations, *operation]\n",
    "        yield operations\n",
    "\n",
    "\n",
    "def run_ensemble(l, analysis_type, corpus):\n",
    "\n",
    "    metrics = pd.DataFrame()\n",
    "\n",
    "    for i in range(1, len(l)+1):\n",
    "        test = list(expressions(l, i))\n",
    "        for t  in test:\n",
    "            if i > 1:\n",
    "                # format Boolean sentence for parse tree \n",
    "                t = '(' + \" \".join(str(x) for x in t).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "\n",
    "            d = get_metrics(t, analysis_type, corpus)\n",
    "            d['merge'] = t\n",
    "            frames = [metrics, pd.DataFrame(d, index=[0]) ]\n",
    "            metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "    \n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    \n",
    "    file_name = corpus + '_all_merge_metrics_'\n",
    "        \n",
    "    geometric_mean(metrics).to_csv(analysisConf.data_dir + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "    print(geometric_mean(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_set(df, analysis_type = 'entity', df_type = 'sys', corpus = None):\n",
    "    \n",
    "    #print(df[0:10])\n",
    "    \n",
    "    # get values for creation of series of type tuple\n",
    "    if 'entity' in analysis_type: \n",
    "        if corpus == 'casi':\n",
    "            arg = df.case, df.overlap\n",
    "        else:    \n",
    "            if df_type == 'sys':\n",
    "                arg = df.begin, df.end, df.note_id\n",
    "            else:\n",
    "                arg = df.start, df.end, df.file\n",
    "            \n",
    "    elif 'cui' in analysis_type:\n",
    "        if df_type == 'sys':\n",
    "            arg = df.cui, df.note_id\n",
    "        else:\n",
    "            arg = df.value, df.file\n",
    "    elif 'full' in analysis_type:\n",
    "        if df_type == 'sys':\n",
    "            arg = df.begin, df.end, df.cui, df.note_id\n",
    "        else:\n",
    "            arg = df.start, df.end, df.value, df.file\n",
    "    \n",
    "    return set(list(zip(*arg)))\n",
    "#     if corpus == 'casi':\n",
    "#         return set(arg)\n",
    "#     else:\n",
    "#         return set(list(zip(*arg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTS -> ensemble:\n",
    "def test_match_consistency(matches, ref_only, ref_n, sys):\n",
    "    \"\"\"test for reference only/match set consistency:\n",
    "        params: match, system and reference only sets\"\"\"\n",
    "   \n",
    "    print('len', len(sys), len(matches), len(matches.union(sys)), len(matches.intersection(sys)))\n",
    "    assert len(matches.union(ref_only)) == ref_n, 'Reference annotation mismatch union'\n",
    "    assert len(matches.intersection(sys)) == len(matches), 'System annotation mismatch intersect'\n",
    "    assert len(matches.union(sys)) == len(sys), 'System annotation mismatch union'\n",
    "    assert len(matches.intersection(ref_only)) == 0, 'Reference annotation mismatch intersect'\n",
    "\n",
    "def test_systems(analysis_type, systems, corpus):\n",
    "    sys = df_to_set(get_sys_data(systems[0], analysis_type, corpus), analysis_type)\n",
    "    test_match_consistency(*get_system_matches(systems[0], analysis_type, corpus), get_ref_n(analysis_type), sys)\n",
    "    print('Match consistency:', len(sys),get_ref_n(analysis_type))\n",
    "\n",
    "def test_metrics(ref, sys_m, match_m):\n",
    "    test = True\n",
    "    reference_n = len(ref)\n",
    "    system_n = len(sys_m)\n",
    "\n",
    "    print('Test metrics:', type(reference_n), type(system_n), type(match_m))\n",
    "\n",
    "    reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, match_m).get_ref_sys()\n",
    "    F, recall, precision, _, _, _, _, _ = Metrics(system_only, reference_only, reference_system_match, system_n).get_confusion_metrics()\n",
    "    F_, recall_, precision_, _, _, _, _, _ = Metrics(system_only, reference_only, reference_system_match, system_n).get_confusion_metrics(test)\n",
    "\n",
    "    assert F[1] == F_, 'F1 issue'\n",
    "    assert recall[1] == recall_, 'recall issue'\n",
    "    assert precision[1] == precision_, 'precision issue'\n",
    "    print(F[1], F_)\n",
    "    print(recall[1], recall_)\n",
    "    print(precision[1], precision_)\n",
    "\n",
    "def test_count(analysis_type, corpus):\n",
    "    # test match counts:\n",
    "    ctakes, _ = get_system_matches('ctakes', analysis_type, corpus)\n",
    "    clamp, _ = get_system_matches('clamp', analysis_type, corpus)\n",
    "    b9, _ = get_system_matches('biomedicus', analysis_type, corpus)\n",
    "    mm, _ = get_system_matches('metamap', analysis_type, corpus)\n",
    "\n",
    "    print('count:', len(mm.intersection(b9.intersection(clamp.intersection(ctakes)))))\n",
    "    \n",
    "def test_ensemble(analysis_type, corpus):\n",
    "    \n",
    "    print('ensemble:')\n",
    "    # Get mixed system_n\n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    ref_ann, data = get_metric_data(training_notes, analysis_type, corpus)\n",
    "\n",
    "    names = ['ctakes', 'biomedicus', 'metamap', 'clamp']\n",
    "    if 'entity' in analysis_type: \n",
    "        cols_to_keep = ['begin', 'end', 'note_id']\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['cui', 'note_id']\n",
    "    elif 'full' in analysis_type:\n",
    "        cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "\n",
    "    biomedicus = data[data[\"system\"]=='biomedicus'][cols_to_keep].copy()\n",
    "    ctakes = data[data[\"system\"]=='ctakes'][cols_to_keep].copy()\n",
    "    clamp = data[data[\"system\"]=='clamp'][cols_to_keep].copy()\n",
    "    metamap = data[data[\"system\"]=='metamap'][cols_to_keep].copy()\n",
    "    quickumls = data[data[\"system\"]=='quick_umls'][cols_to_keep].copy()\n",
    "\n",
    "    print('systems:', len(biomedicus), len(clamp), len(ctakes), len(metamap), len(quickumls))\n",
    "\n",
    "    b9 = set()\n",
    "    cl = set()\n",
    "    ct = set()\n",
    "    mm = set()\n",
    "    qu = set()\n",
    "\n",
    "    b9 = df_to_set(get_sys_data('biomedicus', analysis_type, corpus), analysis_type)\n",
    "    print(len(b9))\n",
    "\n",
    "    ct = df_to_set(get_sys_data('ctakes', analysis_type, corpus), analysis_type)\n",
    "    print(len(ct))\n",
    "\n",
    "    cl = df_to_set(get_sys_data('clamp', analysis_type, corpus), analysis_type)\n",
    "    print(len(cl))\n",
    "\n",
    "    mm = df_to_set(get_sys_data('metamap', analysis_type, corpus), analysis_type)\n",
    "    print(len(mm))\n",
    "\n",
    "    qu = df_to_set(get_sys_data('quick_umls', analysis_type, corpus), analysis_type)\n",
    "    print(len(qu))\n",
    "    \n",
    "    print('various merges:')\n",
    "    print(len(b9), len(cl), len(ct), len(mm), len(qu))\n",
    "    print(len(mm.intersection(b9.intersection(cl.intersection(ct)))))\n",
    "    print(len(mm.union(b9.intersection(cl.intersection(ct)))))\n",
    "    print(len(mm.union(b9.union(cl.intersection(ct)))))\n",
    "    print(len(mm.union(b9.union(cl.union(ct)))))\n",
    "    print(len(b9.intersection(ct)))\n",
    "\n",
    "    sys_m = b9.intersection(ct.intersection(qu))\n",
    "    print('sys_m:', len(sys_m))\n",
    "\n",
    "    # Get match merges:\n",
    "    ct, _ = get_system_matches('ctakes', analysis_type, corpus)\n",
    "    cl, _ = get_system_matches('clamp', analysis_type, corpus)\n",
    "    b9, _ = get_system_matches('biomedicus', analysis_type, corpus)\n",
    "    mm, _ = get_system_matches('metamap', analysis_type, corpus)\n",
    "    qu, _ = get_system_matches('quick_umls', analysis_type, corpus)\n",
    "\n",
    "    match_m = b9.intersection(ct.intersection(qu))\n",
    "    print('match_m:', len(match_m))\n",
    "    # reference df to set\n",
    "    if 'entity' in analysis_type: \n",
    "        cols_to_keep = ['end', 'start','file']\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['value','file']\n",
    "    elif 'full' in analysis_type:\n",
    "        cols_to_keep = ['end', 'start', 'value','file']\n",
    "\n",
    "    ref = df_to_set(ref_ann[cols_to_keep], analysis_type, 'ref')\n",
    "\n",
    "    print('ref:', len(ref))\n",
    "\n",
    "    # test difference:\n",
    "    print('FP:', len(sys_m - match_m), len(sys_m - ref))\n",
    "    assert len(sys_m - match_m) == len(sys_m - ref), 'FP mismatch'\n",
    "    print('FN:', len(ref - match_m), len(ref - sys_m))\n",
    "    assert len(ref - match_m) == len(ref - sys_m), 'FN mismatch'\n",
    "    \n",
    "    test_metrics(ref, sys_m, match_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls'] ('analytical_cui_i2b2_concepts.csv', 'test.i2b2_all')\n",
      "metamap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trivial: metamap 14661 107927\n",
      "len sys merges: 107927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clamp\n",
      "trivial: clamp 23181 52884\n",
      "len sys merges: 52884\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def main():\n",
    "    \n",
    "    #rtype = int(input(\"Run: 1->Single systems; 2->Ensemble; 3->Tests; 4-> MM Test\"))\n",
    "   \n",
    "    '''\n",
    "        corpora: i2b2, mipacq, fv017\n",
    "        analyses: entity only (exact span), cui by document, full (aka (entity and cui on exaact span/exact cui)\n",
    "                  NB: add \"_test\" using mipacq to egnerate small test sample \n",
    "        systems: ctakes, biomedicus, clamp, metamap, quick_umls\n",
    "        \n",
    "        TODO -> Vectorization (entity only and full):\n",
    "                add switch for use of TN on single system performance evaluations \n",
    "                add switch for overlap matching versus exact span\n",
    "             -> Other tasks besides concept extraction\n",
    "             -> Use of https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html\n",
    "        \n",
    "    ''' \n",
    "    #corpus = 'i2b2'\n",
    "    #corpus = 'mipacq'\n",
    "    #corpus = \"casi\"\n",
    "    #analysis_type = 'entity'\n",
    "    #analysis_type = 'full'\n",
    "    analysisConf =  AnalysisConfig()\n",
    "    print(analysisConf.systems, analysisConf.corpus_config())\n",
    "    \n",
    "    if (rtype == 1):\n",
    "        generate_metrics(analysis_type, corpus)\n",
    "    elif (rtype == 2):\n",
    "        #l = ['ctakes','biomedicus','clamp','metamap','quick_umls']\n",
    "        l = ['metamap', 'clamp', 'biomedicus']\n",
    "        run_ensemble(l, analysis_type, corpus) \n",
    "    elif (rtype == 3):\n",
    "        systems = ['biomedicus']\n",
    "        t = ['concept_jaccard_score_false']\n",
    "        test_systems(analysis_type, systems, corpus)  \n",
    "        test_count(analysis_type, corpus)\n",
    "        test_ensemble(analysis_type, corpus)\n",
    "    elif (rtype == 4):\n",
    "        generate_metrics_test(analysis_type, corpus)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dkpro-cassis\n",
    "from cassis import *\n",
    "import numpy as np\n",
    "import collections\n",
    "from typing import List, Set\n",
    "\n",
    "ts_test = \"/Users/gms/development/nlp/nlpie/data/irr_mts/\"\n",
    "dir_test = \"/Users/gms/development/nlp/nlpie/data/irr_mts/\"\n",
    "#fname = \"527982345-v1.txt.xmi\"\n",
    "fname = \"92_1666/alber475.xmi\"\n",
    "case = fname.split('/')[0]\n",
    "view_name = \"_InitialView\"\n",
    "Span = collections.namedtuple('Span',['begin','end','label']) # define named tuple for span object \n",
    "  \n",
    "def get_ann(fname, dir_test, ts_test, view_name):\n",
    "    #t = \"org.apache.ctakes.typesystem.type.textsem.DiseaseDisorderMention\"\n",
    "    t = \"webanno.custom.Term\"\n",
    "    with open(ts_test + 'TypeSystem.xml', 'rb') as f:\n",
    "        typesystem = load_typesystem(f)\n",
    "    with open(dir_test + fname, 'rb') as f:\n",
    "        cas = load_cas_from_xmi(f, typesystem=typesystem)\n",
    "    view = cas.get_view(view_name)\n",
    "    #print([x for x in view.select(t)])\n",
    "    #print(view.sofa_string, len(view.sofa_string))\n",
    "    d = {}\n",
    "    ann = []\n",
    "    labels = set()\n",
    "    attribs = get_attribs(view.select(t))\n",
    "    # only parse if type exists in file\n",
    "    if view.select(t):\n",
    "        for sentence in view.select(t): \n",
    "            for i in range(len(attribs)):\n",
    "                key = attribs[i]\n",
    "                # helper method to get val for given key\n",
    "                val = sentence.__getattribute__(attribs[i])\n",
    "                d[key] = val \n",
    "               \n",
    "                if key == 'termType':\n",
    "                    #print(key, val)\n",
    "                    labels.add(val)\n",
    "                \n",
    "                if i == len(attribs) - 1:\n",
    "                    ann.append( Span(d[\"begin\"], d[\"end\"], d[\"termType\"]))\n",
    "    \n",
    "    #print(ann, labels)\n",
    "    return ann, view.sofa_string, labels\n",
    "# extract attributes from cas Annotation object\n",
    "\n",
    "def get_attribs(v):\n",
    "    attribs = []\n",
    "    for sentence in v:\n",
    "        #print(sentence)\n",
    "        for s in sentence.__dir__():\n",
    "            if '__' not in s:\n",
    "                if s not in attribs:\n",
    "                    #print(s)\n",
    "                    attribs.append(s)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    return attribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
