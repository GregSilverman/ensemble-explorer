{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  Copyright (c) 2019 Regents of the University of Minnesota.\n",
    " \n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    " \n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gevent\n",
    "from scipy import stats  \n",
    "from scipy.stats import norm, mode\n",
    "from scipy.stats.mstats import gmean\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pymysql\n",
    "import time \n",
    "import functools as ft\n",
    "import operator as op\n",
    "from pathlib import Path\n",
    "from itertools import combinations, product, permutations\n",
    "from sqlalchemy.engine import create_engine\n",
    "from datetime import datetime\n",
    "from pythonds.basic.stack import Stack\n",
    "from pythonds.trees.binaryTree import BinaryTree\n",
    "from typing import List, Set, Tuple \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The cell below contains the configurable parameters to ensure that our ensemble explorer runs properaly on your machine. \n",
    "Please read carfully through steps (1-11) before running the rest of the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-1: CHOOSE YOUR CORPUS\n",
    "# TODO: get working with list of corpora\n",
    "#corpora = ['mipacq','i2b2','fairview'] #options for concept extraction include 'fairview', 'mipacq' OR 'i2b2'\n",
    "\n",
    "# cross-system semantic union merge filter for cross system aggregations using custom system annotations file with corpus name and system name using 'ray_test': \n",
    "# need to add semantic type filrering when reading in sys_data\n",
    "#corpus = 'ray_test'\n",
    "#corpus = 'clinical_trial2'\n",
    "#corpus = 'fairview'\n",
    "#corpus = 'mipacq'\n",
    "corpus = 'i2b2'\n",
    "\n",
    "# STEP-2: CHOOSE YOUR DATA DIRECTORY; this is where output data will be saved on your machine\n",
    "data_directory = '/Users/gms/development/nlp/nlpie/data/ensembling-u01/output/' \n",
    "\n",
    "data_out = Path('/Users/gms/development/nlp/nlpie/data/ensembling-u01/output/')\n",
    "\n",
    "# STEP-3: CHOOSE WHICH SYSTEMS YOU'D LIKE TO EVALUATE AGAINST THE CORPUS REFERENCE SET\n",
    "systems = ['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls']\n",
    "#systems = ['biomedicus', 'clamp', 'metamap', 'quick_umls']\n",
    "#systems = ['biomedicus', 'quick_umls']\n",
    "#systems = ['biomedicus', 'ctakes', 'quick_umls']\n",
    "#systems = ['biomedicus', 'clamp', 'ctakes', 'metamap']\n",
    "#systems = ['biomedicus', 'clamp']\n",
    "#systems = ['ctakes', 'quick_umls', 'biomedicus', 'metamap']\n",
    "#systems = ['biomedicus', 'metamap']\n",
    "#systems = ['ray_test']\n",
    "#systems = ['metamap']\n",
    "\n",
    "# STEP-4: CHOOSE TYPE OF RUN:  \n",
    "rtype = 7      # OPTIONS INCLUDE: 2->Ensemble; 3->Tests; 4 -> majority vote; 6 -> add hoc ensemble; 7 -> complementarity\n",
    "               # The Ensemble can include the max system set ['ctakes','biomedicus','clamp','metamap','quick_umls']\n",
    "    \n",
    "# STEP-5: CHOOSE WHAT TYPE OF ANALYSIS YOU'D LIKE TO RUN ON THE CORPUS\n",
    "analysis_type = 'entity' #options include 'entity', 'cui' OR 'full'\n",
    "\n",
    "# STEP-(6A): ENTER DETAILS FOR ACCESSING MANUAL ANNOTATION DATA\n",
    "database_type = 'mysql+pymysql' # We use mysql+pymql as default\n",
    "database_username = 'gms'\n",
    "database_password = 'nej123' \n",
    "database_url = 'localhost' # HINT: use localhost if you're running database on your local machine\n",
    "#database_name = 'clinical_trial' # Enter database name\n",
    "database_name = 'concepts' # Enter database name\n",
    "\n",
    "def ref_data(corpus):\n",
    "    return corpus + '_all' # Enter the table within the database where your reference data is stored\n",
    "\n",
    "table_name = ref_data(corpus)\n",
    "\n",
    "# STEP-(6B): ENTER DETAILS FOR ACCESSING SYSTEM ANNOTATION DATA\n",
    "\n",
    "def sys_data(corpus, analysis_type):\n",
    "    if analysis_type == 'entity':\n",
    "        return 'analytical_'+corpus+'.csv' # OPTIONS include 'analytical_cui_mipacq_concepts.csv' OR 'analytical_cui_i2b2_concepts.csv' \n",
    "    elif analysis_type in ('cui', 'full'):\n",
    "        return 'analytical_'+corpus+'_cui.csv' # OPTIONS include 'analytical_cui_mipacq_concepts.csv' OR 'analytical_cui_i2b2_concepts.csv' \n",
    "        \n",
    "system_annotation = sys_data(corpus, analysis_type)\n",
    "\n",
    "# STEP-7: CREATE A DB CONNECTION POOL\n",
    "engine_request = str(database_type)+'://'+database_username+':'+database_password+\"@\"+database_url+'/'+database_name\n",
    "engine = create_engine(engine_request, pool_pre_ping=True, pool_size=20, max_overflow=30)\n",
    "\n",
    "# STEP-(8A): FILTER BY SEMTYPE\n",
    "filter_semtype = False\n",
    "\n",
    "# STEP-(8B): IF STEP-(8A) == True -> GET REFERENCE SEMTYPES\n",
    "\n",
    "def ref_semtypes(filter_semtype, corpus):\n",
    "    if filter_semtype:\n",
    "        if corpus == 'fairview':\n",
    "            semtypes = ['Drug', 'Finding', 'Anatomy', 'Procedure']\n",
    "        elif corpus == 'i2b2':\n",
    "            semtypes = ['test,treatment', 'problem']\n",
    "        elif corpus == 'mipacq':\n",
    "            semtypes = ['Procedures', 'Disorders,Sign_Symptom', 'Anatomy', 'Chemicals_and_drugs']\n",
    "        elif corpus in ['clinical_trial', 'clinical_trial2']:\n",
    "            semtypes = ['drug,drug::drug_name,drug::drug_dose,dietary_sppplement::dietary_seeelement_name,dietary_supplement::dietary_supplement_dose',\n",
    "                        'temporal_measurement,qualifier,measurement',\n",
    "                        'device',\n",
    "                        'condition,observation', \n",
    "                        'demographics::age,demographics::sex,demographics::race_ethnicity,demographics::bmi,demographics::weight',\n",
    "                        'diet',\n",
    "                        'measurement,qualifier',\n",
    "                        'procedure,observation']\n",
    "        \n",
    "        return semtypes\n",
    "\n",
    "semtypes = ref_semtypes(filter_semtype, corpus)\n",
    "\n",
    "# STEP-9: Set data directory/table for source documents for vectorization\n",
    "src_table = 'sofa'\n",
    "\n",
    "# STEP-10: Specify match type from {'exact', 'overlap'}\n",
    "run_type = 'overlap'\n",
    "\n",
    "# STEP-11: Specify expression type for run (TODO: run all at once; make less kludgey); option single obviates use of rtype = 1\n",
    "expression_type = 'nested' #'nested_with_singleton' # type of merge expression: nested ((A&B)|C), paired ((A&B)|(C&D)), nested_with_singleton ((A&B)|((C&D)|E)) \n",
    "# -> NB: len(systems) for pair must be >= 4, and for nested_with_singleton == 5; single-> skip merges\n",
    "\n",
    "# STEP-12: Specify type of ensemble: merge or vote: used for file naming -> TODO: remove!\n",
    "ensemble_type = 'vote'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "****** TODO \n",
    "-> add majority vote to union for analysis_type = 'full', 'cui': done!\n",
    "-> case for multiple labels on same/overlapping span/same system; disambiguate (order by score if exists and select random for ties): done!\n",
    "-> port to command line: in process... \n",
    "-> rtype 1 -> expression_type = 'single': done!\n",
    "-> refactor vectorized_cooccurences ->  vectorized_cooccurences_test and vectorized_annotations\n",
    "----------------------->\n",
    "-> swap out confused with vectorized_cooccurences\n",
    "-> better control for overlap/exact matching across all single systems and ensembles (e.g., eliminate run_type = for majority-vote control of type of match)\n",
    "-> negation/polarity: in process...\n",
    "-> still need to validate that all semtypes in corpus!\n",
    "-> handle case where intersect merges are empty/any confusion matrix values are 0; specificallly on empty df in evaluate method: done!\n",
    "-> case when system annotations empty from semtype filter; print as 0: done!\n",
    "-> trim whitespace on CSV import -> done for semtypes\n",
    "-> cross-system semantic union merge on aggregation\n",
    "-> other modification, such as 'present'\n",
    "-> clean up configuration process\n",
    "-> allow iteration through all corpora and semtypes\n",
    "-> optimize vecorization (remove confusion?)\n",
    "-> clean up SemanticTypes\n",
    "-> add non-UIMA solution for get_docs\n",
    "-> confusion matrix for multiclass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config class for analysis\n",
    "class AnalysisConfig():\n",
    "    \"\"\"\n",
    "    Configuration object:\n",
    "    systems to use\n",
    "    notes by corpus\n",
    "    paths by output, gold and system location\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self = self    \n",
    "        self.systems = systems\n",
    "        self.data_dir = data_directory\n",
    "    \n",
    "    def corpus_config(self): \n",
    "        usys_data = system_annotation\n",
    "        ref_data = database_name+'.'+table_name\n",
    "        return usys_data, ref_data\n",
    "\n",
    "analysisConf =  AnalysisConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticTypes(object):\n",
    "    '''\n",
    "    Filter semantic types based on: https://metamap.nlm.nih.gov/SemanticTypesAndGroups.shtml\n",
    "    :params: semtypes list from corpus, system to query\n",
    "    :return: list of equivalent system semtypes \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, semtypes, corpus):\n",
    "        self = self\n",
    "        \n",
    "        if corpus == 'clinical_trial2':\n",
    "            sql = \"SELECT st.tui, abbreviation, clamp_name, ctakes_name, biomedicus_name FROM clinical_trial.semantic_groups sg join semantic_types st on sg.tui = st.tui where \" + corpus + \"_name in ({})\"\\\n",
    "                .format(', '.join(['%s' for _ in semtypes]))  \n",
    "        else:\n",
    "            sql = \"SELECT st.tui, abbreviation, clamp_name, ctakes_name FROM concepts.semantic_groups sg join semantic_types st on sg.tui = st.tui where \" + corpus + \"_name in ({})\"\\\n",
    "               .format(', '.join(['%s' for _ in semtypes]))  \n",
    "        \n",
    "        stypes = pd.read_sql(sql, params=[semtypes], con=engine) \n",
    "       \n",
    "        if len(stypes['tui'].tolist()) > 0:\n",
    "            self.biomedicus_types = set(stypes['tui'].tolist())\n",
    "            self.qumls_types = set(stypes['tui'].tolist())\n",
    "        \n",
    "        else:\n",
    "            self.biomedicus_types = None\n",
    "            self.qumls_types = None\n",
    "        \n",
    "        if stypes['clamp_name'].dropna(inplace=True) or len(stypes['clamp_name']) == 0:\n",
    "            self.clamp_types = None\n",
    "        else:\n",
    "            self.clamp_types = set(stypes['clamp_name'].tolist()[0].split(','))\n",
    "         \n",
    "        if stypes['ctakes_name'].dropna(inplace=True) or len(stypes['ctakes_name']) == 0:\n",
    "            self.ctakes_types = None\n",
    "        else:\n",
    "            self.ctakes_types = set(stypes['ctakes_name'].tolist()[0].split(','))\n",
    " \n",
    "        # Kludge for b9 temporal\n",
    "#         if stypes['biomedicus_name'].dropna(inplace=True) or len(stypes['biomedicus_name']) > 0:\n",
    "#             self.biomedicus_types.update(set(stypes['biomedicus_name'].tolist()[0].split(',')))\n",
    "        #else:\n",
    "        #    self.biomedicus_type = None\n",
    "        \n",
    "        if len(stypes['abbreviation'].tolist()) > 0:\n",
    "            self.metamap_types = set(stypes['abbreviation'].tolist())\n",
    "        else:\n",
    "            self.metamap_types = None\n",
    "        \n",
    "        self.reference_types =  set(semtypes)\n",
    "    \n",
    "    def get_system_type(self, system):  \n",
    "        \n",
    "        if system == 'biomedicus':\n",
    "            semtypes = self.biomedicus_types\n",
    "        elif system == 'ctakes':\n",
    "            semtypes = self.ctakes_types\n",
    "        elif system == 'clamp':\n",
    "            semtypes = self.clamp_types\n",
    "        elif system == 'metamap':\n",
    "            semtypes = self.metamap_types\n",
    "        elif system == 'quick_umls':\n",
    "            semtypes = self.qumls_types\n",
    "        elif system == 'reference':\n",
    "            semtypes = self.reference_types\n",
    "            \n",
    "        return semtypes\n",
    "    \n",
    "#print(SemanticTypes(['Drug'], corpus).get_system_type('biomedicus'))\n",
    "#print(SemanticTypes(['Drug'], corpus).get_system_type('quick_umls'))\n",
    "#print(SemanticTypes(['drug'], corpus).get_system_type('clamp'))\n",
    "#print(SemanticTypes(['Anatomy'], 'mipacq').get_system_type('ctakes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semtypes = ['test,treatment']\n",
    "#semtypes = 'drug,drug::drug_name,drug::drug_dose,dietary_supplement::dietary_supplement_name,dietary_supplement::dietary_supplement_dose'\n",
    "#semtypes =  'demographics::age,demographics::sex,demographics::race_ethnicity,demographics::bmi,demographics::weight'\n",
    "#corpus = 'clinical_trial'\n",
    "#sys = 'quick_umls'\n",
    "\n",
    "# is semantic type in particular system\n",
    "def system_semtype_check(sys, semtype, corpus):\n",
    "    st = SemanticTypes([semtype], corpus).get_system_type(sys)\n",
    "    if st:\n",
    "        return sys\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#print(system_semtype_check(sys, semtypes, corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reload_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%cython\n",
    "\n",
    "#import numpy as np # access to Numpy from Python layer\n",
    "#import math\n",
    "\n",
    "class Metrics(object):\n",
    "    \"\"\"\n",
    "    metrics class:\n",
    "    returns an instance with confusion matrix metrics\n",
    "    \"\"\"\n",
    "    def __init__(self, system_only, gold_only, gold_system_match, system_n, neither = 0): # neither: no sys or manual annotation\n",
    "\n",
    "        self = self    \n",
    "        self.system_only = system_only\n",
    "        self.gold_only = gold_only\n",
    "        self.gold_system_match = gold_system_match\n",
    "        self.system_n = system_n\n",
    "        self.neither = neither\n",
    "        \n",
    "    def get_confusion_metrics(self, corpus = None, test = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        compute confusion matrix measures, as per  \n",
    "        https://stats.stackexchange.com/questions/51296/how-do-you-calculate-precision-and-recall-for-multiclass-classification-using-co\n",
    "        \"\"\"\n",
    "#         cdef:\n",
    "#             int TP, FP, FN\n",
    "#             double TM\n",
    "\n",
    "        TP = self.gold_system_match\n",
    "        FP = self.system_only\n",
    "        FN = self.gold_only\n",
    "        \n",
    "        TM = TP/math.sqrt(self.system_n) # TigMetric\n",
    "       \n",
    "        if not test:\n",
    "            \n",
    "            if corpus == 'casi':\n",
    "                recall = TP/(TP + FN)\n",
    "                precision = TP/(TP + FP)\n",
    "                F = 2*(precision*recall)/(precision + recall)\n",
    "            else:\n",
    "                if self.neither == 0:\n",
    "                    confusion = [[0, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "                else:\n",
    "                    confusion = [[self.neither, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "                c = np.asarray(confusion)\n",
    "                \n",
    "                if TP != 0 or FP != 0:\n",
    "                    precision = TP/(TP+FP)\n",
    "                else:\n",
    "                    precision = 0\n",
    "                \n",
    "                if TP != 0 or FN != 0:\n",
    "                    recall = TP/(TP+FN)\n",
    "                else:\n",
    "                    recall = 0\n",
    "                \n",
    "                if precision + recall != 0:\n",
    "                    F = 2*(precision*recall)/(precision + recall)\n",
    "                else:\n",
    "                    F = 0\n",
    "    \n",
    "        else:\n",
    "            precision = TP/(TP+FP)\n",
    "            recall = TP/(TP+FN)\n",
    "            F = 2*(precision*recall)/(precision + recall)\n",
    "        \n",
    "        # Tignanelli Metric\n",
    "        if FN == 0:\n",
    "            TP_FN_R = TP\n",
    "        elif FN > 0:\n",
    "            TP_FN_R = TP/FN\n",
    " \n",
    "        return F, recall, precision, TP, FP, FN, TP_FN_R, TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_set(df, analysis_type = 'entity', df_type = 'sys', corpus = None):\n",
    "    \n",
    "    # get values for creation of series of type tuple\n",
    "    if 'entity' in analysis_type: \n",
    "        if corpus == 'casi':\n",
    "            arg = df.case, df.overlap\n",
    "        else:    \n",
    "            arg = df.begin, df.end, df.case\n",
    "            \n",
    "    elif 'cui' in analysis_type:\n",
    "        arg = df.value, df.case\n",
    "    elif 'full' in analysis_type:\n",
    "        arg = df.begin, df.end, df.value, df.case\n",
    "    \n",
    "    return set(list(zip(*arg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for exact matches\n",
    "#%%cython \n",
    "\n",
    "from __main__ import df_to_set, engine\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "def get_cooccurences(ref, sys, analysis_type: str, corpus: str):\n",
    "    \"\"\"\n",
    "    get cooccurences between system and reference; exact match\n",
    "    \"\"\"\n",
    "    # cooccurences\n",
    "    class Cooccurences(object):\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.ref_system_match = 0\n",
    "            self.ref_only = 0\n",
    "            self.system_only = 0\n",
    "            self.system_n = 0\n",
    "            self.ref_n = 0\n",
    "            self.matches = set()\n",
    "            self.false_negatives = set()\n",
    "            self.corpus = corpus\n",
    "\n",
    "    c = Cooccurences()\n",
    "    \n",
    "    if c.corpus != 'casi':\n",
    "        if analysis_type in ['cui', 'full']:\n",
    "            sys = sys.rename(index=str, columns={\"note_id\": \"case\", \"cui\": \"value\"})\n",
    "            # do not overestimate FP\n",
    "            sys = sys[~sys['value'].isnull()] \n",
    "            ref = ref[~ref['value'].isnull()]\n",
    "        \n",
    "        if 'entity' in analysis_type: \n",
    "            sys = sys.rename(index=str, columns={\"note_id\": \"case\"})\n",
    "            cols_to_keep = ['begin', 'end', 'case']\n",
    "        elif 'cui' in analysis_type: \n",
    "            cols_to_keep = ['value', 'case']\n",
    "        elif 'full' in analysis_type: \n",
    "            cols_to_keep = ['begin', 'end', 'value', 'case']\n",
    "        \n",
    "        sys = sys[cols_to_keep].drop_duplicates()\n",
    "        ref = ref[cols_to_keep].drop_duplicates()\n",
    "        # matches via inner join\n",
    "        tp = pd.merge(sys, ref, how = 'inner', left_on=cols_to_keep, right_on = cols_to_keep) \n",
    "        # reference-only via left outer join\n",
    "        fn = pd.merge(ref, sys, how = 'left', left_on=cols_to_keep, right_on = cols_to_keep, indicator=True) \n",
    "        fn = fn[fn[\"_merge\"] == 'left_only']\n",
    "\n",
    "        tp = tp[cols_to_keep]\n",
    "        fn = fn[cols_to_keep]\n",
    "\n",
    "        # use for metrics \n",
    "        c.matches = c.matches.union(df_to_set(tp, analysis_type, 'ref'))\n",
    "        c.false_negatives = c.false_negatives.union(df_to_set(fn, analysis_type, 'ref'))\n",
    "        c.ref_system_match = len(c.matches)\n",
    "        c.system_only = len(sys) - len(c.matches) # fp\n",
    "        c.system_n = len(sys)\n",
    "        c.ref_n = len(ref)\n",
    "        c.ref_only = len(c.false_negatives)\n",
    "        \n",
    "    else:\n",
    "        sql = \"select `case` from test.amia_2019_analytical_v where overlap = 1 and `system` = %(sys.name)s\"  \n",
    "        tp = pd.read_sql(sql, params={\"sys.name\":sys.name}, con=engine)\n",
    "        \n",
    "        sql = \"select `case` from test.amia_2019_analytical_v where (overlap = 0 or overlap is null) and `system` = %(sys.name)s\"  \n",
    "        fn = pd.read_sql(sql, params={\"sys.name\":sys.name}, con=engine)\n",
    "        \n",
    "        c.matches = df_to_set(tp, 'entity', 'sys', 'casi')\n",
    "        c.fn = df_to_set(fn, 'entity', 'sys', 'casi')\n",
    "        c.ref_system_match = len(c.matches)\n",
    "        c.system_only = len(sys) - len(c.matches)\n",
    "        c.system_n = len(tp) + len(fn)\n",
    "        c.ref_n = len(tp) + len(fn)\n",
    "        c.ref_only = len(fn)\n",
    "        \n",
    "    # sanity check\n",
    "    if len(ref) - c.ref_system_match < 0:\n",
    "        print('Error: ref_system_match > len(ref)!')\n",
    "    if len(ref) != c.ref_system_match + c.ref_only:\n",
    "        print('Error: ref count mismatch!', len(ref), c.ref_system_match, c.ref_only)\n",
    "   \n",
    "    return c "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Relaxed matches using vectorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/49210801/python3-pass-lists-to-function-with-functools-lru-cache\n",
    "def listToTuple(function):\n",
    "    def wrapper(*args):\n",
    "        args = [tuple(x) if type(x) == list else x for x in args]\n",
    "        result = function(*args)\n",
    "        result = tuple(result) if type(result) == list else result\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "@listToTuple\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def label_vector(doc: str, ann: List[int], labels: List[str]) -> np.array:\n",
    "\n",
    "    v = np.zeros(doc)\n",
    "    labels = list(labels)\n",
    "    \n",
    "    for (i, lab) in enumerate(labels):\n",
    "        i += 1  # 0 is reserved for no label\n",
    "        idxs = [np.arange(a.begin, a.end) for a in ann if a.label == lab]\n",
    "        idxs = [j for mask in idxs for j in mask]\n",
    "        v[idxs] = i\n",
    "\n",
    "    return v\n",
    "\n",
    "# confusion matrix elements for vectorized annotation set binary classification\n",
    "# https://kawahara.ca/how-to-compute-truefalse-positives-and-truefalse-negatives-in-python-for-binary-classification-problems/\n",
    "def confused(sys1, ann1):\n",
    "    TP = np.sum(np.logical_and(ann1 == 1, sys1 == ann1))\n",
    "\n",
    "    # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "    TN = np.sum(np.logical_and(ann1 == 0, sys1 == ann1))\n",
    "\n",
    "    # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "    FP = np.sum(np.logical_and(ann1 == 0, sys1 != ann1))\n",
    "\n",
    "    # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "    FN = np.sum(np.logical_and(ann1 == 1, sys1 != ann1))\n",
    "    \n",
    "    #print('con', TP, TN, FP, FN)\n",
    "    \n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_labels(analysis_type, corpus, filter_semtype, semtype):\n",
    "    \n",
    "    if analysis_type == 'entity':\n",
    "        labels = [\"concept\"]\n",
    "    elif analysis_type in ['cui', 'full']:\n",
    "        ann = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "        labels = list(set(ann[\"label\"].tolist()))\n",
    "    \n",
    "    return labels\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def vectorized_cooccurences(r: object, analysis_type: str, corpus: str, filter_semtype, semtype = None) -> np.int64:\n",
    "    docs = get_docs(corpus)\n",
    "    \n",
    "    if filter_semtype:\n",
    "        ann = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "    else: \n",
    "        ann = get_ref_ann(analysis_type, corpus, filter_semtype)\n",
    "        \n",
    "    sys = get_sys_ann(analysis_type, r)\n",
    "    \n",
    "    labels = get_labels(analysis_type, corpus, filter_semtype, semtype)\n",
    "    \n",
    "    sys2 = list()\n",
    "    ann2 = list()\n",
    "    s2 = list()\n",
    "    a2 = list()\n",
    "    \n",
    "    for n in range(len(docs)):\n",
    "        \n",
    "        if analysis_type != 'cui':\n",
    "            a1 = list(ann.loc[ann.case == docs[n][0]].itertuples(index=False))\n",
    "            s1 = list(sys.loc[sys.case == docs[n][0]].itertuples(index=False))\n",
    "            ann1 = label_vector(docs[n][1], a1, labels)\n",
    "            sys1 = label_vector(docs[n][1], s1, labels)\n",
    "\n",
    "            sys2.append(list(sys1))\n",
    "            ann2.append(list(ann1))\n",
    "\n",
    "        else:\n",
    "            a = ann.loc[ann.case == docs[n][0]]['label'].tolist()\n",
    "            s = sys.loc[sys.case == docs[n][0]]['label'].tolist()\n",
    "            x = [1 if x in a else 0 for x in labels]\n",
    "            y = [1 if x in s else 0 for x in labels]\n",
    "            s2.append(y)\n",
    "            a2.append(x)\n",
    "            \n",
    "    if analysis_type != 'cui': #binary and multiclass\n",
    "        a2 = flatten_list(ann2)\n",
    "        s2 = flatten_list(sys2)\n",
    "        report = classification_report(a2, s2, output_dict=True)\n",
    "        macro_precision =  report['macro avg']['precision'] \n",
    "        macro_recall = report['macro avg']['recall']    \n",
    "        macro_f1 = report['macro avg']['f1-score']\n",
    "        \n",
    "        if analysis_type == 'entity':\n",
    "            TN, FP, FN, TP = confusion_matrix(a2, s2).ravel()\n",
    "            return ((TP, TN, FP, FN), (macro_precision, macro_recall, macro_f1))\n",
    "        else:\n",
    "            return ((0, 0, 0, 0), (macro_precision, macro_recall, macro_f1))\n",
    "        \n",
    "    else: # multilabel/multiclass\n",
    "        x_sparse = sparse.csr_matrix(a2)\n",
    "        y_sparse = sparse.csr_matrix(s2)\n",
    "        report = classification_report(x_sparse, y_sparse, output_dict=True)\n",
    "        macro_precision =  report['macro avg']['precision'] \n",
    "        macro_recall = report['macro avg']['recall']    \n",
    "        macro_f1 = report['macro avg']['f1-score']\n",
    "        return ((0, 0, 0, 0), (macro_precision, macro_recall, macro_f1))\n",
    "                                       \n",
    "# http://www.lrec-conf.org/proceedings/lrec2016/pdf/105_Paper.pdf        \n",
    "def vectorized_complementarity(r: object, analysis_type: str, corpus: str, c: tuple, filter_semtype, semtype = None) -> np.int64:\n",
    "    docs = get_docs(corpus)\n",
    "    \n",
    "    out = pd.DataFrame()\n",
    "    \n",
    "    if filter_semtype:\n",
    "        ann = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "    else: \n",
    "        ann = get_ref_ann(analysis_type, corpus, filter_semtype)\n",
    "    \n",
    "    if filter_semtype and SemanticTypes([semtype], corpus).get_system_type(r.sysA) and SemanticTypes([semtype], corpus).get_system_type(r.sysB) or not filter_semtype: \n",
    "        sysA = get_sys_data(r.sysA, analysis_type, corpus, filter_semtype, semtype)\n",
    "        sysA = sysA.rename(index=str, columns={\"note_id\": \"case\"})\n",
    "        sysB = get_sys_data(r.sysB, analysis_type, corpus, filter_semtype, semtype)\n",
    "        sysB = sysB.rename(index=str, columns={\"note_id\": \"case\"})\n",
    "\n",
    "        if analysis_type == 'entity':\n",
    "            sysA[\"label\"] = 'concept'\n",
    "            sysB[\"label\"] = 'concept'\n",
    "            cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "        elif analysis_type == 'full':\n",
    "            sysA[\"label\"] = sysA[\"cui\"]\n",
    "            sysB[\"label\"] = sysB[\"cui\"]\n",
    "            cols_to_keep = ['begin', 'end', 'case', 'value', 'label']\n",
    "        elif analysis_type == 'cui':\n",
    "            sysA[\"label\"] = sysA[\"cui\"]\n",
    "            sysB[\"label\"] = sysA[\"cui\"]\n",
    "            cols_to_keep = ['case', 'cui', 'label']\n",
    "\n",
    "        sysA = sysA[cols_to_keep]\n",
    "        sysB = sysB[cols_to_keep]\n",
    "\n",
    "        if analysis_type == 'entity':\n",
    "            labels = [\"concept\"]\n",
    "        elif analysis_type in ['cui', 'full']:\n",
    "            labels = list(set(ann[\"value\"].tolist()))\n",
    "\n",
    "        sys_a2 = list()\n",
    "        sys_b2 = list()\n",
    "        sys_ab2 = list()\n",
    "        ann2 = list()\n",
    "        s_a2 = list()\n",
    "        s_b2 = list()\n",
    "\n",
    "        sys2 = list()\n",
    "        a2 = list()\n",
    "        \n",
    "        cvals = list()\n",
    "\n",
    "        for n in range(len(docs)):\n",
    "\n",
    "            a1 = list(ann.loc[ann[\"case\"] == docs[n][0]].itertuples(index=False))\n",
    "            ann1 = label_vector(docs[n][1], a1, labels)\n",
    "            ann2.append(list(ann1))\n",
    "\n",
    "            # get for Aright/Awrong and Bright/Bwrong\n",
    "            s_a1 = list(sysA.loc[sysA.case == docs[n][0]].itertuples(index=False))\n",
    "            s_b1 = list(sysB.loc[sysB.case == docs[n][0]].itertuples(index=False))\n",
    "            sys_a1 = label_vector(docs[n][1], s_a1, labels)\n",
    "            sys_b1 = label_vector(docs[n][1], s_b1, labels)\n",
    " \n",
    "            sys_a2.append(list(sys_a1))\n",
    "            sys_b2.append(list(sys_b1))\n",
    "\n",
    "            # intersected list this will give positive values\n",
    "            s_ab1 = list(set(s_a1).intersection(set(s_b1)))\n",
    "            \n",
    "            # in one set or other but not both for negative values\n",
    "            s_ab2 = list(set(s_a1).symmetric_difference(set(s_b1)))\n",
    "            \n",
    "            s_ab1_ab2 = list(set(s_ab1).union(set(s_ab2)))\n",
    "            \n",
    "            sys_ab1 = label_vector(docs[n][1], s_ab1, labels)\n",
    "            sys_ab2.append(list(sys_ab1))\n",
    "            \n",
    "            sys_ab1_ab2 = label_vector(docs[n][1], s_ab1_ab2, labels)\n",
    "            \n",
    "            if len(s_ab1) == 0:\n",
    "                FP = FN = 0\n",
    "                #print(docs[n][0], FP, FN)\n",
    "            else:\n",
    "                _, _, FP, _ = confused(sys_ab1, ann1)\n",
    "                _, _, _, FN = confused(sys_ab1_ab2, ann1)\n",
    "            \n",
    "            cvals.append([FP, FN])\n",
    "\n",
    "        a2 = [item for sublist in ann2 for item in sublist]\n",
    "\n",
    "        # right/wrong for A and B\n",
    "        s_a2 = [item for sublist in sys_a2 for item in sublist]\n",
    "        s_b2 = [item for sublist in sys_b2 for item in sublist]\n",
    "        \n",
    "        FP = np.sum(cvals, axis=0)[0]\n",
    "        FN = np.sum(cvals, axis=0)[1]\n",
    "\n",
    "        _, _, aFP, aFN = confused(np.array(s_a2), np.array(a2))\n",
    "        _, _, bFP, bFN = confused(np.array(s_b2), np.array(a2))\n",
    "\n",
    "        b_over_a, a_over_b = complementarity_measures(FN, FP, aFN, aFP, bFN, bFP)\n",
    "        \n",
    "        b_over_a['system'] = str((r.sysB, r.sysA))\n",
    "        a_over_b['system'] = str((r.sysA, r.sysB))\n",
    "        \n",
    "        frames = [out, pd.DataFrame(b_over_a, index=[0])]\n",
    "        out = pd.concat(frames, ignore_index=True, sort=False) \n",
    "        \n",
    "        frames = [out, pd.DataFrame(a_over_b, index=[0])]\n",
    "        out = pd.concat(frames, ignore_index=True, sort=False) \n",
    "        \n",
    "    else:\n",
    "        print(str(c) + 'has no semtype', semtype)\n",
    "\n",
    "    return out\n",
    "        \n",
    "def complementarity_measures(FN, FP, aFN, aFP, bFN, bFP):\n",
    "    \n",
    "    compA = 1 - (FN+FP)/(aFN + aFP)\n",
    "    compB = 1 - (FN+FP)/(bFN + bFP)\n",
    "   \n",
    "    r_compA = 1 - (FP)/(aFP)\n",
    "    r_compB = 1 - (FP)/(bFP)\n",
    "    \n",
    "    p_compA = 1 - (FN)/(aFN)\n",
    "    p_compB = 1 - (FN)/(bFN)\n",
    "    \n",
    "    f1_compA = 2*(p_compA*r_compA)/(p_compA + r_compA)\n",
    "    f1_compB = 2*(p_compB*r_compB)/(p_compB + r_compB)\n",
    "    \n",
    "    if FN > aFN:\n",
    "        print('a bad n:', FN, aFN)\n",
    "    if FN > bFN:\n",
    "        print('b bad n:', FN, bFN)\n",
    "    if FP > aFP:\n",
    "        print('a bad p:', FP, aFP)\n",
    "    if FP > bFP:\n",
    "        print('b bad p:', FP, bFP)\n",
    "    if FP > min([aFP, bFP]):\n",
    "        print('both bad p:', FP, aFP, bFP)\n",
    "    \n",
    "    b_over_a = {'test': 'COMP(A, B)', 'max_prop_error_reduction': compA, 'p': p_compA, 'r': r_compA, 'F1-score': f1_compA}\n",
    "    a_over_b = {'test': 'COMP(B, A)', 'max_prop_error_reduction': compB, 'p': p_compB, 'r': r_compB, 'F1-score': f1_compB}\n",
    " \n",
    "    return b_over_a, a_over_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ft.lru_cache(maxsize=None)\n",
    "def cm_dict(ref_only: int, system_only: int, ref_system_match: int, system_n: int, ref_n: int) -> dict:\n",
    "    \"\"\"\n",
    "    Generate dictionary of confusion matrix params and measures\n",
    "    :params: ref_only, system_only, reference_system_match -> sets\n",
    "    matches, system_n, reference_n -> counts\n",
    "    :return: dictionary object\n",
    "    \"\"\"\n",
    "\n",
    "    if ref_only + ref_system_match != ref_n:\n",
    "        print('ERROR!')\n",
    "        \n",
    "    # get evaluation metrics\n",
    "    F, recall, precision, TP, FP, FN, TP_FN_R, TM  = Metrics(system_only, ref_only, ref_system_match, system_n).get_confusion_metrics()\n",
    "\n",
    "    d = {\n",
    "         'F1': F, \n",
    "         'precision': precision, \n",
    "         'recall': recall, \n",
    "         'TP': TP, \n",
    "         'FN': FN, \n",
    "         'FP': FP, \n",
    "         'TP/FN': TP_FN_R,\n",
    "         'n_gold': ref_n, \n",
    "         'n_sys': system_n, \n",
    "         'TM': TM\n",
    "    }\n",
    "    \n",
    "    if system_n - FP != TP:\n",
    "        print('inconsistent system n!')\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ft.lru_cache(maxsize=None)\n",
    "def get_metric_data(analysis_type: str, corpus: str):\n",
    "   \n",
    "    usys_file, ref_table = AnalysisConfig().corpus_config()\n",
    "    systems = AnalysisConfig().systems\n",
    "    \n",
    "    sys_ann = pd.read_csv(analysisConf.data_dir + usys_file, dtype={'note_id': str})\n",
    "    \n",
    "    sql = \"SELECT * FROM \" + ref_table #+ \" where semtype in('Anatomy', 'Chemicals_and_drugs')\" \n",
    "    \n",
    "    ref_ann = pd.read_sql(sql, con=engine)\n",
    "    sys_ann = sys_ann.drop_duplicates()\n",
    "    \n",
    "    return ref_ann, sys_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%cython\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "def geometric_mean(metrics):\n",
    "    \"\"\"\n",
    "    1. Get rank average of F1, TP/FN, TM\n",
    "        http://www.datasciencemadesimple.com/rank-dataframe-python-pandas-min-max-dense-rank-group/\n",
    "        https://stackoverflow.com/questions/46686315/in-pandas-how-to-create-a-new-column-with-a-rank-according-to-the-mean-values-o?rq=1\n",
    "    2. Take geomean of rank averages\n",
    "        https://stackoverflow.com/questions/42436577/geometric-mean-applied-on-row\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.DataFrame() \n",
    "\n",
    "    metrics['F1 rank']=metrics['F1'].rank(ascending=0,method='average')\n",
    "    metrics['TP/FN rank']=metrics['TP/FN'].rank(ascending=0,method='average')\n",
    "    metrics['TM rank']=metrics['TM'].rank(ascending=0,method='average')\n",
    "    metrics['Gmean'] = gmean(metrics.iloc[:,-3:],axis=1)\n",
    "\n",
    "    return metrics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidence intervals\n",
    "# https://github.com/sousanunes/confidence_intervals.git\n",
    "\n",
    "def normal_approximation_binomial_confidence_interval(s, n, confidence_level=.95):\n",
    "\t'''Computes the binomial confidence interval of the probability of a success s, \n",
    "\tbased on the sample of n observations. The normal approximation is used,\n",
    "\tappropriate when n is equal to or greater than 30 observations.\n",
    "\tThe confidence level is between 0 and 1, with default 0.95.\n",
    "\tReturns [p_estimate, interval_range, lower_bound, upper_bound].\n",
    "\tFor reference, see Section 5.2 of Tom Mitchel's \"Machine Learning\" book.'''\n",
    "\n",
    "\tp_estimate = (1.0 * s) / n\n",
    "\n",
    "\tinterval_range = norm.interval(confidence_level)[1] * np.sqrt( (p_estimate * (1-p_estimate))/n )\n",
    "\n",
    "\treturn p_estimate, interval_range, p_estimate - interval_range, p_estimate + interval_range\n",
    "\n",
    "\n",
    "def f1_score_confidence_interval(r, p, dr, dp):\n",
    "\t'''Computes the confidence interval for the F1-score measure of classification performance\n",
    "\tbased on the values of recall (r), precision (p), and their respective confidence\n",
    "\tinterval ranges, or absolute uncertainty, about the recall (dr) and the precision (dp).\n",
    "\tDisclaimer: I derived the formula myself based on f(r,p) = 2rp / (r+p).\n",
    "\tNobody has revised my computation. Feedback appreciated!'''\n",
    "\n",
    "\tf1_score = (2.0 * r * p) / (r + p)\n",
    "\n",
    "\tleft_side = np.abs( (2.0 * r * p) / (r + p) )\n",
    "\n",
    "\tright_side = np.sqrt( np.power(dr/r, 2.0) + np.power(dp/p, 2.0) + ((np.power(dr, 2.0)+np.power(dp, 2.0)) / np.power(r + p, 2.0)) )\n",
    "\n",
    "\tinterval_range = left_side * right_side\n",
    "\n",
    "\treturn f1_score, interval_range, f1_score - interval_range, f1_score + interval_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metrics(analysis_type: str, corpus: str, filter_semtype, semtype = None):\n",
    "    start = time.time()\n",
    "\n",
    "    systems = AnalysisConfig().systems\n",
    "    metrics = pd.DataFrame()\n",
    "\n",
    "    __, sys_ann = get_metric_data(analysis_type, corpus)\n",
    "    c = None\n",
    "    \n",
    "    for sys in systems:\n",
    "       \n",
    "        if filter_semtype and semtype:\n",
    "            ref_ann = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "            ref_ann = get_ref_ann(analysis_type, corpus, filter_semtype)\n",
    "            \n",
    "        system_annotations = sys_ann[sys_ann['system'] == sys].copy()\n",
    "\n",
    "        if filter_semtype:\n",
    "            st = SemanticTypes([semtype], corpus).get_system_type(sys)\n",
    "\n",
    "            if st: \n",
    "                system_annotations = sys_ann[sys_ann['semtypes'].isin(st)].copy()\n",
    "        else:\n",
    "            system_annotations = sys_ann.copy()\n",
    "\n",
    "        if (filter_semtype and st) or filter_semtype is False:\n",
    "            system = system_annotations.copy()\n",
    "\n",
    "            if sys == 'quick_umls':\n",
    "                system = system[system.score.astype(float) >= .8]\n",
    "\n",
    "            if sys == 'metamap':\n",
    "                system = system.fillna(0)\n",
    "                system = system[system.score.abs().astype(int) >= 800]\n",
    "\n",
    "            system = system.drop_duplicates()\n",
    "\n",
    "            ref_ann = ref_ann.rename(index=str, columns={\"start\": \"begin\", \"file\": \"case\"})\n",
    "            c = get_cooccurences(ref_ann, system, analysis_type, corpus) # get matches, FN, etc.\n",
    "\n",
    "            \n",
    "            if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "                # get dictionary of confusion matrix metrics\n",
    "                d = cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n)\n",
    "                d['system'] = sys\n",
    "\n",
    "                data = pd.DataFrame(d,  index=[0])\n",
    "                metrics = pd.concat([metrics, data], ignore_index=True)\n",
    "                metrics.drop_duplicates(keep='last', inplace=True)\n",
    "            else:\n",
    "                print(\"NO EXACT MATCHES FOR\", sys)\n",
    "            elapsed = (time.time() - start)\n",
    "            print(\"elapsed:\", sys, elapsed)\n",
    "   \n",
    "    if c:\n",
    "        elapsed = (time.time() - start)\n",
    "        print(geometric_mean(metrics))\n",
    "\n",
    "        now = datetime.now()\n",
    "        timestamp = datetime.timestamp(now)\n",
    "\n",
    "        file_name = 'metrics_'\n",
    "\n",
    "        metrics.to_csv(analysisConf.data_dir + corpus + '_' + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "\n",
    "        print(\"total elapsed time:\", elapsed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ft.lru_cache(maxsize=None)\n",
    "def get_sys_data(system: str, analysis_type: str, corpus: str, filter_semtype, semtype = None) -> pd.DataFrame:\n",
    "   \n",
    "    _, data = get_metric_data(analysis_type, corpus)\n",
    "    \n",
    "    out = data[data['system'] == system].copy()\n",
    "    \n",
    "    if filter_semtype:\n",
    "        st = SemanticTypes([semtype], corpus).get_system_type(system)\n",
    "        print(system, 'st', st)\n",
    "    \n",
    "    if corpus == 'casi':\n",
    "        cols_to_keep = ['case', 'overlap'] \n",
    "        out = out[cols_to_keep].drop_duplicates()\n",
    "        return out\n",
    "        \n",
    "    else:\n",
    "        if filter_semtype:\n",
    "            out = out[out['semtypes'].isin(st)].copy()\n",
    "            \n",
    "        else:\n",
    "            out = out[out['system']== system].copy()\n",
    "            \n",
    "        if system == 'quick_umls':\n",
    "            out = out[(out.score.astype(float) >= 0.8) & (out[\"type\"] == 'concept_jaccard_score_False')]\n",
    "            # fix for leading space on semantic type field\n",
    "            #out = out.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x) \n",
    "            out['semtypes'] = out['semtypes'].str.strip()\n",
    "        \n",
    "        if system == 'metamap':\n",
    "            out = out[out.score.abs().astype(int) >= 800]\n",
    "            \n",
    "        if 'entity' in analysis_type:\n",
    "            cols_to_keep = ['begin', 'end', 'note_id', 'system']\n",
    "        elif 'cui' in analysis_type:\n",
    "            cols_to_keep = ['cui', 'note_id', 'system']\n",
    "        elif 'full' in analysis_type:\n",
    "            cols_to_keep = ['begin', 'end', 'cui', 'note_id', 'system']\n",
    "            \n",
    "        if analysis_type in ['cui','full']:\n",
    "            #out = out[out['cui'] != \"RxNorm=[\"]\n",
    "            out = out[out['cui'].str.startswith(\"C\") == True]\n",
    "\n",
    "        out = out[cols_to_keep]\n",
    "        \n",
    "        return out.drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GENERATE merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disambiguate multiple labeled CUIS on span for union \n",
    "def union_vote(df):\n",
    "    df['length'] = (df.end - df.begin).abs()\n",
    "    \n",
    "    cases = set(df['note_id'].tolist())\n",
    "    \n",
    "    for case in cases:\n",
    "        i = 0\n",
    "        data = []\n",
    "        out = pd.DataFrame()\n",
    "        \n",
    "        test = df[df['note_id']==case].copy()\n",
    "        \n",
    "        for row in test.itertuples():\n",
    "\n",
    "            iix = pd.IntervalIndex.from_arrays(test.begin, test.end, closed='neither')\n",
    "            span_range = pd.Interval(row.begin, row.end)\n",
    "            fx = test[iix.overlaps(span_range)].copy()\n",
    "\n",
    "            maxLength = fx['length'].max()\n",
    "            minLength = fx['length'].min()\n",
    "            maxScore = abs(float(fx['score'].max()))\n",
    "            minScore = abs(float(fx['score'].min()))\n",
    "\n",
    "            if len(fx) > 1: \n",
    "                if i%5000 == 0:\n",
    "                    print('iteration:', i)\n",
    "\n",
    "                # if gretaer scxore or longer span exists, use as tie-breaker\n",
    "                if maxScore > minScore:\n",
    "                    fx = fx[fx['score'] == maxScore]\n",
    "                elif maxLength > minLength:\n",
    "                    fx = fx[fx['length'] == fx['length'].max()]\n",
    "\n",
    "            i += 1\n",
    "            data.append(fx)\n",
    "\n",
    "        out = pd.concat(data, axis=0)\n",
    "   \n",
    "    # Remaining ties: randomly reindex to keep random row when dropping duplicates: https://gist.github.com/cadrev/6b91985a1660f26c2742\n",
    "    out.reset_index(inplace=True)\n",
    "    out = out.reindex(np.random.permutation(out.index))\n",
    "    out = out.drop_duplicates(['begin', 'end', 'note_id', 'length', 'cui'])\n",
    "    \n",
    "    return out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ft.lru_cache(maxsize=None)\n",
    "def process_sentence(pt, sentence, analysis_type, corpus, filter_semtype, semtype = None):\n",
    "    \"\"\"\n",
    "    Recursively evaluate parse tree, \n",
    "    with check for existence before build\n",
    "       :param sentence: to process\n",
    "       :return class of merged annotations, boolean operated system df \n",
    "    \"\"\"\n",
    "    \n",
    "    class Results(object):\n",
    "        def __init__(self):\n",
    "            self.results = set()\n",
    "            self.system_merges = pd.DataFrame()\n",
    "            \n",
    "    r = Results()\n",
    "    \n",
    "    if 'entity' in analysis_type and corpus != 'casi': \n",
    "        cols_to_keep = ['begin', 'end', 'note_id'] # entity only\n",
    "    elif 'full' in analysis_type: \n",
    "        cols_to_keep = ['cui', 'begin', 'end', 'note_id'] # entity only\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['cui', 'note_id'] # entity only\n",
    "    elif corpus == 'casi':\n",
    "        cols_to_keep = ['case', 'overlap']\n",
    "    \n",
    "    def evaluate(parseTree):\n",
    "        oper = {'&': op.and_, '|': op.or_}\n",
    "        \n",
    "        if parseTree:\n",
    "            leftC = gevent.spawn(evaluate, parseTree.getLeftChild())\n",
    "            rightC = gevent.spawn(evaluate, parseTree.getRightChild())\n",
    "            \n",
    "            if leftC.get() is not None and rightC.get() is not None:\n",
    "                system_query = pd.DataFrame()\n",
    "                fn = oper[parseTree.getRootVal()]\n",
    "                \n",
    "                if isinstance(leftC.get(), str):\n",
    "                    # get system as leaf node \n",
    "                    if filter_semtype:\n",
    "                        left_sys = get_sys_data(leftC.get(), analysis_type, corpus, filter_semtype, semtype)\n",
    "                    else:\n",
    "                        left_sys = get_sys_data(leftC.get(), analysis_type, corpus, filter_semtype)\n",
    "                \n",
    "                elif isinstance(leftC.get(), pd.DataFrame):\n",
    "                    l_sys = leftC.get()\n",
    "                \n",
    "                if isinstance(rightC.get(), str):\n",
    "                    # get system as leaf node\n",
    "                    if filter_semtype:\n",
    "                        right_sys = get_sys_data(rightC.get(), analysis_type, corpus, filter_semtype, semtype)\n",
    "                    else:\n",
    "                        right_sys = get_sys_data(rightC.get(), analysis_type, corpus, filter_semtype)\n",
    "                    \n",
    "                elif isinstance(rightC.get(), pd.DataFrame):\n",
    "                    r_sys = rightC.get()\n",
    "                    \n",
    "                if fn == op.or_:\n",
    "\n",
    "                    if isinstance(leftC.get(), str) and isinstance(rightC.get(), str):\n",
    "                        frames = [left_sys, right_sys]\n",
    "\n",
    "                    elif isinstance(leftC.get(), str) and isinstance(rightC.get(), pd.DataFrame):\n",
    "                        frames = [left_sys, r_sys]\n",
    "\n",
    "                    elif isinstance(leftC.get(), pd.DataFrame) and isinstance(rightC.get(), str):\n",
    "                        frames = [l_sys, right_sys]\n",
    "\n",
    "                    elif isinstance(leftC.get(), pd.DataFrame) and isinstance(rightC.get(), pd.DataFrame):\n",
    "                        frames = [l_sys, r_sys]\n",
    "                    \n",
    "                    # add in vote\n",
    "                        \n",
    "                    df = pd.concat(frames,  ignore_index=True)\n",
    "                    #df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "                    \n",
    "#                     if analysis_type == 'full':\n",
    "#                         df = union_vote(df)\n",
    "\n",
    "                if fn == op.and_:\n",
    "                    \n",
    "                    if isinstance(leftC.get(), str) and isinstance(rightC.get(), str):\n",
    "                        if not left_sys.empty and not right_sys.empty:\n",
    "                            df = left_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "                        else:\n",
    "                            df = pd.DataFrame(columns=cols_to_keep)\n",
    "\n",
    "                    elif isinstance(leftC.get(), str) and isinstance(rightC.get(), pd.DataFrame):\n",
    "                        if not left_sys.empty and not r_sys.empty:\n",
    "                            df = left_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "                        else:\n",
    "                            df = pd.DataFrame(columns=cols_to_keep)\n",
    "\n",
    "                    elif isinstance(leftC.get(), pd.DataFrame) and isinstance(rightC.get(), str):\n",
    "                        if not l_sys.empty and not right_sys.empty:\n",
    "                            df = l_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "                        else:\n",
    "                            df = pd.DataFrame(columns=cols_to_keep)\n",
    "\n",
    "                    elif isinstance(leftC.get(), pd.DataFrame) and isinstance(rightC.get(), pd.DataFrame):\n",
    "                        if not l_sys.empty and not r_sys.empty:\n",
    "                            df = l_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "                        else:\n",
    "                            df = pd.DataFrame(columns=cols_to_keep)\n",
    "                \n",
    "                # get combined system results\n",
    "                r.system_merges = df\n",
    "                \n",
    "                if len(df) > 0:\n",
    "                    system_query = system_query.append(df)\n",
    "                else:\n",
    "                    print('wtf!')\n",
    "                    \n",
    "                return system_query\n",
    "            else:\n",
    "                return parseTree.getRootVal()\n",
    "    \n",
    "    if sentence.n_or > 0 or sentence.n_and > 0:\n",
    "        evaluate(pt)  \n",
    "    \n",
    "    # trivial case\n",
    "    elif sentence.n_or == 0 and sentence.n_and == 0:\n",
    "        \n",
    "        if filter_semtype:\n",
    "            r.system_merges = get_sys_data(sentence.sentence, analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "            r.system_merges = get_sys_data(sentence.sentence, analysis_type, corpus, filter_semtype)\n",
    "        \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Incoming Boolean sentences are parsed into a binary tree.\n",
    "\n",
    "Test expressions to parse:\n",
    "\n",
    "sentence = '((((A&B)|C)|D)&E)'\n",
    "\n",
    "sentence = '(E&(D|(C|(A&B))))'\n",
    "\n",
    "sentence = '(((A|(B&C))|(D&(E&F)))|(H&I))'\n",
    "\n",
    "\"\"\"\n",
    "# build parse tree from passed sentence using grammatical rules of Boolean logic\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def buildParseTree(fpexp):\n",
    "    \"\"\"\n",
    "       Iteratively build parse tree from passed sentence using grammatical rules of Boolean logic\n",
    "       :param fpexp: sentence to parse\n",
    "       :return eTree: parse tree representation\n",
    "       Incoming Boolean sentences are parsed into a binary tree.\n",
    "       Test expressions to parse:\n",
    "       sentence = '(A&B)'\n",
    "       sentence = '(A|B)'\n",
    "       sentence = '((A|B)&C)'\n",
    "       \n",
    "    \"\"\"\n",
    "    fplist = fpexp.split()\n",
    "    pStack = Stack()\n",
    "    eTree = BinaryTree('')\n",
    "    pStack.push(eTree)\n",
    "    currentTree = eTree\n",
    "\n",
    "    for i in fplist:\n",
    "\n",
    "        if i == '(':\n",
    "            currentTree.insertLeft('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getLeftChild()\n",
    "        elif i not in ['&', '|', ')']:\n",
    "            currentTree.setRootVal(i)\n",
    "            parent = pStack.pop()\n",
    "            currentTree = parent\n",
    "        elif i in ['&', '|']:\n",
    "            currentTree.setRootVal(i)\n",
    "            currentTree.insertRight('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getRightChild()\n",
    "        elif i == ')':\n",
    "            currentTree = pStack.pop()\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    return eTree\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def make_parse_tree(payload):\n",
    "    \"\"\"\n",
    "    Ensure data to create tree are in correct form\n",
    "    :param sentence: sentence to preprocess\n",
    "    :return pt, parse tree graph\n",
    "            sentence, processed sentence to build tree\n",
    "            a: order\n",
    "    \"\"\"\n",
    "    def preprocess_sentence(sentence):\n",
    "        # prepare statement for case when a boolean AND/OR is given\n",
    "        sentence = payload.replace('(', ' ( '). \\\n",
    "            replace(')', ' ) '). \\\n",
    "            replace('&', ' & '). \\\n",
    "            replace('|', ' | '). \\\n",
    "            replace('  ', ' ')\n",
    "        return sentence\n",
    "\n",
    "    sentence = preprocess_sentence(payload)\n",
    "    print('Processing sentence:', sentence)\n",
    "    \n",
    "    pt = buildParseTree(sentence)\n",
    "    #pt.postorder() \n",
    "    \n",
    "    return pt\n",
    "\n",
    "class Sentence(object):\n",
    "    '''\n",
    "    Details about boolean expression -> number operators and expression\n",
    "    '''\n",
    "    def __init__(self, sentence):\n",
    "        self = self\n",
    "        self.n_and = sentence.count('&')\n",
    "        self.n_or = sentence.count('|')\n",
    "        self.sentence = sentence\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_docs(corpus):\n",
    "    \n",
    "    # KLUDGE!!!\n",
    "    if corpus == 'ray_test':\n",
    "        corpus = 'fairview'\n",
    "        \n",
    "    sql = 'select distinct note_id, sofa from sofas where corpus = %(corpus)s order by note_id'\n",
    "    df = pd.read_sql(sql, params={\"corpus\":corpus}, con=engine)\n",
    "    df.drop_duplicates()\n",
    "    df['len_doc'] = df['sofa'].apply(len)\n",
    "    \n",
    "    subset = df[['note_id', 'len_doc']]\n",
    "    docs = [tuple(x) for x in subset.to_numpy()]\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def set_labels(analysis_type, df):\n",
    "    if analysis_type == 'entity':   \n",
    "        df[\"label\"] = 'concept'\n",
    "    elif analysis_type in ['cui','full']:\n",
    "        df[\"label\"] = df[\"value\"]\n",
    "    \n",
    "    return df\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_ref_ann(analysis_type, corpus, filter_semtype, semtype = None):\n",
    "    \n",
    "    if filter_semtype:\n",
    "        if ',' in semtype:\n",
    "            semtype = semtype.split(',')\n",
    "        else:\n",
    "            semtype = [semtype]\n",
    "        \n",
    "    ann, _ = get_metric_data(analysis_type, corpus)\n",
    "    ann = ann.rename(index=str, columns={\"start\": \"begin\", \"file\": \"case\"})\n",
    "    \n",
    "    if filter_semtype:\n",
    "        ann = ann[ann['semtype'].isin(semtype)]\n",
    "    \n",
    "    ann = set_labels(analysis_type, ann)\n",
    "        \n",
    "    if analysis_type == 'entity':\n",
    "        cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "    elif analysis_type == 'cui':\n",
    "        cols_to_keep = ['case', 'label']\n",
    "    elif analysis_type == 'full':\n",
    "        cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "    ann = ann[cols_to_keep]\n",
    "    \n",
    "    return ann\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_sys_ann(analysis_type, r):\n",
    "    sys = r.system_merges   \n",
    "    \n",
    "    sys = sys.rename(index=str, columns={\"note_id\": \"case\", \"cui\": \"value\"})\n",
    "    \n",
    "    sys = set_labels(analysis_type, sys)\n",
    "    \n",
    "    if analysis_type == 'entity':\n",
    "        cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "    elif analysis_type == 'full':\n",
    "        cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "    elif analysis_type == 'cui':\n",
    "        cols_to_keep = ['case', 'label']\n",
    "    \n",
    "    sys = sys[cols_to_keep]\n",
    "    return sys\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_metrics(boolean_expression: str, analysis_type: str, corpus: str, run_type: str, filter_semtype, semtype = None):\n",
    "    \"\"\"\n",
    "    Traverse binary parse tree representation of Boolean sentence\n",
    "        :params: boolean expression in form of '(<annotator_engine_name1><boolean operator><annotator_engine_name2>)'\n",
    "                 analysis_type (string value of: 'entity', 'cui', 'full') used to filter set of reference and system annotations \n",
    "        :return: dictionary with values needed for confusion matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence = Sentence(boolean_expression)   \n",
    "    pt = make_parse_tree(sentence.sentence)\n",
    "    \n",
    "    if filter_semtype:\n",
    "        r = process_sentence(pt, sentence, analysis_type, corpus, filter_semtype, semtype)\n",
    "    else:\n",
    "        r = process_sentence(pt, sentence, analysis_type, corpus, filter_semtype)\n",
    "        \n",
    "    # vectorize merges using i-o labeling\n",
    "    if run_type == 'overlap':\n",
    "        if filter_semtype:\n",
    "             ((TP, TN, FP, FN),(p,r,f1)) = vectorized_cooccurences(r, analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "            ((TP, TN, FP, FN),(p,r,f1)) = vectorized_cooccurences(r, analysis_type, corpus, filter_semtype)\n",
    "        \n",
    "        print('results:',((TP, TN, FP, FN),(p,r,f1)))\n",
    "        # TODO: validate against ann1/sys1 where val = 1\n",
    "        # total by number chars\n",
    "        system_n = TP + FP\n",
    "        reference_n = TP + FN\n",
    "\n",
    "        if analysis_type == 'entity':\n",
    "            d = cm_dict(FN, FP, TP, system_n, reference_n)\n",
    "        else:\n",
    "            d = dict()\n",
    "            d['F1'] = 0\n",
    "            d['precision'] = 0 \n",
    "            d['recall'] = 0\n",
    "            d['TP/FN'] = 0\n",
    "            d['TM'] = 0\n",
    "            \n",
    "        d['TN'] = TN\n",
    "        d['macro_p'] = p\n",
    "        d['macro_r'] = r\n",
    "        d['macro_f1'] = f1\n",
    "        \n",
    "        # return full metrics\n",
    "        return d\n",
    "\n",
    "    elif run_type == 'exact':\n",
    "        # total by number spans\n",
    "        \n",
    "        if filter_semtype:\n",
    "            ann = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "        else: \n",
    "            ann = get_ref_ann(analysis_type, corpus, filter_semtype)\n",
    "        \n",
    "        c = get_cooccurences(ann, r.system_merges, analysis_type, corpus) # get matches, FN, etc.\n",
    "\n",
    "        if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "            # get dictionary of confusion matrix metrics\n",
    "            d = cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n)\n",
    "        else:\n",
    "            d = None\n",
    "            \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_valid_systems(['biomedicus'], 'Anatomy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all combinations of given list of annotators:\n",
    "def partly_unordered_permutations(lst, k):\n",
    "    elems = set(lst)\n",
    "    for c in combinations(lst, k):\n",
    "        for d in permutations(elems - set(c)):\n",
    "            yield c + d\n",
    "            \n",
    "def expressions(l, n):\n",
    "    for (operations, *operands), operators in product(\n",
    "            combinations(l, n), product(('&', '|'), repeat=n-1)):\n",
    "        for operation in zip(operators, operands):\n",
    "            operations = [operations, *operation]\n",
    "        yield operations\n",
    "\n",
    "# get list of systems with a semantic type in grouping\n",
    "def get_valid_systems(systems, semtype):\n",
    "    test = []\n",
    "    for sys in systems:\n",
    "        st = system_semtype_check(sys, semtype, corpus)\n",
    "        if st:\n",
    "            test.append(sys)\n",
    "\n",
    "    return test\n",
    "\n",
    "# permute system combinations and evaluate system merges for performance\n",
    "def run_ensemble(systems, analysis_type, corpus, filter_semtype, expression_type, semtype = None):\n",
    "    metrics = pd.DataFrame()\n",
    "    \n",
    "    # pass single system to evaluate\n",
    "    if expression_type == 'single':\n",
    "        for system in systems:\n",
    "            if filter_semtype:\n",
    "                d = get_metrics(system, analysis_type, corpus, run_type, filter_semtype, semtype)\n",
    "            else:\n",
    "                d = get_metrics(system, analysis_type, corpus, run_type, filter_semtype)\n",
    "            d['merge'] = system\n",
    "            d['n_terms'] = 1\n",
    "\n",
    "            frames = [metrics, pd.DataFrame(d, index=[0])]\n",
    "            metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "    \n",
    "    elif expression_type == 'nested':\n",
    "        for l in partly_unordered_permutations(systems, 2):\n",
    "            print('processing merge combo:', l)\n",
    "            for i in range(1, len(l)+1):\n",
    "                test = list(expressions(l, i))\n",
    "                for t in test:\n",
    "                    if i > 1:\n",
    "                        # format Boolean sentence for parse tree \n",
    "                        t = '(' + \" \".join(str(x) for x in t).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "\n",
    "                    if filter_semtype:\n",
    "                        d = get_metrics(t, analysis_type, corpus, run_type, filter_semtype, semtype)\n",
    "                    else:\n",
    "                        d = get_metrics(t, analysis_type, corpus, run_type, filter_semtype)\n",
    "\n",
    "                    d['merge'] = t\n",
    "                    d['n_terms'] = i\n",
    "\n",
    "                    frames = [metrics, pd.DataFrame(d, index=[0])]\n",
    "                    metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "    \n",
    "    # kludges to fill in missing forms\n",
    "    elif expression_type == 'nested_with_singleton' and len(systems) == 5:\n",
    "        # form (((a&b)|c)&(d|e))\n",
    "        \n",
    "        nested = list(expressions(systems, 3))\n",
    "        test = list(expressions(systems, 2))\n",
    "        to_do_terms = []\n",
    "    \n",
    "        for n in nested:\n",
    "            # format Boolean sentence for parse tree \n",
    "            n = '(' + \" \".join(str(x) for x in n).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "\n",
    "            for t in test:\n",
    "                t = '(' + \" \".join(str(x) for x in t).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "\n",
    "                new_and = '(' + n +'&'+ t + ')'\n",
    "                new_or = '(' + n +'|'+ t + ')'\n",
    "\n",
    "                if new_and.count('biomedicus') != 2 and new_and.count('clamp') != 2 and new_and.count('ctakes') != 2 and new_and.count('metamap') != 2 and new_and.count('quick_umls') != 2:\n",
    "\n",
    "                    if new_and.count('&') != 4 and new_or.count('|') != 4:\n",
    "                        #print(new_and)\n",
    "                        #print(new_or)\n",
    "                        to_do_terms.append(new_or)\n",
    "                        to_do_terms.append(new_and)\n",
    "        \n",
    "        print('nested_with_singleton', len(to_do_terms))\n",
    "        for term in to_do_terms:\n",
    "            if filter_semtype:\n",
    "                d = get_metrics(term, analysis_type, corpus, run_type, filter_semtype, semtype)\n",
    "            else:\n",
    "                d = get_metrics(term, analysis_type, corpus, run_type, filter_semtype)\n",
    "                \n",
    "            n = term.count('&')\n",
    "            m = term.count('|')\n",
    "            d['merge'] = term\n",
    "            d['n_terms'] = m + n + 1\n",
    "\n",
    "            frames = [metrics, pd.DataFrame(d, index=[0])]\n",
    "            metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "                        \n",
    "    elif expression_type == 'paired':\n",
    "        m = list(expressions(systems, 2))\n",
    "        test = list(expressions(m, 2))\n",
    "\n",
    "        to_do_terms = []\n",
    "        for t in test:\n",
    "            # format Boolean sentence for parse tree \n",
    "            t = '(' + \" \".join(str(x) for x in t).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "            if t.count('biomedicus') != 2 and t.count('clamp') != 2 and t.count('ctakes') != 2 and t.count('metamap') != 2 and t.count('quick_umls') != 2:\n",
    "                if t.count('&') != 3 and t.count('|') != 3:\n",
    "                    to_do_terms.append(t)\n",
    "                    if len(systems) == 5:\n",
    "                        for i in systems:\n",
    "                            if i not in t:\n",
    "                                #print('('+t+'&'+i+')')\n",
    "                                #print('('+t+'|'+i+')')\n",
    "                                new_and = '('+t+'&'+i+')'\n",
    "                                new_or = '('+t+'|'+i+')'\n",
    "                                to_do_terms.append(new_and)\n",
    "                                to_do_terms.append(new_or)\n",
    "                            \n",
    "        print('paired', len(to_do_terms))\n",
    "        for term in to_do_terms:\n",
    "            if filter_semtype:\n",
    "                d = get_metrics(term, analysis_type, corpus, run_type, filter_semtype, semtype)\n",
    "            else:\n",
    "                d = get_metrics(term, analysis_type, corpus, run_type, filter_semtype)\n",
    "                \n",
    "            n = term.count('&')\n",
    "            m = term.count('|')\n",
    "            d['merge'] = term\n",
    "            d['n_terms'] = m + n + 1\n",
    "\n",
    "            frames = [metrics, pd.DataFrame(d, index=[0])]\n",
    "            metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "        \n",
    "    return metrics\n",
    "\n",
    "# write to file\n",
    "def generate_ensemble_metrics(metrics, analysis_type, corpus, ensemble_type, filter_semtype, semtype = None):\n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    \n",
    "    file_name = corpus + '_all_'\n",
    "   \n",
    "    # drop exact matches:\n",
    "    metrics = metrics.drop_duplicates()\n",
    "    \n",
    "    if ensemble_type == 'merge':\n",
    "        metrics = metrics.sort_values(by=['n_terms', 'merge'])\n",
    "        file_name += 'merge_'\n",
    "    elif ensemble_type == 'vote':\n",
    "        file_name += 'vote_'\n",
    "    \n",
    "    #metrics = metrics.drop_duplicates(subset=['TP', 'FN', 'FP', 'n_sys', 'precision', 'recall', 'F', 'TM', 'TP/FN', 'TM', 'n_terms'])\n",
    "\n",
    "    file = file_name + analysis_type + '_' + run_type +'_'\n",
    "    \n",
    "    if filter_semtype:\n",
    "        file += semtype\n",
    "        \n",
    "    if ensemble_type != 'vote':\n",
    "        geometric_mean(metrics).to_csv(analysisConf.data_dir + file + str(timestamp) + '.csv')\n",
    "        print(geometric_mean(metrics))\n",
    "    \n",
    "    \n",
    "# control ensemble run\n",
    "def ensemble_control(systems, analysis_type, corpus, run_type, filter_semtype, semtypes = None):\n",
    "    if filter_semtype:\n",
    "        for semtype in semtypes:\n",
    "            test = get_valid_systems(systems, semtype)\n",
    "            print('SYSTEMS FOR SEMTYPE', semtype, 'ARE', test)\n",
    "            metrics = run_ensemble(test, analysis_type, corpus, filter_semtype, expression_type, semtype)\n",
    "            if (expression_type == 'nested_with_singleton' and len(test) == 5) or expression_type in ['nested', 'paired', 'single']:\n",
    "                generate_ensemble_metrics(metrics, analysis_type, corpus, ensemble_type, filter_semtype, semtype)\n",
    "    else:\n",
    "        metrics = run_ensemble(systems, analysis_type, corpus, filter_semtype, expression_type)\n",
    "        generate_ensemble_metrics(metrics, analysis_type, corpus, ensemble_type, filter_semtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ad hoc query for performance evaluation\n",
    "def get_merge_data(boolean_expression: str, analysis_type: str, corpus: str, run_type: str, filter_semtype, semtype = None):\n",
    "    \"\"\"\n",
    "    Traverse binary parse tree representation of Boolean sentence\n",
    "        :params: boolean expression in form of '(<annotator_engine_name1><boolean operator><annotator_engine_name2>)'\n",
    "                 analysis_type (string value of: 'entity', 'cui', 'full') used to filter set of reference and system annotations \n",
    "        :return: dictionary with values needed for confusion matrix\n",
    "    \"\"\"\n",
    "    if filter_semtype:\n",
    "        ann = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "    else: \n",
    "        ann = get_ref_ann(analysis_type, corpus, filter_semtype)\n",
    "    \n",
    "    sentence = Sentence(boolean_expression)   \n",
    "\n",
    "    pt = make_parse_tree(sentence.sentence)\n",
    "\n",
    "    r = process_sentence(pt, sentence, analysis_type, corpus, filter_semtype, semtype)\n",
    "\n",
    "    if run_type == 'overlap' and rtype != 6:\n",
    "        if filter_semtype:\n",
    "             ((TP, TN, FP, FN),(p,r,f1)) = vectorized_cooccurences(r, analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "             ((TP, TN, FP, FN),(p,r,f1)) = vectorized_cooccurences(r, analysis_type, corpus, filter_semtype)\n",
    "\n",
    "        # TODO: validate against ann1/sys1 where val = 1\n",
    "        # total by number chars\n",
    "        system_n = TP + FP\n",
    "        reference_n = TP + FN\n",
    "\n",
    "        d = cm_dict(FN, FP, TP, system_n, reference_n)\n",
    "        print(d)\n",
    "        \n",
    "    elif run_type == 'exact':\n",
    "        c = get_cooccurences(ann, r.system_merges, analysis_type, corpus) # get matches, FN, etc.\n",
    "\n",
    "        if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "            # get dictionary of confusion matrix metrics\n",
    "            d = cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n)\n",
    "\n",
    "            print('cm', d)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # get matched data from merge\n",
    "    return r.system_merges # merge_eval(reference_only, system_only, reference_system_match, system_n, reference_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# majority vote \n",
    "\n",
    "def vectorized_annotations(ann, analysis_type, labels):\n",
    "    \n",
    "    docs = get_docs(corpus)\n",
    "    out= []\n",
    "    \n",
    "    for n in range(len(docs)):\n",
    "        if analysis_type != 'cui':\n",
    "            a1 = list(ann.loc[ann.case == docs[n][0]].itertuples(index=False))\n",
    "            a = label_vector(docs[n][1], a1, labels)\n",
    "            out.append(a)\n",
    "\n",
    "        else:\n",
    "            a = ann.loc[ann.case == docs[n][0]]['label'].tolist()\n",
    "            x = [1 if x in a else 0 for x in labels]\n",
    "        \n",
    "            out.append(x)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_reference_vector(analysis_type, corpus, filter_semtype, semtype = None):\n",
    "    ref_ann = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "\n",
    "    df = ref_ann.copy()\n",
    "    \n",
    "    if 'entity' in analysis_type: \n",
    "        cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "    elif 'cui' in analysis_type: \n",
    "        cols_to_keep = ['case', 'label']\n",
    "    elif 'full' in analysis_type: \n",
    "        cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "     \n",
    "    labels = get_labels(analysis_type, corpus, filter_semtype, semtype)\n",
    "    \n",
    "    df = df.drop_duplicates(subset=cols_to_keep)\n",
    "    ref = df[cols_to_keep].copy()\n",
    "    test = vectorized_annotations(ref, analysis_type, labels)\n",
    "    \n",
    "    if analysis_type != 'cui':\n",
    "        ref =  np.asarray(flatten_list(test), dtype=np.int32) \n",
    "    else: \n",
    "        ref =  np.asarray(test, dtype=np.int32)\n",
    "\n",
    "    return ref\n",
    "\n",
    "def majority_overlap_sys(systems, analysis_type, corpus, filter_semtype, semtype = None):\n",
    "    \n",
    "    d = {}\n",
    "    sys_test = []\n",
    "    \n",
    "    if 'entity' in analysis_type: \n",
    "        cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "    elif 'cui' in analysis_type: \n",
    "        cols_to_keep = ['case', 'label']\n",
    "    elif 'full' in analysis_type: \n",
    "        cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "   \n",
    "    for system in systems:\n",
    "        print(system)\n",
    "        sys_ann = get_sys_data(system, analysis_type, corpus, filter_semtype, semtype)\n",
    "        df = sys_ann.copy()\n",
    "        df = df.dropna()\n",
    "        \n",
    "        df = df.rename(index=str, columns={\"note_id\": \"case\", \"cui\": \"value\"})\n",
    "        df = set_labels(analysis_type, df)\n",
    "        labels = get_labels(analysis_type, corpus, filter_semtype, semtype)\n",
    "        \n",
    "        sys = df[df['system']==system][cols_to_keep].copy()\n",
    "        test = vectorized_annotations(sys, analysis_type, labels)\n",
    "      \n",
    "        if analysis_type != 'cui':\n",
    "            vector = np.asarray(flatten_list(test), dtype=np.int32) \n",
    "        else:\n",
    "            vector = test\n",
    "        \n",
    "        d[system] = vector\n",
    "        sys_test.append(d[system])\n",
    "        \n",
    "    \n",
    "    n = int(len(systems) / 2)\n",
    "    if analysis_type != 'full':\n",
    "        output = sum(np.array(sys_test))\n",
    "        if ((len(systems) % 2) != 0):\n",
    "            vote = np.where(output > n, 1, 0)\n",
    "        else:\n",
    "            vote = np.where(output > n, 1, \n",
    "             (np.where(output == n, random.randint(0, 1), 0)))\n",
    "        \n",
    "    elif analysis_type == 'full':\n",
    "#         vote = mode(np.stack(sys_test), axis=0)\n",
    "#         vote = vote.mode[0]\n",
    "\n",
    "        vote = list()\n",
    "        X = np.vstack(sys_test)\n",
    "#         for i in range(X.shape[1]):\n",
    "#             unique, counts = np.unique(X[:, i], return_counts=True)\n",
    "#             print(systems, unique, counts)\n",
    "            \n",
    "#             idx = np.argmax(counts)\n",
    "#             if counts[idx] > n:\n",
    "#                 vote.append(unique[idx])\n",
    "#             else:\n",
    "#                 vote.append(random.choice(unique))\n",
    "        \n",
    "        # \n",
    "        for i in range(X.shape[1]):\n",
    "            unique, counts = np.unique(X[:, i], return_counts=True)\n",
    "            idx = np.argmax(counts)\n",
    "            if counts[idx] > n or len(counts) == 1:\n",
    "                vote.append(unique[idx])\n",
    "            elif len(counts) == X.shape[0]:\n",
    "                vote.append(random.choice(unique))\n",
    "            else:\n",
    "                # take highest, non-majority\n",
    "                m = 1\n",
    "                val = 0\n",
    "                for j in range(len(list(counts))):\n",
    "                    if counts[j] > m:\n",
    "                        m = counts[j]\n",
    "                        val = unique[j]\n",
    "\n",
    "                if j == len(list(counts)) - 1:\n",
    "                    if m == 1:\n",
    "                        vote.append(random.choice(unique))\n",
    "                    else:\n",
    "                        vote.append(val)\n",
    "        \n",
    "    return vote\n",
    "\n",
    "\n",
    "#@ft.lru_cache(maxsize=None)\n",
    "def vectorized_cooccurences_test(ref, vote, analysis_type: str, corpus: str, filter_semtype, semtype = None) -> np.int64:\n",
    "   \n",
    "    if analysis_type != 'cui':\n",
    "        report = classification_report(ref, vote, output_dict=True)\n",
    "        macro_precision =  report['macro avg']['precision'] \n",
    "        macro_recall = report['macro avg']['recall']    \n",
    "        macro_f1 = report['macro avg']['f1-score']\n",
    "        if analysis_type == 'entity':\n",
    "            TN, FP, FN, TP = confusion_matrix(ref, vote).ravel()\n",
    "            return ((TP, TN, FP, FN), (macro_precision, macro_recall, macro_f1))\n",
    "        else:\n",
    "            return ((0, 0, 0, 0), (macro_precision, macro_recall, macro_f1))\n",
    "            \n",
    "    else:\n",
    "        x_sparse = sparse.csr_matrix(ref)\n",
    "        y_sparse = sparse.csr_matrix(vote)\n",
    "        report = classification_report(x_sparse, y_sparse, output_dict=True)\n",
    "        macro_precision =  report['macro avg']['precision'] \n",
    "        macro_recall = report['macro avg']['recall']    \n",
    "        macro_f1 = report['macro avg']['f1-score']\n",
    "        return ((0, 0, 0, 0), (macro_precision, macro_recall, macro_f1))\n",
    "\n",
    "    \n",
    "def majority_overlap_vote_out(ref, vote, corpus):   \n",
    "   \n",
    "    ((TP, TN, FP, FN),(p,r,f1)) = vectorized_cooccurences_test(ref, vote, analysis_type, corpus, filter_semtype, semtype = None)\n",
    "    \n",
    "    if analysis_type == 'entity':\n",
    "        #TP, TN, FP, FN = confused(vote, ref)\n",
    "        print((TP, TN, FP, FN),(p,r,f1))\n",
    "        system_n = TP + FP\n",
    "        reference_n = TP + FN\n",
    "\n",
    "        d = cm_dict(FN, FP, TP, system_n, reference_n)\n",
    "\n",
    "        # get CI:\n",
    "        [r, dr, r_lower_bound, r_upper_bound] = normal_approximation_binomial_confidence_interval(TP, TP + FN)\n",
    "        [p, dp, p_lower_bound, p_upper_bound] = normal_approximation_binomial_confidence_interval(TP, TP + FP)\n",
    "        [f, df, f_lower_bound, f_upper_bound] = f1_score_confidence_interval(r, p, dr, dp)\n",
    "\n",
    "        d['r_upper_bound'] = r_upper_bound\n",
    "        d['r_lower_bound'] = r_lower_bound\n",
    "\n",
    "        d['p_upper_bound'] = p_upper_bound\n",
    "        d['p_lower_bound'] = p_lower_bound\n",
    "\n",
    "        d['f_upper_bound'] = f_upper_bound\n",
    "        d['f_lower_bound'] = f_lower_bound\n",
    "\n",
    "        d['TN'] = TN\n",
    "        d['corpus'] = corpus\n",
    "        print(d)\n",
    "    \n",
    "    else:\n",
    "        d = {}\n",
    "        d['precision'] = p\n",
    "        d['recall'] = r\n",
    "        d['F1'] = f1\n",
    "    \n",
    "    metrics = pd.DataFrame(d, index=[0])\n",
    "    \n",
    "    return metrics\n",
    "    \n",
    "# control vote run\n",
    "def majority_vote(systems, analysis_type, corpus, run_type, filter_semtype, semtypes = None):\n",
    "    print(semtypes, systems)\n",
    "    if filter_semtype:\n",
    "        \n",
    "        metrics = pd.DataFrame()\n",
    "        for semtype in semtypes:\n",
    "            test = get_valid_systems(systems, semtype)\n",
    "            print('SYSYEMS FOR SEMTYPE', semtype, 'ARE', test)\n",
    "            \n",
    "            if run_type == 'overlap':\n",
    "                ref = get_reference_vector(analysis_type, corpus, filter_semtype, semtype)\n",
    "                vote = majority_overlap_sys(test, analysis_type, corpus, filter_semtype, semtype)\n",
    "                out = majority_overlap_vote_out(ref, vote, corpus)\n",
    "                #generate_ensemble_metrics(metrics, analysis_type, corpus, ensemble_type, filter_semtype, semtype)\n",
    "            elif run_type == 'exact':\n",
    "                sys = majority_exact_sys(test, analysis_type, corpus, filter_semtype, semtype)\n",
    "                d = majority_exact_vote_out(sys, filter_semtype, semtype)\n",
    "                metrics = pd.DataFrame(d, index=[0])\n",
    "            elif run_type == 'cui':\n",
    "                sys = majority_cui_sys(test, analysis_type, corpus, filter_semtype, semtype)\n",
    "                d = majority_cui_vote_out(sys, filter_semtype, semtype)\n",
    "                metrics = pd.DataFrame(d, index=[0])\n",
    "            \n",
    "            out['semgroup'] = semtype\n",
    "            out['systems'] = ','.join(test)\n",
    "            generate_ensemble_metrics(out, analysis_type, corpus, ensemble_type, filter_semtype, semtype)\n",
    "            frames = [metrics, out]\n",
    "            metrics = pd.concat(frames, ignore_index=True, sort=False)\n",
    "                \n",
    "    else:\n",
    "        if run_type == 'overlap':\n",
    "            ref = get_reference_vector(analysis_type, corpus, filter_semtype)\n",
    "            vote = majority_overlap_sys(systems, analysis_type, corpus, filter_semtype)\n",
    "            metrics = majority_overlap_vote_out(ref, vote, corpus)\n",
    "            \n",
    "        elif run_type == 'exact':\n",
    "            sys = majority_exact_sys(systems, analysis_type, corpus, filter_semtype)\n",
    "            d = majority_exact_vote_out(sys, filter_semtype)\n",
    "            metrics = pd.DataFrame(d, index=[0])\n",
    "            \n",
    "        elif run_type == 'cui':\n",
    "            sys = majority_cui_sys(systems, analysis_type, corpus, filter_semtype)\n",
    "            d = majority_cui_vote_out(sys, filter_semtype)\n",
    "            metrics = pd.DataFrame(d, index=[0])\n",
    "            \n",
    "        metrics['systems'] = ','.join(systems)\n",
    "        generate_ensemble_metrics(metrics, analysis_type, corpus, ensemble_type, filter_semtype)\n",
    "    \n",
    "    print(metrics)\n",
    "    \n",
    "    return metrics\n",
    "    \n",
    "def majority_cui_sys(systems, analysis_type, corpus, filter_semtype, semtype = None):\n",
    "   \n",
    "    cols_to_keep = ['cui', 'note_id', 'system']\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for system in systems:\n",
    "        if filter_semtype:\n",
    "            sys = get_sys_data(system, analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "            sys = get_sys_data(system, analysis_type, corpus, filter_semtype)\n",
    "            \n",
    "        sys = sys[sys['system'] == system][cols_to_keep].drop_duplicates()\n",
    "        \n",
    "        frames = [df, sys]\n",
    "        df = pd.concat(frames)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def majority_cui_vote_out(sys, filter_semtype, semtype = None):\n",
    "    \n",
    "    sys = sys.astype(str)\n",
    "    sys['value_cui'] = list(zip(sys.cui, sys.note_id.astype(str)))\n",
    "    sys['count'] = sys.groupby(['value_cui'])['value_cui'].transform('count')\n",
    "\n",
    "    n = int(len(systems) / 2)\n",
    "    if ((len(systems) % 2) != 0):\n",
    "        sys = sys[sys['count'] > n]\n",
    "    else:\n",
    "        # https://stackoverflow.com/questions/23330654/update-a-dataframe-in-pandas-while-iterating-row-by-row\n",
    "        for i in sys.index:\n",
    "            if sys.at[i, 'count'] == n:\n",
    "                sys.at[i, 'count'] = random.choice([1,len(systems)])\n",
    "        sys = sys[sys['count'] > n]\n",
    "\n",
    "    sys = sys.drop_duplicates(subset=['value_cui', 'cui', 'note_id'])\n",
    "    ref = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "\n",
    "    c = get_cooccurences(ref, sys, analysis_type, corpus) # get matches, FN, etc.\n",
    "\n",
    "    if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "        # get dictionary of confusion matrix metrics\n",
    "        print(cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n))\n",
    "        return cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n)\n",
    "    \n",
    "\n",
    "def majority_exact_sys(systems, analysis_type, corpus, filter_semtype, semtype = None):\n",
    "   \n",
    "    cols_to_keep = ['begin', 'end', 'note_id', 'system']\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for system in systems:\n",
    "        if filter_semtype:\n",
    "            sys = get_sys_data(system, analysis_type, corpus, filter_semtype, semtype)\n",
    "        else:\n",
    "            sys = get_sys_data(system, analysis_type, corpus, filter_semtype)\n",
    "            \n",
    "        sys = sys[sys['system'] == system][cols_to_keep].drop_duplicates()\n",
    "        \n",
    "        frames = [df, sys]\n",
    "        df = pd.concat(frames)\n",
    "        \n",
    "    return df\n",
    "        \n",
    "def majority_exact_vote_out(sys, filter_semtype, semtype = None):\n",
    "    sys['span'] = list(zip(sys.begin, sys.end, sys.note_id.astype(str)))\n",
    "    sys['count'] = sys.groupby(['span'])['span'].transform('count')\n",
    "\n",
    "    n = int(len(systems) / 2)\n",
    "    if ((len(systems) % 2) != 0):\n",
    "        sys = sys[sys['count'] > n]\n",
    "    else:\n",
    "        # https://stackoverflow.com/questions/23330654/update-a-dataframe-in-pandas-while-iterating-row-by-row\n",
    "        for i in sys.index:\n",
    "            if sys.at[i, 'count'] == n:\n",
    "                sys.at[i, 'count'] = random.choice([1,len(systems)])\n",
    "        sys = sys[sys['count'] > n]\n",
    "\n",
    "    sys = sys.drop_duplicates(subset=['span', 'begin', 'end', 'note_id'])\n",
    "    ref = get_ref_ann(analysis_type, corpus, filter_semtype, semtype)\n",
    "\n",
    "    c = get_cooccurences(ref, sys, analysis_type, corpus) # get matches, FN, etc.\n",
    "\n",
    "    if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "        # get dictionary of confusion matrix metrics\n",
    "        print(cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n))\n",
    "        return cm_dict(c.ref_only, c.system_only, c.ref_system_match, c.system_n, c.ref_n)\n",
    "    \n",
    "#ensemble_type = 'vote'        \n",
    "#filter_semtype = False\n",
    "#majority_vote(systems, analysis_type, corpus, run_type, filter_semtype, semtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls'] ('analytical_i2b2.csv', 'concepts.i2b2_all')\n",
      "          test  max_prop_error_reduction         p         r  F1-score  \\\n",
      "0   COMP(A, B)                  0.871511  0.805321  0.937028  0.866196   \n",
      "1   COMP(B, A)                  0.666648  0.297365  0.872194  0.443518   \n",
      "2   COMP(A, B)                  0.365804  0.163411  0.566136  0.253617   \n",
      "3   COMP(B, A)                  0.247469  0.209206  0.311090  0.250172   \n",
      "4   COMP(A, B)                  0.394371  0.397028  0.391740  0.394367   \n",
      "5   COMP(B, A)                  0.648927  0.408364  0.749048  0.528566   \n",
      "6   COMP(A, B)                  0.296031  0.291872  0.300148  0.295952   \n",
      "7   COMP(B, A)                  0.339856  0.141141  0.464049  0.216449   \n",
      "8   COMP(A, B)                  0.667723  0.263898  0.892495  0.407348   \n",
      "9   COMP(B, A)                  0.848028  0.807212  0.915893  0.858125   \n",
      "10  COMP(A, B)                  0.676563  0.378853  0.842270  0.522628   \n",
      "11  COMP(B, A)                  0.927733  0.831134  0.967937  0.894334   \n",
      "12  COMP(A, B)                  0.717725  0.419032  0.883979  0.568553   \n",
      "13  COMP(B, A)                  0.897972  0.804767  0.956223  0.873982   \n",
      "14  COMP(A, B)                  0.426487  0.410398  0.453238  0.430755   \n",
      "15  COMP(B, A)                  0.719822  0.387980  0.857934  0.534324   \n",
      "16  COMP(A, B)                  0.320109  0.370318  0.236627  0.288748   \n",
      "17  COMP(B, A)                  0.462691  0.192058  0.631831  0.294574   \n",
      "18  COMP(A, B)                  0.640118  0.481287  0.706223  0.572451   \n",
      "19  COMP(B, A)                  0.417824  0.358821  0.454699  0.401110   \n",
      "\n",
      "                          system    semgroup  \n",
      "0        ('clamp', 'biomedicus')  All groups  \n",
      "1        ('biomedicus', 'clamp')  All groups  \n",
      "2       ('ctakes', 'biomedicus')  All groups  \n",
      "3       ('biomedicus', 'ctakes')  All groups  \n",
      "4      ('metamap', 'biomedicus')  All groups  \n",
      "5      ('biomedicus', 'metamap')  All groups  \n",
      "6   ('quick_umls', 'biomedicus')  All groups  \n",
      "7   ('biomedicus', 'quick_umls')  All groups  \n",
      "8            ('ctakes', 'clamp')  All groups  \n",
      "9            ('clamp', 'ctakes')  All groups  \n",
      "10          ('metamap', 'clamp')  All groups  \n",
      "11          ('clamp', 'metamap')  All groups  \n",
      "12       ('quick_umls', 'clamp')  All groups  \n",
      "13       ('clamp', 'quick_umls')  All groups  \n",
      "14         ('metamap', 'ctakes')  All groups  \n",
      "15         ('ctakes', 'metamap')  All groups  \n",
      "16      ('quick_umls', 'ctakes')  All groups  \n",
      "17      ('ctakes', 'quick_umls')  All groups  \n",
      "18     ('quick_umls', 'metamap')  All groups  \n",
      "19     ('metamap', 'quick_umls')  All groups  \n",
      " done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "         22085998 function calls (21784987 primitive calls) in 58.226 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "     7631   21.162    0.003   21.162    0.003 {pandas._libs.ops.scalar_compare}\n",
       "     6556    4.197    0.001    9.705    0.001 <ipython-input-236-d810081bbf10>:4(label_vector)\n",
       "71216/55904    3.838    0.000    3.906    0.000 {built-in method numpy.array}\n",
       "     6556    3.458    0.001    3.458    0.001 <ipython-input-236-d810081bbf10>:14(<listcomp>)\n",
       "       10    2.821    0.282   55.570    5.557 <ipython-input-236-d810081bbf10>:109(vectorized_complementarity)\n",
       "        1    2.626    2.626   58.229   58.229 <ipython-input-250-93e2c218b0d7>:106(ad_hoc)\n",
       "    12700    2.506    0.000   12.233    0.001 <ipython-input-235-51033ee3970e>:3(wrapper)\n",
       "  1575444    2.067    0.000    2.067    0.000 {built-in method __new__ of type object at 0x1045e7778}\n",
       "  1625596    1.560    0.000    1.560    0.000 {built-in method numpy.arange}\n",
       "   7621/1    0.538    0.000   58.229   58.229 {built-in method builtins.exec}\n",
       "     7620    0.516    0.000    1.137    0.000 __init__.py:316(namedtuple)\n",
       "     6556    0.475    0.000    2.016    0.000 <ipython-input-236-d810081bbf10>:13(<listcomp>)\n",
       "2269666/2269660    0.447    0.000    0.739    0.000 {built-in method builtins.isinstance}\n",
       "        1    0.380    0.380    0.383    0.383 {method 'read' of 'pandas._libs.parsers.TextReader' objects}\n",
       "  1567308    0.369    0.000    2.523    0.000 __init__.py:403(_make)\n",
       "     5020    0.339    0.000    0.618    0.000 <ipython-input-236-d810081bbf10>:21(confused)\n",
       "    58971    0.317    0.000    0.317    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
       "       10    0.303    0.030    0.303    0.030 <ipython-input-236-d810081bbf10>:195(<listcomp>)\n",
       "       42    0.300    0.007    0.300    0.007 managers.py:1959(<listcomp>)\n",
       "       10    0.289    0.029    0.289    0.029 <ipython-input-236-d810081bbf10>:198(<listcomp>)\n",
       "       10    0.288    0.029    0.288    0.029 <ipython-input-236-d810081bbf10>:199(<listcomp>)\n",
       "2728776/2466676    0.269    0.000    0.356    0.000 {built-in method builtins.len}\n",
       "      192    0.265    0.001    0.265    0.001 {method 'copy' of 'numpy.ndarray' objects}\n",
       "    91893    0.204    0.000    0.363    0.000 generic.py:5276(__setattr__)\n",
       "   789784    0.202    0.000    0.291    0.000 generic.py:10(_check)\n",
       "    30487    0.167    0.000    2.029    0.000 indexing.py:1364(_getitem_lowerdim)\n",
       "1205099/1205096    0.163    0.000    0.166    0.000 {built-in method builtins.getattr}\n",
       "    20480    0.147    0.000    0.150    0.000 base.py:3897(__contains__)\n",
       "    15497    0.128    0.000    0.886    0.000 algorithms.py:1565(take_nd)\n",
       "38211/38210    0.122    0.000    1.121    0.000 series.py:183(__init__)\n",
       "   156984    0.118    0.000    0.314    0.000 common.py:1565(is_extension_array_dtype)\n",
       "      341    0.118    0.000    0.120    0.000 {pandas._libs.lib.infer_dtype}\n",
       "    30553    0.115    0.000    0.458    0.000 managers.py:979(iget)\n",
       "   157083    0.113    0.000    0.160    0.000 dtypes.py:75(find)\n",
       "   214197    0.107    0.000    0.219    0.000 {pandas._libs.lib.is_list_like}\n",
       "    38155    0.105    0.000    0.335    0.000 base.py:1026(__iter__)\n",
       "    53928    0.100    0.000    0.213    0.000 blocks.py:118(__init__)\n",
       "       17    0.098    0.006    0.099    0.006 {method 'factorize' of 'pandas._libs.hashtable.StringHashTable' objects}\n",
       "     7903    0.097    0.000    0.227    0.000 managers.py:212(_rebuild_blknos_and_blklocs)\n",
       "    31809    0.091    0.000    0.091    0.000 {built-in method numpy.empty}\n",
       "    99127    0.091    0.000    0.393    0.000 common.py:99(is_bool_indexer)\n",
       "    61461    0.089    0.000    0.089    0.000 {method 'settimeout' of '_socket.socket' objects}\n",
       "    30643    0.088    0.000    0.282    0.000 connections.py:1195(_read_row_from_packet)\n",
       "66191/66169    0.088    0.000    0.330    0.000 generic.py:5257(__getattr__)\n",
       "     7671    0.087    0.000    0.087    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
       "    55308    0.087    0.000    0.226    0.000 _dtype.py:319(_name_get)\n",
       "    53928    0.084    0.000    0.586    0.000 blocks.py:3027(make_block)\n",
       "    38107    0.084    0.000    5.914    0.000 indexing.py:1754(__getitem__)\n",
       "   122214    0.081    0.000    0.111    0.000 generic.py:409(_get_axis_name)\n",
       "    38210    0.080    0.000    0.380    0.000 managers.py:1467(__init__)\n",
       "    30487    0.079    0.000    1.174    0.000 frame.py:2722(_ixs)\n",
       "     7699    0.077    0.000    0.077    0.000 {pandas._libs.algos.take_2d_axis1_object_object}\n",
       "     7666    0.077    0.000    0.077    0.000 {method 'take' of 'numpy.ndarray' objects}\n",
       "     7666    0.075    0.000    0.138    0.000 indexers.py:173(maybe_convert_indices)\n",
       "   576113    0.072    0.000    0.072    0.000 {built-in method builtins.issubclass}\n",
       "    45972    0.072    0.000    0.072    0.000 generic.py:190(__init__)\n",
       "      100    0.071    0.001    0.137    0.001 blocks.py:3085(_merge_blocks)\n",
       "    15442    0.068    0.000    0.233    0.000 cast.py:347(maybe_promote)\n",
       "     7666    0.068    0.000    2.135    0.000 managers.py:1373(take)\n",
       "    72209    0.068    0.000    0.167    0.000 common.py:1708(_is_dtype_type)\n",
       "   238663    0.068    0.000    0.093    0.000 base.py:615(__len__)\n",
       "    15497    0.067    0.000    0.198    0.000 algorithms.py:1436(_get_take_nd_function)\n",
       "     7639    0.066    0.000    0.066    0.000 {pandas._libs.algos.take_2d_axis1_int64_int64}\n",
       "    30487    0.066    0.000    1.478    0.000 indexing.py:2116(_getitem_axis)\n",
       "     7620    0.065    0.000    4.292    0.001 frame.py:955(itertuples)\n",
       "      189    0.064    0.000    0.067    0.000 cast.py:1458(construct_1d_object_array_from_listlike)\n",
       "   122214    0.063    0.000    0.203    0.000 generic.py:422(_get_axis)\n",
       "    30716    0.063    0.000    0.290    0.000 connections.py:648(_read_packet)\n",
       "    61132    0.063    0.000    0.110    0.000 series.py:428(name)\n",
       "      285    0.062    0.000    0.062    0.000 {built-in method numpy.concatenate}\n",
       "   215011    0.062    0.000    0.062    0.000 {built-in method _abc._abc_instancecheck}\n",
       "   110620    0.062    0.000    0.086    0.000 numerictypes.py:293(issubclass_)\n",
       "    61432    0.062    0.000    0.195    0.000 connections.py:687(_read_bytes)\n",
       "       13    0.056    0.004    0.056    0.004 {method 'factorize' of 'pandas._libs.hashtable.Int64HashTable' objects}\n",
       "    53928    0.056    0.000    0.062    0.000 blocks.py:251(mgr_locs)\n",
       "    60974    0.053    0.000    0.382    0.000 indexing.py:1976(_validate_key)\n",
       "   215011    0.051    0.000    0.113    0.000 abc.py:137(__instancecheck__)\n",
       "       46    0.050    0.001    0.060    0.001 managers.py:1828(_stack_arrays)\n",
       "    84393    0.049    0.000    0.252    0.000 {built-in method builtins.hasattr}\n",
       "    76327    0.049    0.000    0.077    0.000 managers.py:1548(dtype)\n",
       "    55310    0.048    0.000    0.139    0.000 numerictypes.py:365(issubdtype)\n",
       "    15344    0.048    0.000    1.073    0.000 blocks.py:1271(take_nd)\n",
       "    15901    0.048    0.000    0.137    0.000 blocks.py:343(ftype)\n",
       "   122183    0.048    0.000    0.161    0.000 protocol.py:168(read_length_coded_string)\n",
       "   122183    0.047    0.000    0.055    0.000 protocol.py:63(read)\n",
       "    60974    0.047    0.000    0.187    0.000 indexing.py:2045(_validate_integer)\n",
       "    51903    0.046    0.000    0.200    0.000 common.py:222(is_object_dtype)\n",
       "    30487    0.046    0.000    0.083    0.000 indexing.py:2016(_is_scalar_access)\n",
       "    38211    0.045    0.000    0.092    0.000 series.py:376(_set_axis)\n",
       "     2540    0.045    0.000    0.045    0.000 {method 'symmetric_difference' of 'set' objects}\n",
       "     7666    0.044    0.000    2.443    0.000 generic.py:3300(take)\n",
       "    31363    0.044    0.000    0.109    0.000 managers.py:163(shape)\n",
       "    38100    0.042    0.000    2.736    0.000 frame.py:1026(<genexpr>)\n",
       "    30553    0.042    0.000    0.441    0.000 frame.py:3073(_box_col_values)\n",
       "    91456    0.041    0.000    0.064    0.000 managers.py:1565(internal_values)\n",
       "    30487    0.039    0.000    0.421    0.000 indexing.py:695(_has_valid_tuple)\n",
       "    30702    0.039    0.000    0.062    0.000 base.py:3911(__getitem__)\n",
       "    91461    0.038    0.000    0.061    0.000 indexing.py:1756(<genexpr>)\n",
       "    46085    0.036    0.000    0.351    0.000 blocks.py:275(make_block_same_class)\n",
       "     7668    0.035    0.000    0.084    0.000 cast.py:1088(maybe_castable)\n",
       "       11    0.035    0.003    0.603    0.055 connections.py:1182(_read_rowdata_packet)\n",
       "     6557    0.035    0.000    0.035    0.000 {built-in method numpy.zeros}\n",
       "     7978    0.035    0.000    0.206    0.000 blocks.py:2981(get_block_type)\n",
       "        1    0.035    0.035    0.035    0.035 {method 'get_labels_groupby' of 'pandas._libs.hashtable.Int64HashTable' objects}\n",
       "     2540    0.034    0.000    0.034    0.000 {method 'intersection' of 'set' objects}\n",
       "     7620    0.034    0.000    3.211    0.000 indexing.py:1902(_getitem_axis)\n",
       "    23198    0.033    0.000    0.115    0.000 blocks.py:2591(__init__)\n",
       "     7789    0.033    0.000    0.253    0.000 construction.py:388(sanitize_array)\n",
       "     7681    0.033    0.000    0.381    0.000 frame.py:2767(__getitem__)\n",
       "    53491    0.033    0.000    0.060    0.000 series.py:432(name)\n",
       "   122218    0.033    0.000    0.059    0.000 protocol.py:150(read_length_encoded_integer)\n",
       "     7639    0.032    0.000    0.285    0.000 indexers.py:278(check_array_indexer)\n",
       "    52130    0.032    0.000    0.124    0.000 base.py:247(is_dtype)\n",
       "    91461    0.032    0.000    0.126    0.000 indexers.py:23(is_list_like_indexer)\n",
       "    30487    0.032    0.000    0.130    0.000 indexing.py:710(_is_nested_tuple_indexer)\n",
       "    15901    0.032    0.000    0.085    0.000 _dtype.py:46(__str__)\n",
       "     7666    0.031    0.000    1.647    0.000 managers.py:1224(reindex_indexer)\n",
       "    38555    0.031    0.000    0.160    0.000 construction.py:337(extract_array)\n",
       "        5    0.031    0.006    0.528    0.106 <ipython-input-242-3da3b5c39d42>:1(get_sys_data)\n",
       "   161030    0.029    0.000    0.029    0.000 {method 'get' of 'dict' objects}\n",
       "     7633    0.029    0.000   22.297    0.003 __init__.py:518(wrapper)\n",
       "   183100    0.029    0.000    0.029    0.000 managers.py:1520(_block)\n",
       "    76327    0.029    0.000    0.105    0.000 series.py:414(dtype)\n",
       "     7853    0.028    0.000    0.520    0.000 managers.py:122(__init__)\n",
       "    15275    0.028    0.000    0.047    0.000 numpy_.py:138(__init__)\n",
       "     7780    0.028    0.000    0.087    0.000 managers.py:329(_verify_integrity)\n",
       "        7    0.028    0.004    0.028    0.004 {pandas._libs.hashtable.duplicated_int64}\n",
       "    20109    0.027    0.000    0.254    0.000 fromnumeric.py:69(_wrapreduction)\n",
       "    60974    0.027    0.000    0.069    0.000 indexing.py:2394(is_label_like)\n",
       "    99158    0.027    0.000    0.131    0.000 inference.py:220(is_array_like)\n",
       "    15394    0.027    0.000    0.047    0.000 generic.py:5235(__finalize__)\n",
       "    15459    0.027    0.000    0.179    0.000 base.py:4046(equals)\n",
       "    15957    0.027    0.000    0.027    0.000 {method 'fill' of 'numpy.ndarray' objects}\n",
       "    91456    0.026    0.000    0.091    0.000 series.py:480(_values)\n",
       "   122219    0.026    0.000    0.026    0.000 protocol.py:117(read_uint8)\n",
       "     7666    0.026    0.000    2.680    0.000 generic.py:3399(_take_with_is_copy)\n",
       "    69652    0.026    0.000    0.037    0.000 inference.py:358(is_hashable)\n",
       "     7633    0.025    0.000   21.342    0.003 array_ops.py:202(comparison_op)\n",
       "     7631    0.025    0.000   21.211    0.003 array_ops.py:42(comp_method_OBJECT_ARRAY)\n",
       "     7634    0.024    0.000    0.791    0.000 __init__.py:448(_construct_result)\n",
       "   122142    0.024    0.000    0.024    0.000 {method 'decode' of 'bytes' objects}\n",
       "    20100    0.024    0.000    0.282    0.000 fromnumeric.py:1966(sum)\n",
       "    91461    0.023    0.000    0.064    0.000 indexing.py:716(<genexpr>)\n",
       "     7761    0.023    0.000    0.168    0.000 frame.py:414(__init__)\n",
       "    30553    0.023    0.000    0.070    0.000 generic.py:3227(_set_as_cached)\n",
       "    76308    0.023    0.000    0.031    0.000 common.py:330(apply_if_callable)\n",
       "    30487    0.022    0.000    2.472    0.000 indexing.py:2065(_getitem_tuple)\n",
       "     7666    0.022    0.000    0.199    0.000 base.py:744(take)\n",
       "    12700    0.022    0.000    0.022    0.000 <ipython-input-235-51033ee3970e>:4(<listcomp>)\n",
       "    15905    0.022    0.000    0.079    0.000 common.py:1435(is_bool_dtype)\n",
       "     7620    0.021    0.000    2.865    0.000 indexing.py:1779(_getbool_axis)\n",
       "    54189    0.021    0.000    0.043    0.000 base.py:5293(ensure_index)\n",
       "    53928    0.021    0.000    0.021    0.000 blocks.py:129(_check_ndim)\n",
       "     2543    0.021    0.000    0.021    0.000 {method 'union' of 'set' objects}\n",
       "    61432    0.021    0.000    0.042    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
       "    94089    0.021    0.000    0.065    0.000 managers.py:165(<genexpr>)\n",
       "     7798    0.021    0.000    0.082    0.000 base.py:526(_shallow_copy)\n",
       "     7639    0.021    0.000    1.078    0.000 managers.py:1260(<listcomp>)\n",
       "   107998    0.020    0.000    0.020    0.000 blocks.py:339(dtype)\n",
       "    30811    0.020    0.000    0.086    0.000 {built-in method builtins.any}\n",
       "62498/47187    0.020    0.000    0.173    0.000 numeric.py:469(asarray)\n",
       "      301    0.020    0.000    0.020    0.000 {method 'recv_into' of '_socket.socket' objects}\n",
       "    30553    0.020    0.000    0.020    0.000 blocks.py:365(iget)\n",
       "    30487    0.019    0.000    1.193    0.000 indexing.py:627(_get_loc)\n",
       "    70538    0.018    0.000    0.027    0.000 common.py:208(<lambda>)\n",
       "     7634    0.018    0.000   22.344    0.003 common.py:49(new_method)\n",
       "    38681    0.018    0.000    0.130    0.000 {method 'any' of 'numpy.ndarray' objects}\n",
       "    30487    0.017    0.000    0.020    0.000 common.py:284(is_null_slice)\n",
       "    54212    0.016    0.000    0.022    0.000 managers.py:167(ndim)\n",
       "   101273    0.016    0.000    0.016    0.000 blocks.py:247(mgr_locs)\n",
       "    70538    0.016    0.000    0.016    0.000 common.py:206(classes)\n",
       "    47122    0.016    0.000    0.022    0.000 common.py:1672(_get_dtype)\n",
       "        7    0.016    0.002    0.060    0.009 sorting.py:21(get_group_index)\n",
       "    30607    0.016    0.000    0.029    0.000 generic.py:515(ndim)\n",
       "    31274    0.016    0.000    0.032    0.000 base.py:3638(values)\n",
       "    15275    0.015    0.000    0.015    0.000 numpy_.py:42(__init__)\n",
       "    31282    0.015    0.000    0.015    0.000 {method 'view' of 'numpy.ndarray' objects}\n",
       "    30508    0.015    0.000    0.037    0.000 managers.py:314(__len__)\n",
       "     7643    0.015    0.000    0.015    0.000 {built-in method pandas._libs.lib.is_datetime_array}\n",
       "     7885    0.015    0.000    0.161    0.000 managers.py:655(_consolidate_check)\n",
       "    15275    0.015    0.000    0.080    0.000 series.py:515(array)\n",
       "    25018    0.014    0.000    0.021    0.000 common.py:1844(pandas_dtype)\n",
       "     8191    0.014    0.000    0.029    0.000 common.py:252(is_sparse)\n",
       "    99407    0.014    0.000    0.014    0.000 {pandas._libs.lib.is_integer}\n",
       "     7639    0.014    0.000    0.367    0.000 indexing.py:2286(check_bool_indexer)\n",
       "    77465    0.014    0.000    0.014    0.000 {pandas._libs.lib.is_scalar}\n",
       "   166958    0.014    0.000    0.014    0.000 {method 'append' of 'list' objects}\n",
       "    38211    0.014    0.000    0.014    0.000 series.py:403(_set_subtyp)\n",
       "90567/90480    0.014    0.000    0.014    0.000 {built-in method builtins.hash}\n",
       "    30487    0.014    0.000    0.014    0.000 frame.py:515(axes)\n",
       "    15275    0.013    0.000    0.061    0.000 blocks.py:219(array_values)\n",
       "        2    0.013    0.007    0.013    0.007 construction.py:484(<listcomp>)\n",
       "    15442    0.013    0.000    0.049    0.000 cast.py:503(_ensure_dtype_type)\n",
       "    38681    0.013    0.000    0.112    0.000 _methods.py:42(_any)\n",
       "    30702    0.013    0.000    0.017    0.000 common.py:151(cast_scalar_indexer)\n",
       "     8074    0.013    0.000    0.031    0.000 base.py:472(_simple_new)\n",
       "    24721    0.012    0.000    0.073    0.000 common.py:403(is_datetime64tz_dtype)\n",
       "       13    0.012    0.001    0.012    0.001 {pandas._libs.algos.take_2d_axis0_float64_float64}\n",
       "    17850    0.012    0.000    0.027    0.000 <frozen importlib._bootstrap>:1009(_handle_fromlist)\n",
       "     9014    0.012    0.000    0.033    0.000 dtypes.py:1124(is_dtype)\n",
       "       31    0.012    0.000    0.227    0.007 algorithms.py:585(factorize)\n",
       "     7634    0.012    0.000    0.066    0.000 dispatch.py:21(should_extension_dispatch)\n",
       "     7639    0.012    0.000    0.012    0.000 {built-in method builtins.repr}\n",
       "    12702    0.012    0.000    0.195    0.000 base.py:3949(_can_hold_identifiers_and_holds_name)\n",
       "     7678    0.011    0.000    2.748    0.000 {method 'extend' of 'list' objects}\n",
       "    91456    0.011    0.000    0.011    0.000 blocks.py:213(internal_values)\n",
       "    61674    0.011    0.000    0.011    0.000 {method 'startswith' of 'str' objects}\n",
       "    15488    0.011    0.000    0.030    0.000 {pandas._libs.lib.values_from_object}\n",
       "     7740    0.011    0.000    0.022    0.000 missing.py:402(array_equivalent)\n",
       "    84222    0.011    0.000    0.016    0.000 strings.py:1691(<lambda>)\n",
       "       65    0.011    0.000    0.011    0.000 {pandas._libs.algos.take_2d_axis0_object_object}\n",
       "    30487    0.011    0.000    0.011    0.000 indexing.py:93(iloc)\n",
       "     7735    0.011    0.000    0.019    0.000 {method 'join' of 'str' objects}\n",
       "    61063    0.010    0.000    0.010    0.000 {pandas._libs.lib.item_from_zerodim}\n",
       "    15695    0.010    0.000    0.026    0.000 missing.py:132(_isna_new)\n",
       "     7639    0.010    0.000    0.085    0.000 series.py:707(__array__)\n",
       "        1    0.010    0.010    0.026    0.026 {pandas._libs.lib.map_infer_mask}\n",
       "        7    0.010    0.001    0.010    0.001 {pandas._libs.lib.maybe_convert_objects}\n",
       "    15462    0.010    0.000    0.023    0.000 common.py:685(is_dtype_equal)\n",
       "     7636    0.010    0.000    0.017    0.000 numpy_.py:417(to_numpy)\n",
       "     8701    0.010    0.000    0.025    0.000 common.py:372(is_datetime64_dtype)\n",
       "     8381    0.009    0.000    0.025    0.000 base.py:5393(maybe_extract_name)\n",
       "    12704    0.009    0.000    0.043    0.000 base.py:1679(is_object)\n",
       "        1    0.009    0.009    1.425    1.425 <ipython-input-238-ae69ae0eb560>:1(get_metric_data)\n",
       "    30722    0.009    0.000    0.009    0.000 {built-in method _struct.unpack}\n",
       "     7728    0.009    0.000    0.184    0.000 generic.py:5331(_protect_consolidate)\n",
       "    15459    0.009    0.000    0.011    0.000 base.py:573(is_)\n",
       "     7658    0.009    0.000    0.015    0.000 generic.py:3627(_set_is_copy)\n",
       "     7643    0.009    0.000    0.034    0.000 base.py:1737(is_all_dates)\n",
       "    23420    0.009    0.000    0.013    0.000 managers.py:331(<genexpr>)\n",
       "    38100    0.009    0.000    0.009    0.000 __init__.py:388(<genexpr>)\n",
       "    30654    0.008    0.000    0.014    0.000 connections.py:1137(_check_packet_is_eof)\n",
       "    30717    0.008    0.000    0.015    0.000 protocol.py:214(check_error)\n",
       "    76517    0.008    0.000    0.008    0.000 {built-in method builtins.callable}\n",
       "    38582    0.008    0.000    0.008    0.000 managers.py:232(items)\n",
       "     7853    0.008    0.000    0.015    0.000 managers.py:128(<listcomp>)\n",
       "        1    0.008    0.008    0.483    0.483 parsers.py:416(_read)\n",
       "    20109    0.008    0.000    0.008    0.000 fromnumeric.py:70(<dictcomp>)\n",
       "    68580    0.008    0.000    0.008    0.000 {method 'isidentifier' of 'str' objects}\n",
       "     7789    0.008    0.000    0.096    0.000 construction.py:506(_try_cast)\n",
       "     7739    0.008    0.000    0.008    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
       "     7729    0.007    0.000    0.009    0.000 generic.py:219(_init_mgr)\n",
       "  615/274    0.007    0.000    0.143    0.001 base.py:276(__new__)\n",
       "     7885    0.007    0.000    0.145    0.000 managers.py:656(<listcomp>)\n",
       "     7796    0.007    0.000    0.010    0.000 generic.py:396(_get_axis_number)\n",
       "    15524    0.007    0.000    0.007    0.000 {pandas._libs.algos.ensure_int64}\n",
       "     7728    0.007    0.000    0.174    0.000 generic.py:5344(f)\n",
       "     7833    0.007    0.000    0.007    0.000 {method 'replace' of 'str' objects}\n",
       "     7881    0.007    0.000    0.018    0.000 base.py:505(_get_attributes_dict)\n",
       "     9090    0.007    0.000    0.027    0.000 dtypes.py:917(is_dtype)\n",
       "    68580    0.007    0.000    0.007    0.000 {method '__contains__' of 'frozenset' objects}\n",
       "    30717    0.007    0.000    0.007    0.000 protocol.py:211(is_error_packet)\n",
       "     9653    0.007    0.000    0.032    0.000 common.py:542(is_categorical_dtype)\n",
       "     7634    0.007    0.000    0.019    0.000 __init__.py:101(get_op_result_name)\n",
       "     8139    0.007    0.000    0.007    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
       "       31    0.007    0.000    0.007    0.000 sorting.py:62(maybe_lift)\n",
       "    46256    0.007    0.000    0.007    0.000 {pandas._libs.lib.is_float}\n",
       "     9015    0.007    0.000    0.018    0.000 common.py:441(is_timedelta64_dtype)\n",
       "        2    0.007    0.003    0.680    0.340 sql.py:336(read_sql)\n",
       "    23217    0.006    0.000    0.007    0.000 managers.py:647(is_consolidated)\n",
       "    30716    0.006    0.000    0.006    0.000 protocol.py:56(__init__)\n",
       "    15364    0.006    0.000    0.006    0.000 generic.py:238(attrs)\n",
       "     7659    0.006    0.000    0.025    0.000 construction.py:570(is_empty_data)\n",
       "    15544    0.006    0.000    0.006    0.000 {built-in method pandas._libs.missing.checknull}\n",
       "    15695    0.006    0.000    0.032    0.000 missing.py:49(isna)\n",
       "     7871    0.006    0.000    0.019    0.000 {built-in method builtins.sum}\n",
       "     7738    0.006    0.000    0.016    0.000 generic.py:426(_get_block_manager_axis)\n",
       "    15490    0.006    0.000    0.151    0.000 managers.py:943(_consolidate_inplace)\n",
       "    30676    0.006    0.000    0.006    0.000 protocol.py:190(is_eof_packet)\n",
       "     7647    0.006    0.000    0.075    0.000 base.py:766(_assert_take_fillable)\n",
       "       42    0.006    0.000    0.513    0.012 managers.py:184(rename_axis)\n",
       "     7728    0.006    0.000    0.190    0.000 generic.py:5341(_consolidate_inplace)\n",
       "     7639    0.006    0.000    0.013    0.000 numpy_.py:183(__array__)\n",
       "    84222    0.006    0.000    0.006    0.000 {method 'strip' of 'str' objects}\n",
       "    15480    0.005    0.000    0.019    0.000 base.py:3700(_internal_get_values)\n",
       "    61152    0.005    0.000    0.005    0.000 {method 'add' of 'set' objects}\n",
       "     9362    0.005    0.000    0.038    0.000 common.py:506(is_interval_dtype)\n",
       "    12704    0.005    0.000    0.012    0.000 generic.py:491(_info_axis)\n",
       "     7978    0.005    0.000    0.041    0.000 common.py:339(is_categorical)\n",
       "        7    0.005    0.001    0.379    0.054 frame.py:4772(drop_duplicates)\n",
       "     9438    0.005    0.000    0.032    0.000 common.py:472(is_period_dtype)\n",
       "     7672    0.005    0.000    0.012    0.000 generic.py:3581(_get_item_cache)\n",
       "     7881    0.005    0.000    0.011    0.000 base.py:509(<dictcomp>)\n",
       "       31    0.005    0.000    0.210    0.007 algorithms.py:456(_factorize_array)\n",
       "        1    0.004    0.004    0.004    0.004 {method 'factorize' of 'pandas._libs.hashtable.Float64HashTable' objects}\n",
       "     7847    0.004    0.000    0.197    0.000 inference.py:96(is_iterator)\n",
       "       26    0.004    0.000    0.004    0.000 {pandas._libs.algos.take_2d_axis0_int64_int64}\n",
       "     7647    0.004    0.000    0.004    0.000 indexing.py:231(loc)\n",
       "        6    0.004    0.001    0.004    0.001 result.py:1192(<listcomp>)\n",
       "    30487    0.004    0.000    0.004    0.000 base.py:626(ndim)\n",
       "    15646    0.004    0.000    0.004    0.000 blocks.py:335(shape)\n",
       "        2    0.004    0.002    0.004    0.002 {pandas._libs.lib.to_object_array_tuples}\n",
       "     8110    0.004    0.000    0.004    0.000 base.py:592(_reset_identity)\n",
       "    15344    0.004    0.000    0.004    0.000 blocks.py:243(fill_value)\n",
       "     7728    0.004    0.000    0.152    0.000 managers.py:927(consolidate)\n",
       "    28006    0.004    0.000    0.004    0.000 {method 'items' of 'dict' objects}\n",
       "    23136    0.003    0.000    0.004    0.000 base.py:3670(_values)\n",
       "     7964    0.003    0.000    0.026    0.000 numeric.py:541(asanyarray)\n",
       "     7620    0.003    0.000    0.010    0.000 indexing.py:1870(_get_partial_string_timestamp_match_key)\n",
       "     7620    0.003    0.000    0.003    0.000 {built-in method sys._getframe}\n",
       "      107    0.003    0.000    0.003    0.000 {built-in method pandas._libs.missing.isnaobj}\n",
       "     7873    0.003    0.000    0.003    0.000 {method 'update' of 'dict' objects}\n",
       "     7621    0.003    0.000    0.003    0.000 {built-in method sys.intern}\n",
       "     7667    0.003    0.000    0.003    0.000 function.py:42(__call__)\n",
       "    15419    0.002    0.000    0.002    0.000 {pandas._libs.algos.ensure_platform_int}\n",
       "     7930    0.002    0.000    0.002    0.000 base.py:638(dtype)\n",
       "     8054    0.002    0.000    0.002    0.000 base.py:1182(name)\n",
       "        2    0.002    0.001    0.038    0.019 frame.py:1549(from_records)\n",
       "     7727    0.002    0.000    0.002    0.000 frame.py:399(_constructor)\n",
       "     7648    0.002    0.000    0.002    0.000 series.py:359(_constructor)\n",
       "     7785    0.002    0.000    0.002    0.000 {pandas._libs.algos.ensure_object}\n",
       "      603    0.001    0.000    0.071    0.000 common.py:219(asarray_tuplesafe)\n",
       "     7683    0.001    0.000    0.001    0.000 base.py:1383(nlevels)\n",
       "        8    0.001    0.000    0.001    0.000 numeric.py:2551(array_equal)\n",
       "        7    0.001    0.000    0.322    0.046 frame.py:4829(duplicated)\n",
       "        1    0.001    0.001    0.001    0.001 blocks.py:368(set)\n",
       "     1950    0.001    0.000    0.001    0.000 base.py:194(construct_from_string)\n",
       "       49    0.001    0.000    0.001    0.000 concat.py:28(get_mgr_concatenation_plan)\n",
       "        7    0.001    0.000    0.001    0.000 {built-in method _operator.inv}\n",
       "       34    0.001    0.000    0.066    0.002 managers.py:1700(form_blocks)\n",
       "        2    0.001    0.000    0.040    0.020 sql.py:121(_wrap_result)\n",
       "        1    0.001    0.001    0.001    0.001 {built-in method _socket.getaddrinfo}\n",
       "      151    0.001    0.000    0.007    0.000 missing.py:225(_isna_ndarraylike)\n",
       "       51    0.001    0.000    0.002    0.000 {pandas._libs.lib.clean_index_list}\n",
       "       75    0.001    0.000    0.001    0.000 {method 'get_indexer' of 'pandas._libs.index.IndexEngine' objects}\n",
       "       76    0.001    0.000    0.274    0.004 managers.py:368(apply)\n",
       "      176    0.001    0.000    0.003    0.000 cast.py:1209(maybe_cast_to_datetime)\n",
       "       27    0.001    0.000    0.023    0.001 managers.py:1274(_slice_take_blocks_ax0)\n",
       "      259    0.001    0.000    0.001    0.000 blocks.py:3053(_extend_blocks)\n",
       "        1    0.001    0.001    0.001    0.001 parsers.py:1864(__init__)\n",
       "       32    0.001    0.000    0.016    0.000 managers.py:1158(insert)\n",
       "       60    0.001    0.000    0.002    0.000 concat.py:268(_get_empty_dtype_and_na)\n",
       "     1230    0.001    0.000    0.006    0.000 common.py:987(is_datetime64_any_dtype)\n",
       "       32    0.001    0.000    0.001    0.000 index_tricks.py:316(__getitem__)\n",
       "      136    0.001    0.000    0.007    0.000 concat.py:145(is_na)\n",
       "       75    0.001    0.000    0.014    0.000 base.py:2706(get_indexer)\n",
       "      520    0.001    0.000    0.001    0.000 _internal.py:886(npy_ctypes_check)\n",
       "       30    0.001    0.000    0.012    0.000 concat.py:292(__init__)\n",
       "      715    0.001    0.000    0.002    0.000 common.py:775(is_integer_dtype)\n",
       "      827    0.001    0.000    0.001    0.000 common.py:1401(is_float_dtype)\n",
       "      203    0.001    0.000    0.032    0.000 base.py:5463(_maybe_cast_data_without_dtype)\n",
       "      109    0.001    0.000    0.001    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
       "      197    0.001    0.000    0.001    0.000 {method 'get_loc' of 'pandas._libs.index.IndexEngine' objects}\n",
       "       21    0.001    0.000    0.589    0.028 generic.py:932(rename)\n",
       "        1    0.001    0.001    0.036    0.036 sorting.py:373(compress_group_index)\n",
       "      320    0.000    0.000    0.001    0.000 blocks.py:169(_consolidate_key)\n",
       "       96    0.000    0.000    0.001    0.000 numeric.py:676(require)\n",
       "     1671    0.000    0.000    0.001    0.000 common.py:216(<lambda>)\n",
       "      158    0.000    0.000    0.266    0.002 blocks.py:694(copy)\n",
       "       30    0.000    0.000    0.023    0.001 managers.py:1988(concatenate_block_managers)\n",
       "        1    0.000    0.000    1.453    1.453 <ipython-input-245-2ac8e15c2bd2>:116(get_ref_ann)\n",
       "      301    0.000    0.000    0.021    0.000 socket.py:575(readinto)\n",
       "      507    0.000    0.000    0.001    0.000 common.py:830(is_signed_integer_dtype)\n",
       "       50    0.000    0.000    0.139    0.003 managers.py:1875(_consolidate)\n",
       "      114    0.000    0.000    0.001    0.000 base.py:602(_engine)\n",
       "       30    0.000    0.000    0.025    0.001 concat.py:454(get_result)\n",
       "       33    0.000    0.000    0.018    0.001 managers.py:1037(set)\n",
       "      120    0.000    0.000    0.001    0.000 concat.py:434(combine_concat_plans)\n",
       "       27    0.000    0.000    0.000    0.000 {method 'sendall' of '_socket.socket' objects}\n",
       "       98    0.000    0.000    0.005    0.000 concat.py:173(get_reindexed_values)\n",
       "       61    0.000    0.000    0.001    0.000 base.py:1656(is_unique)\n",
       "       33    0.000    0.000    0.044    0.001 frame.py:2922(__setitem__)\n",
       "       11    0.000    0.000    0.604    0.055 connections.py:1149(_read_result_packet)\n",
       "       27    0.000    0.000    0.014    0.001 indexing.py:1503(_get_listlike_indexer)\n",
       "       42    0.000    0.000    0.405    0.010 managers.py:1941(_transform_index)\n",
       "       98    0.000    0.000    0.001    0.000 concat.py:126(needs_filling)\n",
       "       33    0.000    0.000    0.024    0.001 frame.py:3570(_sanitize_column)\n",
       "       32    0.000    0.000    0.120    0.004 construction.py:213(init_dict)\n",
       "        8    0.000    0.000    0.000    0.000 socket.py:337(send)\n",
       "       68    0.000    0.000    0.000    0.000 numerictypes.py:578(_can_coerce_all)\n",
       "     1671    0.000    0.000    0.000    0.000 common.py:211(classes_and_not_datetimelike)\n",
       "      442    0.000    0.000    0.001    0.000 common.py:887(is_unsigned_integer_dtype)\n",
       "       32    0.000    0.000    0.006    0.000 base.py:3809(_coerce_scalar_to_index)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'close' of '_io.TextIOWrapper' objects}\n",
       "      142    0.000    0.000    0.002    0.000 cast.py:1102(maybe_infer_to_datetimelike)\n",
       "      143    0.000    0.000    0.000    0.000 shape_base.py:83(atleast_2d)\n",
       "      235    0.000    0.000    0.000    0.000 dtypes.py:715(construct_from_string)\n",
       "      235    0.000    0.000    0.001    0.000 base.py:5409(_maybe_cast_with_dtype)\n",
       "  465/464    0.000    0.000    0.008    0.000 {built-in method builtins.all}\n",
       "       54    0.000    0.000    0.000    0.000 {pandas._libs.algos.take_1d_int64_int64}\n",
       "      215    0.000    0.000    0.007    0.000 concat.py:388(<genexpr>)\n",
       "       73    0.000    0.000    0.276    0.004 managers.py:766(copy)\n",
       "      182    0.000    0.000    0.000    0.000 cast.py:549(infer_dtype_from_scalar)\n",
       "       79    0.000    0.000    0.001    0.000 indexing.py:2261(convert_to_index_sliceable)\n",
       "       66    0.000    0.000    0.002    0.000 managers.py:950(get)\n",
       "      165    0.000    0.000    0.001    0.000 base.py:2637(get_loc)\n",
       "      195    0.000    0.000    0.000    0.000 dtypes.py:1072(construct_from_string)\n",
       "       32    0.000    0.000    0.002    0.000 managers.py:1963(_fast_count_smallints)\n",
       "      195    0.000    0.000    0.001    0.000 dtypes.py:867(construct_from_string)\n",
       "      245    0.000    0.000    0.000    0.000 {method 'match' of 're.Pattern' objects}\n",
       "       50    0.000    0.000    0.000    0.000 {method 'argsort' of 'numpy.ndarray' objects}\n",
       "      146    0.000    0.000    0.002    0.000 base.py:660(view)\n",
       "       34    0.000    0.000    0.045    0.001 construction.py:300(_homogenize)\n",
       "      321    0.000    0.000    0.002    0.000 common.py:1647(_is_dtype)\n",
       "      221    0.000    0.000    0.001    0.000 base.py:853(_ndarray_values)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:628(__array_ufunc__)\n",
       "      120    0.000    0.000    0.001    0.000 cast.py:1418(construct_1d_arraylike_from_scalar)\n",
       "       50    0.000    0.000    0.000    0.000 blocks.py:3098(<listcomp>)\n",
       "      214    0.000    0.000    0.002    0.000 common.py:575(is_string_dtype)\n",
       "       38    0.000    0.000    0.001    0.000 concat.py:31(get_dtype_kinds)\n",
       "       60    0.000    0.000    0.009    0.000 concat.py:236(concatenate_join_units)\n",
       "       38    0.000    0.000    0.002    0.000 concat.py:72(concat_compat)\n",
       "      195    0.000    0.000    0.000    0.000 dtypes.py:334(construct_from_string)\n",
       "       31    0.000    0.000    0.046    0.001 algorithms.py:264(_check_object_for_strings)\n",
       "       51    0.000    0.000    0.007    0.000 base.py:544(_shallow_copy_with_infer)\n",
       "       33    0.000    0.000    0.043    0.001 frame.py:2988(_set_item)\n",
       "       31    0.000    0.000    0.175    0.006 generic.py:5706(copy)\n",
       "       33    0.000    0.000    0.001    0.000 range.py:83(__new__)\n",
       "       79    0.000    0.000    0.007    0.000 concat.py:375(is_uniform_join_units)\n",
       "       27    0.000    0.000    0.001    0.000 indexing.py:1600(_validate_read_indexer)\n",
       "       27    0.000    0.000    0.006    0.000 base.py:3100(reindex)\n",
       "      125    0.000    0.000    0.000    0.000 generic.py:3294(_clear_item_cache)\n",
       "       51    0.000    0.000    0.001    0.000 {built-in method builtins.sorted}\n",
       "      460    0.000    0.000    0.000    0.000 range.py:675(__len__)\n",
       "      257    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
       "        1    0.000    0.000    0.000    0.000 <ipython-input-245-2ac8e15c2bd2>:104(<listcomp>)\n",
       "      146    0.000    0.000    0.002    0.000 managers.py:784(copy_func)\n",
       "       66    0.000    0.000    0.002    0.000 frame.py:3066(_box_item_values)\n",
       "       32    0.000    0.000    0.012    0.000 base.py:4972(insert)\n",
       "       39    0.000    0.000    0.001    0.000 numeric.py:53(__new__)\n",
       "      320    0.000    0.000    0.002    0.000 managers.py:1881(<lambda>)\n",
       "      301    0.000    0.000    0.000    0.000 {method '_checkReadable' of '_io._IOBase' objects}\n",
       "       69    0.000    0.000    0.002    0.000 numeric.py:107(_shallow_copy)\n",
       "      214    0.000    0.000    0.002    0.000 common.py:605(condition)\n",
       "   261/87    0.000    0.000    0.000    0.000 typing.py:661(__hash__)\n",
       "     1104    0.000    0.000    0.000    0.000 format.py:394(len)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'writelines' of '_io._IOBase' objects}\n",
       "        4    0.000    0.000    0.000    0.000 {pandas._libs.lib.map_infer}\n",
       "      256    0.000    0.000    0.000    0.000 protocol.py:122(read_uint16)\n",
       "       23    0.000    0.000    0.008    0.000 managers.py:1811(_multi_blockify)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method io.open}\n",
       "       98    0.000    0.000    0.000    0.000 {built-in method pandas._libs.lib.is_bool_array}\n",
       "       21    0.000    0.000    0.589    0.028 frame.py:4000(rename)\n",
       "       87    0.000    0.000    0.001    0.000 printing.py:162(pprint_thing)\n",
       "       42    0.000    0.000    0.000    0.000 managers.py:171(set_axis)\n",
       "       19    0.000    0.000    0.128    0.007 frame.py:2824(_getitem_bool_array)\n",
       "       48    0.000    0.000    0.013    0.000 base.py:4488(get_indexer_for)\n",
       "        1    0.000    0.000    0.000    0.000 {pandas._libs.writers.write_csv_rows}\n",
       "       34    0.000    0.000    0.118    0.003 construction.py:56(arrays_to_mgr)\n",
       "       34    0.000    0.000    0.069    0.002 managers.py:1667(create_block_manager_from_arrays)\n",
       "       30    0.000    0.000    0.037    0.001 concat.py:65(concat)\n",
       "       96    0.000    0.000    0.000    0.000 numeric.py:748(<setcomp>)\n",
       "       46    0.000    0.000    0.003    0.000 frame.py:890(items)\n",
       "       25    0.000    0.000    0.001    0.000 connections.py:744(_execute_command)\n",
       "       10    0.000    0.000    0.000    0.000 <ipython-input-236-d810081bbf10>:223(complementarity_measures)\n",
       "      462    0.000    0.000    0.001    0.000 common.py:613(<genexpr>)\n",
       "       64    0.000    0.000    0.001    0.000 function_base.py:4641(append)\n",
       "      154    0.000    0.000    0.002    0.000 common.py:608(is_excluded_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'connect' of '_socket.socket' objects}\n",
       "       60    0.000    0.000    0.008    0.000 generic.py:5349(_consolidate)\n",
       "      195    0.000    0.000    0.000    0.000 dtype.py:182(construct_from_string)\n",
       "       62    0.000    0.000    0.001    0.000 algorithms.py:64(_ensure_data)\n",
       "       75    0.000    0.000    0.001    0.000 base.py:1667(is_boolean)\n",
       "       50    0.000    0.000    0.062    0.001 shape_base.py:229(vstack)\n",
       "       81    0.000    0.000    0.000    0.000 config.py:83(_get_single_key)\n",
       "      118    0.000    0.000    0.000    0.000 inference.py:395(is_sequence)\n",
       "       11    0.000    0.000    0.001    0.000 connections.py:1213(_get_descriptions)\n",
       "      256    0.000    0.000    0.000    0.000 {built-in method _struct.unpack_from}\n",
       "       33    0.000    0.000    0.018    0.001 generic.py:3623(_set_item)\n",
       "      135    0.000    0.000    0.000    0.000 managers.py:1831(_asarray_compat)\n",
       "       80    0.000    0.000    0.000    0.000 format.py:1310(base_formatter)\n",
       "       31    0.000    0.000    0.005    0.000 algorithms.py:171(_reconstruct_data)\n",
       "      146    0.000    0.000    0.001    0.000 {method 'all' of 'numpy.ndarray' objects}\n",
       "       31    0.000    0.000    0.227    0.007 frame.py:4861(f)\n",
       "       12    0.000    0.000    0.000    0.000 protocol.py:283(__init__)\n",
       "       19    0.000    0.000    0.001    0.000 api.py:149(union_indexes)\n",
       "       34    0.000    0.000    0.001    0.000 numerictypes.py:602(find_common_type)\n",
       "       36    0.000    0.000    0.000    0.000 range.py:131(_simple_new)\n",
       "        1    0.000    0.000    0.000    0.000 cast.py:799(astype_nansafe)\n",
       "       32    0.000    0.000    0.001    0.000 base.py:5387(default_index)\n",
       "       98    0.000    0.000    0.000    0.000 blocks.py:2597(is_bool)\n",
       "      126    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
       "       98    0.000    0.000    0.001    0.000 concat.py:135(dtype)\n",
       "       23    0.000    0.000    0.000    0.000 protocol.py:237(_parse_field_descriptor)\n",
       "      302    0.000    0.000    0.000    0.000 socket.py:614(readable)\n",
       "       46    0.000    0.000    0.000    0.000 protocol.py:180(read_struct)\n",
       "        1    0.000    0.000    0.000    0.000 {function socket.close at 0x1053ec510}\n",
       "       76    0.000    0.000    0.001    0.000 common.py:1282(needs_i8_conversion)\n",
       "       60    0.000    0.000    0.005    0.000 concat.py:247(<listcomp>)\n",
       "       30    0.000    0.000    0.001    0.000 concat.py:526(_get_concat_axis)\n",
       "       87    0.000    0.000    0.000    0.000 printing.py:193(as_escaped_string)\n",
       "       77    0.000    0.000    0.000    0.000 {pandas._libs.internals.get_blkno_placements}\n",
       "        3    0.000    0.000    0.001    0.000 format.py:1206(_format_strings)\n",
       "      300    0.000    0.000    0.000    0.000 format.py:1753(just)\n",
       "        8    0.000    0.000    0.623    0.078 base.py:1163(_execute_context)\n",
       "       78    0.000    0.000    0.000    0.000 managers.py:199(_is_single_block)\n",
       "       73    0.000    0.000    0.002    0.000 managers.py:790(<listcomp>)\n",
       "       30    0.000    0.000    0.003    0.000 concat.py:513(_get_new_axes)\n",
       "      157    0.000    0.000    0.000    0.000 config.py:566(_get_deprecated_option)\n",
       "       31    0.000    0.000    0.021    0.001 cast.py:1390(cast_scalar_to_array)\n",
       "       64    0.000    0.000    0.000    0.000 fromnumeric.py:1583(ravel)\n",
       "       50    0.000    0.000    0.000    0.000 shape_base.py:220(_warn_for_nonsequence)\n",
       "      148    0.000    0.000    0.000    0.000 base.py:429(<genexpr>)\n",
       "      301    0.000    0.000    0.000    0.000 {method '_checkClosed' of '_io._IOBase' objects}\n",
       "       50    0.000    0.000    0.000    0.000 fromnumeric.py:942(argsort)\n",
       "      147    0.000    0.000    0.000    0.000 concat.py:115(__init__)\n",
       "      151    0.000    0.000    0.000    0.000 construction.py:245(<genexpr>)\n",
       "       50    0.000    0.000    0.000    0.000 fromnumeric.py:54(_wrapfunc)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'close' of 'pandas._libs.parsers.TextReader' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method posix.listdir}\n",
       "       38    0.000    0.000    0.003    0.000 frame.py:4887(<genexpr>)\n",
       "       30    0.000    0.000    0.001    0.000 api.py:107(_get_combined_index)\n",
       "       60    0.000    0.000    0.001    0.000 format.py:1223(_format)\n",
       "        4    0.000    0.000    0.001    0.000 format.py:1340(get_result_as_array)\n",
       "       30    0.000    0.000    0.003    0.000 concat.py:516(<listcomp>)\n",
       "       65    0.000    0.000    0.000    0.000 {method 'transpose' of 'numpy.ndarray' objects}\n",
       "      215    0.000    0.000    0.000    0.000 concat.py:384(<genexpr>)\n",
       "        4    0.000    0.000    0.001    0.000 format.py:1355(format_values_with)\n",
       "      315    0.000    0.000    0.000    0.000 format.py:1744(<genexpr>)\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method posix.stat}\n",
       "       27    0.000    0.000    0.000    0.000 managers.py:1970(_preprocess_slice_or_indexer)\n",
       "       21    0.000    0.000    0.589    0.028 _decorators.py:225(wrapper)\n",
       "       30    0.000    0.000    0.002    0.000 concat.py:520(_get_comb_axis)\n",
       "       76    0.000    0.000    0.000    0.000 missing.py:601(clean_reindex_fill_method)\n",
       "      110    0.000    0.000    0.001    0.000 base.py:1730(inferred_type)\n",
       "       42    0.000    0.000    0.000    0.000 common.py:464(get_rename_function)\n",
       "      120    0.000    0.000    0.000    0.000 concat.py:401(<genexpr>)\n",
       "      195    0.000    0.000    0.000    0.000 {method 'search' of 're.Pattern' objects}\n",
       "      289    0.000    0.000    0.000    0.000 numerictypes.py:587(<listcomp>)\n",
       "       31    0.000    0.000    0.000    0.000 construction.py:249(<listcomp>)\n",
       "       77    0.000    0.000    0.000    0.000 common.py:1108(is_datetime_or_timedelta_dtype)\n",
       "       31    0.000    0.000    0.046    0.001 algorithms.py:255(_get_data_algo)\n",
       "      137    0.000    0.000    0.000    0.000 cast.py:1483(construct_1d_ndarray_preserving_na)\n",
       "       19    0.000    0.000    0.000    0.000 api.py:278(<setcomp>)\n",
       "       30    0.000    0.000    0.000    0.000 concat.py:378(<listcomp>)\n",
       "       13    0.000    0.000    0.622    0.048 connections.py:1073(read)\n",
       "       50    0.000    0.000    0.000    0.000 shape_base.py:283(<listcomp>)\n",
       "       60    0.000    0.000    0.000    0.000 frame.py:532(shape)\n",
       "       30    0.000    0.000    0.000    0.000 generic.py:384(_from_axes)\n",
       "       30    0.000    0.000    0.000    0.000 api.py:93(_get_distinct_objs)\n",
       "       47    0.000    0.000    0.000    0.000 {pandas._libs.lib.array_equivalent_object}\n",
       "       77    0.000    0.000    0.000    0.000 config.py:101(_get_option)\n",
       "       33    0.000    0.000    0.000    0.000 common.py:183(all_none)\n",
       "      103    0.000    0.000    0.000    0.000 blocks.py:225(get_values)\n",
       "       31    0.000    0.000    0.001    0.000 construction.py:252(<listcomp>)\n",
       "       54    0.000    0.000    0.000    0.000 {built-in method numpy.can_cast}\n",
       "      107    0.000    0.000    0.000    0.000 common.py:1369(is_string_like_dtype)\n",
       "       31    0.000    0.000    0.001    0.000 algorithms.py:247(_get_values_for_rank)\n",
       "       87    0.000    0.000    0.000    0.000 typing.py:245(inner)\n",
       "        8    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1356(find_spec)\n",
       "       79    0.000    0.000    0.000    0.000 config.py:551(_get_root)\n",
       "       66    0.000    0.000    0.000    0.000 common.py:179(ensure_python_int)\n",
       "      121    0.000    0.000    0.000    0.000 common.py:275(maybe_iterable_to_list)\n",
       "       75    0.000    0.000    0.000    0.000 base.py:4505(_maybe_promote)\n",
       "       34    0.000    0.000    0.000    0.000 langhelpers.py:852(__get__)\n",
       "      146    0.000    0.000    0.001    0.000 _methods.py:45(_all)\n",
       "       27    0.000    0.000    0.000    0.000 base.py:2966(_convert_listlike_indexer)\n",
       "      177    0.000    0.000    0.000    0.000 concat.py:391(<genexpr>)\n",
       "        1    0.000    0.000    0.004    0.004 format.py:708(_to_str_columns)\n",
       "      152    0.000    0.000    0.000    0.000 concat.py:459(_next_or_none)\n",
       "       27    0.000    0.000    0.001    0.000 connections.py:710(_write_bytes)\n",
       "        7    0.000    0.000    0.001    0.000 generic.py:1911(__array_wrap__)\n",
       "        7    0.000    0.000    0.002    0.000 format.py:1097(format_array)\n",
       "       30    0.000    0.000    0.002    0.000 api.py:65(get_objs_combined_axis)\n",
       "       11    0.000    0.000    0.001    0.000 range.py:155(_data)\n",
       "       23    0.000    0.000    0.055    0.002 managers.py:1797(_simple_blockify)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:1343(_handle_dbapi_exception)\n",
       "       68    0.000    0.000    0.000    0.000 frame.py:1037(__len__)\n",
       "       19    0.000    0.000    0.000    0.000 api.py:260(get_consensus_names)\n",
       "       13    0.000    0.000    0.002    0.000 construction.py:331(extract_index)\n",
       "      174    0.000    0.000    0.000    0.000 typing.py:319(__hash__)\n",
       "       16    0.000    0.000    0.000    0.000 numeric.py:2656(seterr)\n",
       "       15    0.000    0.000    0.000    0.000 format.py:1759(<listcomp>)\n",
       "       35    0.000    0.000    0.000    0.000 managers.py:1562(external_values)\n",
       "      125    0.000    0.000    0.000    0.000 {method 'clear' of 'dict' objects}\n",
       "       21    0.000    0.000    0.000    0.000 printing.py:69(<listcomp>)\n",
       "       19    0.000    0.000    0.000    0.000 generic.py:1797(__hash__)\n",
       "       19    0.000    0.000    0.000    0.000 api.py:221(_sanitize_and_check)\n",
       "       30    0.000    0.000    0.001    0.000 managers.py:2001(<listcomp>)\n",
       "        8    0.000    0.000    0.000    0.000 default.py:862(_init_statement)\n",
       "       36    0.000    0.000    0.000    0.000 base.py:1186(name)\n",
       "       60    0.000    0.000    0.000    0.000 concat.py:398(_is_uniform_reindex)\n",
       "        4    0.000    0.000    0.010    0.003 base.py:748(_checkout)\n",
       "      124    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}\n",
       "      155    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
       "      107    0.000    0.000    0.000    0.000 common.py:1398(<lambda>)\n",
       "        7    0.000    0.000    0.002    0.000 generic.py:1440(__neg__)\n",
       "       15    0.000    0.000    0.001    0.000 format.py:1731(_make_fixed_width)\n",
       "       13    0.000    0.000    0.623    0.048 connections.py:508(query)\n",
       "        6    0.000    0.000    0.000    0.000 result.py:215(__init__)\n",
       "      221    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}\n",
       "       14    0.000    0.000    0.000    0.000 {pandas._libs.lib.infer_datetimelike_array}\n",
       "       38    0.000    0.000    0.000    0.000 base.py:1217(_get_names)\n",
       "      114    0.000    0.000    0.000    0.000 concat.py:120(<genexpr>)\n",
       "        1    0.000    0.000   58.229   58.229 <ipython-input-250-93e2c218b0d7>:2(main)\n",
       "        1    0.000    0.000    0.002    0.002 connections.py:564(connect)\n",
       "       12    0.000    0.000    0.000    0.000 {built-in method numpy.copyto}\n",
       "        3    0.000    0.000    0.000    0.000 printing.py:25(adjoin)\n",
       "      147    0.000    0.000    0.000    0.000 format.py:764(<genexpr>)\n",
       "        5    0.000    0.000    0.000    0.000 base.py:69(__init__)\n",
       "        4    0.000    0.000    0.010    0.002 base.py:481(checkout)\n",
       "        8    0.000    0.000    0.000    0.000 sorting.py:54(_int64_cut_off)\n",
       "       13    0.000    0.000    0.000    0.000 printing.py:65(<listcomp>)\n",
       "       19    0.000    0.000    0.000    0.000 blocks.py:354(concat_same_type)\n",
       "       16    0.000    0.000    0.000    0.000 numeric.py:2758(geterr)\n",
       "        2    0.000    0.000    0.014    0.007 construction.py:488(_list_to_arrays)\n",
       "      202    0.000    0.000    0.000    0.000 {pandas._libs.lib.is_bool}\n",
       "       12    0.000    0.000    0.002    0.000 connections.py:393(_read_ok_packet)\n",
       "     14/4    0.000    0.000    0.000    0.000 langhelpers.py:273(get_cls_kwargs)\n",
       "        8    0.000    0.000    0.623    0.078 base.py:1138(_execute_text)\n",
       "        1    0.000    0.000    0.001    0.001 csvs.py:325(_save_chunk)\n",
       "       84    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
       "       97    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
       "       76    0.000    0.000    0.000    0.000 config.py:230(__call__)\n",
       "       18    0.000    0.000    0.000    0.000 type_api.py:483(_cached_result_processor)\n",
       "       76    0.000    0.000    0.000    0.000 missing.py:73(clean_fill_method)\n",
       "       76    0.000    0.000    0.000    0.000 concat.py:119(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:371(_force_close)\n",
       "        1    0.000    0.000    0.000    0.000 console.py:77(in_ipython_frontend)\n",
       "       13    0.000    0.000    0.623    0.048 cursors.py:151(execute)\n",
       "        7    0.000    0.000    0.003    0.000 format.py:945(_format_col)\n",
       "       11    0.000    0.000    0.000    0.000 cursors.py:116(_escape_args)\n",
       "       34    0.000    0.000    0.000    0.000 numerictypes.py:654(<listcomp>)\n",
       "       27    0.000    0.000    0.000    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
       "       67    0.000    0.000    0.000    0.000 _validators.py:207(validate_bool_kwarg)\n",
       "        7    0.000    0.000    0.000    0.000 base.py:2436(difference)\n",
       "        1    0.000    0.000    0.005    0.005 frame.py:738(to_string)\n",
       "       33    0.000    0.000    0.000    0.000 generic.py:3656(_check_setitem_copy)\n",
       "       46    0.000    0.000    0.000    0.000 managers.py:1837(_shape_compat)\n",
       "       15    0.000    0.000    0.000    0.000 format.py:464(_get_formatter)\n",
       "        1    0.000    0.000    0.383    0.383 parsers.py:2035(read)\n",
       "     20/3    0.000    0.000    0.001    0.000 visitors.py:85(_compiler_dispatch)\n",
       "      123    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
       "       83    0.000    0.000    0.000    0.000 missing.py:299(notna)\n",
       "       30    0.000    0.000    0.000    0.000 api.py:89(<listcomp>)\n",
       "       32    0.000    0.000    0.000    0.000 {built-in method numpy.bincount}\n",
       "        1    0.000    0.000    0.002    0.002 generic.py:3042(to_csv)\n",
       "       60    0.000    0.000    0.000    0.000 blocks.py:300(__len__)\n",
       "        3    0.000    0.000    0.001    0.000 blocks.py:554(astype)\n",
       "        3    0.000    0.000    0.001    0.000 compiler.py:1999(visit_select)\n",
       "      269    0.000    0.000    0.000    0.000 {method 'ljust' of 'str' objects}\n",
       "       13    0.000    0.000    0.000    0.000 {built-in method numpy.min_scalar_type}\n",
       "       12    0.000    0.000    0.000    0.000 numeric.py:293(full)\n",
       "       46    0.000    0.000    0.000    0.000 protocol.py:264(get_column_length)\n",
       "       14    0.000    0.000    0.000    0.000 deprecations.py:117(warned)\n",
       "       10    0.000    0.000    0.000    0.000 elements.py:717(__getattr__)\n",
       "        1    0.000    0.000    0.000    0.000 csvs.py:228(_save_header)\n",
       "      106    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
       "      287    0.000    0.000    0.000    0.000 {method 'rjust' of 'str' objects}\n",
       "       33    0.000    0.000    0.000    0.000 weakref.py:395(__getitem__)\n",
       "       32    0.000    0.000    0.000    0.000 base.py:4722(_maybe_cast_indexer)\n",
       "       13    0.000    0.000    0.623    0.048 cursors.py:324(_query)\n",
       "       24    0.000    0.000    0.000    0.000 result.py:461(_colnames_from_description)\n",
       "      114    0.000    0.000    0.000    0.000 base.py:609(<lambda>)\n",
       "       34    0.000    0.000    0.000    0.000 base.py:621(__array__)\n",
       "        7    0.000    0.000    0.000    0.000 managers.py:1568(get_values)\n",
       "       86    0.000    0.000    0.000    0.000 managers.py:1815(<lambda>)\n",
       "       30    0.000    0.000    0.000    0.000 concat.py:507(_get_result_dim)\n",
       "       11    0.000    0.000    0.000    0.000 protocol.py:308(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 queue.py:92(put)\n",
       "        7    0.000    0.000    0.001    0.000 base.py:2223(do_rollback)\n",
       "      172    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}\n",
       "        8    0.000    0.000    0.000    0.000 iostream.py:195(schedule)\n",
       "       38    0.000    0.000    0.000    0.000 common.py:190(any_not_none)\n",
       "       27    0.000    0.000    0.000    0.000 base.py:3003(_convert_arr_indexer)\n",
       "       14    0.000    0.000    0.000    0.000 generic.py:1852(empty)\n",
       "       79    0.000    0.000    0.000    0.000 concat.py:562(<genexpr>)\n",
       "       13    0.000    0.000    0.000    0.000 connections.py:1053(__init__)\n",
       "        3    0.000    0.000    0.001    0.000 compiler.py:1761(_label_select_column)\n",
       "        4    0.000    0.000    0.001    0.000 base.py:845(_reset)\n",
       "       24    0.000    0.000    0.000    0.000 result.py:560(_merge_cols_by_none)\n",
       "       31    0.000    0.000    0.000    0.000 algorithms.py:205(_ensure_arraylike)\n",
       "       72    0.000    0.000    0.000    0.000 numeric.py:83(_validate_dtype)\n",
       "       35    0.000    0.000    0.000    0.000 series.py:438(values)\n",
       "       13    0.000    0.000    0.622    0.048 connections.py:720(_read_query_result)\n",
       "        2    0.000    0.000    0.000    0.000 schema.py:3753(__init__)\n",
       "        3    0.000    0.000    0.001    0.000 compiler.py:500(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:507(checkin)\n",
       "       40    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:56(_path_join)\n",
       "       27    0.000    0.000    0.000    0.000 _methods.py:34(_sum)\n",
       "       84    0.000    0.000    0.000    0.000 common.py:471(f)\n",
       "       76    0.000    0.000    0.000    0.000 concat.py:104(<genexpr>)\n",
       "       42    0.000    0.000    0.000    0.000 generic.py:1899(<genexpr>)\n",
       "        4    0.000    0.000    0.000    0.000 format.py:1375(<listcomp>)\n",
       "        1    0.000    0.000    0.002    0.002 connections.py:183(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:786(_request_authentication)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:973(_get_server_information)\n",
       "       23    0.000    0.000    0.000    0.000 protocol.py:253(description)\n",
       "        6    0.000    0.000    0.000    0.000 default.py:947(should_autocommit)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'getvalue' of '_io.StringIO' objects}\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:882(_find_spec)\n",
       "        1    0.000    0.000    0.000    0.000 socket.py:139(__init__)\n",
       "        8    0.000    0.000    0.000    0.000 fromnumeric.py:2664(prod)\n",
       "       81    0.000    0.000    0.000    0.000 config.py:533(_select_options)\n",
       "       30    0.000    0.000    0.000    0.000 common.py:169(not_none)\n",
       "        1    0.000    0.000    0.005    0.005 format.py:512(get_result)\n",
       "       30    0.000    0.000    0.000    0.000 concat.py:559(<listcomp>)\n",
       "        1    0.000    0.000    0.039    0.039 <ipython-input-245-2ac8e15c2bd2>:91(get_docs)\n",
       "       76    0.000    0.000    0.000    0.000 config.py:607(_warn_if_deprecated)\n",
       "       90    0.000    0.000    0.000    0.000 common.py:173(<genexpr>)\n",
       "       33    0.000    0.000    0.000    0.000 frame.py:3046(_ensure_valid_index)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:834(_interleave)\n",
       "       60    0.000    0.000    0.000    0.000 format.py:1220(<lambda>)\n",
       "        6    0.000    0.000    0.010    0.002 construction.py:586(convert)\n",
       "       13    0.000    0.000    0.000    0.000 cursors.py:135(mogrify)\n",
       "       11    0.000    0.000    0.000    0.000 cursors.py:341(_do_get_result)\n",
       "        3    0.000    0.000    0.000    0.000 selectable.py:2687(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:780(visit_label)\n",
       "        3    0.000    0.000    0.002    0.001 default.py:342(check_unicode)\n",
       "      154    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}\n",
       "       46    0.000    0.000    0.000    0.000 {method 'unpack_from' of 'Struct' objects}\n",
       "        2    0.000    0.000    0.000    0.000 config.py:109(_set_option)\n",
       "       50    0.000    0.000    0.000    0.000 blocks.py:3099(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:1043(_get_formatted_index)\n",
       "        1    0.000    0.000    0.474    0.474 parsers.py:1131(read)\n",
       "        4    0.000    0.000    0.001    0.000 base.py:645(_finalize_fairy)\n",
       "        2    0.000    0.000    0.001    0.001 base.py:2312(has_table)\n",
       "        6    0.000    0.000    0.000    0.000 iostream.py:382(write)\n",
       "       50    0.000    0.000    0.000    0.000 shape_base.py:209(_arrays_for_stack_dispatcher)\n",
       "        1    0.000    0.000    0.005    0.005 frame.py:662(__repr__)\n",
       "       23    0.000    0.000    0.000    0.000 protocol.py:233(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method posix.getcwd}\n",
       "        7    0.000    0.000    0.000    0.000 threading.py:335(notify)\n",
       "       38    0.000    0.000    0.000    0.000 {built-in method _struct.pack}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'writerow' of '_csv.writer' objects}\n",
       "        2    0.000    0.000    0.662    0.331 sql.py:1167(read_query)\n",
       "        7    0.000    0.000    0.001    0.000 connections.py:422(rollback)\n",
       "        4    0.000    0.000    0.000    0.000 queue.py:135(get)\n",
       "        6    0.000    0.000    0.000    0.000 result.py:740(_init_metadata)\n",
       "        1    0.000    0.000    0.000    0.000 csvs.py:34(__init__)\n",
       "        3    0.000    0.000    0.001    0.000 generic.py:5563(astype)\n",
       "        4    0.000    0.000    0.000    0.000 format.py:1787(<listcomp>)\n",
       "        2    0.000    0.000    0.027    0.014 construction.py:437(to_arrays)\n",
       "       26    0.000    0.000    0.000    0.000 cursors.py:89(_nextset)\n",
       "        8    0.000    0.000    0.000    0.000 type_api.py:505(_dialect_info)\n",
       "        4    0.000    0.000    0.001    0.000 base.py:869(close)\n",
       "        1    0.000    0.000    0.010    0.010 base.py:622(__connect)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:1278(visit_typeclause)\n",
       "      100    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
       "        2    0.000    0.000    0.006    0.003 {built-in method builtins.print}\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1240(_get_spec)\n",
       "        6    0.000    0.000    0.000    0.000 parse.py:409(urlsplit)\n",
       "        1    0.000    0.000    0.000    0.000 socket.py:652(close)\n",
       "       27    0.000    0.000    0.000    0.000 base.py:3043(_convert_list_indexer)\n",
       "       38    0.000    0.000    0.000    0.000 concat.py:91(is_nonempty)\n",
       "       76    0.000    0.000    0.000    0.000 concat.py:105(<genexpr>)\n",
       "        4    0.000    0.000    0.000    0.000 range.py:387(_shallow_copy)\n",
       "       80    0.000    0.000    0.000    0.000 format.py:1783(_is_number)\n",
       "       13    0.000    0.000    0.000    0.000 cursors.py:40(__init__)\n",
       "       35    0.000    0.000    0.000    0.000 protocol.py:186(is_ok_packet)\n",
       "        6    0.000    0.000    0.000    0.000 result.py:441(<listcomp>)\n",
       "       64    0.000    0.000    0.000    0.000 fromnumeric.py:2847(ndim)\n",
       "       81    0.000    0.000    0.000    0.000 config.py:594(_translate_key)\n",
       "        8    0.000    0.000    0.000    0.000 common.py:1225(is_datetimelike_v_numeric)\n",
       "       66    0.000    0.000    0.000    0.000 common.py:187(<genexpr>)\n",
       "       34    0.000    0.000    0.000    0.000 printing.py:60(justify)\n",
       "        3    0.000    0.000    0.000    0.000 range.py:316(dtype)\n",
       "        2    0.000    0.000    0.000    0.000 exc.py:390(instance)\n",
       "        2    0.000    0.000    0.000    0.000 result.py:588(_key_fallback)\n",
       "        8    0.000    0.000    0.000    0.000 default.py:1034(create_cursor)\n",
       "        1    0.000    0.000    0.004    0.004 mysqldb.py:136(_check_unicode_returns)\n",
       "        1    0.000    0.000    0.002    0.002 csvs.py:157(save)\n",
       "        8    0.000    0.000    0.000    0.000 threading.py:1080(is_alive)\n",
       "        6    0.000    0.000    0.000    0.000 parse.py:361(urlparse)\n",
       "        1    0.000    0.000    0.000    0.000 socket.py:221(makefile)\n",
       "        7    0.000    0.000    0.000    0.000 generic.py:343(_construct_axes_dict)\n",
       "       86    0.000    0.000    0.000    0.000 numeric.py:155(is_all_dates)\n",
       "        8    0.000    0.000    0.000    0.000 format.py:443(_get_adjustment)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:870(_join_multiline)\n",
       "        7    0.000    0.000    0.002    0.000 format.py:1202(get_result)\n",
       "       84    0.000    0.000    0.000    0.000 format.py:1410(<genexpr>)\n",
       "        1    0.000    0.000    0.483    0.483 parsers.py:530(parser_f)\n",
       "        2    0.000    0.000    0.000    0.000 sql.py:1068(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 _auth.py:186(scramble_caching_sha2)\n",
       "        4    0.000    0.000    0.000    0.000 type_api.py:1440(adapt_type)\n",
       "        1    0.000    0.000    0.000    0.000 registry.py:53(_collection_gced)\n",
       "        8    0.000    0.000    0.623    0.078 base.py:922(execute)\n",
       "        2    0.000    0.000    0.012    0.006 base.py:2133(run_callable)\n",
       "        5    0.000    0.000    0.000    0.000 types.py:69(__init__)\n",
       "       36    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _operator.and_}\n",
       "       16    0.000    0.000    0.000    0.000 {built-in method numpy.seterrobj}\n",
       "       27    0.000    0.000    0.000    0.000 base.py:5360(_ensure_has_len)\n",
       "        1    0.000    0.000    0.005    0.005 strings.py:2040(__init__)\n",
       "       34    0.000    0.000    0.000    0.000 format.py:397(justify)\n",
       "        4    0.000    0.000    0.001    0.000 format.py:1431(_format_strings)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:885(_get_options_with_defaults)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1517(_maybe_dedup_names)\n",
       "        9    0.000    0.000    0.000    0.000 cursors.py:280(fetchone)\n",
       "        4    0.000    0.000    0.010    0.003 base.py:2223(_contextual_connect)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:2157(_setup_select_stack)\n",
       "        4    0.000    0.000    0.001    0.000 base.py:831(_checkin)\n",
       "        4    0.000    0.000    0.010    0.002 impl.py:111(_do_get)\n",
       "        6    0.000    0.000    0.000    0.000 result.py:714(__init__)\n",
       "        6    0.000    0.000    0.001    0.000 result.py:869(_soft_close)\n",
       "       18    0.000    0.000    0.000    0.000 default.py:1051(get_result_processor)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.__build_class__}\n",
       "       40    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:58(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1404(_fill_cache)\n",
       "        8    0.000    0.000    0.000    0.000 numeric.py:3054(__init__)\n",
       "        8    0.000    0.000    0.000    0.000 inference.py:299(is_dict_like)\n",
       "       35    0.000    0.000    0.000    0.000 blocks.py:202(external_values)\n",
       "       76    0.000    0.000    0.000    0.000 managers.py:419(<dictcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:986(_get_formatted_column_labels)\n",
       "        7    0.000    0.000    0.000    0.000 series.py:520(_internal_get_values)\n",
       "       13    0.000    0.000    0.000    0.000 cursors.py:51(close)\n",
       "       26    0.000    0.000    0.000    0.000 cursors.py:106(nextset)\n",
       "        4    0.000    0.000    0.000    0.000 langhelpers.py:1136(constructor_copy)\n",
       "        9    0.000    0.000    0.000    0.000 <string>:1(__init__)\n",
       "        1    0.000    0.000    0.005    0.005 default.py:286(initialize)\n",
       "      9/3    0.000    0.000    0.001    0.000 compiler.py:349(process)\n",
       "        8    0.000    0.000    0.000    0.000 numeric.py:3058(__enter__)\n",
       "        8    0.000    0.000    0.000    0.000 numeric.py:3063(__exit__)\n",
       "        1    0.000    0.000    0.000    0.000 console.py:8(get_console_size)\n",
       "        1    0.000    0.000    0.001    0.001 strings.py:2120(_wrap_result)\n",
       "       19    0.000    0.000    0.000    0.000 api.py:242(<setcomp>)\n",
       "        1    0.000    0.000    0.001    0.001 common.py:314(get_handle)\n",
       "       21    0.000    0.000    0.000    0.000 series.py:548(__len__)\n",
       "        4    0.000    0.000    0.001    0.000 connections.py:532(ping)\n",
       "        7    0.000    0.000    0.000    0.000 langhelpers.py:253(_inspect_func_args)\n",
       "       21    0.000    0.000    0.000    0.000 langhelpers.py:1145(<genexpr>)\n",
       "        3    0.000    0.000    0.000    0.000 selectable.py:2959(_froms)\n",
       "        3    0.000    0.000    0.000    0.000 <string>:1(select)\n",
       "        9    0.000    0.000    0.000    0.000 sqltypes.py:140(__init__)\n",
       "       13    0.000    0.000    0.000    0.000 attr.py:267(__bool__)\n",
       "        1    0.000    0.000    0.002    0.002 default.py:331(_check_unicode_returns)\n",
       "        6    0.000    0.000    0.000    0.000 result.py:334(_merge_cursor_description)\n",
       "        2    0.000    0.000    0.000    0.000 pymysql.py:64(is_disconnect)\n",
       "        1    0.000    0.000    0.001    0.001 socket.py:691(create_connection)\n",
       "        7    0.000    0.000    0.000    0.000 generic.py:345(<dictcomp>)\n",
       "        1    0.000    0.000    0.005    0.005 format.py:806(write_result)\n",
       "        4    0.000    0.000    0.000    0.000 format.py:1798(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:3719(apply)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:3857(_reduce)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:939(_clean_options)\n",
       "        2    0.000    0.000    0.001    0.000 sql.py:100(_parse_date_columns)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:729(_rollback_impl)\n",
       "        3    0.000    0.000    0.001    0.000 compiler.py:274(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 result.py:1279(first)\n",
       "        8    0.000    0.000    0.000    0.000 default.py:1002(_use_server_side_cursor)\n",
       "        6    0.000    0.000    0.000    0.000 default.py:1092(get_result_proxy)\n",
       "        3    0.000    0.000    0.000    0.000 types.py:689(_adapt_string_for_cast)\n",
       "      101    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
       "       40    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:222(_verbose_message)\n",
       "        1    0.000    0.000    0.001    0.001 socket.py:731(getaddrinfo)\n",
       "       15    0.000    0.000    0.000    0.000 common.py:1614(is_complex_dtype)\n",
       "        7    0.000    0.000    0.000    0.000 base.py:870(empty)\n",
       "        1    0.000    0.000    0.029    0.029 strings.py:204(_map_object)\n",
       "        7    0.000    0.000    0.000    0.000 blocks.py:240(to_dense)\n",
       "        2    0.000    0.000    0.000    0.000 blocks.py:673(to_native_types)\n",
       "        3    0.000    0.000    0.000    0.000 format.py:400(adjoin)\n",
       "        7    0.000    0.000    0.000    0.000 format.py:1176(__init__)\n",
       "        6    0.000    0.000    0.000    0.000 common.py:88(stringify_path)\n",
       "        1    0.000    0.000    0.001    0.001 series.py:313(_init_dict)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1345(__init__)\n",
       "       13    0.000    0.000    0.000    0.000 connections.py:481(cursor)\n",
       "       63    0.000    0.000    0.000    0.000 cursors.py:71(_get_db)\n",
       "       14    0.000    0.000    0.000    0.000 protocol.py:86(advance)\n",
       "        5    0.000    0.000    0.000    0.000 _collections.py:140(__new__)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:864(anon_label)\n",
       "        1    0.000    0.000    0.000    0.000 elements.py:3066(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:3831(__init__)\n",
       "        5    0.000    0.000    0.000    0.000 base.py:77(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 compiler.py:3477(_requires_quotes)\n",
       "        6    0.000    0.000    0.000    0.000 compiler.py:3529(quote)\n",
       "        4    0.000    0.000    0.000    0.000 impl.py:102(_do_return_conn)\n",
       "        6    0.000    0.000    0.004    0.001 result.py:1178(process_rows)\n",
       "       11    0.000    0.000    0.623    0.057 default.py:551(do_execute)\n",
       "        6    0.000    0.000    0.000    0.000 base.py:1174(should_autocommit_text)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:1316(visit_cast)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2184(_get_server_version_info)\n",
       "        1    0.000    0.000    0.008    0.008 base.py:2347(initialize)\n",
       "       31    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
       "       32    0.000    0.000    0.000    0.000 {built-in method numpy.geterrobj}\n",
       "       76    0.000    0.000    0.000    0.000 common.py:194(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 range.py:200(_format_with_header)\n",
       "       27    0.000    0.000    0.000    0.000 indexing.py:1344(_convert_for_reindex)\n",
       "       19    0.000    0.000    0.000    0.000 blocks.py:359(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 parsers.py:1174(_is_potential_multi_index)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:74(_makefile)\n",
       "       11    0.000    0.000    0.000    0.000 cursors.py:355(_show_warnings)\n",
       "        2    0.000    0.000    0.000    0.000 compat.py:123(reraise)\n",
       "        2    0.000    0.000    0.000    0.000 exc.py:462(__init__)\n",
       "        7    0.000    0.000    0.000    0.000 elements.py:705(comparator)\n",
       "        6    0.000    0.000    0.000    0.000 type_api.py:440(dialect_impl)\n",
       "        4    0.000    0.000    0.000    0.000 type_api.py:524(adapt)\n",
       "        5    0.000    0.000    0.000    0.000 base.py:116(_for_class)\n",
       "        1    0.000    0.000    0.008    0.008 strategies.py:194(first_connect)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:183(execution_options)\n",
       "        8    0.000    0.000    0.000    0.000 base.py:1327(_safe_close_cursor)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:2186(_compose_select_body)\n",
       "        4    0.000    0.000    0.010    0.003 base.py:345(connect)\n",
       "        2    0.000    0.000    0.005    0.002 result.py:1195(fetchall)\n",
       "        6    0.000    0.000    0.000    0.000 default.py:1108(_setup_crud_result_proxy)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:2260(is_disconnect)\n",
       "        2    0.000    0.000    0.000    0.000 types.py:510(__init__)\n",
       "        1    0.000    0.000    0.001    0.001 csvs.py:308(_save)\n",
       "       80    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}\n",
       "        8    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:157(_get_module_lock)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:978(_find_and_load)\n",
       "        2    0.000    0.000    0.000    0.000 warnings.py:474(__enter__)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'setsockopt' of '_socket.socket' objects}\n",
       "        6    0.000    0.000    0.000    0.000 iostream.py:307(_is_master_process)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:968(_format_with_header)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:1024(_format_native_types)\n",
       "        8    0.000    0.000    0.000    0.000 format.py:391(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:615(_chk_truncate)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:144(get_filepath_or_buffer)\n",
       "        2    0.000    0.000    0.000    0.000 sql.py:570(pandasSQL_builder)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:304(<dictcomp>)\n",
       "       13    0.000    0.000    0.000    0.000 cursors.py:332(_clear_result)\n",
       "       12    0.000    0.000    0.000    0.000 protocol.py:297(__getattr__)\n",
       "        3    0.000    0.000    0.000    0.000 langhelpers.py:897(expire_instance)\n",
       "        2    0.000    0.000    0.000    0.000 exc.py:328(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:4299(apply_map)\n",
       "        3    0.000    0.000    0.000    0.000 selectable.py:3641(_columns_plus_names)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:123(_join)\n",
       "        5    0.000    0.000    0.000    0.000 base.py:295(__get__)\n",
       "        2    0.000    0.000    0.617    0.309 base.py:2149(execute)\n",
       "        5    0.000    0.000    0.000    0.000 log.py:59(_should_log_info)\n",
       "        4    0.000    0.000    0.001    0.000 base.py:987(close)\n",
       "        6    0.000    0.000    0.000    0.000 result.py:266(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:1747(_extend_string)\n",
       "        1    0.000    0.000    0.001    0.001 base.py:2750(_detect_casing)\n",
       "        9    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1203(_path_importer_cache)\n",
       "        6    0.000    0.000    0.000    0.000 posixpath.py:232(expanduser)\n",
       "       10    0.000    0.000    0.000    0.000 __init__.py:1619(isEnabledFor)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method _hashlib.new}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method now}\n",
       "        1    0.000    0.000    0.000    0.000 socket.py:563(__init__)\n",
       "        1    0.000    0.000    0.002    0.002 __init__.py:88(Connect)\n",
       "        7    0.000    0.000    0.000    0.000 common.py:1326(is_numeric_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 cast.py:1342(find_common_type)\n",
       "        2    0.000    0.000    0.002    0.001 frame.py:3590(reindexer)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:5519(dtypes)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:40(is_url)\n",
       "       19    0.000    0.000    0.000    0.000 managers.py:2018(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 format.py:1786(_cond)\n",
       "        7    0.000    0.000    0.000    0.000 common.py:58(_expand_user)\n",
       "        3    0.000    0.000    0.000    0.000 common.py:259(infer_compression)\n",
       "        2    0.000    0.000    0.010    0.005 construction.py:574(_convert_object_array)\n",
       "        1    0.000    0.000    0.001    0.001 parsers.py:792(__init__)\n",
       "        1    0.000    0.000    0.001    0.001 parsers.py:1112(_make_engine)\n",
       "        2    0.000    0.000    0.000    0.000 sql.py:39(_is_sqlalchemy_connectable)\n",
       "        2    0.000    0.000    0.617    0.309 sql.py:1085(execute)\n",
       "        2    0.000    0.000    0.012    0.006 sql.py:1339(has_table)\n",
       "        2    0.000    0.000    0.000    0.000 err.py:100(raise_mysql_exception)\n",
       "        1    0.000    0.000    0.000    0.000 _auth.py:34(scramble_native_password)\n",
       "       13    0.000    0.000    0.000    0.000 protocol.py:77(read_all)\n",
       "        6    0.000    0.000    0.000    0.000 base.py:38(_from_objects)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:2395(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 selectable.py:2988(_get_display_froms)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:94(__getattr__)\n",
       "        5    0.000    0.000    0.000    0.000 base.py:119(_for_instance)\n",
       "        1    0.000    0.000    0.008    0.008 attr.py:279(exec_once)\n",
       "        3    0.000    0.000    0.001    0.000 base.py:1287(_cursor_execute)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:1542(_truncated_identifier)\n",
       "        4    0.000    0.000    0.000    0.000 log.py:56(_should_log_debug)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:580(get_connection)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2172(get_isolation_level)\n",
       "        3    0.000    0.000    0.000    0.000 types.py:674(__init__)\n",
       "       14    0.000    0.000    0.000    0.000 {method 'discard' of 'set' objects}\n",
       "        8    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:74(_path_stat)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:948(_find_and_load_unlocked)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1433(<setcomp>)\n",
       "        8    0.000    0.000    0.000    0.000 threading.py:1038(_wait_for_tstate_lock)\n",
       "        6    0.000    0.000    0.000    0.000 {method 'digest' of '_hashlib.HASH' objects}\n",
       "        1    0.000    0.000    0.000    0.000 pathlib.py:62(parse_parts)\n",
       "        1    0.000    0.000    0.000    0.000 pathlib.py:633(_parse_args)\n",
       "        1    0.000    0.000    0.000    0.000 socket.py:412(_real_close)\n",
       "        2    0.000    0.000    0.000    0.000 socket.py:416(close)\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method sqlalchemy.cutils._distill_params}\n",
       "       19    0.000    0.000    0.000    0.000 inference.py:325(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 array_ops.py:305(logical_op)\n",
       "        1    0.000    0.000    0.000    0.000 nanops.py:234(_get_values)\n",
       "        1    0.000    0.000    0.005    0.005 strings.py:2051(_validate)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:4299(reindex)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:9533(abs)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:540(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 format.py:1278(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 format.py:1775(_trim_zeros_float)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:1917(_binify)\n",
       "        2    0.000    0.000    0.000    0.000 parsers.py:367(_validate_integer)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1969(close)\n",
       "        2    0.000    0.000    0.000    0.000 connections.py:637(write_packet)\n",
       "        3    0.000    0.000    0.001    0.000 elements.py:464(_compiler)\n",
       "        5    0.000    0.000    0.000    0.000 elements.py:4139(__new__)\n",
       "       10    0.000    0.000    0.000    0.000 type_api.py:1430(to_instance)\n",
       "        2    0.000    0.000    0.000    0.000 schema.py:4027(_bind_to)\n",
       "        2    0.000    0.000    0.000    0.000 sqltypes.py:411(__init__)\n",
       "       13    0.000    0.000    0.000    0.000 base.py:370(connection)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:1564(_process_anon)\n",
       "        4    0.000    0.000    0.000    0.000 compiler.py:3425(_escape_identifier)\n",
       "        4    0.000    0.000    0.000    0.000 compiler.py:3464(quote_identifier)\n",
       "        4    0.000    0.000    0.000    0.000 default.py:409(type_descriptor)\n",
       "       17    0.000    0.000    0.000    0.000 base.py:1753(attr)\n",
       "        1    0.000    0.000    0.001    0.001 base.py:2794(_detect_sql_mode)\n",
       "        4    0.000    0.000    0.001    0.000 mysqldb.py:120(do_ping)\n",
       "        8    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:78(acquire)\n",
       "        8    0.000    0.000    0.000    0.000 weakref.py:435(__contains__)\n",
       "       12    0.000    0.000    0.000    0.000 parse.py:109(_coerce_args)\n",
       "        1    0.000    0.000    0.000    0.000 pathlib.py:687(_make_child)\n",
       "        8    0.000    0.000    0.000    0.000 iostream.py:93(_event_pipe)\n",
       "        8    0.000    0.000    0.000    0.000 {method 'max' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.000    0.000 config.py:174(get_default_val)\n",
       "        1    0.000    0.000    0.000    0.000 {pandas._libs.ops_dispatch.maybe_dispatch_ufunc_to_dunder_op}\n",
       "        2    0.000    0.000    0.000    0.000 array_ops.py:326(fill_bool)\n",
       "        5    0.000    0.000    0.000    0.000 base.py:104(__setattr__)\n",
       "        7    0.000    0.000    0.000    0.000 base.py:666(size)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:951(format)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:980(<listcomp>)\n",
       "        1    0.000    0.000    0.029    0.029 strings.py:122(_na_map)\n",
       "        3    0.000    0.000    0.000    0.000 printing.py:46(<listcomp>)\n",
       "        1    0.000    0.000    0.030    0.030 strings.py:2980(strip)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:11341(logical_func)\n",
       "        5    0.000    0.000    0.000    0.000 range.py:444(equals)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:248(get_dtypes)\n",
       "        3    0.000    0.000    0.001    0.000 managers.py:581(astype)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:879(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _csv.writer}\n",
       "        2    0.000    0.000    0.000    0.000 charset.py:43(by_name)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:305(<dictcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:401(_send_autocommit_mode)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:890(_process_auth)\n",
       "       13    0.000    0.000    0.000    0.000 connections.py:1069(__del__)\n",
       "        1    0.000    0.000    0.000    0.000 _auth.py:48(_my_crypt)\n",
       "       11    0.000    0.000    0.000    0.000 cursors.py:127(<dictcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 cursors.py:299(fetchall)\n",
       "       11    0.000    0.000    0.000    0.000 protocol.py:208(is_load_local_packet)\n",
       "        2    0.000    0.000    0.000    0.000 _collections.py:151(union)\n",
       "        3    0.000    0.000    0.000    0.000 _collections.py:361(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 langhelpers.py:925(__getattr__)\n",
       "        1    0.000    0.000    0.000    0.000 elements.py:54(collate)\n",
       "        3    0.000    0.000    0.001    0.000 elements.py:399(compile)\n",
       "        1    0.000    0.000    0.008    0.008 attr.py:291(__call__)\n",
       "        4    0.000    0.000    0.010    0.003 base.py:2259(_wrap_pool_connect)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:399(process)\n",
       "        1    0.000    0.000    0.000    0.000 compiler.py:1318(_generate_generic_binary)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:366(_return_conn)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:949(cursor)\n",
       "        4    0.000    0.000    0.000    0.000 queue.py:195(_full)\n",
       "        4    0.000    0.000    0.000    0.000 queue.py:199(_put)\n",
       "        3    0.000    0.000    0.000    0.000 queue.py:203(_get)\n",
       "        7    0.000    0.000    0.000    0.000 default.py:943(no_parameters)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:1954(visit_CHAR)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:2906(__getitem__)\n",
       "       14    0.000    0.000    0.000    0.000 {method 'get' of 'mappingproxy' objects}\n",
       "        4    0.000    0.000    0.000    0.000 {method 'difference' of 'set' objects}\n",
       "        7    0.000    0.000    0.000    0.000 {method 'update' of 'set' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.__import__}\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:873(_find_spec_legacy)\n",
       "        1    0.000    0.000    0.000    0.000 weakref.py:356(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 weakref.py:408(__setitem__)\n",
       "        4    0.000    0.000    0.000    0.000 enum.py:526(__new__)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method _hashlib.openssl_sha256}\n",
       "        6    0.000    0.000    0.000    0.000 iostream.py:320(_schedule_flush)\n",
       "       34    0.000    0.000    0.000    0.000 numerictypes.py:655(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 config.py:393(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 {pandas._libs.algos.take_1d_object_object}\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:544(wrapper)\n",
       "        1    0.000    0.000    0.005    0.005 accessor.py:183(__get__)\n",
       "        7    0.000    0.000    0.000    0.000 base.py:2581(_assert_can_do_setop)\n",
       "        3    0.000    0.000    0.000    0.000 generic.py:255(_validate_dtype)\n",
       "        2    0.000    0.000    0.000    0.000 range.py:170(_int64index)\n",
       "        1    0.000    0.000    0.000    0.000 blocks.py:2045(to_native_types)\n",
       "        3    0.000    0.000    0.000    0.000 format.py:1801(_has_names)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:687(construct_return)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:4027(reindex)\n",
       "        6    0.000    0.000    0.000    0.000 parsers.py:1191(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1842(_do_date_conversions)\n",
       "        2    0.000    0.000    0.000    0.000 connections.py:448(escape)\n",
       "        3    0.000    0.000    0.000    0.000 _collections.py:396(__iter__)\n",
       "        3    0.000    0.000    0.000    0.000 _collections.py:755(unique_list)\n",
       "        1    0.000    0.000    0.008    0.008 langhelpers.py:1440(go)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:2340(literal_column)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:2455(_from_objects)\n",
       "        6    0.000    0.000    0.000    0.000 elements.py:3941(_get_table)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:4184(__new__)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:261(__init__)\n",
       "        1    0.000    0.000    0.002    0.002 strategies.py:106(connect)\n",
       "       10    0.000    0.000    0.000    0.000 base.py:155(_root)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:355(closed)\n",
       "        1    0.000    0.000    0.002    0.002 base.py:914(scalar)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:834(visit_column)\n",
       "        3    0.000    0.000    0.001    0.000 compiler.py:2082(<listcomp>)\n",
       "        1    0.000    0.000    0.002    0.002 default.py:379(<setcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:715(__init__)\n",
       "        6    0.000    0.000    0.000    0.000 result.py:263(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 result.py:901(close)\n",
       "        4    0.000    0.000    0.000    0.000 result.py:1146(_fetchone_impl)\n",
       "        2    0.000    0.000    0.000    0.000 result.py:1161(_fetchall_impl)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:1350(get_select_precolumns)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:2054(_quote_free_identifiers)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2809(_detect_ansiquotes)\n",
       "        4    0.000    0.000    0.000    0.000 csvs.py:135(<genexpr>)\n",
       "        5    0.000    0.000    0.000    0.000 {method 'pop' of 'list' objects}\n",
       "        5    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method sys.exc_info}\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:58(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 warnings.py:453(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 contextlib.py:237(helper)\n",
       "        1    0.000    0.000    0.000    0.000 re.py:271(_compile)\n",
       "        4    0.000    0.000    0.000    0.000 enum.py:284(__call__)\n",
       "        3    0.000    0.000    0.000    0.000 inspect.py:72(isclass)\n",
       "        3    0.000    0.000    0.000    0.000 hashlib.py:139(__hash_new)\n",
       "        2    0.000    0.000    0.000    0.000 typing.py:707(__instancecheck__)\n",
       "        2    0.000    0.000    0.000    0.000 typing.py:710(__subclasscheck__)\n",
       "        9    0.000    0.000    0.000    0.000 typing.py:890(cast)\n",
       "        1    0.000    0.000    0.000    0.000 socket.py:406(_decref_socketios)\n",
       "        8    0.000    0.000    0.000    0.000 _methods.py:26(_amax)\n",
       "        1    0.000    0.000    0.000    0.000 converters.py:12(escape_item)\n",
       "        1    0.000    0.000    0.000    0.000 construction.py:590(create_series_with_explicit_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 nanops.py:396(nanany)\n",
       "        1    0.000    0.000    0.000    0.000 console.py:53(in_interactive_session)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:993(to_native_types)\n",
       "        7    0.000    0.000    0.000    0.000 base.py:2210(_validate_sort_keyword)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:4076(identical)\n",
       "        1    0.000    0.000    0.000    0.000 frame.py:653(_info_repr)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:356(_construct_axes_from_arguments)\n",
       "        2    0.000    0.000    0.000    0.000 generic.py:4530(<genexpr>)\n",
       "        4    0.000    0.000    0.000    0.000 range.py:210(start)\n",
       "        3    0.000    0.000    0.000    0.000 blocks.py:187(is_categorical_astype)\n",
       "        1    0.000    0.000    0.000    0.000 blocks.py:2655(should_store)\n",
       "        2    0.000    0.000    0.000    0.000 format.py:1035(show_row_idx_names)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:1062(<listcomp>)\n",
       "        8    0.000    0.000    0.000    0.000 format.py:1790(<genexpr>)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:120(is_s3_url)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:127(is_gcs_url)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:4400(isna)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:882(close)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1551(_make_index)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:3258(_process_date_conversion)\n",
       "        3    0.000    0.000    0.000    0.000 util.py:11(int2byte)\n",
       "        2    0.000    0.000    0.000    0.000 compat.py:378(raise_from_cause)\n",
       "        2    0.000    0.000    0.000    0.000 elements.py:681(self_group)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:3944(_set_table)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:4340(_select_iterables)\n",
       "        7    0.000    0.000    0.000    0.000 type_api.py:60(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 type_api.py:307(_has_column_expression)\n",
       "        4    0.000    0.000    0.000    0.000 type_api.py:521(_gen_dialect_impl)\n",
       "        3    0.000    0.000    0.000    0.000 selectable.py:2120(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 <string>:1(cast)\n",
       "        1    0.000    0.000    0.000    0.000 default.py:270(_type_memos)\n",
       "        1    0.000    0.000    0.002    0.002 default.py:452(connect)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:169(_clone)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:364(invalidated)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:865(_autorollback)\n",
       "        2    0.000    0.000    0.001    0.001 base.py:1591(run_callable)\n",
       "        1    0.000    0.000    0.000    0.000 compiler.py:1168(_get_operator_dispatch)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:1758(_add_to_result_map)\n",
       "        1    0.000    0.000    0.010    0.010 base.py:425(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 queue.py:191(_empty)\n",
       "        6    0.000    0.000    0.000    0.000 result.py:864(_cursor_description)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:1778(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:2057(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:2296(_compat_first)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:2902(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 pymysql.py:51(supports_server_side_cursors)\n",
       "        3    0.000    0.000    0.000    0.000 pymysql.py:76(_extract_error_code)\n",
       "        1    0.000    0.000    0.000    0.000 idna.py:147(encode)\n",
       "        1    0.000    0.000    0.001    0.001 <ipython-input-245-2ac8e15c2bd2>:108(set_labels)\n",
       "        2    0.000    0.000    0.000    0.000 <ipython-input-228-d3bc5f4776a7>:14(corpus_config)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'find' of 'bytes' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'translate' of 'str' objects}\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method _warnings._filters_mutated}\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method _imp.release_lock}\n",
       "        7    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
       "        7    0.000    0.000    0.000    0.000 {built-in method posix.fspath}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'split' of 're.Pattern' objects}\n",
       "       12    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n",
       "        8    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {function SocketIO.close at 0x1053ece18}\n",
       "        8    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:36(_relax_case)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:151(__exit__)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:176(cb)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:369(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:719(find_spec)\n",
       "        6    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:855(__enter__)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1272(find_spec)\n",
       "        2    0.000    0.000    0.000    0.000 warnings.py:493(__exit__)\n",
       "        1    0.000    0.000    0.000    0.000 contextlib.py:81(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 contextlib.py:116(__exit__)\n",
       "        3    0.000    0.000    0.000    0.000 weakref.py:358(remove)\n",
       "        2    0.000    0.000    0.000    0.000 six.py:184(find_module)\n",
       "       12    0.000    0.000    0.000    0.000 parse.py:98(_noop)\n",
       "        1    0.000    0.000    0.000    0.000 pathlib.py:676(_format_parsed_parts)\n",
       "        1    0.000    0.000    0.000    0.000 pathlib.py:693(__str__)\n",
       "        1    0.000    0.000    0.000    0.000 pathlib.py:703(__fspath__)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'timestamp' of 'datetime.datetime' objects}\n",
       "        4    0.000    0.000    0.000    0.000 socket.py:97(_intenum_converter)\n",
       "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2171(all)\n",
       "        1    0.000    0.000    0.000    0.000 converters.py:47(escape_bool)\n",
       "        1    0.000    0.000    0.000    0.000 config.py:402(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 config.py:401(__enter__)\n",
       "        2    0.000    0.000    0.000    0.000 config_init.py:347(_deprecate_negative_int_max_colwidth)\n",
       "        3    0.000    0.000    0.000    0.000 cast.py:1381(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:427(_align_method_SERIES)\n",
       "        1    0.000    0.000    0.000    0.000 array_ops.py:264(na_logical_op)\n",
       "        1    0.000    0.000    0.000    0.000 nanops.py:329(_na_ok_dtype)\n",
       "        1    0.000    0.000    0.029    0.029 strings.py:1676(str_strip)\n",
       "        1    0.000    0.000    0.030    0.030 strings.py:1946(wrapper)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:1803(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:5412(values)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:7264(isna)\n",
       "        1    0.000    0.000    0.000    0.000 range.py:685(__getitem__)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:249(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:798(as_array)\n",
       "        3    0.000    0.000    0.000    0.000 managers.py:1582(_consolidate_inplace)\n",
       "        2    0.000    0.000    0.000    0.000 format.py:476(get_buffer)\n",
       "       14    0.000    0.000    0.000    0.000 format.py:757(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:876(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:1021(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 format.py:1027(has_index_names)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:1039(show_col_idx_names)\n",
       "        4    0.000    0.000    0.000    0.000 format.py:1290(_value_formatter)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:226(get_compression_method)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:658(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 parsers.py:1170(_is_index_col)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1979(_set_noconvert_columns)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:2108(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 sql.py:57(_convert_params)\n",
       "        2    0.000    0.000    0.000    0.000 sql.py:552(_engine_builder)\n",
       "        1    0.000    0.000    0.000    0.000 converters.py:68(_escape_unicode)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:383(autocommit)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:469(escape_string)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:946(_get_auth_plugin_handler)\n",
       "        1    0.000    0.000    0.000    0.000 util.py:4(byte2int)\n",
       "       11    0.000    0.000    0.000    0.000 cursors.py:76(_check_executed)\n",
       "        2    0.000    0.000    0.000    0.000 protocol.py:94(rewind)\n",
       "        3    0.000    0.000    0.000    0.000 _collections.py:733(__missing__)\n",
       "        3    0.000    0.000    0.000    0.000 _collections.py:759(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 exc.py:24(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 langhelpers.py:59(__enter__)\n",
       "        2    0.000    0.000    0.000    0.000 langhelpers.py:62(__exit__)\n",
       "        6    0.000    0.000    0.000    0.000 base.py:39(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:751(_select_iterable)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:1271(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 elements.py:3098(_from_objects)\n",
       "        2    0.000    0.000    0.000    0.000 elements.py:4322(_string_or_unprintable)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:4345(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 elements.py:4544(_literal_as_binds)\n",
       "        1    0.000    0.000    0.000    0.000 operators.py:1399(is_boolean)\n",
       "        5    0.000    0.000    0.000    0.000 type_api.py:269(result_processor)\n",
       "        2    0.000    0.000    0.000    0.000 <string>:1(<lambda>)\n",
       "        3    0.000    0.000    0.000    0.000 selectable.py:3002(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 selectable.py:3663(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 attr.py:255(__call__)\n",
       "        1    0.000    0.000    0.000    0.000 attr.py:276(_memoized_attr__exec_once_mutex)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:180(__exit__)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:709(in_transaction)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:410(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 compiler.py:1261(visit_binary)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:3598(format_label)\n",
       "        1    0.000    0.000    0.010    0.010 base.py:296(_create_connection)\n",
       "        6    0.000    0.000    0.000    0.000 base.py:958(__getattr__)\n",
       "        1    0.000    0.000    0.000    0.000 impl.py:142(_inc_overflow)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:23(find_module)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2309(_get_default_schema_name)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:2381(_is_mariadb)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:2407(_supports_cast)\n",
       "        1    0.000    0.000    0.000    0.000 mysqldb.py:214(_detect_charset)\n",
       "        3    0.000    0.000    0.000    0.000 <ipython-input-228-d3bc5f4776a7>:9(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'insert' of 'list' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'reverse' of 'list' objects}\n",
       "       11    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'issuperset' of 'set' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'find' of 'str' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'partition' of 'str' objects}\n",
       "        9    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _imp.is_builtin}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _imp.is_frozen}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method gc.get_referents}\n",
       "        3    0.000    0.000    0.000    0.000 {method 'popleft' of 'collections.deque' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:103(release)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:143(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:147(__enter__)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:792(find_spec)\n",
       "        6    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:859(__exit__)\n",
       "        1    0.000    0.000    0.000    0.000 codecs.py:186(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 abc.py:141(__subclasscheck__)\n",
       "        1    0.000    0.000    0.000    0.000 contextlib.py:107(__enter__)\n",
       "        1    0.000    0.000    0.000    0.000 re.py:232(compile)\n",
       "        8    0.000    0.000    0.000    0.000 threading.py:507(is_set)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:1361(debug)\n",
       "        6    0.000    0.000    0.000    0.000 <string>:1(__new__)\n",
       "        1    0.000    0.000    0.000    0.000 pathlib.py:100(join_parsed_parts)\n",
       "        1    0.000    0.000    0.000    0.000 pathlib.py:666(_from_parsed_parts)\n",
       "        1    0.000    0.000    0.000    0.000 pathlib.py:900(__truediv__)\n",
       "        1    0.000    0.000    0.000    0.000 pathlib.py:1010(_init)\n",
       "        1    0.000    0.000    0.000    0.000 interactiveshell.py:698(get_ipython)\n",
       "        1    0.000    0.000    0.000    0.000 <string>:1(<module>)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        1    0.000    0.000    0.000    0.000 config.py:407(__exit__)\n",
       "        1    0.000    0.000    0.000    0.000 inference.py:130(is_file_like)\n",
       "        2    0.000    0.000    0.000    0.000 cast.py:1367(<genexpr>)\n",
       "        3    0.000    0.000    0.000    0.000 cast.py:1370(<genexpr>)\n",
       "        2    0.000    0.000    0.000    0.000 cast.py:1376(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 nanops.py:189(_maybe_get_mask)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:4091(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 frame.py:1249(to_numpy)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:381(<dictcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method pandas._libs.writers.word_len}\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:1062(value_getitem)\n",
       "        3    0.000    0.000    0.000    0.000 managers.py:1544(index)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:1576(is_consolidated)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:1872(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:1855(_interleaved_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:458(should_show_dimensions)\n",
       "        1    0.000    0.000    0.005    0.005 format.py:909(to_string)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:1031(has_column_names)\n",
       "        2    0.000    0.000    0.010    0.005 construction.py:592(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 series.py:661(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 series.py:678(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:923(_check_file_or_buffer)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1156(_create_index)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1271(_validate_usecols_arg)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1321(_validate_parse_dates_arg)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:2110(<dictcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:3213(_make_date_converter)\n",
       "        2    0.000    0.000    0.000    0.000 sql.py:68(_process_parse_dates_argument)\n",
       "        1    0.000    0.000    0.000    0.000 charset.py:40(by_id)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:101(lenenc_int)\n",
       "        2    0.000    0.000    0.000    0.000 connections.py:96(pack_int24)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:389(get_autocommit)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:462(literal)\n",
       "        1    0.000    0.000    0.000    0.000 connections.py:964(character_set_name)\n",
       "        1    0.000    0.000    0.000    0.000 protocol.py:142(read_string)\n",
       "        3    0.000    0.000    0.000    0.000 _collections.py:730(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:3950(_from_objects)\n",
       "        1    0.000    0.000    0.000    0.000 elements.py:4062(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 elements.py:4559(_interpret_as_column_or_from)\n",
       "        1    0.000    0.000    0.000    0.000 operators.py:1376(is_comparison)\n",
       "        1    0.000    0.000    0.000    0.000 attr.py:340(for_modify)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:352(__str__)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:708(default_from)\n",
       "        1    0.000    0.000    0.000    0.000 compiler.py:872(visit_collation)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:890(escape_literal_column)\n",
       "        1    0.000    0.000    0.000    0.000 compiler.py:3579(format_collation)\n",
       "        2    0.000    0.000    0.000    0.000 result.py:760(keys)\n",
       "        2    0.000    0.000    0.000    0.000 result.py:1306(scalar)\n",
       "        2    0.000    0.000    0.000    0.000 default.py:477(set_connection_execution_options)\n",
       "        2    0.000    0.000    0.000    0.000 default.py:1089(handle_dbapi_exception)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2385(_is_mysql)\n",
       "        1    0.000    0.000    0.000    0.000 <ipython-input-250-93e2c218b0d7>:99(Results)\n",
       "        1    0.000    0.000    0.000    0.000 <ipython-input-250-93e2c218b0d7>:100(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'split' of 'bytes' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'with_traceback' of 'BaseException' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'isdigit' of 'str' objects}\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method builtins.globals}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.ord}\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method _imp.acquire_lock}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method sys.getfilesystemencoding}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
       "        7    0.000    0.000    0.000    0.000 {method '_is_owned' of '_thread.RLock' objects}\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:416(parent)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'update' of '_hashlib.HASH' objects}\n",
       "        1    0.000    0.000    0.000    0.000 parse.py:394(_checknetloc)\n",
       "        1    0.000    0.000    0.000    0.000 pathlib.py:290(splitroot)\n",
       "        4    0.000    0.000    0.000    0.000 config.py:583(_get_registered_option)\n",
       "        2    0.000    0.000    0.000    0.000 config.py:793(inner)\n",
       "        2    0.000    0.000    0.000    0.000 {pandas._libs.algos.ensure_float64}\n",
       "        2    0.000    0.000    0.000    0.000 cast.py:1374(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:124(_maybe_match_name)\n",
       "        1    0.000    0.000    0.000    0.000 nanops.py:166(_get_fill_value)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:99(_freeze)\n",
       "        4    0.000    0.000    0.000    0.000 range.py:233(stop)\n",
       "        4    0.000    0.000    0.000    0.000 range.py:256(step)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:660(is_mixed_type)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:78(validate_header_arg)\n",
       "        2    0.000    0.000    0.000    0.000 series.py:659(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:396(_validate_names)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:1432(_has_complex_date_col)\n",
       "        2    0.000    0.000    0.000    0.000 parsers.py:1545(_maybe_make_multi_index_columns)\n",
       "        1    0.000    0.000    0.000    0.000 parsers.py:3346(_clean_na_values)\n",
       "        1    0.000    0.000    0.000    0.000 charset.py:18(encoding)\n",
       "        1    0.000    0.000    0.000    0.000 protocol.py:60(get_all_data)\n",
       "        1    0.000    0.000    0.000    0.000 protocol.py:196(is_auth_switch_request)\n",
       "        5    0.000    0.000    0.000    0.000 _collections.py:145(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 langhelpers.py:56(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:177(__enter__)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:479(_still_open_and_connection_is_valid)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:419(type)\n",
       "        3    0.000    0.000    0.000    0.000 compiler.py:2080(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:875(is_valid)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2367(_warn_for_known_db_issues)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%time\n",
    "def main():\n",
    "    '''\n",
    "        corpora: i2b2, mipacq, fv017\n",
    "        analyses: entity only (exact span), cui by document, full (aka (entity and cui on exaact span/exact cui)\n",
    "        systems: ctakes, biomedicus, clamp, metamap, quick_umls\n",
    "        \n",
    "        TODO -> Vectorization (entity only) -> done:\n",
    "                add switch for use of TN on single system performance evaluations -> done\n",
    "                add switch for overlap matching versus exact span -> done\n",
    "             -> Other tasks besides concept extraction\n",
    "        \n",
    "    ''' \n",
    "    analysisConf =  AnalysisConfig()\n",
    "    print(analysisConf.systems, analysisConf.corpus_config())\n",
    "   \n",
    "    '''\n",
    "    NB: For single system evaluation select expression_type = 'single'\n",
    "    '''\n",
    "#     if (rtype == 1):\n",
    "#         print(semtypes, systems)\n",
    "#         if filter_semtype:\n",
    "#             for semtype in semtypes:\n",
    "#                 test = get_valid_systems(systems, semtype)\n",
    "#                 print('SYSYEMS FOR SEMTYPE', semtype, 'ARE', test)\n",
    "#                 generate_metrics(analysis_type, corpus, filter_semtype, semtype)\n",
    "            \n",
    "#         else:\n",
    "#             generate_metrics(analysis_type, corpus, filter_semtype)\n",
    "        \n",
    "    if (rtype == 2):\n",
    "        print('run_type:', run_type)\n",
    "        if filter_semtype:\n",
    "            print(semtypes)\n",
    "            ensemble_control(analysisConf.systems, analysis_type, corpus, run_type, filter_semtype, semtypes)\n",
    "        else:\n",
    "            ensemble_control(analysisConf.systems, analysis_type, corpus, run_type, filter_semtype)\n",
    "#     elif (rtype == 3):\n",
    "#         t = ['concept_jaccard_score_false']\n",
    "#         test_systems(analysis_type, analysisConf.systems, corpus)  \n",
    "#         test_count(analysis_type, corpus)\n",
    "#         test_ensemble(analysis_type, corpus)\n",
    "    elif (rtype == 4):\n",
    "        \n",
    "        out = pd.DataFrame()\n",
    "        for i in range(2, len(systems) + 1):\n",
    "            for s in combinations(systems, i):\n",
    "                print(s)\n",
    "                if filter_semtype:\n",
    "                    metrics = majority_vote(s, analysis_type, corpus, run_type, filter_semtype, semtypes)\n",
    "                else:\n",
    "                    metrics = majority_vote(s, analysis_type, corpus, run_type, filter_semtype)\n",
    "                    \n",
    "                frames = [out, metrics]\n",
    "                out = pd.concat(frames, ignore_index=True, sort=False)\n",
    "                \n",
    "        now = datetime.now()\n",
    "        timestamp = datetime.timestamp(now)\n",
    "        \n",
    "        file = corpus + '_vote_' + analysis_type + '_' + str(filter_semtype) + '_' + str(timestamp) +'.csv'\n",
    "        out.to_csv(data_out / file)\n",
    "                \n",
    "    elif (rtype == 5): # with evaluation\n",
    "        \n",
    "        # control filter_semtype in get_sys_data, get_ref_n and generate_metrics. TODO consolidate. \n",
    "        # # run single ad hoc statement\n",
    "        statement = '((ctakes&biomedicus)|metamap)'\n",
    "\n",
    "        def ad_hoc(analysis_type, corpus, statement):\n",
    "            sys = get_merge_data(statement, analysis_type, corpus, run_type, filter_semtype)\n",
    "            sys = sys.rename(index=str, columns={\"note_id\": \"case\"})\n",
    "            sys['label'] = 'concept'\n",
    "\n",
    "            ref = get_reference_vector(analysis_type, corpus, filter_semtype)\n",
    "            sys = vectorized_annotations(sys)\n",
    "            sys = np.asarray(flatten_list(list(sys)), dtype=np.int32)\n",
    "\n",
    "            return ref, sys\n",
    "\n",
    "        ref, sys = ad_hoc(analysis_type, corpus, statement)\n",
    "        \n",
    "    elif (rtype == 6): # 5 w/o evaluation\n",
    "        \n",
    "        statement = '(ctakes|biomedicus)'\n",
    "\n",
    "        def ad_hoc(analysis_type, corpus, statement):\n",
    "            sys = get_merge_data(statement, analysis_type, corpus, run_type, filter_semtype)\n",
    "            sys = sys.rename(index=str, columns={\"note_id\": \"case\"})\n",
    "\n",
    "            return sys\n",
    "\n",
    "        sys = ad_hoc(analysis_type, corpus, statement).sort_values(by=['case', 'begin'])\n",
    "        \n",
    "        print(statement + '\\n', sys.head(100))\n",
    "        sys.to_csv('test.csv')\n",
    "    \n",
    "    elif (rtype == 7): # complementarity ala http://www.lrec-conf.org/proceedings/lrec2016/pdf/105_Paper.pdf\n",
    "       \n",
    "        class Results(object):\n",
    "            def __init__(self):\n",
    "                self.sysA = ''\n",
    "                self.sysB = ''\n",
    "            \n",
    "        r = Results()\n",
    "        \n",
    "        def ad_hoc(analysis_type, corpus, systems):\n",
    "            #sys = get_merge_data(statement, analysis_type, corpus, run_type, filter_semtype)\n",
    "            \n",
    "            df = pd.DataFrame()\n",
    "            for c in list(combinations(systems, 2)):\n",
    "                \n",
    "                r.sysA = c[0]\n",
    "                r.sysB = c[1]\n",
    "\n",
    "                if filter_semtype:\n",
    "                    for semtype in semtypes:\n",
    "                        out = vectorized_complementarity(r, analysis_type, corpus, c, filter_semtype, semtype)\n",
    "                        out['semgroup'] = semtype\n",
    "                        frames = [df, out]\n",
    "                        df = pd.concat(frames, ignore_index=True, sort=False) \n",
    "                        \n",
    "                else:\n",
    "                    out = vectorized_complementarity(r, analysis_type, corpus, c, filter_semtype)\n",
    "                    out['semgroup'] = 'All groups'\n",
    "                    frames = [df, out]\n",
    "                    df = pd.concat(frames, ignore_index=True, sort=False) \n",
    "                    \n",
    "            #return sys\n",
    "            now = datetime.now()\n",
    "            timestamp = datetime.timestamp(now)\n",
    "           \n",
    "            file = 'complement_' + corpus + '_filter_semtype_' + str(filter_semtype) + '_' + str(timestamp) +'.csv'\n",
    "            df.to_csv(data_out / file)\n",
    "            print(df)\n",
    "            \n",
    "        ad_hoc(analysis_type, corpus, systems)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    %prun main()\n",
    "    print('done!')\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
