{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  Copyright (c) 2019 Regents of the University of Minnesota.\n",
    " \n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    " \n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gevent\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import numpy as np\n",
    "import math\n",
    "import pymysql\n",
    "import time \n",
    "import functools as ft\n",
    "import glob   \n",
    "import operator as op\n",
    "import shelve\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from pathlib import Path\n",
    "from itertools import combinations, product, permutations\n",
    "from sqlalchemy.engine import create_engine\n",
    "from datetime import datetime\n",
    "from ast import literal_eval\n",
    "from scipy import stats  \n",
    "from scipy.stats.mstats import gmean\n",
    "from pythonds.basic.stack import Stack\n",
    "from pythonds.trees.binaryTree import BinaryTree\n",
    "from collections import defaultdict\n",
    "from typing import List, Set, Tuple "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The cell below contains the configurable parameters to ensure that our ensemble explorer runs properaly on your machine. Please read carfully through steps (1-7) before running the rest of the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-1: CHOOSE YOUR CORPUS\n",
    "# TODO: allow for list of corpora\n",
    "corpus = 'fairview' #options include 'fairview', 'mipacq' OR 'i2b2'\n",
    "\n",
    "# STEP-2: CHOOSE YOUR DATA DIRECTORY; this is where output data will be saved on your machine\n",
    "data_directory = '/Users/gms/development/nlp/nlpie/data/ensembling-u01/output/' \n",
    "\n",
    "# STEP-3: CHOOSE WHICH SYSTEMS YOU'D LIKE TO PROCESS THE CORPUS\n",
    "\n",
    "systems = ['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls']\n",
    "systems = ['clamp', 'ctakes']\n",
    "\n",
    "# STEP-4: CHOOSE TYPE OF RUN\n",
    "rtype = 1      # OPTIONS INCLUDE: 1->Single systems; 2->Ensemble; 3->Tests; 4-> MM Test\n",
    "               # The Ensemble includes ['ctakes','biomedicus','clamp','metamap','quick_umls']\n",
    "    \n",
    "# STEP-5: CHOOSE WHAT TYPE OF ANALYSIS YOU'D LIKE TO RUN ON THE CORPUS\n",
    "analysis_type = 'entity' #options include 'entity' OR 'full'\n",
    "\n",
    "# STEP-(6A): ENTER DETAILS FOR ACCESSING MANUAL ANNOTATION DATA\n",
    "database_type = 'mysql+pymysql' # We use mysql+pymql as default\n",
    "database_username = 'gms'\n",
    "database_password = 'nej123' \n",
    "database_url = 'localhost' # HINT: use localhost if you're running database on your local machine\n",
    "database_name = 'concepts' # Enter database name\n",
    "table_name = corpus + '_all' # Enter the table within the database where your reference data is stored\n",
    "\n",
    "# STEP-(6B): ENTER DIRECTORY FOR ACCESSING SYSTEM ANNOTATION DATA\n",
    "system_annotation = 'analytical_'+corpus+'.csv' # OPTIONS include 'analytical_cui_mipacq_concepts.csv' OR 'analytical_cui_i2b2_concepts.csv' \n",
    "\n",
    "# STEP-7: WE'LL CREATE A 'SYSTEM OUTPUT'DIRECTORY FOR YOU INSIDE THE DIRECTORY YOU SPECIFIED IN (STEP 2)\n",
    "single_sys_dir = Path(data_directory + \"single_system_out\")\n",
    "single_sys_dir.mkdir(parents=True, exist_ok=True)\n",
    "dir_out = Path(data_directory + 'single_system_out/')\n",
    "\n",
    "# STEP-8: CREATE A DB CONNECTION POOL\n",
    "engine_request = str(database_type)+'://'+database_username+':'+database_password+\"@\"+database_url+'/'+database_name\n",
    "engine = create_engine(engine_request, pool_pre_ping=True, pool_size=20, max_overflow=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config class for analysis\n",
    "class AnalysisConfig():\n",
    "    \"\"\"\n",
    "    Configuration object:\n",
    "    systems to use\n",
    "    notes by corpus\n",
    "    paths by output, gold and system location\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self = self    \n",
    "        self.systems = systems\n",
    "        self.data_dir = data_directory\n",
    "    \n",
    "    def corpus_config(self): \n",
    "        usys_data = system_annotation\n",
    "        ref_data = database_name+'.'+table_name\n",
    "        return usys_data, ref_data\n",
    "        \n",
    "\n",
    "analysisConf =  AnalysisConfig()\n",
    "usys, ref = analysisConf.corpus_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticTypes():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self = self\n",
    "        self.biomedicus_types = ['placeholder']\n",
    "        self.ctakes_types = {'DiseaseDisorderMention'}\n",
    "        self.clamp_types = {'problem'}\n",
    "        self.reference_types = {'Finding'}\n",
    "    \n",
    "    def get_system_type(self, system):  \n",
    "        \n",
    "        if system == 'biomedicus':\n",
    "            semtypes = self.biomedicus_types\n",
    "        elif system == 'ctakes':\n",
    "            semtypes = self.ctakes_types\n",
    "        elif system == 'clamp':\n",
    "            semtypes = self.clamp_types\n",
    "        elif system == 'reference':\n",
    "            semtypes = self.reference_types\n",
    "            \n",
    "        return semtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation class for systems\n",
    "class AnnotationSystems():\n",
    "    \"\"\"   \n",
    "    System annotations of interest for UMLS concept extraction\n",
    "    NB: ctakes combines all \"mentions\" annotation types\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\" \n",
    "        annotation base types\n",
    "        \"\"\"   \n",
    "        \n",
    "        self.biomedicus_types = [\"biomedicus.v2.UmlsConcept\"]\n",
    "        self.clamp_types = [\"edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA\"]\n",
    "        self.ctakes_types = ['ctakes_mentions']\n",
    "        self.metamap_types = [\"org.metamap.uima.ts.Candidate\"]\n",
    "        self.quick_umls_types = ['concept_jaccard_score_False']\n",
    "       \n",
    "    def get_system_type(self, system):\n",
    "        \n",
    "        \"\"\"\n",
    "        return system types\n",
    "        \"\"\"\n",
    "        \n",
    "        if system == \"biomedicus\":\n",
    "            view = \"Analysis\"\n",
    "        else:\n",
    "            view = \"_InitialView\"\n",
    "\n",
    "        if system == 'biomedicus':\n",
    "            types = self.biomedicus_types\n",
    "\n",
    "        elif system == 'clamp':\n",
    "            types = self.clamp_types\n",
    "\n",
    "        elif system == 'ctakes':\n",
    "            types = self.ctakes_types\n",
    "\n",
    "        elif system == 'metamap':\n",
    "            types = self.metamap_types\n",
    "        \n",
    "        elif system == \"quick_umls\":\n",
    "            types = self.quick_umls_types\n",
    "            \n",
    "        return types, view\n",
    "    \n",
    "annSys = AnnotationSystems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np # access to Numpy from Python layer\n",
    "import math\n",
    "\n",
    "class Metrics(object):\n",
    "    \"\"\"\n",
    "    metrics class:\n",
    "    returns an instance with confusion matrix metrics\n",
    "    \"\"\"\n",
    "    def __init__(self, system_only, gold_only, gold_system_match, system_n, neither = 0): # neither: no sys or manual annotation\n",
    "\n",
    "        self = self    \n",
    "        self.system_only = system_only\n",
    "        self.gold_only = gold_only\n",
    "        self.gold_system_match = gold_system_match\n",
    "        self.system_n = system_n\n",
    "        self.neither = neither\n",
    "        \n",
    "    def get_confusion_metrics(self, corpus = None, test = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        compute confusion matrix measures, as per  \n",
    "        https://stats.stackexchange.com/questions/51296/how-do-you-calculate-precision-and-recall-for-multiclass-classification-using-co\n",
    "        \"\"\"\n",
    "        cdef:\n",
    "            int TP, FP, FN\n",
    "            double TM\n",
    "\n",
    "        TP = self.gold_system_match\n",
    "        FP = self.system_only\n",
    "        FN = self.gold_only\n",
    "        \n",
    "        TM = TP/math.sqrt(self.system_n) # TigMetric\n",
    "       \n",
    "        if not test:\n",
    "            \n",
    "            if corpus == 'casi':\n",
    "                recall = TP/(TP + FN)\n",
    "                precision = TP/(TP + FP)\n",
    "                F = 2*(precision*recall)/(precision + recall)\n",
    "            else:\n",
    "                if self.neither == 0:\n",
    "                    confusion = [[0, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "                else:\n",
    "                    confusion = [[self.neither, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "                c = np.asarray(confusion)\n",
    "                recall = np.diag(c) / np.sum(c, axis = 1)\n",
    "                precision = np.diag(c) / np.sum(c, axis = 0)\n",
    "                F = 2*(precision*recall)/(precision + recall)\n",
    "        else:\n",
    "            precision = TP/(TP+FP)\n",
    "            recall = TP/(TP+FN)\n",
    "            F = 2*(precision*recall)/(precision + recall)\n",
    "        \n",
    "        # Tignanelli Metric\n",
    "        if FN == 0:\n",
    "            TP_FN_R = TP\n",
    "        elif FN > 0:\n",
    "            TP_FN_R = TP/FN\n",
    " \n",
    "        return F, recall, precision, TP, FP, FN, TP_FN_R, TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out(name: str, analysis_type: str, c: object):\n",
    "   \n",
    "    \"\"\"\n",
    "    write matching and reference-only sets to file for use in merging combinations\n",
    "    \"\"\"\n",
    "    \n",
    "    # write output to file\n",
    "    dir_out = analysisConf.data_dir + 'single_system_out/'\n",
    "    with open(dir_out + name + '_' + analysis_type + '_' + c.corpus + '_matches.txt', 'w') as f:\n",
    "        for item in list(c.matches):\n",
    "            f.write(\"%s\\n\" % str(item))\n",
    "\n",
    "    # write to file\n",
    "    with open(dir_out + name + '_' + analysis_type + '_' + c.corpus + '_ref_only.txt', 'w') as f:\n",
    "        for item in list(c.false_negatives):\n",
    "            f.write(\"%s\\n\" % str(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_set(df, analysis_type = 'entity', df_type = 'sys', corpus = None):\n",
    "    \n",
    "    # get values for creation of series of type tuple\n",
    "    if 'entity' in analysis_type: \n",
    "        if corpus == 'casi':\n",
    "            arg = df.case, df.overlap\n",
    "        else:    \n",
    "            if df_type == 'sys':\n",
    "                arg = df.begin, df.end, df.note_id\n",
    "            else:\n",
    "                arg = df.start, df.end, df.file\n",
    "            \n",
    "    elif 'cui' in analysis_type:\n",
    "        if df_type == 'sys':\n",
    "            arg = df.cui, df.note_id\n",
    "        else:\n",
    "            arg = df.value, df.file\n",
    "    elif 'full' in analysis_type:\n",
    "        if df_type == 'sys':\n",
    "            arg = df.begin, df.end, df.cui, df.note_id\n",
    "        else:\n",
    "            arg = df.start, df.end, df.value, df.file\n",
    "    \n",
    "    return set(list(zip(*arg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython \n",
    "\n",
    "from __main__ import write_out, df_to_set, engine\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "def get_cooccurences(ref, sys, analysis_type: str, corpus: str, single_sys = True, name = None):\n",
    "    \"\"\"\n",
    "    get coocurences between system and reference; exact match; TODO: add relaxed\n",
    "    \"\"\"\n",
    "    # cooccurences\n",
    "    class Coocurences(object):\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.ref_system_match = 0\n",
    "            self.ref_only = 0\n",
    "            self.system_only = 0\n",
    "            self.system_n = 0\n",
    "            self.ref_n = 0\n",
    "            self.matches = set()\n",
    "            self.false_negatives = set()\n",
    "            self.corpus = corpus\n",
    "            self.cases = set(ref[\"file\"].tolist()) # cases to label \n",
    "\n",
    "    c = Coocurences()\n",
    "    \n",
    "    if corpus != 'casi':\n",
    "        if 'entity' in analysis_type and single_sys: # mipacq n -> 16793\n",
    "            cols_to_keep = ['begin', 'end', 'note_id']\n",
    "            sys = sys[cols_to_keep].drop_duplicates()\n",
    "            ref = ref[['start', 'end', 'file']].drop_duplicates()\n",
    "            sys.name = name\n",
    "        elif 'cui' in analysis_type and single_sys: # mipacq n -> 10799\n",
    "            cols_to_keep = ['cui', 'note_id']\n",
    "            sys = sys[cols_to_keep].drop_duplicates()\n",
    "            # do not overestimate FP\n",
    "            sys = sys[~sys['cui'].isnull()] \n",
    "            ref = ref[['value', 'file']].drop_duplicates()\n",
    "            ref = ref[~ref['value'].isnull()]\n",
    "            sys.name = name\n",
    "        elif 'full' in analysis_type and single_sys: # mipacq n -> 17393\n",
    "            cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "            sys = sys[cols_to_keep].drop_duplicates()\n",
    "            sys = sys[~sys['cui'].isnull()]\n",
    "            ref = ref[['start', 'end', 'value', 'file']].drop_duplicates()\n",
    "            ref = ref[~ref['value'].isnull()]\n",
    "            sys.name = name\n",
    "\n",
    "        # matches via inner join\n",
    "        matches = pd.merge(sys, ref, how = 'inner', left_on=['begin','end','note_id'], right_on = ['start','end','file']) \n",
    "        # reference-only via left outer join\n",
    "        fn = pd.merge(ref, sys, how = 'left', left_on=['start','end','file'], right_on = ['begin','end','note_id']) \n",
    "\n",
    "        fn = fn[fn['begin'].isnull()] # get as outer join with no match\n",
    "\n",
    "        if 'entity' in analysis_type and single_sys:\n",
    "            cols_to_keep = ['start', 'end', 'file']\n",
    "        else:\n",
    "            cols_to_keep = ['start', 'end', 'value', 'file']\n",
    "\n",
    "        matches = matches[cols_to_keep]\n",
    "        fn = fn[cols_to_keep]\n",
    "\n",
    "        # use for metrics \n",
    "        c.matches = c.matches.union(df_to_set(matches, analysis_type, 'ref'))\n",
    "        c.false_negatives = c.false_negatives.union(df_to_set(fn, analysis_type, 'ref'))\n",
    "        c.ref_system_match = len(c.matches)\n",
    "        c.system_only = len(sys) - len(c.matches)\n",
    "        c.system_n = len(sys)\n",
    "        c.ref_n = len(ref)\n",
    "        c.ref_only = len(c.false_negatives)\n",
    "        \n",
    "    else:\n",
    "        sql = \"select `case` from test.amia_2019_analytical_v where overlap = 1 and `system` = %(sys.name)s\"  \n",
    "        \n",
    "        matches = pd.read_sql(sql, params={\"sys.name\":sys.name}, con=engine)\n",
    "        \n",
    "        sql = \"select `case` from test.amia_2019_analytical_v where (overlap = 0 or overlap is null) and `system` = %(sys.name)s\"  \n",
    "        \n",
    "        fn = pd.read_sql(sql, params={\"sys.name\":sys.name}, con=engine)\n",
    "        \n",
    "        c.matches = df_to_set(matches, 'entity', 'sys', 'casi')\n",
    "        c.fn = df_to_set(fn, 'entity', 'sys', 'casi')\n",
    "        c.ref_system_match = len(c.matches)\n",
    "        c.system_only = len(sys) - len(c.matches)\n",
    "        c.system_n = len(matches) + len(fn)\n",
    "        c.ref_n = len(matches) + len(fn)\n",
    "        c.ref_only = len(fn)\n",
    "        \n",
    "    # sanity check\n",
    "    if len(ref) - c.ref_system_match < 0:\n",
    "        print('Error: ref_system_match > len(ref)!')\n",
    "    if len(ref) != c.ref_system_match + c.ref_only:\n",
    "        print('Error: ref count mismatch!')\n",
    "   \n",
    "    # save TP/FN\n",
    "    if single_sys and corpus != 'casi':\n",
    "        print(analysis_type)\n",
    "        write_out(sys.name, analysis_type, c)\n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%cython \n",
    "\n",
    "#from __main__ import write_out\n",
    "\n",
    "#import numpy as np # access to Numpy from Python layer\n",
    "def label_vector(doc: str, ann: List[int], labels: List[str]) -> np.array:\n",
    "\n",
    "    #print(ann, doc, labels)\n",
    "\n",
    "    v = np.zeros(doc)\n",
    "    labels = list(labels)\n",
    "    \n",
    "    for (i, lab) in enumerate(labels):\n",
    "        i += 1  # 0 is reserved for no label\n",
    "        idxs = [np.arange(a.begin, a.end) for a in ann if a.label == lab]\n",
    "            \n",
    "        idxs = [j for mask in idxs for j in mask]\n",
    "        v[idxs] = i\n",
    "\n",
    "    return v\n",
    "\n",
    "# test confusion matrix elements for vectorized annotation set; includes TN\n",
    "def confused(sys1, ann1):\n",
    "    TP = np.sum(np.logical_and(sys1 >= 1, ann1 == sys1 ))\n",
    "\n",
    "    # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "    TN = np.sum(np.logical_and(sys1 == 0, ann1 == 0))\n",
    "\n",
    "    # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "    FP = np.sum(np.logical_and(sys1 >= 1, ann1 == 0))\n",
    "\n",
    "    # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "    FN = np.sum(np.logical_and(sys1 == 0, ann1 >= 1))\n",
    "    \n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "\n",
    "def get_cooccurences_vec(ref, sys, analysis_type: str, corpus: str, single_sys = True, name = None):\n",
    "    \"\"\"\n",
    "    get coocurences between system and reference; exact match; TODO: add relaxed\n",
    "    \"\"\"\n",
    "    # test cooccurences\n",
    "    class Coocurences(object):\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.ref_system_match = 0\n",
    "            self.ref_only = 0\n",
    "            self.system_only = 0\n",
    "            self.system_n = 0\n",
    "            self.ref_n = 0\n",
    "            self.matches = set()\n",
    "            self.false_negatives = set()\n",
    "            self.corpus = corpus\n",
    "            self.cases = set(ref[\"file\"].tolist()) # cases to label \n",
    "\n",
    "    c = Coocurences()\n",
    "    \n",
    "    # vectorization and i-o labeling\n",
    "    def test_io():\n",
    "        test = c.cases\n",
    "        if analysis_type == 'entity':\n",
    "            docs = [(x, len(open(\"/Users/gms/development/nlp/nlpie/data/ensembling-u01/i2b2/source_data/test_data/\" + x + \".txt\", 'r').read())) for x in test]\n",
    "\n",
    "        ann = ref.copy()\n",
    "        ann = ann.rename(index=str, columns={\"start\": \"begin\", \"file\": \"case\"}).copy()\n",
    "        cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "        if analysis_type == 'entity':\n",
    "            labels = [\"concept\"]\n",
    "            ann[\"label\"] = 'concept'\n",
    "            ann = ann[cols_to_keep].copy()\n",
    "\n",
    "        sys_ = sys.rename(index=str, columns={\"note_id\": \"case\"}).copy()\n",
    "        \n",
    "        # need for enttity-only\n",
    "        if analysis_type == 'entity':\n",
    "            sys_[\"label\"] = 'concept'\n",
    "        \n",
    "        sys_ = sys_[cols_to_keep]\n",
    "       \n",
    "        tp = []\n",
    "        tn = []\n",
    "        fp = []\n",
    "        fn = []\n",
    "        cvals = []\n",
    "        out = []\n",
    "        t = []\n",
    "        d = defaultdict(list)\n",
    "        \n",
    "        for n in range(len(docs)):\n",
    "            a1 = [i for i in ann[ann[\"case\"] == docs[n][0]].copy().itertuples(index=False)]\n",
    "            s1 = [i for i in sys_[sys_[\"case\"] == docs[n][0]].copy().itertuples(index=False)]\n",
    "\n",
    "            ann1 = label_vector(docs[n][1], a1, labels)\n",
    "            sys1 = label_vector(docs[n][1], s1, labels)\n",
    "            \n",
    "            TP, TN, FP, FN = confused(sys1, ann1)\n",
    "            cvals.append([TP, TN, FP, FN])\n",
    "                 \n",
    "            d['sys'].append(list([int(i) for i in sys1]))\n",
    "            d['oracle'].append(list([int(i) for i in ann1]))\n",
    "            d['case'].append(docs[n][0])\n",
    "            \n",
    "            '''\n",
    "            print(\"tn:\", np.intersect1d(np.where(ann1 == 0)[0], np.where(sys1 == 0)[0]),  \n",
    "                  \"tp:\", np.intersect1d(np.where(ann1 == 1)[0], np.where(sys1 == 1)[0]), \n",
    "                  \"fn:\", np.intersect1d(np.where(ann1 == 1)[0], np.where(sys1 == 0)[0]), \n",
    "                  \"fp:\", np.intersect1d(np.where(ann1 == 0)[0], np.where(sys1 == 1)[0]))\n",
    "            '''\n",
    "        d['labels'] = labels\n",
    "        \n",
    "        corp = shelve.open('/Users/gms/Desktop/' + sys.name + '_' + corpus + '.dat')\n",
    "        \n",
    "        for k in d:\n",
    "            corp[k] = d[k]\n",
    "        \n",
    "        corp.close()\n",
    "        return cvals\n",
    "    \n",
    "    if corpus == 'i2b2':\n",
    "        TP, TN, FP, FN = np.sum(test_io(), axis=0)\n",
    "        F, recall, precision, TP, FP, FN, TP_FN_R, TM = Metrics(FP, FN, TP, len(sys), TN).get_confusion_metrics() #no TN\n",
    "        print('test_io():', TP, TN, FP, FN, np.mean(F), np.mean(recall), np.mean(precision))\n",
    "    \n",
    "    # non-vectorized:\n",
    "    if corpus != 'casi':\n",
    "        if 'entity' in analysis_type and single_sys: # mipacq n -> 16793\n",
    "            cols_to_keep = ['begin', 'end', 'note_id']\n",
    "            sys = sys[cols_to_keep].drop_duplicates()\n",
    "            ref = ref[['start', 'end', 'file']].drop_duplicates()\n",
    "            sys.name = name\n",
    "        elif 'cui' in analysis_type and single_sys: # mipacq n -> 10799\n",
    "            cols_to_keep = ['cui', 'note_id']\n",
    "            sys = sys[cols_to_keep].drop_duplicates()\n",
    "            # do not overestimate FP\n",
    "            sys = sys[~sys['cui'].isnull()] \n",
    "            ref = ref[['value', 'file']].drop_duplicates()\n",
    "            ref = ref[~ref['value'].isnull()]\n",
    "            sys.name = name\n",
    "        elif 'full' in analysis_type and single_sys: # mipacq n -> 17393\n",
    "            cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "            sys = sys[cols_to_keep].drop_duplicates()\n",
    "            sys = sys[~sys['cui'].isnull()]\n",
    "            ref = ref[['start', 'end', 'value', 'file']].drop_duplicates()\n",
    "            ref = ref[~ref['value'].isnull()]\n",
    "            sys.name = name\n",
    "\n",
    "        # matches via inner join\n",
    "        matches = pd.merge(sys, ref, how = 'inner', left_on=['begin','end','note_id'], right_on = ['start','end','file']) \n",
    "        # reference-only via left outer join\n",
    "        fn = pd.merge(ref, sys, how = 'left', left_on=['start','end','file'], right_on = ['begin','end','note_id']) \n",
    "\n",
    "        fn = fn[fn['begin'].isnull()] # get as outer join with no match\n",
    "\n",
    "        if 'entity' in analysis_type and single_sys:\n",
    "            cols_to_keep = ['start', 'end', 'file']\n",
    "        else:\n",
    "            cols_to_keep = ['start', 'end', 'value', 'file']\n",
    "\n",
    "        matches = matches[cols_to_keep]\n",
    "        fn = fn[cols_to_keep]\n",
    "\n",
    "        # use for metrics \n",
    "        c.matches = c.matches.union(df_to_set(matches, analysis_type, 'ref'))\n",
    "        c.false_negatives = c.false_negatives.union(df_to_set(fn, analysis_type, 'ref'))\n",
    "        c.ref_system_match = len(c.matches)\n",
    "        c.system_only = len(sys) - len(c.matches)\n",
    "        c.system_n = len(sys)\n",
    "        c.ref_n = len(ref)\n",
    "        c.ref_only = len(c.false_negatives)\n",
    "        \n",
    "    else:\n",
    "        sql = \"select `case` from test.amia_2019_analytical_v where overlap = 1 and `system` = %(sys.name)s\"  \n",
    "        \n",
    "        matches = pd.read_sql(sql, params={\"sys.name\":sys.name}, con=engine)\n",
    "        \n",
    "        sql = \"select `case` from test.amia_2019_analytical_v where (overlap = 0 or overlap is null) and `system` = %(sys.name)s\"  \n",
    "        \n",
    "        fn = pd.read_sql(sql, params={\"sys.name\":sys.name}, con=engine)\n",
    "        \n",
    "        c.matches = df_to_set(matches, 'entity', 'sys', 'casi')\n",
    "        c.fn = df_to_set(fn, 'entity', 'sys', 'casi')\n",
    "        c.ref_system_match = len(c.matches)\n",
    "        c.system_only = len(sys) - len(c.matches)\n",
    "        c.system_n = len(matches) + len(fn)\n",
    "        c.ref_n = len(matches) + len(fn)\n",
    "        c.ref_only = len(fn)\n",
    "        \n",
    "    # sanity check\n",
    "    if len(ref) - c.ref_system_match < 0:\n",
    "        print('Error: ref_system_match > len(ref)!')\n",
    "    if len(ref) != c.ref_system_match + c.ref_only:\n",
    "        print('Error: ref count mismatch!')\n",
    "   \n",
    "    # save TP/FN\n",
    "    if single_sys and corpus != 'casi':\n",
    "        print(analysis_type)\n",
    "        write_out(sys.name, analysis_type, c)\n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging test for i-o labeled data\n",
    "import numpy as np\n",
    "import shelve\n",
    "# load shelve\n",
    "def read_shelve():\n",
    "    corp = shelve.open('/Users/gms/Desktop/test.dat')\n",
    "\n",
    "    return corp\n",
    "        \n",
    "#test = read_shelve()\n",
    "\n",
    "def test_merge_vector(test):\n",
    "    # get sample for testing\n",
    "    for case in test['case'][3:5]:\n",
    "        for i in range(len(test['case'][3:5])):\n",
    "            if i == 3:\n",
    "                t0 = test['oracle'][3][0:750]\n",
    "            else:\n",
    "                t1 = test['oracle'][4][0:750]\n",
    "\n",
    "            #print('case:', case, test['sys'][i], test['oracle'][i], confused(np.array(test['sys'][i]), np.array(test['oracle'][i])))\n",
    "        #print(t0, t1)\n",
    "\n",
    "    t0 = np.array(test['oracle'][3][0:750])\n",
    "    t1 = np.array(test['oracle'][5][0:750])\n",
    "\n",
    "    l0 = list(t0)\n",
    "    l1 = list(t1)\n",
    "    \n",
    "    l0 = [0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0] \n",
    "    l1 = [0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0]\n",
    "    \n",
    "    print(l0, l1)\n",
    "\n",
    "    def intersection(lst1, lst2): \n",
    "        out = list()\n",
    "        if isinstance(lst1, set) and isinstance(lst2, set):\n",
    "            out = (set(lst1) & set(lst2))\n",
    "        elif isinstance(lst1, set) and isinstance(lst2, np.int64):\n",
    "            out = (set(lst1) & set([lst2]))\n",
    "        elif isinstance(lst1, np.int64) and isinstance(lst2, set):\n",
    "            out = (set([lst1]) & set(lst2))\n",
    "        elif isinstance(lst1, np.int64) and isinstance(lst2, np.int64):\n",
    "            out = (set([lst1]) & set([lst2]))\n",
    "        #if len(out) > 1:\n",
    "        return out\n",
    "        #elif len(out) == 1:\n",
    "        #    return out[0]\n",
    "        #else:\n",
    "        #    return 0\n",
    "\n",
    "    def union(lst1, lst2): \n",
    "        out = list()\n",
    "        if isinstance(lst1, set) and isinstance(lst2, set):\n",
    "            out = set(lst1) | set(lst2)\n",
    "        elif isinstance(lst1, set) and isinstance(lst2, np.int64):\n",
    "            out = set(lst1) | set([lst2])\n",
    "        elif isinstance(lst1, np.int64) and isinstance(lst2, set):\n",
    "            out = set([lst1]) | set(lst2)\n",
    "        elif isinstance(lst1, np.int64) and isinstance(lst2, np.int64):\n",
    "            out = set([lst1]) | set([lst2])\n",
    "        #if len(out) == 1:\n",
    "        #    #out = out[0]\n",
    "        return out\n",
    "\n",
    "    # union and intersect\n",
    "    def umerges(l0, l1):\n",
    "        #un = [0]*len(l0)\n",
    "        #for i in range(len(l0)):\n",
    "        #    un[i] = union(l0[i], l1[i])\n",
    "\n",
    "        return [union(l0[i], l1[i]) for i in range(len(l0))]\n",
    "\n",
    "    %timeit un = umerges(l0, l1)\n",
    "    \n",
    "    x = umerges(l0, l1)\n",
    "    \n",
    "\n",
    "    #l2 = [1, {1, 4}, {3}, {2, 4}, {1}, 0, 2, 3, {0, 8}, {1, 8}]\n",
    "    \n",
    "    #print(umerges(x, l2))\n",
    "    \n",
    "    def imerges(l0, l1):\n",
    "        #inter = [0]*len(l0)\n",
    "        #for i in range(len(l0)):\n",
    "        \n",
    "\n",
    "        return [intersection(l0[i], l1[i]) for i in range(len(l0))]\n",
    "    \n",
    "    %timeit x = imerges(l0, l1)\n",
    "    \n",
    "    '''\n",
    "    union = [\n",
    "        ( [set(x) | set(y)] if isinstance(x, list) and isinstance(y, list)\n",
    "          else [set(x) | set([y])] if isinstance(x, list) and isinstance(y, int)\n",
    "          else [set([x]) | set(y)] if isinstance(x, int) and isinstance(y, list)\n",
    "          else [set([x]) | set([y])])\n",
    "\n",
    "         for x, y in zip(l0, l1)\n",
    "    ]\n",
    "\n",
    "    # unpack map object\n",
    "    #*y, = list(map(list, zip(*union)))\n",
    "    #%timeit list(map(list, zip(*union)))\n",
    "\n",
    "    intersection = [\n",
    "        ( [set(x) & set(y)] if isinstance(x, list) and isinstance(y, list)\n",
    "          else [set(x) & set([y])] if isinstance(x, list) and isinstance(y, int)\n",
    "          else [set([x]) & set(y)] if isinstance(x, int) and isinstance(y, list)\n",
    "          else [set([x]) & set([y])])\n",
    "          for x, y in zip(l0, l1)\n",
    "\n",
    "    ]\n",
    "\n",
    "    #*x, = list(map(list, zip(*intersection)))\n",
    "    #%timeit list(map(list, zip(*intersection)))\n",
    "    '''\n",
    "#test_merge_vector(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blah -=\n",
    "# https://kawahara.ca/how-to-compute-truefalse-positives-and-truefalse-negatives-in-python-for-binary-classification-problems/\n",
    "def confusing(sys1, ann1):\n",
    "    TP = np.sum(np.logical_and(sys1 == 1, ann1 == sys1))\n",
    "\n",
    "    # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "    TN = np.sum(np.logical_and(sys1 == 0, ann1 == 0))\n",
    "\n",
    "    # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "    FP = np.sum(np.logical_and(sys1 == 1, ann1 == 0))\n",
    "\n",
    "    # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "    FN = np.sum(np.logical_and(np.logical_or(sys1 == 0, sys1 is None), ann1 == 1))\n",
    "    \n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "#%%cython\n",
    "#import numpy as np # access to Numpy from Python layer\n",
    "#import time\n",
    "#from __main__ import read_shelve\n",
    "def imerge(l0, l1):\n",
    "\n",
    "    return [\n",
    "        ( [\n",
    "            set(x) & set(y)] if isinstance(x, list) and  isinstance(y, list)\n",
    "            else [set(x) & set([y])] if isinstance(x, list) and  isinstance(y, np.int64)\n",
    "            else [set(x) & y] if isinstance(x, list) and isinstance(y, set)\n",
    "            else [x & y] if isinstance(x, set) and isinstance(y, set)\n",
    "            else [x & set(y)] if isinstance(x, set) and isinstance(y, list)\n",
    "            else [x & set([y])] if isinstance(x, set) and isinstance(y, np.int64)\n",
    "            else [set([x]) & set(y)] if isinstance(x, np.int64) and  isinstance(y, list)\n",
    "            else [set([x]) & y] if isinstance(x, np.int64) and isinstance(y, set)\n",
    "            else [set([x]) & set([y])])\n",
    "        for x, y in zip(l0, l1)\n",
    "    ]\n",
    "\n",
    "def umerge(l0, l1):\n",
    "\n",
    "    return [\n",
    "        ( [\n",
    "            set(x) | set(y)] if isinstance(x, list) and  isinstance(y, list)\n",
    "            else [set(x) | set([y])] if isinstance(x, list) and  isinstance(y, np.int64)\n",
    "            else [set(x) | y] if isinstance(x, list) and isinstance(y, set)\n",
    "            else [x | y] if isinstance(x, set) and isinstance(y, set)\n",
    "            else [x | set(y)] if isinstance(x, set) and isinstance(y, list)\n",
    "            else [x | set([y])] if isinstance(x, set) and isinstance(y, np.int64)\n",
    "            else [set([x]) | y] if isinstance(x, np.int64) and isinstance(y, set)\n",
    "            else [set([x]) | set(y)] if isinstance(x, np.int64) and  isinstance(y, list)\n",
    "            else [set([x]) | set([y])])\n",
    "        for x, y in zip(l0, l1)\n",
    "    ]\n",
    "\n",
    "def imerge_int(l0, l1):\n",
    "    return [\n",
    "        ( [\n",
    "            set(x) & set(y)] if isinstance(x, list) and  isinstance(y, list)\n",
    "            else [set(x) & set([y])] if isinstance(x, list) and isinstance(y, int)\n",
    "            else [set(x) & y] if isinstance(x, list) and isinstance(y, set)\n",
    "            else [x & y] if isinstance(x, set) and isinstance(y, set)\n",
    "            else [x & set(y)] if isinstance(x, set) and isinstance(y, list)\n",
    "            else [x & set([y])] if isinstance(x, set) and isinstance(y, int)\n",
    "            else [set([x]) & set(y)] if isinstance(x, int) and  isinstance(y, list)\n",
    "            else [set([x]) & y] if isinstance(x, int) and isinstance(y, set)\n",
    "            else [set([x]) & set([y])])\n",
    "        for x, y in zip(l0, l1)\n",
    "    ]\n",
    "\n",
    "def umerge_int(l0, l1):\n",
    "    return [\n",
    "        ( [\n",
    "            set(x) | set(y)] if isinstance(x, list) and  isinstance(y, list)\n",
    "            else [set(x) | set([y])] if isinstance(x, list) and  isinstance(y, int)\n",
    "            else [set(x) | y] if isinstance(x, list) and isinstance(y, set)\n",
    "            else [x | y] if isinstance(x, set) and isinstance(y, set)\n",
    "            else [x | set(y)] if isinstance(x, set) and isinstance(y, list)\n",
    "            else [x | set([y])] if isinstance(x, set) and isinstance(y, int)\n",
    "            else [set([x]) | y] if isinstance(x, int) and isinstance(y, set)\n",
    "            else [set([x]) | set(y)] if isinstance(x, int) and  isinstance(y, list)\n",
    "            else [set([x]) | set([y])])\n",
    "        for x, y in zip(l0, l1)\n",
    "    ]\n",
    "\n",
    "def test_merge_shelve():\n",
    "    ctakes = shelve.open('/Users/gms/Desktop/ctakes_' + corpus + '.dat')\n",
    "    clamp = shelve.open('/Users/gms/Desktop/clamp_' + corpus + '.dat')\n",
    "    mm = shelve.open('/Users/gms/Desktop/metamap_' + corpus + '.dat')\n",
    "\n",
    "    print(test['case'][0:2])\n",
    "\n",
    "    #t0 = np.array(test['oracle'][3][0:750])\n",
    "    #t1 = np.array(test['oracle'][5][0:750])\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    sys = []\n",
    "    oracles = []\n",
    "    confuzz = []\n",
    "    for i in range(len(ctakes['case'])):\n",
    "        t0 = np.array(ctakes['sys'][i])\n",
    "        t1 = np.array(clamp['sys'][i])\n",
    "        t2 = np.array(mm['sys'][i])\n",
    "        oracle = np.array(ctakes['oracle'][i])\n",
    "\n",
    "        #print(ctakes['case'][i])\n",
    "\n",
    "        l0 = list(t0)\n",
    "        l1 = list(t1)\n",
    "        l2 = list(t2)\n",
    "\n",
    "        z = *map(list, zip(*umerge(l0, l1))),\n",
    "\n",
    "        #%time  *map(list, zip(*umerge(l0, l1))),\n",
    "        #%time  *map(list, zip(*umerge(z[0], l1))),\n",
    "\n",
    "        t = *map(list, zip(*umerge(z[0], l2))),\n",
    "\n",
    "        from functools import reduce \n",
    "        import itertools\n",
    "        import operator\n",
    "\n",
    "        test = [list(i) for i in t[0]]\n",
    "\n",
    "        replaced = [[none] if len(wd) == 0  else wd for wd in t[0]]\n",
    "\n",
    "        replaced = [[1] if wd == {0, 1}  else wd for wd in replaced]\n",
    "\n",
    "\n",
    "        sys.append(replaced)\n",
    "\n",
    "        tp, tn, fp, fn = confusing(np.array(list(itertools.chain.from_iterable(replaced))), oracle)\n",
    "\n",
    "        confuzz.append((tp, tn, fp, fn))\n",
    "        f, recall, precision, tp, fp, fn, tp_fn_r, tm = Metrics(fp, fn, tp, len(oracle), tn).get_confusion_metrics() #no tn\n",
    "    print(' --- ')\n",
    "\n",
    "    print(len(list(itertools.chain.from_iterable(oracles))), len(list(itertools.chain.from_iterable(sys))))\n",
    "    print(list(map(sum, zip(*confuzz))))\n",
    "    z = list(map(sum, zip(*confuzz)))\n",
    "    # tp, tn, fp, fn -> (fp, fn, tp, len(oracle), tn)\n",
    "    f, recall, precision, tp, fp, fn, tp_fn_r, tm = Metrics(z[2], z[3], z[0], len(list(itertools.chain.from_iterable(sys))), z[1]).get_confusion_metrics() #no tn\n",
    "    print('test_io():', tp, tn, fp, fn, np.mean(f), np.mean(recall), np.mean(precision))\n",
    "    elapsed = (time.perf_counter() - start)\n",
    "    print('time 1:', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ft.lru_cache(maxsize=None)\n",
    "def get_metric_data(analysis_type: str, corpus: str):\n",
    "   \n",
    "    usys_file, ref_table = AnalysisConfig().corpus_config()\n",
    "    systems = AnalysisConfig().systems\n",
    "    \n",
    "    sys_ann = pd.read_csv(analysisConf.data_dir + usys_file, dtype={'note_id': str})\n",
    "    \n",
    "    sql = \"SELECT * FROM \" + ref_table  \n",
    "    \n",
    "    ref_ann = pd.read_sql(sql, con=engine)\n",
    "    sys_ann = sys_ann.drop_duplicates()\n",
    "    \n",
    "    return ref_ann, sys_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "def geometric_mean(metrics):\n",
    "    \"\"\"\n",
    "    1. Get rank average of F1, TP/FN, TM\n",
    "        http://www.datasciencemadesimple.com/rank-dataframe-python-pandas-min-max-dense-rank-group/\n",
    "        https://stackoverflow.com/questions/46686315/in-pandas-how-to-create-a-new-column-with-a-rank-according-to-the-mean-values-o?rq=1\n",
    "    2. Take geomean of 2.\n",
    "        https://stackoverflow.com/questions/42436577/geometric-mean-applied-on-row\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.DataFrame() \n",
    "\n",
    "    metrics['F1 rank']=metrics['F'].rank(ascending=0,method='average')\n",
    "    metrics['TP/FN rank']=metrics['TP/FN'].rank(ascending=0,method='average')\n",
    "    metrics['TM rank']=metrics['TM'].rank(ascending=0,method='average')\n",
    "    metrics['Gmean'] = gmean(metrics.iloc[:,-3:],axis=1)\n",
    "\n",
    "    return metrics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metrics(analysis_type: str, corpus: str, single_sys = None, filter_semtype = True):\n",
    "    start = time.time()\n",
    "\n",
    "    systems = AnalysisConfig().systems\n",
    "    metrics = pd.DataFrame()\n",
    "\n",
    "    ref_ann, sys_ann = get_metric_data(analysis_type, corpus)\n",
    "    \n",
    "    for sys in systems:\n",
    "        \n",
    "        if filter_semtype:\n",
    "            st = SemanticTypes().get_system_type(sys)\n",
    "            ref_ann = ref_ann[ref_ann['semtype'].isin(SemanticTypes().get_system_type('reference'))]\n",
    "            \n",
    "            print(st, SemanticTypes().get_system_type('reference'))\n",
    "        \n",
    "        types, _ = AnnotationSystems().get_system_type(sys) # system types for iterable\n",
    "        for t in types:\n",
    "            print(t)\n",
    "            system = pd.DataFrame()\n",
    "\n",
    "            if filter_semtype:\n",
    "                system_annotations = sys_ann[sys_ann['semtypes'].isin(st)].copy()\n",
    "                \n",
    "            else:\n",
    "                system_annotations = sys_ann.copy()\n",
    "\n",
    "            system = system_annotations[system_annotations['type'] == str(t)]\n",
    "\n",
    "            if sys == 'quick_umls':\n",
    "                system = system[system.score.astype(float) >= .8]\n",
    "\n",
    "            if sys == 'metamap':\n",
    "                system = system[system.score.abs().astype(int) >= 800]\n",
    "\n",
    "#                 if sys == 'biomedicus':\n",
    "#                     system = system[system.score.abs().astype(int) >= 0.6]\n",
    "\n",
    "            system = system.drop_duplicates()\n",
    "            system.name = sys\n",
    "\n",
    "            c = get_cooccurences(ref_ann, system, analysis_type, corpus, True, system.name) # get matches, FN, etc.\n",
    "\n",
    "            print(c.ref_n, c.ref_only, c.system_n, c.system_only, c.ref_system_match)\n",
    "\n",
    "        if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "            F, recall, precision, TP, FP, FN, TP_FN_R, TM = Metrics(c.system_only, c.ref_only, c.ref_system_match, c.system_n).get_confusion_metrics(corpus)\n",
    "\n",
    "            if corpus == 'casi':\n",
    "                if sys == 'biomedicus':\n",
    "                    t = 'biomedicus.v2.Acronym'\n",
    "\n",
    "                d = {'system': sys, \n",
    "                     'type': t, \n",
    "                     'F': F, \n",
    "                     'precision': precision, \n",
    "                     'recall': recall, \n",
    "                     'FN': FN, \n",
    "                     'TP/FN': TP_FN_R,\n",
    "                     'n_gold': c.ref_n, \n",
    "                     'n_sys': c.system_n, \n",
    "                     'TM': TM}\n",
    "            else:\n",
    "                d = {'system': sys, \n",
    "                     'type': t, \n",
    "                     'F': F[1], \n",
    "                     'precision': precision[1], \n",
    "                     'recall': recall[1], \n",
    "                     'TP': TP, \n",
    "                     'FN': FN, \n",
    "                     'FP': FP, \n",
    "                     'TP/FN': TP_FN_R,\n",
    "                     'n_gold': c.ref_n, \n",
    "                     'n_sys': c.system_n, \n",
    "                     'TM': TM}\n",
    "\n",
    "            data = pd.DataFrame(d,  index=[0])\n",
    "            metrics = pd.concat([metrics, data], ignore_index=True)\n",
    "            metrics.drop_duplicates(keep='last', inplace=True)\n",
    "        else:\n",
    "            print(\"NO EXACT MATCHES FOR\", t)\n",
    "        elapsed = (time.time() - start)\n",
    "        print(\"elapsed:\", sys, elapsed)\n",
    "     \n",
    "    elapsed = (time.time() - start)\n",
    "    print(geometric_mean(metrics))\n",
    "    \n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    \n",
    "    if single_sys is None:\n",
    "        file_name = 'metrics_'\n",
    "    \n",
    "    metrics.to_csv(analysisConf.data_dir + corpus + '_' + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "    \n",
    "    print(\"total elapsed time:\", elapsed) \n",
    "\n",
    "# use to iterate through mm scores\n",
    "def generate_metrics_mm(analysis_type: str, corpus: str, single_sys = None):\n",
    "    start = time.time()\n",
    "    #systems = [\"biomedicus\",\"ctakes\",\"metamap\",\"clamp\",\"quick_umls\"]\n",
    "    systems = AnalysisConfig().systems\n",
    "    #systems = [\"quick_umls\"]\n",
    "    metrics = pd.DataFrame()\n",
    "\n",
    "    ref_ann, sys_ann = get_metric_data(analysis_type, corpus)\n",
    "    \n",
    "    sys_ann = sys_ann[(sys_ann.score.notnull()) & (sys_ann['system'] == 'metamap')]\n",
    "    sys_ann = sys_ann[['begin', 'end', 'note_id', 'system', 'score']].drop_duplicates()\n",
    "    sys_ann.score = sys_ann.score.astype(int)\n",
    "    \n",
    "    for sys in systems:\n",
    "        types, __ = AnnotationSystems().get_system_type(sys) # system types for iterable\n",
    "        for t in types:\n",
    "            print(t)\n",
    "\n",
    "            for i in range(500, 1050, 50): \n",
    "\n",
    "                sys_ann = sys_ann[(sys_ann[\"score\"] >= i)].copy()\n",
    "\n",
    "                sys_ann.name = sys + str(i)\n",
    "\n",
    "                c = get_cooccurences(ref_ann, sys_ann, analysis_type, corpus, True, sys_ann.name) # get matches, FN, etc.\n",
    "\n",
    "                print(c.ref_n, c.ref_only, c.system_n, c.system_only, c.ref_system_match)\n",
    "\n",
    "                #print(i, len(system))\n",
    "\n",
    "                if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "                    F, recall, precision, TP, FP, FN, TP_FN_R, TM = Metrics(c.system_only, c.ref_only, c.ref_system_match, c.system_n).get_confusion_metrics()\n",
    "                    d = {'system': sys + '_score_' + str(i), \n",
    "                         'type': t, \n",
    "                         'F': F[1], \n",
    "                         'precision': precision[1], \n",
    "                         'recall': recall[1], \n",
    "                         'TP': TP, \n",
    "                         'FN': FN, \n",
    "                         'FP': FP, \n",
    "                         'TP/FN': TP_FN_R,\n",
    "                         'n_gold': c.ref_n, \n",
    "                         'n_sys': c.system_n, \n",
    "                         'TM': TM}\n",
    "\n",
    "                    data = pd.DataFrame(d,  index=[0])\n",
    "                    metrics = pd.concat([metrics, data], ignore_index=True)\n",
    "                    metrics.drop_duplicates(keep='last', inplace=True)\n",
    "                else:\n",
    "                    print(\"NO EXACT MATCHES FOR\", t)\n",
    "                elapsed = (time.time() - start)\n",
    "                print(\"elapsed:\", sys, elapsed)\n",
    "     \n",
    "    elapsed = (time.time() - start)\n",
    "    print(geometric_mean(metrics))\n",
    "    \n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    # UIMA or QuickUMLS\n",
    "    if single_sys is None:\n",
    "        file_name = 'mm_metrics_'\n",
    "    metrics.to_csv(analysisConf.data_dir + corpus + '_' + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "    \n",
    "    print(\"total elapsed time:\", elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in system matches from file\n",
    "def get_ref_n(analysis_type: str, corpus, filter_semtypes = True) -> int:\n",
    "    \n",
    "    ref_ann, _ = get_metric_data(analysis_type, corpus)\n",
    "    \n",
    "    if filter_semtypes:\n",
    "        ref_ann = ref_ann[ref_ann['semtype'].isin(SemanticTypes().get_system_type('reference'))]\n",
    "            \n",
    "    if corpus == 'casi':\n",
    "        return len(ref_ann)\n",
    "        \n",
    "    else:\n",
    "        # do not overestimate fn\n",
    "        if 'entity' in analysis_type:\n",
    "            ref_ann = ref_ann[['start', 'end', 'file']].drop_duplicates()\n",
    "        elif 'cui' in analysis_type:\n",
    "            ref_ann = ref_ann[['value', 'file']].drop_duplicates()\n",
    "        elif 'full' in analysis_type:\n",
    "            ref_ann = ref_ann[['start', 'end', 'value', 'file']].drop_duplicates()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        ref_n = len(ref_ann.drop_duplicates())\n",
    "\n",
    "        return ref_n\n",
    "\n",
    "def get_sys_data(system: str, analysis_type: str, corpus: str, filter_semtypes = True) -> pd.DataFrame:\n",
    "   \n",
    "    _, data = get_metric_data(analysis_type, corpus)\n",
    "    \n",
    "    out = data[data['system'] == system].copy()\n",
    "    \n",
    "    if filter_semtypes:\n",
    "        st = SemanticTypes().get_system_type(system)\n",
    "    \n",
    "    if corpus == 'casi':\n",
    "        cols_to_keep = ['case', 'overlap'] \n",
    "        #cols_to_keep = ['case', 'begin', 'end'] \n",
    "        out = out[cols_to_keep].drop_duplicates()\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    else:\n",
    "        if filter_semtypes:\n",
    "            out = data[data['semtypes'].isin(st)].copy()\n",
    "                \n",
    "        else:\n",
    "            out = data[data['system']== system].copy()\n",
    "\n",
    "        if system == 'quick_umls':\n",
    "            out = out[(out.score.astype(float) >= 0.8) & (out[\"type\"] == 'concept_jaccard_score_False')]\n",
    "        \n",
    "        if system == 'metamap':\n",
    "            out = out[out.score.abs().astype(int) >= 800]\n",
    "        \n",
    "#         if system == 'biomedicus':\n",
    "#             out = out[out.score.abs().astype(int) >= 0.6]\n",
    "            \n",
    "\n",
    "        if 'entity' in analysis_type:\n",
    "            cols_to_keep = ['begin', 'end', 'note_id']\n",
    "        elif 'cui' in analysis_type:\n",
    "            cols_to_keep = ['cui', 'note_id']\n",
    "        elif 'full' in analysis_type:\n",
    "            cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "\n",
    "        out = out[cols_to_keep]\n",
    "\n",
    "        return out.drop_duplicates()\n",
    "\n",
    "@ft.lru_cache(maxsize=None)\n",
    "def get_system_matches(system: str, analysis_type: str, corpus: str):\n",
    "   \n",
    "    if corpus == 'casi':\n",
    "        \n",
    "        sql = \"select `case`, overlap from test.amia_2019_cases where overlap = 1 and `system` = %(system)s\"  \n",
    "        data_matches = df_to_set(pd.read_sql(sql, params={\"system\":system}, con=engine), 'entity', 'sys', 'casi')\n",
    "        \n",
    "        sql = \"select `case`, overlap from test.amia_2019_cases where (overlap = 0 or overlap is null) and `system` = %(system)s\"  \n",
    "        data_fn = df_to_set(pd.read_sql(sql, params={\"system\":system}, con=engine), 'entity', 'sys', 'casi')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        dir_test = analysisConf.data_dir + 'single_system_out/'\n",
    "\n",
    "        file = dir_test + system + '_' + analysis_type + '_' + corpus + '_matches.txt'\n",
    "        data_matches = set(literal_eval(line.strip()) for line in open(file))\n",
    "\n",
    "        file = dir_test + system + '_' + analysis_type + '_' + corpus + '_ref_only.txt'\n",
    "        data_fn = set(literal_eval(line.strip()) for line in open(file)) \n",
    "\n",
    "    return data_matches, data_fn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GENERATE merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetTotals(object):\n",
    "    \"\"\" \n",
    "    returns an instance with merged match set numbers using either union or intersection of elements in set \n",
    "    \"\"\"\n",
    "    def __init__(self, ref_n, sys_n, match_set):\n",
    "\n",
    "        self = self    \n",
    "        self.ref_ann = ref_n\n",
    "        self.sys_n = sys_n\n",
    "        self.match_set = match_set\n",
    "\n",
    "    def get_ref_sys(self):\n",
    "\n",
    "        ref_only = self.ref_ann - len(self.match_set)\n",
    "        sys_only = self.sys_n - len(self.match_set)\n",
    "\n",
    "        return ref_only, sys_only, len(self.match_set), self.match_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: refactor generate_metrics()\n",
    "\n",
    "def merge_eval(ref_only: int, system_only: int, ref_system_match: int, system_n: int, ref_n: int) -> dict:\n",
    "    \"\"\"\n",
    "    Generate confusion matrix params\n",
    "    :params: ref_only, system_only, reference_system_match -> sets\n",
    "    matches, system_n, reference_n -> counts\n",
    "    :return: dictionary object\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if ref_only + ref_system_match != ref_n:\n",
    "        print('ERROR!')\n",
    "\n",
    "    # get evaluation metrics\n",
    "    d = {}\n",
    "    \n",
    "    F, recall, precision, TP, FP, FN, TP_FN_R, TM  = Metrics(system_only, ref_only, ref_system_match, system_n).get_confusion_metrics()\n",
    "\n",
    "    d = {\n",
    "         'F': F[1], \n",
    "         'precision': precision[1], \n",
    "         'recall': recall[1], \n",
    "         'TP': TP, \n",
    "         'FN': FN, \n",
    "         'FP': FP, \n",
    "         'TP/FN': TP_FN_R,\n",
    "         'n_gold': ref_n, \n",
    "         'n_sys': system_n, \n",
    "         'TM': TM\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if system_n - FP != TP:\n",
    "        print('inconsistent system n!')\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ft.lru_cache(maxsize=None)\n",
    "def process_sentence(pt, sentence, analysis_type, corpus):\n",
    "    \"\"\"\n",
    "    Recursively evaluate parse tree, \n",
    "    with check for existence before build\n",
    "       :param sentence: to process\n",
    "       :return class of merged annotations, boolean operated system df \n",
    "    \"\"\"\n",
    "    \n",
    "    class Results(object):\n",
    "        def __init__(self):\n",
    "            self.results = set()\n",
    "            #self.operations = []\n",
    "            self.system_merges = pd.DataFrame()\n",
    "            \n",
    "    r = Results()\n",
    "    \n",
    "    if 'entity' in analysis_type and corpus != 'casi': \n",
    "        cols_to_keep = ['begin', 'end', 'note_id'] # entity only\n",
    "    elif 'full' in analysis_type: \n",
    "        cols_to_keep = ['cui', 'begin', 'end', 'note_id'] # entity only\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['cui', 'note_id'] # entity only\n",
    "    elif corpus == 'casi':\n",
    "        cols_to_keep = ['case', 'overlap']\n",
    "    \n",
    "    def evaluate(parseTree):\n",
    "        oper = {'&': op.and_, '|': op.or_}\n",
    "        \n",
    "        if parseTree:\n",
    "            leftC = gevent.spawn(evaluate, parseTree.getLeftChild())\n",
    "            rightC = gevent.spawn(evaluate, parseTree.getRightChild())\n",
    "            \n",
    "            if leftC.get() and rightC.get():\n",
    "                query = set()\n",
    "                system_query = pd.DataFrame()\n",
    "                fn = oper[parseTree.getRootVal()]\n",
    "                \n",
    "                if isinstance(leftC.get(), str):\n",
    "                    # get system as leaf node \n",
    "                    left, _ = get_system_matches(leftC.get(), analysis_type, corpus)\n",
    "                    left_sys = get_sys_data(leftC.get(), analysis_type, corpus)\n",
    "                \n",
    "                elif isinstance(leftC.get(), tuple):\n",
    "                    left = leftC.get()[0]\n",
    "                    l_sys = leftC.get()[1]\n",
    "                \n",
    "                if isinstance(rightC.get(), str):\n",
    "                    # get system as leaf node\n",
    "                    right, _ = get_system_matches(rightC.get(), analysis_type, corpus)\n",
    "                    right_sys = get_sys_data(rightC.get(), analysis_type, corpus)\n",
    "                    \n",
    "                elif isinstance(rightC.get(), tuple):\n",
    "                    right = rightC.get()[0]\n",
    "                    r_sys = rightC.get()[1]\n",
    "                    \n",
    "                # create match set based on boolean operation\n",
    "                match_set = fn(left, right)\n",
    "               \n",
    "                if corpus != 'casi':\n",
    "                    if fn == op.or_:\n",
    "                        r.results = r.results.union(match_set)\n",
    "\n",
    "                        if isinstance(leftC.get(), str) and isinstance(rightC.get(), str):\n",
    "                            frames = [left_sys, right_sys]\n",
    "                            df = pd.concat(frames,  ignore_index=True)\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                        elif isinstance(leftC.get(), str) and isinstance(rightC.get(), tuple):\n",
    "                            frames = [left_sys, r_sys]\n",
    "                            df = pd.concat(frames,  ignore_index=True)\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                        elif isinstance(leftC.get(), tuple) and isinstance(rightC.get(), str):\n",
    "                            frames = [l_sys, right_sys]\n",
    "                            df = pd.concat(frames,  ignore_index=True)\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                        elif isinstance(leftC.get(), tuple) and isinstance(rightC.get(), tuple):\n",
    "                            frames = [l_sys, r_sys]\n",
    "                            df = pd.concat(frames,  ignore_index=True)\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                    if fn == op.and_:\n",
    "                        if len(r.results) == 0:\n",
    "                            r.results = match_set\n",
    "                        r.results = r.results.intersection(match_set)\n",
    "\n",
    "                        if isinstance(leftC.get(), str) and isinstance(rightC.get(), str):\n",
    "                            df = left_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                        elif isinstance(leftC.get(), str) and isinstance(rightC.get(), tuple):\n",
    "                            df = left_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                        elif isinstance(leftC.get(), tuple) and isinstance(rightC.get(), str):\n",
    "                            df = l_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "\n",
    "                        elif isinstance(leftC.get(), tuple) and isinstance(rightC.get(), tuple):\n",
    "                            df = l_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df[cols_to_keep].drop_duplicates(subset=cols_to_keep)\n",
    "                else:\n",
    "                    if fn == op.or_:\n",
    "                        r.results = r.results.union(match_set)\n",
    "\n",
    "                        if isinstance(leftC, str) and isinstance(rightC, str):\n",
    "                            df = left_sys.append(right_sys)\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, str) and isinstance(rightC, tuple):\n",
    "                            df = left_sys.append(r_sys)\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, tuple) and isinstance(rightC, str):\n",
    "                            df = right_sys.append(l_sys)\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, tuple) and isinstance(rightC, tuple):\n",
    "                            df = l_sys.append(r_sys)\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                    if fn == op.and_:\n",
    "                        if len(r.results) == 0:\n",
    "                            r.results = match_set\n",
    "                        r.results = r.results.intersection(match_set)\n",
    "\n",
    "                        if isinstance(leftC, str) and isinstance(rightC, str):\n",
    "                            df = left_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, str) and isinstance(rightC, tuple):\n",
    "                            df = left_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, tuple) and isinstance(rightC, str):\n",
    "                            df = l_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df.drop_duplicates()\n",
    "\n",
    "                        elif isinstance(leftC, tuple) and isinstance(rightC, tuple):\n",
    "                            df = l_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                            df = df.drop_duplicates()\n",
    "                \n",
    "                # get matched results\n",
    "                query.update(r.results)\n",
    "                \n",
    "                # get combined system results\n",
    "                r.system_merges = df\n",
    "                \n",
    "                if len(df) > 0:\n",
    "                    system_query = system_query.append(df)\n",
    "                else:\n",
    "                    print('wtf!')\n",
    "                    \n",
    "                return query, system_query\n",
    "            else:\n",
    "                return parseTree.getRootVal()\n",
    "    \n",
    "    if sentence.n_or > 0 or sentence.n_and > 0:\n",
    "        evaluate(pt)  \n",
    "    \n",
    "    # trivial case\n",
    "    elif sentence.n_or == 0 and sentence.n_and == 0:\n",
    "        r.results, _ = get_system_matches(sentence.sentence, analysis_type, corpus)\n",
    "        r.system_merges = get_sys_data(sentence.sentence, analysis_type, corpus)\n",
    "        #print('trivial:', sentence.sentence, len(r.results), len(r.system_merges))\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Incoming Boolean sentences are parsed into a binary tree.\n",
    "\n",
    "Test expressions to parse:\n",
    "\n",
    "sentence = '((((A&B)|C)|D)&E)'\n",
    "\n",
    "sentence = '(E&(D|(C|(A&B))))'\n",
    "\n",
    "sentence = '(((A|(B&C))|(D&(E&F)))|(H&I))'\n",
    "\n",
    "\"\"\"\n",
    "# build parse tree from passed sentence\n",
    "# using grammatical rules of Boolean logic\n",
    "def buildParseTree(fpexp):\n",
    "    \"\"\"\n",
    "       Iteratively build parse tree from passed sentence using grammatical rules of Boolean logic\n",
    "       :param fpexp: sentence to parse\n",
    "       :return eTree: parse tree representation\n",
    "       Incoming Boolean sentences are parsed into a binary tree.\n",
    "       Test expressions to parse:\n",
    "       sentence = '(A&B)'\n",
    "       sentence = '(A|B)'\n",
    "       sentence = '((A|B)&C)'\n",
    "       \n",
    "    \"\"\"\n",
    "    fplist = fpexp.split()\n",
    "    pStack = Stack()\n",
    "    eTree = BinaryTree('')\n",
    "    pStack.push(eTree)\n",
    "    currentTree = eTree\n",
    "\n",
    "    for i in fplist:\n",
    "\n",
    "        if i == '(':\n",
    "            currentTree.insertLeft('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getLeftChild()\n",
    "        elif i not in ['&', '|', ')']:\n",
    "            currentTree.setRootVal(i)\n",
    "            parent = pStack.pop()\n",
    "            currentTree = parent\n",
    "        elif i in ['&', '|']:\n",
    "            currentTree.setRootVal(i)\n",
    "            currentTree.insertRight('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getRightChild()\n",
    "        elif i == ')':\n",
    "            currentTree = pStack.pop()\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    return eTree\n",
    "\n",
    "def make_parse_tree(payload):\n",
    "    \"\"\"\n",
    "    Ensure data to create tree are in standard form\n",
    "    :param sentence: sentence to preprocess\n",
    "    :return pt, parse tree graph\n",
    "            sentence, processed sentence to build tree\n",
    "            a: order\n",
    "    \"\"\"\n",
    "    def preprocess_sentence(sentence):\n",
    "        # prepare statement for case when a boolean AND/OR is given\n",
    "        sentence = payload.replace('(', ' ( '). \\\n",
    "            replace(')', ' ) '). \\\n",
    "            replace('&', ' & '). \\\n",
    "            replace('|', ' | '). \\\n",
    "            replace('  ', ' ')\n",
    "        return sentence\n",
    "\n",
    "    sentence = preprocess_sentence(payload)\n",
    "    print(sentence)\n",
    "    \n",
    "    pt = buildParseTree(sentence)\n",
    "    #pt.postorder() \n",
    "    \n",
    "    return pt\n",
    "\n",
    "class Sentence(object):\n",
    "\n",
    "    def __init__(self, sentence):\n",
    "        self = self\n",
    "        self.n_and = sentence.count('&')\n",
    "        self.n_or = sentence.count('|')\n",
    "        self.sentence = sentence\n",
    "    \n",
    "def get_metrics(boolean_expression: str, analysis_type: str, corpus: str):\n",
    "    \"\"\"\n",
    "    Traverse binary parse tree representation of Boolean sentence\n",
    "        :params: boolean expression in form of '(<annotator_engine_name1><boolean operator><annotator_engine_name2>)'\n",
    "                 analysis_type (string value of: 'entity', 'cui', 'full') used to filter set of reference and system annotations \n",
    "        :return: dictionary with values needed for confusion matrix\n",
    "    \"\"\"\n",
    "    sentence = Sentence(boolean_expression)   \n",
    "\n",
    "    pt = make_parse_tree(sentence.sentence)\n",
    "\n",
    "    r = process_sentence(pt, sentence, analysis_type, corpus)\n",
    "    \n",
    "    #print('len sys merges:', len(r.system_merges))\n",
    "    system_n = len(r.system_merges)\n",
    "    reference_n = get_ref_n(analysis_type, corpus)\n",
    "\n",
    "    reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, r.results).get_ref_sys()\n",
    "\n",
    "    #print (r.system_merges)\n",
    "    \n",
    "    # get overall TP/TF and various other counts for running confusion matrix metric analysis\n",
    "    return merge_eval(reference_only, system_only, reference_system_match, system_n, reference_n)\n",
    "\n",
    "def get_merge_data(boolean_expression: str, analysis_type: str, corpus: str):\n",
    "    \"\"\"\n",
    "    Traverse binary parse tree representation of Boolean sentence\n",
    "        :params: boolean expression in form of '(<annotator_engine_name1><boolean operator><annotator_engine_name2>)'\n",
    "                 analysis_type (string value of: 'entity', 'cui', 'full') used to filter set of reference and system annotations \n",
    "        :return: dictionary with values needed for confusion matrix\n",
    "    \"\"\"\n",
    "    sentence = Sentence(boolean_expression)   \n",
    "\n",
    "    pt = make_parse_tree(sentence.sentence)\n",
    "\n",
    "    r = process_sentence(pt, sentence, analysis_type, corpus)\n",
    "    \n",
    "    #print('len sys merges:', len(r.system_merges))\n",
    "    system_n = len(r.system_merges)\n",
    "    reference_n = get_ref_n(analysis_type, corpus)\n",
    "\n",
    "    reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, r.results).get_ref_sys()\n",
    "\n",
    "    print(merge_eval(reference_only, system_only, reference_system_match, system_n, reference_n))\n",
    "    # get matched data from merge\n",
    "    return r.system_merges # merge_eval(reference_only, system_only, reference_system_match, system_n, reference_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ( ctakes & clamp ) \n",
      "{'F': 0.23781377139786597, 'precision': 0.48157935644333905, 'recall': 0.15789205443147647, 'TP': 3098, 'FN': 16523, 'FP': 3335, 'TP/FN': 0.18749621739393574, 'n_gold': 19621, 'n_sys': 6433, 'TM': 38.62554654967958}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# control filter_semtypes in get_sys_data, get_fef_n and generate_metrics. TODO consolidate. \n",
    "\n",
    "# run single statement\n",
    "statement = '(ctakes&clamp)'\n",
    "analysis_type = 'entity'\n",
    "corpus = 'fairview'\n",
    "matches = get_merge_data(statement, analysis_type, corpus)\n",
    "#print(matches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "70\n",
      "['diabetes']\n",
      "['diabetes']\n",
      "['diabetes']\n",
      "['diabetes', 'type', 'ii', ':']\n",
      "['diabetes']\n",
      "['diabetes']\n",
      "['diabetes']\n",
      "['diabetes', 'type', 'ii', ':']\n",
      "['diabetes']\n",
      "['diabetes']\n",
      "['diabetes']\n",
      "['diabetes']\n",
      "['diabetes', '\\r\\n\\r']\n",
      "['diabetes']\n",
      "['diabetes']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "sql = \"select distinct note_id, sofa from concepts.sofas where corpus = 'fairview'\"\n",
    "\n",
    "docs = pd.read_sql(sql, con=engine)\n",
    "\n",
    "d = {}\n",
    "\n",
    "for row in docs.itertuples():\n",
    "    d[row.note_id] = row.sofa\n",
    "    \n",
    "print(len(d))\n",
    "\n",
    "test = matches[matches['note_id'] == '0000200926']\n",
    "print(len(test))\n",
    "\n",
    "doc = nlp(d['0000200926'])\n",
    "\n",
    "for row in test.itertuples():\n",
    "    my_str = [token.text.strip('\\n').lower() for token in doc if token.idx >= (row.begin) and token.idx <= (row.end)]\n",
    "    if 'diabetes' in my_str:\n",
    "        print(my_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all combinations of given list of annotators:\n",
    "def expressions(l, n):\n",
    "    for (operations, *operands), operators in product(\n",
    "            combinations(l, n), product(('&', '|'), repeat=n - 1)):\n",
    "        for operation in zip(operators, operands):\n",
    "            operations = [operations, *operation]\n",
    "        yield operations\n",
    "        \n",
    "def run_ensemble(l, analysis_type, corpus):\n",
    "\n",
    "    metrics = pd.DataFrame()\n",
    "\n",
    "    print('CORPUS---->', corpus)\n",
    "    \n",
    "    #for i in l:\n",
    "    #    d = get_metrics(i, analysis_type, corpus)\n",
    "    \n",
    "    #for s in list(permutations(l)):\n",
    "    for i in range(1, len(l)+1):\n",
    "        test = list(expressions(l, i))\n",
    "        for t in test:\n",
    "            if i > 1:\n",
    "                # format Boolean sentence for parse tree \n",
    "                t = '(' + \" \".join(str(x) for x in t).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "\n",
    "            d = get_metrics(t, analysis_type, corpus)\n",
    "            d['merge'] = t\n",
    "            frames = [metrics, pd.DataFrame(d, index=[0]) ]\n",
    "            metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "    \n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    \n",
    "    file_name = corpus + '_all_merge_metrics_'\n",
    "        \n",
    "    geometric_mean(metrics).to_csv(analysisConf.data_dir + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "    print(geometric_mean(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTS -> ensemble:\n",
    "def test_match_consistency(matches, ref_only, ref_n, sys):\n",
    "    \"\"\"test for reference only/match set consistency:\n",
    "        params: match, system and reference only sets\"\"\"\n",
    "   \n",
    "    print('len', len(sys), len(matches), len(matches.union(sys)), len(matches.intersection(sys)))\n",
    "    assert len(matches.union(ref_only)) == ref_n, 'Reference annotation mismatch union'\n",
    "    assert len(matches.intersection(sys)) == len(matches), 'System annotation mismatch intersect'\n",
    "    assert len(matches.union(sys)) == len(sys), 'System annotation mismatch union'\n",
    "    assert len(matches.intersection(ref_only)) == 0, 'Reference annotation mismatch intersect'\n",
    "\n",
    "def test_systems(analysis_type, systems, corpus):\n",
    "    sys = df_to_set(get_sys_data(systems[0], analysis_type, corpus), analysis_type)\n",
    "    test_match_consistency(*get_system_matches(systems[0], analysis_type, corpus), get_ref_n(analysis_type), sys)\n",
    "    print('Match consistency:', len(sys),get_ref_n(analysis_type))\n",
    "\n",
    "def test_metrics(ref, sys_m, match_m):\n",
    "    test = True\n",
    "    reference_n = len(ref)\n",
    "    system_n = len(sys_m)\n",
    "\n",
    "    print('Test metrics:', type(reference_n), type(system_n), type(match_m))\n",
    "\n",
    "    reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, match_m).get_ref_sys()\n",
    "    F, recall, precision, _, _, _, _, _ = Metrics(system_only, reference_only, reference_system_match, system_n).get_confusion_metrics()\n",
    "    F_, recall_, precision_, _, _, _, _, _ = Metrics(system_only, reference_only, reference_system_match, system_n).get_confusion_metrics(test)\n",
    "\n",
    "    assert F[1] == F_, 'F1 issue'\n",
    "    assert recall[1] == recall_, 'recall issue'\n",
    "    assert precision[1] == precision_, 'precision issue'\n",
    "    print(F[1], F_)\n",
    "    print(recall[1], recall_)\n",
    "    print(precision[1], precision_)\n",
    "\n",
    "def test_count(analysis_type, corpus):\n",
    "    # test match counts:\n",
    "    ctakes, _ = get_system_matches('ctakes', analysis_type, corpus)\n",
    "    clamp, _ = get_system_matches('clamp', analysis_type, corpus)\n",
    "    b9, _ = get_system_matches('biomedicus', analysis_type, corpus)\n",
    "    mm, _ = get_system_matches('metamap', analysis_type, corpus)\n",
    "\n",
    "    print('count:', len(mm.intersection(b9.intersection(clamp.intersection(ctakes)))))\n",
    "    \n",
    "def test_ensemble(analysis_type, corpus):\n",
    "    \n",
    "    print('ensemble:')\n",
    "    # Get mixed system_n\n",
    "    ref_ann, data = get_metric_data(analysis_type, corpus)\n",
    "\n",
    "    names = ['ctakes', 'biomedicus', 'metamap', 'clamp']\n",
    "    if 'entity' in analysis_type: \n",
    "        cols_to_keep = ['begin', 'end', 'note_id']\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['cui', 'note_id']\n",
    "    elif 'full' in analysis_type:\n",
    "        cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "\n",
    "    biomedicus = data[data[\"system\"]=='biomedicus'][cols_to_keep].copy()\n",
    "    ctakes = data[data[\"system\"]=='ctakes'][cols_to_keep].copy()\n",
    "    clamp = data[data[\"system\"]=='clamp'][cols_to_keep].copy()\n",
    "    metamap = data[data[\"system\"]=='metamap'][cols_to_keep].copy()\n",
    "    quickumls = data[data[\"system\"]=='quick_umls'][cols_to_keep].copy()\n",
    "\n",
    "    print('systems:', len(biomedicus), len(clamp), len(ctakes), len(metamap), len(quickumls))\n",
    "\n",
    "    b9 = set()\n",
    "    cl = set()\n",
    "    ct = set()\n",
    "    mm = set()\n",
    "    qu = set()\n",
    "\n",
    "    b9 = df_to_set(get_sys_data('biomedicus', analysis_type, corpus), analysis_type)\n",
    "    print(len(b9))\n",
    "\n",
    "    ct = df_to_set(get_sys_data('ctakes', analysis_type, corpus), analysis_type)\n",
    "    print(len(ct))\n",
    "\n",
    "    cl = df_to_set(get_sys_data('clamp', analysis_type, corpus), analysis_type)\n",
    "    print(len(cl))\n",
    "\n",
    "    mm = df_to_set(get_sys_data('metamap', analysis_type, corpus), analysis_type)\n",
    "    print(len(mm))\n",
    "\n",
    "    qu = df_to_set(get_sys_data('quick_umls', analysis_type, corpus), analysis_type)\n",
    "    print(len(qu))\n",
    "    \n",
    "    print('various merges:')\n",
    "    print(len(b9), len(cl), len(ct), len(mm), len(qu))\n",
    "    print(len(mm.intersection(b9.intersection(cl.intersection(ct)))))\n",
    "    print(len(mm.union(b9.intersection(cl.intersection(ct)))))\n",
    "    print(len(mm.union(b9.union(cl.intersection(ct)))))\n",
    "    print(len(mm.union(b9.union(cl.union(ct)))))\n",
    "    print(len(b9.intersection(ct)))\n",
    "\n",
    "    sys_m = b9.intersection(ct.intersection(qu))\n",
    "    print('sys_m:', len(sys_m))\n",
    "\n",
    "    # Get match merges:\n",
    "    ct, _ = get_system_matches('ctakes', analysis_type, corpus)\n",
    "    cl, _ = get_system_matches('clamp', analysis_type, corpus)\n",
    "    b9, _ = get_system_matches('biomedicus', analysis_type, corpus)\n",
    "    mm, _ = get_system_matches('metamap', analysis_type, corpus)\n",
    "    qu, _ = get_system_matches('quick_umls', analysis_type, corpus)\n",
    "\n",
    "    match_m = b9.intersection(ct.intersection(qu))\n",
    "    print('match_m:', len(match_m))\n",
    "    # reference df to set\n",
    "    if 'entity' in analysis_type: \n",
    "        cols_to_keep = ['end', 'start','file']\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['value','file']\n",
    "    elif 'full' in analysis_type:\n",
    "        cols_to_keep = ['end', 'start', 'value','file']\n",
    "\n",
    "    ref = df_to_set(ref_ann[cols_to_keep], analysis_type, 'ref')\n",
    "\n",
    "    print('ref:', len(ref))\n",
    "\n",
    "    # test difference:\n",
    "    print('FP:', len(sys_m - match_m), len(sys_m - ref))\n",
    "    assert len(sys_m - match_m) == len(sys_m - ref), 'FP mismatch'\n",
    "    print('FN:', len(ref - match_m), len(ref - sys_m))\n",
    "    assert len(ref - match_m) == len(ref - sys_m), 'FN mismatch'\n",
    "    \n",
    "    test_metrics(ref, sys_m, match_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clamp', 'ctakes'] ('analytical_fairview.csv', 'concepts.fairview_all')\n",
      "{'problem'} {'Finding'}\n",
      "edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA\n",
      "entity\n",
      "19621 11077 24188 15644 8544\n",
      "elapsed: clamp 0.13944387435913086\n",
      "{'DiseaseDisorderMention'} {'Finding'}\n",
      "ctakes_mentions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:47: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity\n",
      "19621 14385 23537 18301 5236\n",
      "elapsed: ctakes 0.2764739990234375\n",
      "   system                                              type         F  \\\n",
      "0   clamp  edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA  0.390057   \n",
      "1  ctakes                                   ctakes_mentions  0.242643   \n",
      "\n",
      "   precision    recall    TP     FN     FP     TP/FN  n_gold  n_sys  \\\n",
      "0   0.353233  0.435452  8544  11077  15644  0.771328   19621  24188   \n",
      "1   0.222458  0.266857  5236  14385  18301  0.363990   19621  23537   \n",
      "\n",
      "          TM  F1 rank  TP/FN rank  TM rank  Gmean  \n",
      "0  54.936534      1.0         1.0      1.0    1.0  \n",
      "1  34.129041      2.0         2.0      2.0    2.0  \n",
      "total elapsed time: 0.2769651412963867\n",
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         160530 function calls (159032 primitive calls) in 0.293 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "        2    0.041    0.020    0.049    0.024 <ipython-input-8-9b8b6b3d138d>:1(write_out)\n",
       "        4    0.039    0.010    0.039    0.010 {pandas._libs.hashtable.ismember_object}\n",
       "       24    0.027    0.001    0.027    0.001 {method 'factorize' of 'pandas._libs.hashtable.Int64Factorizer' objects}\n",
       "        4    0.014    0.003    0.015    0.004 <ipython-input-9-f806c6488fbf>:1(df_to_set)\n",
       "       18    0.008    0.000    0.008    0.000 {method 'factorize' of 'pandas._libs.hashtable.StringHashTable' objects}\n",
       "       24    0.008    0.000    0.008    0.000 {method 'factorize' of 'pandas._libs.hashtable.Int64HashTable' objects}\n",
       "        8    0.007    0.001    0.007    0.001 {method 'factorize' of 'pandas._libs.hashtable.Factorizer' objects}\n",
       "       36    0.007    0.000    0.007    0.000 {method 'copy' of 'numpy.ndarray' objects}\n",
       "    39242    0.007    0.000    0.007    0.000 {method 'write' of '_io.TextIOWrapper' objects}\n",
       "        1    0.007    0.007    0.290    0.290 <ipython-input-16-d071cedb9345>:1(generate_metrics)\n",
       "        2    0.005    0.002    0.162    0.081 {_cython_magic_9ad4c7c9c59354d0db34df36c127cfd7.get_cooccurences}\n",
       "    19693    0.004    0.000    0.008    0.000 {built-in method builtins.isinstance}\n",
       "      123    0.004    0.000    0.005    0.000 {pandas._libs.lib.infer_dtype}\n",
       "       15    0.004    0.000    0.004    0.000 {pandas._libs.algos.take_2d_axis1_object_object}\n",
       "        8    0.004    0.000    0.054    0.007 frame.py:4605(drop_duplicates)\n",
       "        1    0.003    0.003    0.293    0.293 <ipython-input-26-76ba0dfc63f4>:8(main)\n",
       "        2    0.003    0.001    0.003    0.001 {pandas._libs.ops.scalar_compare}\n",
       "        2    0.002    0.001    0.003    0.001 {pandas._libs.join.left_outer_join}\n",
       "       17    0.002    0.000    0.005    0.000 blocks.py:3131(_merge_blocks)\n",
       "      326    0.002    0.000    0.002    0.000 {built-in method numpy.empty}\n",
       "       48    0.002    0.000    0.002    0.000 {built-in method numpy.concatenate}\n",
       "       25    0.002    0.000    0.002    0.000 {pandas._libs.algos.take_2d_axis1_int64_int64}\n",
       "     7940    0.002    0.000    0.003    0.000 generic.py:7(_check)\n",
       "     1310    0.002    0.000    0.006    0.000 dtypes.py:68(find)\n",
       "       21    0.002    0.000    0.002    0.000 {pandas._libs.algos.take_2d_axis0_object_object}\n",
       "       62    0.002    0.000    0.002    0.000 socket.py:337(send)\n",
       "        8    0.002    0.000    0.002    0.000 {pandas._libs.hashtable.duplicated_int64}\n",
       "     1952    0.002    0.000    0.002    0.000 {method 'format' of 'str' objects}\n",
       "     1832    0.002    0.000    0.004    0.000 common.py:1845(_is_dtype_type)\n",
       "12250/12246    0.001    0.000    0.002    0.000 {built-in method builtins.getattr}\n",
       "      382    0.001    0.000    0.001    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
       "     1961    0.001    0.000    0.005    0.000 base.py:75(is_dtype)\n",
       "      133    0.001    0.000    0.019    0.000 algorithms.py:1544(take_nd)\n",
       "  868/833    0.001    0.000    0.002    0.000 {built-in method numpy.array}\n",
       "     2869    0.001    0.000    0.001    0.000 {built-in method builtins.hasattr}\n",
       "       24    0.001    0.000    0.001    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
       "        5    0.001    0.000    0.001    0.000 {built-in method io.open}\n",
       "        2    0.001    0.001    0.002    0.001 {pandas._libs.join.inner_join}\n",
       "    95/81    0.001    0.000    0.008    0.000 base.py:253(__new__)\n",
       "      381    0.001    0.000    0.002    0.000 generic.py:5069(__setattr__)\n",
       "      659    0.001    0.000    0.003    0.000 _dtype.py:319(_name_get)\n",
       "       32    0.001    0.000    0.001    0.000 {method 'take' of 'numpy.ndarray' objects}\n",
       "        4    0.001    0.000    0.001    0.000 merge.py:1701(_get_join_keys)\n",
       "        8    0.001    0.000    0.002    0.000 sorting.py:20(get_group_index)\n",
       "5354/4429    0.001    0.000    0.001    0.000 {built-in method builtins.len}\n",
       "        4    0.001    0.000    0.040    0.010 algorithms.py:407(<lambda>)\n",
       "     6801    0.001    0.000    0.001    0.000 {built-in method builtins.issubclass}\n",
       "     1249    0.001    0.000    0.005    0.000 common.py:1702(is_extension_array_dtype)\n",
       "        9    0.001    0.000    0.001    0.000 {pandas._libs.algos.take_2d_axis0_float64_float64}\n",
       "       58    0.001    0.000    0.002    0.000 managers.py:186(_rebuild_blknos_and_blklocs)\n",
       "     1096    0.001    0.000    0.002    0.000 common.py:1981(pandas_dtype)\n",
       "   199/12    0.001    0.000    0.001    0.000 {built-in method _abc._abc_subclasscheck}\n",
       "       24    0.001    0.000    0.022    0.001 managers.py:1329(take)\n",
       "      167    0.001    0.000    0.001    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
       "     1318    0.001    0.000    0.001    0.000 numerictypes.py:293(issubclass_)\n",
       "       24    0.001    0.000    0.001    0.000 indexing.py:2575(maybe_convert_indices)\n",
       "      133    0.001    0.000    0.002    0.000 algorithms.py:1417(_get_take_nd_function)\n",
       "       12    0.001    0.000    0.001    0.000 {method 'factorize' of 'pandas._libs.hashtable.Float64HashTable' objects}\n",
       "       16    0.001    0.000    0.035    0.002 merge.py:1617(_factorize_keys)\n",
       "      659    0.001    0.000    0.002    0.000 numerictypes.py:365(issubdtype)\n",
       "      264    0.001    0.000    0.001    0.000 blocks.py:78(__init__)\n",
       "     1360    0.001    0.000    0.001    0.000 integer.py:80(construct_from_string)\n",
       "      109    0.001    0.000    0.002    0.000 cast.py:255(maybe_promote)\n",
       "  194/182    0.001    0.000    0.001    0.000 generic.py:5053(__getattr__)\n",
       "      132    0.000    0.000    0.004    0.000 series.py:152(__init__)\n",
       "      343    0.000    0.000    0.001    0.000 common.py:160(is_sparse)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'tolist' of 'numpy.ndarray' objects}\n",
       "      971    0.000    0.000    0.001    0.000 <frozen importlib._bootstrap>:1009(_handle_fromlist)\n",
       "       22    0.000    0.000    0.000    0.000 {built-in method posix.stat}\n",
       "      335    0.000    0.000    0.001    0.000 dtypes.py:973(is_dtype)\n",
       "      104    0.000    0.000    0.002    0.000 managers.py:963(iget)\n",
       "       71    0.000    0.000    0.037    0.001 frame.py:2893(__getitem__)\n",
       "       54    0.000    0.000    0.001    0.000 sorting.py:55(maybe_lift)\n",
       "      644    0.000    0.000    0.002    0.000 common.py:1809(_get_dtype)\n",
       "        3    0.000    0.000    0.000    0.000 {pandas._libs.algos.rank_1d_float64}\n",
       "      374    0.000    0.000    0.004    0.000 common.py:1578(is_bool_dtype)\n",
       "      264    0.000    0.000    0.004    0.000 blocks.py:3080(make_block)\n",
       "       54    0.000    0.000    0.030    0.001 algorithms.py:559(factorize)\n",
       "       99    0.000    0.000    0.002    0.000 blocks.py:3034(get_block_type)\n",
       "       15    0.000    0.000    0.000    0.000 {pandas._libs.algos.take_2d_axis0_int64_int64}\n",
       "       15    0.000    0.000    0.000    0.000 {built-in method pandas._libs.missing.isnaobj}\n",
       "      383    0.000    0.000    0.001    0.000 dtypes.py:827(is_dtype)\n",
       "      880    0.000    0.000    0.003    0.000 common.py:434(is_datetime64tz_dtype)\n",
       "      184    0.000    0.000    0.001    0.000 dtypes.py:786(construct_from_string)\n",
       "      455    0.000    0.000    0.001    0.000 common.py:131(is_object_dtype)\n",
       "        6    0.000    0.000    0.011    0.002 managers.py:2029(concatenate_block_managers)\n",
       "      120    0.000    0.000    0.001    0.000 base.py:504(_simple_new)\n",
       "       26    0.000    0.000    0.002    0.000 concat.py:261(get_empty_dtype_and_na)\n",
       "        2    0.000    0.000    0.000    0.000 {pandas._libs.algos.take_2d_axis0_int64_float64}\n",
       "      312    0.000    0.000    0.001    0.000 dtypes.py:672(construct_from_string)\n",
       "      212    0.000    0.000    0.001    0.000 managers.py:139(shape)\n",
       "        4    0.000    0.000    0.041    0.010 merge.py:730(_get_join_indexers)\n",
       "       32    0.000    0.000    0.000    0.000 {pandas._libs.algos.take_1d_int64_int64}\n",
       "      192    0.000    0.000    0.000    0.000 {method 'get_loc' of 'pandas._libs.index.IndexEngine' objects}\n",
       "      132    0.000    0.000    0.001    0.000 managers.py:1443(__init__)\n",
       "     1244    0.000    0.000    0.000    0.000 common.py:119(<lambda>)\n",
       "       69    0.000    0.000    0.014    0.000 blocks.py:1217(take_nd)\n",
       "       52    0.000    0.000    0.003    0.000 construction.py:537(sanitize_array)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'close' of '_io.TextIOWrapper' objects}\n",
       "      103    0.000    0.000    0.005    0.000 generic.py:3056(_get_item_cache)\n",
       "      133    0.000    0.000    0.001    0.000 _dtype.py:46(__str__)\n",
       "      264    0.000    0.000    0.000    0.000 blocks.py:199(mgr_locs)\n",
       "       13    0.000    0.000    0.005    0.000 managers.py:1241(_slice_take_blocks_ax0)\n",
       "      177    0.000    0.000    0.000    0.000 generic.py:127(__init__)\n",
       "       42    0.000    0.000    0.002    0.000 missing.py:183(_isna_ndarraylike)\n",
       "      333    0.000    0.000    0.002    0.000 inference.py:253(is_list_like)\n",
       "      372    0.000    0.000    0.000    0.000 {pandas._libs.lib.is_scalar}\n",
       "      119    0.000    0.000    0.003    0.000 algorithms.py:38(_ensure_data)\n",
       "      162    0.000    0.000    0.000    0.000 {built-in method numpy.arange}\n",
       "      217    0.000    0.000    0.000    0.000 base.py:3918(__contains__)\n",
       "     1244    0.000    0.000    0.000    0.000 common.py:117(classes)\n",
       "  639/606    0.000    0.000    0.002    0.000 numeric.py:469(asarray)\n",
       "      457    0.000    0.000    0.001    0.000 {built-in method _abc._abc_instancecheck}\n",
       "        8    0.000    0.000    0.040    0.005 frame.py:4639(duplicated)\n",
       "      814    0.000    0.000    0.000    0.000 base.py:652(__len__)\n",
       "       51    0.000    0.000    0.004    0.000 managers.py:97(__init__)\n",
       "      184    0.000    0.000    0.000    0.000 dtypes.py:929(construct_from_string)\n",
       "       11    0.000    0.000    0.000    0.000 concat.py:22(get_mgr_concatenation_plan)\n",
       "      504    0.000    0.000    0.001    0.000 common.py:572(is_categorical_dtype)\n",
       "      248    0.000    0.000    0.001    0.000 common.py:403(is_datetime64_dtype)\n",
       "      306    0.000    0.000    0.001    0.000 {method 'any' of 'numpy.ndarray' objects}\n",
       "       31    0.000    0.000    0.000    0.000 concat.py:117(needs_filling)\n",
       "        4    0.000    0.000    0.041    0.010 merge.py:1104(_get_join_indexers)\n",
       "       88    0.000    0.000    0.002    0.000 frame.py:3342(_box_item_values)\n",
       "      263    0.000    0.000    0.001    0.000 common.py:923(is_signed_integer_dtype)\n",
       "       84    0.000    0.000    0.001    0.000 common.py:222(asarray_tuplesafe)\n",
       "      588    0.000    0.000    0.000    0.000 common.py:127(<lambda>)\n",
       "       54    0.000    0.000    0.022    0.000 algorithms.py:434(_factorize_array)\n",
       "       28    0.000    0.000    0.000    0.000 {method 'get_indexer' of 'pandas._libs.index.IndexEngine' objects}\n",
       "      162    0.000    0.000    0.000    0.000 base.py:3940(__getitem__)\n",
       "       89    0.000    0.000    0.002    0.000 base.py:4051(equals)\n",
       "      383    0.000    0.000    0.001    0.000 common.py:503(is_period_dtype)\n",
       "       88    0.000    0.000    0.002    0.000 managers.py:934(get)\n",
       "      132    0.000    0.000    0.000    0.000 series.py:354(_set_axis)\n",
       "      809    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
       "       12    0.000    0.000    0.001    0.000 <frozen importlib._bootstrap>:882(_find_spec)\n",
       "       62    0.000    0.000    0.002    0.000 iostream.py:195(schedule)\n",
       "      153    0.000    0.000    0.001    0.000 series.py:392(name)\n",
       "      241    0.000    0.000    0.001    0.000 common.py:1545(is_float_dtype)\n",
       "      228    0.000    0.000    0.000    0.000 {method 'match' of 're.Pattern' objects}\n",
       "       54    0.000    0.000    0.006    0.000 algorithms.py:132(_reconstruct_data)\n",
       "       45    0.000    0.000    0.007    0.000 frame.py:378(__init__)\n",
       "      301    0.000    0.000    0.004    0.000 base.py:5318(ensure_index)\n",
       "      164    0.000    0.000    0.002    0.000 missing.py:105(_isna_new)\n",
       "      283    0.000    0.000    0.000    0.000 {method 'view' of 'numpy.ndarray' objects}\n",
       "       28    0.000    0.000    0.004    0.000 base.py:2715(get_indexer)\n",
       "       10    0.000    0.000    0.002    0.000 {pandas._libs.lib.clean_index_list}\n",
       "       44    0.000    0.000    0.001    0.000 managers.py:306(_verify_integrity)\n",
       "      422    0.000    0.000    0.001    0.000 common.py:536(is_interval_dtype)\n",
       "    42/12    0.000    0.000    0.003    0.000 <frozen importlib._bootstrap>:978(_find_and_load)\n",
       "       20    0.000    0.000    0.001    0.000 <frozen importlib._bootstrap_external>:1356(find_spec)\n",
       "      104    0.000    0.000    0.002    0.000 frame.py:3349(_box_col_values)\n",
       "      245    0.000    0.000    0.001    0.000 common.py:472(is_timedelta64_dtype)\n",
       "      228    0.000    0.000    0.000    0.000 dtypes.py:452(construct_from_string)\n",
       "      238    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
       "       84    0.000    0.000    0.001    0.000 blocks.py:2626(__init__)\n",
       "       59    0.000    0.000    0.003    0.000 iostream.py:382(write)\n",
       "      636    0.000    0.000    0.000    0.000 managers.py:141(<genexpr>)\n",
       "      189    0.000    0.000    0.001    0.000 blocks.py:225(make_block_same_class)\n",
       "       22    0.000    0.000    0.028    0.001 generic.py:3323(_take)\n",
       "      145    0.000    0.000    0.000    0.000 config.py:78(_get_single_key)\n",
       "      264    0.000    0.000    0.000    0.000 blocks.py:89(_check_ndim)\n",
       "        9    0.000    0.000    0.001    0.000 format.py:1045(get_result_as_array)\n",
       "      457    0.000    0.000    0.001    0.000 abc.py:137(__instancecheck__)\n",
       "      133    0.000    0.000    0.001    0.000 blocks.py:312(ftype)\n",
       "        6    0.000    0.000    0.000    0.000 base.py:1587(is_monotonic_increasing)\n",
       "       28    0.000    0.000    0.000    0.000 cast.py:832(maybe_castable)\n",
       "       54    0.000    0.000    0.031    0.001 frame.py:4666(f)\n",
       "        6    0.000    0.000    0.001    0.000 managers.py:1696(form_blocks)\n",
       "       25    0.000    0.000    0.000    0.000 base.py:1658(is_unique)\n",
       "      269    0.000    0.000    0.000    0.000 base.py:3608(values)\n",
       "       37    0.000    0.000    0.000    0.000 base.py:566(_shallow_copy)\n",
       "       51    0.000    0.000    0.002    0.000 managers.py:599(_consolidate_check)\n",
       "       28    0.000    0.000    0.018    0.001 managers.py:1198(reindex_indexer)\n",
       "      170    0.000    0.000    0.000    0.000 sparse.py:196(construct_from_string)\n",
       "       80    0.000    0.000    0.001    0.000 common.py:93(is_bool_indexer)\n",
       "      188    0.000    0.000    0.000    0.000 base.py:2650(get_loc)\n",
       "      157    0.000    0.000    0.001    0.000 common.py:1078(is_datetime64_any_dtype)\n",
       "       52    0.000    0.000    0.001    0.000 missing.py:360(array_equivalent)\n",
       "      208    0.000    0.000    0.000    0.000 generic.py:363(_get_axis_name)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method marshal.loads}\n",
       "        9    0.000    0.000    0.001    0.000 format.py:1060(format_values_with)\n",
       "      392    0.000    0.000    0.000    0.000 managers.py:143(ndim)\n",
       "      196    0.000    0.000    0.000    0.000 generic.py:377(_get_axis)\n",
       "       31    0.000    0.000    0.003    0.000 concat.py:165(get_reindexed_values)\n",
       "        7    0.000    0.000    0.000    0.000 numeric.py:2551(array_equal)\n",
       "      694    0.000    0.000    0.000    0.000 blocks.py:195(mgr_locs)\n",
       "       33    0.000    0.000    0.001    0.000 concat.py:137(is_na)\n",
       "      650    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n",
       "       62    0.000    0.000    0.003    0.000 frame.py:742(iteritems)\n",
       "      174    0.000    0.000    0.000    0.000 generic.py:349(_get_axis_number)\n",
       "        7    0.000    0.000    0.006    0.001 managers.py:1887(_consolidate)\n",
       "      288    0.000    0.000    0.000    0.000 config.py:561(_get_deprecated_option)\n",
       "      129    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:416(parent)\n",
       "       28    0.000    0.000    0.003    0.000 base.py:784(take)\n",
       "       95    0.000    0.000    0.000    0.000 _internal.py:886(npy_ctypes_check)\n",
       "      143    0.000    0.000    0.000    0.000 config.py:546(_get_root)\n",
       "      143    0.000    0.000    0.000    0.000 {pandas._libs.lib.values_from_object}\n",
       "      292    0.000    0.000    0.000    0.000 format.py:301(len)\n",
       "      149    0.000    0.000    0.000    0.000 series.py:399(name)\n",
       "      306    0.000    0.000    0.001    0.000 _methods.py:42(_any)\n",
       "       17    0.000    0.000    0.001    0.000 indexing.py:960(_getitem_lowerdim)\n",
       "       26    0.000    0.000    0.007    0.000 concat.py:230(concatenate_join_units)\n",
       "      588    0.000    0.000    0.000    0.000 common.py:122(classes_and_not_datetimelike)\n",
       "      192    0.000    0.000    0.000    0.000 common.py:980(is_unsigned_integer_dtype)\n",
       "        1    0.000    0.000    0.006    0.006 format.py:503(_to_str_columns)\n",
       "       34    0.000    0.000    0.001    0.000 format.py:1384(_make_fixed_width)\n",
       "      254    0.000    0.000    0.000    0.000 inference.py:438(is_hashable)\n",
       "       42    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:157(_get_module_lock)\n",
       "       28    0.000    0.000    0.000    0.000 cast.py:1193(construct_1d_object_array_from_listlike)\n",
       "       23    0.000    0.000    0.001    0.000 base.py:1117(__iter__)\n",
       "       24    0.000    0.000    0.001    0.000 generic.py:1657(_get_label_or_level_values)\n",
       "      143    0.000    0.000    0.001    0.000 config.py:96(_get_option)\n",
       "      136    0.000    0.000    0.000    0.000 common.py:746(is_dtype_equal)\n",
       "      382    0.000    0.000    0.000    0.000 blocks.py:308(dtype)\n",
       "      134    0.000    0.000    0.001    0.000 common.py:262(is_categorical)\n",
       "       25    0.000    0.000    0.002    0.000 base.py:584(_shallow_copy_with_infer)\n",
       "      140    0.000    0.000    0.000    0.000 {method 'fill' of 'numpy.ndarray' objects}\n",
       "       95    0.000    0.000    0.000    0.000 common.py:1784(_is_dtype)\n",
       "       57    0.000    0.000    0.005    0.000 algorithms.py:217(_get_data_algo)\n",
       "       17    0.000    0.000    0.003    0.000 format.py:848(format_array)\n",
       "       11    0.000    0.000    0.009    0.001 {built-in method builtins.print}\n",
       "        4    0.000    0.000    0.002    0.000 base.py:2357(intersection)\n",
       "       52    0.000    0.000    0.002    0.000 construction.py:684(_try_cast)\n",
       "       62    0.000    0.000    0.000    0.000 blocks.py:128(_consolidate_key)\n",
       "       95    0.000    0.000    0.000    0.000 common.py:868(is_integer_dtype)\n",
       "      196    0.000    0.000    0.000    0.000 generic.py:450(ndim)\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method _operator.inv}\n",
       "        1    0.000    0.000    0.000    0.000 {pandas._libs.writers.write_csv_rows}\n",
       "    42/12    0.000    0.000    0.002    0.000 <frozen importlib._bootstrap>:948(_find_and_load_unlocked)\n",
       "      104    0.000    0.000    0.000    0.000 blocks.py:332(iget)\n",
       "       14    0.000    0.000    0.024    0.002 frame.py:2952(_getitem_bool_array)\n",
       "       70    0.000    0.000    0.000    0.000 base.py:547(_get_attributes_dict)\n",
       "        8    0.000    0.000    0.004    0.001 indexing.py:1271(_convert_to_indexer)\n",
       "        4    0.000    0.000    0.000    0.000 function_base.py:4220(delete)\n",
       "      122    0.000    0.000    0.000    0.000 __init__.py:221(iteritems)\n",
       "       54    0.000    0.000    0.030    0.001 _decorators.py:146(wrapper)\n",
       "      280    0.000    0.000    0.000    0.000 managers.py:206(items)\n",
       "       23    0.000    0.000    0.000    0.000 numeric.py:34(__new__)\n",
       "      164    0.000    0.000    0.002    0.000 missing.py:25(isna)\n",
       "      104    0.000    0.000    0.000    0.000 generic.py:3070(_set_as_cached)\n",
       "       13    0.000    0.000    0.000    0.000 {pandas._libs.algos.take_2d_axis1_float64_float64}\n",
       "       59    0.000    0.000    0.000    0.000 iostream.py:307(_is_master_process)\n",
       "      160    0.000    0.000    0.000    0.000 common.py:144(cast_scalar_indexer)\n",
       "      160    0.000    0.000    0.000    0.000 managers.py:308(<genexpr>)\n",
       "       16    0.000    0.000    0.004    0.000 format.py:702(_format_col)\n",
       "      475    0.000    0.000    0.000    0.000 {built-in method builtins.hash}\n",
       "       17    0.000    0.000    0.000    0.000 numeric.py:676(require)\n",
       "       38    0.000    0.000    0.000    0.000 numeric.py:2656(seterr)\n",
       "      101    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:56(_path_join)\n",
       "       14    0.000    0.000    0.000    0.000 base.py:786(array)\n",
       "      160    0.000    0.000    0.002    0.000 {built-in method builtins.all}\n",
       "       71    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
       "        4    0.000    0.000    0.002    0.000 managers.py:1134(insert)\n",
       "       78    0.000    0.000    0.000    0.000 managers.py:1522(dtype)\n",
       "       99    0.000    0.000    0.000    0.000 managers.py:1549(internal_values)\n",
       "        4    0.000    0.000    0.052    0.013 merge.py:541(get_result)\n",
       "       38    0.000    0.000    0.000    0.000 numeric.py:2758(geterr)\n",
       "      144    0.000    0.000    0.000    0.000 inference.py:304(is_array_like)\n",
       "        8    0.000    0.000    0.004    0.000 indexing.py:1108(_get_listlike_indexer)\n",
       "       11    0.000    0.000    0.000    0.000 blocks.py:3145(<listcomp>)\n",
       "       62    0.000    0.000    0.003    0.000 frame.py:4685(<genexpr>)\n",
       "       27    0.000    0.000    0.001    0.000 cast.py:953(maybe_cast_to_datetime)\n",
       "       31    0.000    0.000    0.000    0.000 blocks.py:3100(_extend_blocks)\n",
       "       60    0.000    0.000    0.001    0.000 concat.py:379(<genexpr>)\n",
       "      144    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
       "       48    0.000    0.000    0.000    0.000 common.py:1198(is_datetime_or_timedelta_dtype)\n",
       "      209    0.000    0.000    0.000    0.000 {pandas._libs.algos.ensure_int64}\n",
       "       33    0.000    0.000    0.000    0.000 format.py:356(_get_formatter)\n",
       "       27    0.000    0.000    0.001    0.000 concat.py:367(is_uniform_join_units)\n",
       "      196    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}\n",
       "       16    0.000    0.000    0.000    0.000 fromnumeric.py:69(_wrapreduction)\n",
       "       34    0.000    0.000    0.000    0.000 concat.py:425(combine_concat_plans)\n",
       "       28    0.000    0.000    0.000    0.000 generic.py:1546(_is_label_reference)\n",
       "       62    0.000    0.000    0.000    0.000 range.py:510(__len__)\n",
       "       12    0.000    0.000    0.001    0.000 <frozen importlib._bootstrap_external>:1240(_get_spec)\n",
       "       16    0.000    0.000    0.012    0.001 managers.py:1233(<listcomp>)\n",
       "       62    0.000    0.000    0.000    0.000 threading.py:1080(is_alive)\n",
       "       20    0.000    0.000    0.000    0.000 printing.py:185(as_escaped_unicode)\n",
       "       24    0.000    0.000    0.000    0.000 generic.py:1607(_check_label_or_level_ambiguity)\n",
       "      268    0.000    0.000    0.000    0.000 managers.py:1488(_block)\n",
       "       24    0.000    0.000    0.000    0.000 cast.py:1151(construct_1d_arraylike_from_scalar)\n",
       "        4    0.000    0.000    0.007    0.002 merge.py:777(_get_merge_keys)\n",
       "       42    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:78(acquire)\n",
       "       44    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:873(_find_spec_legacy)\n",
       "       29    0.000    0.000    0.000    0.000 base.py:643(_engine)\n",
       "      170    0.000    0.000    0.000    0.000 {method 'search' of 're.Pattern' objects}\n",
       "       26    0.000    0.000    0.000    0.000 concat.py:388(is_uniform_reindex)\n",
       "        4    0.000    0.000    0.000    0.000 index_tricks.py:316(__getitem__)\n",
       "      292    0.000    0.000    0.000    0.000 __init__.py:291(strlen)\n",
       "        2    0.000    0.000    0.000    0.000 {pandas._libs.lib.map_infer}\n",
       "       16    0.000    0.000    0.001    0.000 frame.py:2829(_ixs)\n",
       "       11    0.000    0.000    0.000    0.000 {method 'argsort' of 'numpy.ndarray' objects}\n",
       "       28    0.000    0.000    0.000    0.000 base.py:4459(_maybe_promote)\n",
       "       14    0.000    0.000    0.000    0.000 numpy_.py:35(__init__)\n",
       "       24    0.000    0.000    0.000    0.000 generic.py:1513(_is_level_reference)\n",
       "       60    0.000    0.000    0.000    0.000 generic.py:5036(__finalize__)\n",
       "       51    0.000    0.000    0.001    0.000 managers.py:600(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method posix.listdir}\n",
       "      122    0.000    0.000    0.000    0.000 base.py:633(_reset_identity)\n",
       "        8    0.000    0.000    0.000    0.000 indexing.py:1209(_validate_read_indexer)\n",
       "        6    0.000    0.000    0.000    0.000 managers.py:1841(_stack_arrays)\n",
       "       42    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:103(release)\n",
       "       88    0.000    0.000    0.000    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
       "       67    0.000    0.000    0.000    0.000 base.py:963(_ndarray_values)\n",
       "        9    0.000    0.000    0.000    0.000 concat.py:20(get_dtype_kinds)\n",
       "       18    0.000    0.000    0.000    0.000 format.py:1019(base_formatter)\n",
       "        4    0.000    0.000    0.000    0.000 merge.py:647(_maybe_add_join_keys)\n",
       "       55    0.000    0.000    0.000    0.000 {built-in method builtins.sum}\n",
       "       93    0.000    0.000    0.000    0.000 numeric.py:541(asanyarray)\n",
       "       23    0.000    0.000    0.001    0.000 numeric.py:67(_shallow_copy)\n",
       "       51    0.000    0.000    0.000    0.000 managers.py:98(<listcomp>)\n",
       "      101    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:58(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:914(get_data)\n",
       "        6    0.000    0.000    0.006    0.001 construction.py:170(init_dict)\n",
       "      109    0.000    0.000    0.000    0.000 common.py:316(apply_if_callable)\n",
       "       22    0.000    0.000    0.000    0.000 numerictypes.py:654(<listcomp>)\n",
       "       44    0.000    0.000    0.000    0.000 numerictypes.py:578(_can_coerce_all)\n",
       "       24    0.000    0.000    0.000    0.000 cast.py:341(infer_dtype_from_scalar)\n",
       "       12    0.000    0.000    0.001    0.000 base.py:3089(reindex)\n",
       "       17    0.000    0.000    0.002    0.000 indexing.py:1485(__getitem__)\n",
       "       14    0.000    0.000    0.001    0.000 indexing.py:2475(check_bool_indexer)\n",
       "      123    0.000    0.000    0.000    0.000 blocks.py:304(shape)\n",
       "       61    0.000    0.000    0.000    0.000 managers.py:1546(external_values)\n",
       "        4    0.000    0.000    0.007    0.002 merge.py:474(__init__)\n",
       "       89    0.000    0.000    0.000    0.000 base.py:613(is_)\n",
       "       75    0.000    0.000    0.000    0.000 frame.py:937(__len__)\n",
       "       31    0.000    0.000    0.006    0.000 generic.py:5122(_protect_consolidate)\n",
       "        6    0.000    0.000    0.006    0.001 managers.py:318(apply)\n",
       "       62    0.000    0.000    0.000    0.000 threading.py:1038(_wait_for_tstate_lock)\n",
       "       20    0.000    0.000    0.000    0.000 printing.py:156(pprint_thing)\n",
       "       75    0.000    0.000    0.006    0.000 managers.py:927(_consolidate_inplace)\n",
       "      101    0.000    0.000    0.000    0.000 {built-in method builtins.any}\n",
       "       70    0.000    0.000    0.000    0.000 base.py:551(<dictcomp>)\n",
       "       42    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:58(__init__)\n",
       "       22    0.000    0.000    0.000    0.000 numerictypes.py:602(find_common_type)\n",
       "      291    0.000    0.000    0.000    0.000 {pandas._libs.lib.is_float}\n",
       "       48    0.000    0.000    0.001    0.000 common.py:1431(needs_i8_conversion)\n",
       "      132    0.000    0.000    0.000    0.000 series.py:382(_set_subtyp)\n",
       "       99    0.000    0.000    0.000    0.000 series.py:476(_values)\n",
       "      143    0.000    0.000    0.001    0.000 config.py:226(__call__)\n",
       "       21    0.000    0.000    0.000    0.000 inference.py:470(is_sequence)\n",
       "       28    0.000    0.000    0.000    0.000 base.py:1669(is_boolean)\n",
       "        2    0.000    0.000    0.000    0.000 concat.py:237(__init__)\n",
       "      232    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
       "      126    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x10ac56778}\n",
       "       62    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
       "       42    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:147(__enter__)\n",
       "       29    0.000    0.000    0.000    0.000 shape_base.py:83(atleast_2d)\n",
       "        4    0.000    0.000    0.000    0.000 printing.py:15(adjoin)\n",
       "        4    0.000    0.000    0.041    0.010 algorithms.py:370(isin)\n",
       "       31    0.000    0.000    0.000    0.000 concat.py:126(dtype)\n",
       "      369    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
       "       59    0.000    0.000    0.000    0.000 iostream.py:320(_schedule_flush)\n",
       "       24    0.000    0.000    0.001    0.000 cast.py:846(maybe_infer_to_datetimelike)\n",
       "       61    0.000    0.000    0.000    0.000 series.py:434(values)\n",
       "        4    0.000    0.000    0.041    0.010 series.py:3947(isin)\n",
       "       39    0.000    0.000    0.000    0.000 generic.py:144(_init_mgr)\n",
       "        1    0.000    0.000    0.001    0.001 csvs.py:290(_save_chunk)\n",
       "      119    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}\n",
       "      143    0.000    0.000    0.000    0.000 config.py:602(_warn_if_deprecated)\n",
       "       29    0.000    0.000    0.000    0.000 generic.py:381(_get_block_manager_axis)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'writelines' of '_io._IOBase' objects}\n",
       "       93    0.000    0.000    0.000    0.000 frame.py:474(axes)\n",
       "       17    0.000    0.000    0.000    0.000 indexing.py:217(_has_valid_tuple)\n",
       "       33    0.000    0.000    0.000    0.000 format.py:1421(_cond)\n",
       "      122    0.000    0.000    0.000    0.000 {built-in method pandas._libs.missing.checknull}\n",
       "       54    0.000    0.000    0.000    0.000 algorithms.py:163(_ensure_arraylike)\n",
       "       14    0.000    0.000    0.000    0.000 numpy_.py:127(__init__)\n",
       "       24    0.000    0.000    0.001    0.000 generic.py:3461(xs)\n",
       "        4    0.000    0.000    0.003    0.001 generic.py:3787(_drop_axis)\n",
       "       14    0.000    0.000    0.000    0.000 {method 'clear' of 'dict' objects}\n",
       "   199/12    0.000    0.000    0.001    0.000 abc.py:141(__subclasscheck__)\n",
       "      327    0.000    0.000    0.000    0.000 {pandas._libs.lib.is_integer}\n",
       "       66    0.000    0.000    0.000    0.000 common.py:605(is_string_dtype)\n",
       "        2    0.000    0.000    0.003    0.002 ops.py:1660(wrapper)\n",
       "      104    0.000    0.000    0.000    0.000 base.py:3663(get_values)\n",
       "       17    0.000    0.000    0.001    0.000 indexing.py:2205(_getitem_axis)\n",
       "       85    0.000    0.000    0.000    0.000 blocks.py:191(fill_value)\n",
       "        4    0.000    0.000    0.059    0.015 merge.py:37(merge)\n",
       "       11    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
       "       73    0.000    0.000    0.000    0.000 printing.py:50(justify)\n",
       "       31    0.000    0.000    0.006    0.000 generic.py:5132(_consolidate_inplace)\n",
       "       52    0.000    0.000    0.000    0.000 arrays.py:7(extract_array)\n",
       "      193    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}\n",
       "        8    0.000    0.000    0.001    0.000 generic.py:1439(__neg__)\n",
       "       34    0.000    0.000    0.000    0.000 indexing.py:2056(_validate_key)\n",
       "       32    0.000    0.000    0.000    0.000 indexing.py:2116(_validate_integer)\n",
       "       78    0.000    0.000    0.000    0.000 series.py:406(dtype)\n",
       "        1    0.000    0.000    0.000    0.000 csvs.py:4(<module>)\n",
       "      144    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
       "        4    0.000    0.000    0.002    0.001 generic.py:4113(reindex)\n",
       "        4    0.000    0.000    0.002    0.000 generic.py:4469(_reindex_with_indexers)\n",
       "       31    0.000    0.000    0.006    0.000 generic.py:5135(f)\n",
       "        1    0.000    0.000    0.004    0.004 {_cython_magic_a8678e4eba7059b7e523b0769dde009b.geometric_mean}\n",
       "      145    0.000    0.000    0.000    0.000 config.py:528(_select_options)\n",
       "       49    0.000    0.000    0.000    0.000 inference.py:121(is_iterator)\n",
       "       38    0.000    0.000    0.000    0.000 common.py:1472(is_numeric_dtype)\n",
       "        6    0.000    0.000    0.006    0.001 generic.py:5699(copy)\n",
       "       14    0.000    0.000    0.000    0.000 series.py:669(__array__)\n",
       "        4    0.000    0.000    0.042    0.010 merge.py:737(_get_join_info)\n",
       "        4    0.000    0.000    0.000    0.000 merge.py:889(_maybe_coerce_merge_keys)\n",
       "        4    0.000    0.000    0.000    0.000 concat.py:486(_concat_index_asobject)\n",
       "       31    0.000    0.000    0.005    0.000 managers.py:911(consolidate)\n",
       "       17    0.000    0.000    0.002    0.000 format.py:927(get_result)\n",
       "      122    0.000    0.000    0.000    0.000 format.py:1418(_is_number)\n",
       "        4    0.000    0.000    0.000    0.000 posixpath.py:232(expanduser)\n",
       "       62    0.000    0.000    0.000    0.000 iostream.py:93(_event_pipe)\n",
       "        8    0.000    0.000    0.000    0.000 sorting.py:47(_int64_cut_off)\n",
       "        4    0.000    0.000    0.001    0.000 base.py:4939(drop)\n",
       "       26    0.000    0.000    0.000    0.000 indexing.py:2450(convert_to_index_sliceable)\n",
       "       26    0.000    0.000    0.000    0.000 managers.py:174(_is_single_block)\n",
       "       62    0.000    0.000    0.000    0.000 managers.py:1893(<lambda>)\n",
       "       26    0.000    0.000    0.003    0.000 concat.py:240(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 common.py:120(_stringify_path)\n",
       "      202    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}\n",
       "       42    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:176(cb)\n",
       "       38    0.000    0.000    0.000    0.000 {built-in method numpy.seterrobj}\n",
       "       66    0.000    0.000    0.000    0.000 common.py:634(condition)\n",
       "        7    0.000    0.000    0.000    0.000 common.py:1320(is_datetimelike_v_numeric)\n",
       "       48    0.000    0.000    0.000    0.000 generic.py:1895(<genexpr>)\n",
       "       17    0.000    0.000    0.000    0.000 indexing.py:2089(_is_scalar_access)\n",
       "        6    0.000    0.000    0.002    0.000 managers.py:1663(create_block_manager_from_arrays)\n",
       "        2    0.000    0.000    0.004    0.002 concat.py:383(get_result)\n",
       "        4    0.000    0.000    0.000    0.000 _bootlocale.py:33(getpreferredencoding)\n",
       "       12    0.000    0.000    0.000    0.000 fromnumeric.py:2664(prod)\n",
       "      106    0.000    0.000    0.000    0.000 managers.py:591(is_consolidated)\n",
       "        6    0.000    0.000    0.002    0.000 construction.py:254(_homogenize)\n",
       "       69    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}\n",
       "    30/10    0.000    0.000    0.001    0.000 {built-in method builtins.__import__}\n",
       "      164    0.000    0.000    0.000    0.000 {built-in method _imp.acquire_lock}\n",
       "       80    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:855(__enter__)\n",
       "        3    0.000    0.000    0.000    0.000 function_base.py:1149(diff)\n",
       "      145    0.000    0.000    0.000    0.000 config.py:589(_translate_key)\n",
       "       25    0.000    0.000    0.000    0.000 common.py:702(is_datetimelike)\n",
       "       83    0.000    0.000    0.000    0.000 base.py:676(dtype)\n",
       "       47    0.000    0.000    0.001    0.000 base.py:1729(inferred_type)\n",
       "       56    0.000    0.000    0.000    0.000 generic.py:1576(<genexpr>)\n",
       "       13    0.000    0.000    0.000    0.000 managers.py:2015(_preprocess_slice_or_indexer)\n",
       "       73    0.000    0.000    0.000    0.000 format.py:304(justify)\n",
       "       84    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n",
       "      102    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:222(_verbose_message)\n",
       "       19    0.000    0.000    0.000    0.000 numeric.py:3054(__init__)\n",
       "       19    0.000    0.000    0.000    0.000 numeric.py:3058(__enter__)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:1010(<listcomp>)\n",
       "       17    0.000    0.000    0.000    0.000 indexing.py:229(_is_nested_tuple_indexer)\n",
       "        4    0.000    0.000    0.000    0.000 managers.py:2008(_fast_count_smallints)\n",
       "       60    0.000    0.000    0.000    0.000 concat.py:376(<genexpr>)\n",
       "       14    0.000    0.000    0.000    0.000 series.py:490(get_values)\n",
       "        1    0.000    0.000    0.000    0.000 csvs.py:30(__init__)\n",
       "       80    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:859(__exit__)\n",
       "       19    0.000    0.000    0.000    0.000 numeric.py:3063(__exit__)\n",
       "        5    0.000    0.000    0.000    0.000 concat.py:101(_concat_compat)\n",
       "       14    0.000    0.000    0.000    0.000 generic.py:1814(__hash__)\n",
       "      135    0.000    0.000    0.000    0.000 numeric.py:113(is_all_dates)\n",
       "       17    0.000    0.000    0.002    0.000 indexing.py:2141(_getitem_tuple)\n",
       "       56    0.000    0.000    0.000    0.000 indexing.py:2689(is_list_like_indexer)\n",
       "       33    0.000    0.000    0.000    0.000 format.py:1422(<listcomp>)\n",
       "       90    0.000    0.000    0.000    0.000 format.py:1423(<genexpr>)\n",
       "      144    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
       "       42    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:151(__exit__)\n",
       "        1    0.000    0.000    0.000    0.000 {pandas._libs.lib.maybe_convert_objects}\n",
       "      122    0.000    0.000    0.000    0.000 {pandas._libs.algos.ensure_platform_int}\n",
       "        4    0.000    0.000    0.002    0.000 managers.py:1019(set)\n",
       "       14    0.000    0.000    0.000    0.000 managers.py:1556(get_values)\n",
       "       52    0.000    0.000    0.000    0.000 concat.py:391(<genexpr>)\n",
       "       18    0.000    0.000    0.000    0.000 format.py:338(_get_adjustment)\n",
       "       17    0.000    0.000    0.000    0.000 format.py:912(__init__)\n",
       "      102    0.000    0.000    0.000    0.000 format.py:1392(<genexpr>)\n",
       "       68    0.000    0.000    0.000    0.000 format.py:1401(just)\n",
       "        9    0.000    0.000    0.000    0.000 format.py:1412(_trim_zeros)\n",
       "      164    0.000    0.000    0.000    0.000 {built-in method _imp.release_lock}\n",
       "       35    0.000    0.000    0.000    0.000 common.py:1643(is_extension_type)\n",
       "       50    0.000    0.000    0.000    0.000 base.py:1237(_get_names)\n",
       "        8    0.000    0.000    0.000    0.000 base.py:2445(difference)\n",
       "       16    0.000    0.000    0.000    0.000 base.py:3975(_can_hold_identifiers_and_holds_name)\n",
       "       24    0.000    0.000    0.000    0.000 {pandas._libs.internals.get_blkno_placements}\n",
       "        2    0.000    0.000    0.000    0.000 format.py:931(_format_strings)\n",
       "       17    0.000    0.000    0.000    0.000 numeric.py:748(<setcomp>)\n",
       "        4    0.000    0.000    0.006    0.001 generic.py:1729(_drop_labels_or_levels)\n",
       "       16    0.000    0.000    0.000    0.000 generic.py:3175(_set_is_copy)\n",
       "        6    0.000    0.000    0.000    0.000 generic.py:3840(_update_inplace)\n",
       "        6    0.000    0.000    0.005    0.001 construction.py:43(arrays_to_mgr)\n",
       "        9    0.000    0.000    0.000    0.000 format.py:1078(<listcomp>)\n",
       "       24    0.000    0.000    0.000    0.000 format.py:1427(<listcomp>)\n",
       "        8    0.000    0.000    0.001    0.000 series.py:730(__array_wrap__)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method posix.getcwd}\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1404(_fill_cache)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:1736(is_all_dates)\n",
       "        4    0.000    0.000    0.000    0.000 frame.py:3565(_sanitize_column)\n",
       "       16    0.000    0.000    0.000    0.000 generic.py:1848(empty)\n",
       "        3    0.000    0.000    0.001    0.000 generic.py:8324(ranker)\n",
       "        8    0.000    0.000    0.000    0.000 indexing.py:256(_convert_scalar_indexer)\n",
       "        4    0.000    0.000    0.000    0.000 managers.py:1810(_multi_blockify)\n",
       "       34    0.000    0.000    0.000    0.000 format.py:1407(<listcomp>)\n",
       "       80    0.000    0.000    0.000    0.000 {method 'rjust' of 'str' objects}\n",
       "    31/11    0.000    0.000    0.001    0.000 <frozen importlib._bootstrap>:211(_call_with_frames_removed)\n",
       "       21    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1203(_path_importer_cache)\n",
       "       11    0.000    0.000    0.002    0.000 shape_base.py:229(vstack)\n",
       "       32    0.000    0.000    0.000    0.000 missing.py:530(clean_reindex_fill_method)\n",
       "       33    0.000    0.000    0.000    0.000 base.py:646(<lambda>)\n",
       "       12    0.000    0.000    0.000    0.000 base.py:700(view)\n",
       "        4    0.000    0.000    0.004    0.001 generic.py:3759(drop)\n",
       "       51    0.000    0.000    0.000    0.000 indexing.py:1487(<genexpr>)\n",
       "       34    0.000    0.000    0.000    0.000 blocks.py:175(get_values)\n",
       "       34    0.000    0.000    0.000    0.000 concat.py:104(__init__)\n",
       "       55    0.000    0.000    0.000    0.000 concat.py:381(<genexpr>)\n",
       "       48    0.000    0.000    0.000    0.000 printing.py:59(<listcomp>)\n",
       "        8    0.000    0.000    0.000    0.000 base.py:2849(_convert_scalar_indexer)\n",
       "        4    0.000    0.000    0.001    0.000 base.py:3988(append)\n",
       "       32    0.000    0.000    0.000    0.000 managers.py:291(__len__)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:642(_join_multiline)\n",
       "       34    0.000    0.000    0.000    0.000 {method 'all' of 'numpy.ndarray' objects}\n",
       "       76    0.000    0.000    0.000    0.000 {built-in method numpy.geterrobj}\n",
       "       11    0.000    0.000    0.000    0.000 shape_base.py:283(<listcomp>)\n",
       "       20    0.000    0.000    0.000    0.000 missing.py:259(notna)\n",
       "      111    0.000    0.000    0.000    0.000 base.py:3632(_values)\n",
       "        4    0.000    0.000    0.002    0.000 frame.py:3356(__setitem__)\n",
       "        4    0.000    0.000    0.000    0.000 generic.py:297(_construct_axes_from_arguments)\n",
       "       26    0.000    0.000    0.000    0.000 generic.py:426(_info_axis)\n",
       "       14    0.000    0.000    0.005    0.000 blocks.py:749(copy)\n",
       "        6    0.000    0.000    0.006    0.001 managers.py:710(copy)\n",
       "       48    0.000    0.000    0.000    0.000 format.py:541(<genexpr>)\n",
       "      2/1    0.000    0.000    0.293    0.293 {built-in method builtins.exec}\n",
       "       42    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:143(__init__)\n",
       "       22    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:74(_path_stat)\n",
       "      132    0.000    0.000    0.000    0.000 _collections_abc.py:392(__subclasshook__)\n",
       "       16    0.000    0.000    0.000    0.000 fromnumeric.py:70(<dictcomp>)\n",
       "       23    0.000    0.000    0.000    0.000 base.py:3806(_coerce_to_ndarray)\n",
       "        4    0.000    0.000    0.001    0.000 base.py:4919(insert)\n",
       "        4    0.000    0.000    0.002    0.001 frame.py:3794(reindex)\n",
       "       51    0.000    0.000    0.000    0.000 indexing.py:230(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:651(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 merge.py:614(_maybe_restore_index_levels)\n",
       "      204    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}\n",
       "      109    0.000    0.000    0.000    0.000 {built-in method builtins.callable}\n",
       "       11    0.000    0.000    0.000    0.000 fromnumeric.py:942(argsort)\n",
       "       25    0.000    0.000    0.000    0.000 printing.py:55(<listcomp>)\n",
       "       14    0.000    0.000    0.000    0.000 common.py:1118(is_datetime64_ns_dtype)\n",
       "       73    0.000    0.000    0.000    0.000 {pandas._libs.algos.ensure_object}\n",
       "        2    0.000    0.000    0.000    0.000 base.py:998(_format_with_header)\n",
       "       18    0.000    0.000    0.000    0.000 base.py:1681(is_object)\n",
       "        8    0.000    0.000    0.000    0.000 base.py:2965(_convert_listlike_indexer)\n",
       "       16    0.000    0.000    0.001    0.000 indexing.py:143(_get_loc)\n",
       "        5    0.000    0.000    0.000    0.000 blocks.py:1982(to_native_types)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:381(__init__)\n",
       "        1    0.000    0.000    0.001    0.001 format.py:739(_get_formatted_column_labels)\n",
       "       15    0.000    0.000    0.000    0.000 _collections_abc.py:72(_check_methods)\n",
       "       33    0.000    0.000    0.000    0.000 six.py:184(find_module)\n",
       "        3    0.000    0.000    0.000    0.000 parse.py:409(urlsplit)\n",
       "       34    0.000    0.000    0.000    0.000 _methods.py:45(_all)\n",
       "        4    0.000    0.000    0.000    0.000 twodim_base.py:216(diag)\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method pandas._libs.lib.is_bool_array}\n",
       "       27    0.000    0.000    0.000    0.000 cast.py:1218(construct_1d_ndarray_preserving_na)\n",
       "        4    0.000    0.000    0.002    0.000 frame.py:3433(_set_item)\n",
       "        8    0.000    0.000    0.000    0.000 blocks.py:2633(is_bool)\n",
       "       16    0.000    0.000    0.000    0.000 {method 'add' of 'pandas._libs.internals.BlockPlacement' objects}\n",
       "        1    0.000    0.000    0.006    0.006 format.py:582(to_string)\n",
       "        6    0.000    0.000    0.000    0.000 format.py:1140(<listcomp>)\n",
       "       72    0.000    0.000    0.000    0.000 format.py:1424(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.__build_class__}\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method _locale.nl_langinfo}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'read' of '_io.FileIO' objects}\n",
       "       12    0.000    0.000    0.001    0.000 <frozen importlib._bootstrap_external>:1272(find_spec)\n",
       "        8    0.000    0.000    0.000    0.000 fromnumeric.py:1583(ravel)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:3926(contains)\n",
       "        8    0.000    0.000    0.000    0.000 generic.py:1844(__contains__)\n",
       "        3    0.000    0.000    0.000    0.000 range.py:272(_shallow_copy)\n",
       "       24    0.000    0.000    0.000    0.000 managers.py:1844(_asarray_compat)\n",
       "        4    0.000    0.000    0.002    0.000 managers.py:1959(items_overlap_with_suffix)\n",
       "        1    0.000    0.000    0.000    0.000 _methods.py:58(_mean)\n",
       "        2    0.000    0.000    0.001    0.000 base.py:1096(tolist)\n",
       "       12    0.000    0.000    0.000    0.000 base.py:2590(_assert_can_do_setop)\n",
       "        8    0.000    0.000    0.003    0.000 base.py:4447(get_indexer_for)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:4909(delete)\n",
       "       37    0.000    0.000    0.000    0.000 frame.py:361(_constructor)\n",
       "        4    0.000    0.000    0.002    0.001 frame.py:3729(_reindex_axes)\n",
       "        4    0.000    0.000    0.002    0.001 frame.py:3754(_reindex_columns)\n",
       "      115    0.000    0.000    0.000    0.000 blocks.py:165(internal_values)\n",
       "       16    0.000    0.000    0.000    0.000 managers.py:1552(formatting_values)\n",
       "        6    0.000    0.000    0.000    0.000 managers.py:2041(<listcomp>)\n",
       "       18    0.000    0.000    0.000    0.000 format.py:298(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:1553(_binify)\n",
       "       11    0.000    0.000    0.000    0.000 __init__.py:23(find_module)\n",
       "       12    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:369(__init__)\n",
       "       62    0.000    0.000    0.000    0.000 threading.py:507(is_set)\n",
       "       15    0.000    0.000    0.000    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
       "        4    0.000    0.000    0.000    0.000 fromnumeric.py:1395(diagonal)\n",
       "       11    0.000    0.000    0.000    0.000 shape_base.py:220(_warn_for_nonsequence)\n",
       "       76    0.000    0.000    0.000    0.000 {pandas._libs.lib.is_bool}\n",
       "        4    0.000    0.000    0.001    0.000 base.py:3830(_coerce_scalar_to_index)\n",
       "       14    0.000    0.000    0.000    0.000 numpy_.py:170(__array__)\n",
       "        6    0.000    0.000    0.000    0.000 generic.py:3115(_maybe_update_cacher)\n",
       "       14    0.000    0.000    0.000    0.000 generic.py:3149(_clear_item_cache)\n",
       "       14    0.000    0.000    0.000    0.000 blocks.py:184(to_dense)\n",
       "       26    0.000    0.000    0.000    0.000 blocks.py:255(__len__)\n",
       "        4    0.000    0.000    0.000    0.000 format.py:307(adjoin)\n",
       "        9    0.000    0.000    0.001    0.000 format.py:1128(_format_strings)\n",
       "        2    0.000    0.000    0.004    0.002 concat.py:24(concat)\n",
       "        8    0.000    0.000    0.000    0.000 apipkg.py:133(__makeattr)\n",
       "        1    0.000    0.000    0.002    0.002 csvs.py:130(save)\n",
       "       59    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
       "       62    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
       "       84    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}\n",
       "        4    0.000    0.000    0.000    0.000 copy.py:66(copy)\n",
       "        8    0.000    0.000    0.000    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
       "       14    0.000    0.000    0.000    0.000 common.py:1167(is_timedelta64_ns_dtype)\n",
       "        4    0.000    0.000    0.000    0.000 _validators.py:230(validate_axis_style_args)\n",
       "       16    0.000    0.000    0.000    0.000 common.py:273(maybe_make_list)\n",
       "       32    0.000    0.000    0.000    0.000 missing.py:71(clean_fill_method)\n",
       "       68    0.000    0.000    0.000    0.000 base.py:1396(nlevels)\n",
       "        2    0.000    0.000    0.000    0.000 concat.py:84(_get_frame_result_type)\n",
       "        4    0.000    0.000    0.004    0.001 frame.py:3819(drop)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:3155(_slice)\n",
       "        8    0.000    0.000    0.000    0.000 generic.py:4341(<genexpr>)\n",
       "        3    0.000    0.000    0.001    0.000 generic.py:8282(rank)\n",
       "        2    0.000    0.000    0.000    0.000 range.py:136(_simple_new)\n",
       "       34    0.000    0.000    0.000    0.000 indexing.py:2695(is_label_like)\n",
       "       61    0.000    0.000    0.000    0.000 blocks.py:161(external_values)\n",
       "       16    0.000    0.000    0.000    0.000 blocks.py:171(formatting_values)\n",
       "        9    0.000    0.000    0.000    0.000 format.py:992(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 concat.py:440(_get_new_axes)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:793(get_code)\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method numpy.copyto}\n",
       "       11    0.000    0.000    0.000    0.000 fromnumeric.py:54(_wrapfunc)\n",
       "        4    0.000    0.000    0.000    0.000 fromnumeric.py:1966(sum)\n",
       "        8    0.000    0.000    0.000    0.000 function_base.py:4641(append)\n",
       "        1    0.000    0.000    0.000    0.000 console.py:47(get_console_size)\n",
       "       10    0.000    0.000    0.000    0.000 {pandas._libs.lib.array_equivalent_object}\n",
       "       15    0.000    0.000    0.000    0.000 common.py:1513(is_string_like_dtype)\n",
       "       27    0.000    0.000    0.000    0.000 {pandas._libs.algos.ensure_float64}\n",
       "        1    0.000    0.000    0.000    0.000 {pandas._libs.algos.take_1d_object_object}\n",
       "        4    0.000    0.000    0.000    0.000 common.py:246(index_labels_to_array)\n",
       "       18    0.000    0.000    0.000    0.000 common.py:279(is_null_slice)\n",
       "        4    0.000    0.000    0.000    0.000 sorting.py:124(is_int64_overflow_possible)\n",
       "        4    0.000    0.000    0.001    0.000 base.py:4017(_concat)\n",
       "        1    0.000    0.000    0.006    0.006 frame.py:614(__unicode__)\n",
       "        1    0.000    0.000    0.002    0.002 generic.py:2882(to_csv)\n",
       "        4    0.000    0.000    0.002    0.000 generic.py:3171(_set_item)\n",
       "        1    0.000    0.000    0.000    0.000 indexing.py:264(_convert_slice_indexer)\n",
       "       32    0.000    0.000    0.000    0.000 format.py:535(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:801(_get_formatted_index)\n",
       "        4    0.000    0.000    0.000    0.000 merge.py:1704(<lambda>)\n",
       "        6    0.000    0.000    0.000    0.000 <ipython-input-4-d36ff41301f6>:3(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 stats.py:256(gmean)\n",
       "       69    0.000    0.000    0.000    0.000 {method 'ljust' of 'str' objects}\n",
       "       20    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
       "       20    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:36(_relax_case)\n",
       "        4    0.000    0.000    0.000    0.000 numeric.py:175(ones)\n",
       "       17    0.000    0.000    0.000    0.000 {method 'max' of 'numpy.ndarray' objects}\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method numpy.bincount}\n",
       "       15    0.000    0.000    0.000    0.000 common.py:1542(<lambda>)\n",
       "       16    0.000    0.000    0.000    0.000 _validators.py:221(validate_bool_kwarg)\n",
       "       12    0.000    0.000    0.000    0.000 base.py:802(_assert_take_fillable)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:5408(default_index)\n",
       "        4    0.000    0.000    0.000    0.000 generic.py:5140(_consolidate)\n",
       "        2    0.000    0.000    0.000    0.000 range.py:69(__new__)\n",
       "        2    0.000    0.000    0.000    0.000 blocks.py:730(to_native_types)\n",
       "       24    0.000    0.000    0.000    0.000 managers.py:706(nblocks)\n",
       "       12    0.000    0.000    0.000    0.000 managers.py:729(<lambda>)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:769(<listcomp>)\n",
       "        6    0.000    0.000    0.000    0.000 format.py:1138(_format_strings)\n",
       "        4    0.000    0.000    0.001    0.000 construction.py:284(extract_index)\n",
       "       12    0.000    0.000    0.000    0.000 merge.py:1731(_should_fill)\n",
       "        1    0.000    0.000    0.000    0.000 csvs.py:191(_save_header)\n",
       "        1    0.000    0.000    0.000    0.000 warnings.py:35(_formatwarnmsg_impl)\n",
       "        9    0.000    0.000    0.000    0.000 {method 'transpose' of 'numpy.ndarray' objects}\n",
       "        7    0.000    0.000    0.000    0.000 common.py:1752(is_complex_dtype)\n",
       "        6    0.000    0.000    0.000    0.000 common.py:212(dict_keys_to_ordered_list)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:1050(_format_native_types)\n",
       "        8    0.000    0.000    0.000    0.000 base.py:1672(is_integer)\n",
       "        1    0.000    0.000    0.006    0.006 frame.py:678(to_string)\n",
       "       28    0.000    0.000    0.000    0.000 generic.py:1567(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:5457(dtypes)\n",
       "        8    0.000    0.000    0.000    0.000 indexing.py:2676(is_nested_tuple)\n",
       "       27    0.000    0.000    0.000    0.000 format.py:1107(<genexpr>)\n",
       "        9    0.000    0.000    0.000    0.000 format.py:1430(<listcomp>)\n",
       "       17    0.000    0.000    0.000    0.000 series.py:338(_constructor)\n",
       "        2    0.000    0.000    0.000    0.000 concat.py:309(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 concat.py:475(_get_concat_axis)\n",
       "       38    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}\n",
       "       11    0.000    0.000    0.000    0.000 {method 'partition' of 'str' objects}\n",
       "       12    0.000    0.000    0.000    0.000 {built-in method _imp.is_frozen}\n",
       "        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:271(cache_from_source)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method now}\n",
       "        4    0.000    0.000    0.000    0.000 {method 'diagonal' of 'numpy.ndarray' objects}\n",
       "        4    0.000    0.000    0.000    0.000 printing.py:36(<listcomp>)\n",
       "        2    0.000    0.000    0.003    0.001 ops.py:1591(_comp_method_OBJECT_ARRAY)\n",
       "        2    0.000    0.000    0.003    0.001 ops.py:1615(na_op)\n",
       "        4    0.000    0.000    0.000    0.000 common.py:199(count_not_none)\n",
       "        8    0.000    0.000    0.000    0.000 base.py:759(size)\n",
       "        8    0.000    0.000    0.000    0.000 base.py:978(empty)\n",
       "        6    0.000    0.000    0.000    0.000 base.py:658(__array__)\n",
       "        8    0.000    0.000    0.000    0.000 base.py:2999(_convert_arr_indexer)\n",
       "        3    0.000    0.000    0.000    0.000 frame.py:3585(reindexer)\n",
       "        3    0.000    0.000    0.000    0.000 generic.py:277(_construct_axes_dict)\n",
       "       10    0.000    0.000    0.000    0.000 generic.py:3205(_check_setitem_copy)\n",
       "        4    0.000    0.000    0.000    0.000 generic.py:4378(_needs_reindex_multi)\n",
       "        2    0.000    0.000    0.000    0.000 api.py:87(_get_combined_index)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:431(_chk_truncate)\n",
       "        4    0.000    0.000    0.000    0.000 format.py:945(_format)\n",
       "        1    0.000    0.000    0.001    0.001 common.py:314(_get_handle)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _csv.writer}\n",
       "       16    0.000    0.000    0.000    0.000 series.py:483(_formatting_values)\n",
       "       14    0.000    0.000    0.000    0.000 series.py:591(__len__)\n",
       "        2    0.000    0.000    0.000    0.000 concat.py:464(_get_comb_axis)\n",
       "       12    0.000    0.000    0.000    0.000 merge.py:799(<lambda>)\n",
       "        8    0.000    0.000    0.000    0.000 merge.py:1738(_any)\n",
       "        1    0.000    0.000    0.001    0.001 csvs.py:272(_save)\n",
       "       35    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
       "       25    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}\n",
       "       60    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'getvalue' of '_io.StringIO' objects}\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:663(_load_unlocked)\n",
       "       12    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:792(find_spec)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:523(_compile_bytecode)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:574(spec_from_file_location)\n",
       "        6    0.000    0.000    0.000    0.000 parse.py:109(_coerce_args)\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method numpy.can_cast}\n",
       "       11    0.000    0.000    0.000    0.000 shape_base.py:209(_arrays_for_stack_dispatcher)\n",
       "       12    0.000    0.000    0.000    0.000 base.py:475(<genexpr>)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:1024(to_native_types)\n",
       "       12    0.000    0.000    0.000    0.000 base.py:2249(_validate_sort_keyword)\n",
       "        8    0.000    0.000    0.000    0.000 base.py:3035(_convert_list_indexer)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:4025(_concat_same_dtype)\n",
       "       12    0.000    0.000    0.000    0.000 base.py:5381(_ensure_has_len)\n",
       "        5    0.000    0.000    0.000    0.000 concat.py:151(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 concat.py:496(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 concat.py:503(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 generic.py:334(<dictcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 generic.py:1578(_is_label_or_level_reference)\n",
       "       24    0.000    0.000    0.000    0.000 generic.py:1693(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 api.py:73(_get_distinct_objs)\n",
       "        4    0.000    0.000    0.000    0.000 range.py:89(ensure_int)\n",
       "       11    0.000    0.000    0.000    0.000 blocks.py:3146(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:684(get_slice)\n",
       "        6    0.000    0.000    0.000    0.000 construction.py:210(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:260(_infer_compression)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'writerow' of '_csv.writer' objects}\n",
       "        4    0.000    0.000    0.000    0.000 merge.py:1011(_validate_specification)\n",
       "        2    0.000    0.000    0.000    0.000 <ipython-input-5-b9d6816890d4>:8(__init__)\n",
       "        6    0.000    0.000    0.000    0.000 <ipython-input-4-d36ff41301f6>:10(get_system_type)\n",
       "       29    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
       "       17    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:504(_init_module_attrs)\n",
       "       12    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:719(find_spec)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:438(_classify_pyc)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:722(exec_module)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1351(_get_spec)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:1433(<setcomp>)\n",
       "        5    0.000    0.000    0.000    0.000 codecs.py:186(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 _collections_abc.py:271(__subclasshook__)\n",
       "       21    0.000    0.000    0.000    0.000 _collections_abc.py:302(__subclasshook__)\n",
       "        3    0.000    0.000    0.000    0.000 parse.py:361(urlparse)\n",
       "       14    0.000    0.000    0.000    0.000 common.py:1195(<lambda>)\n",
       "        2    0.000    0.000    0.000    0.000 ops.py:43(get_op_result_name)\n",
       "        4    0.000    0.000    0.002    0.001 _decorators.py:195(wrapper)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:4012(<setcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:4071(identical)\n",
       "        5    0.000    0.000    0.000    0.000 base.py:5398(<genexpr>)\n",
       "       10    0.000    0.000    0.000    0.000 concat.py:120(is_nonempty)\n",
       "       10    0.000    0.000    0.000    0.000 concat.py:136(<genexpr>)\n",
       "        4    0.000    0.000    0.000    0.000 frame.py:3416(_ensure_valid_index)\n",
       "        3    0.000    0.000    0.000    0.000 generic.py:178(_validate_dtype)\n",
       "        2    0.000    0.000    0.000    0.000 generic.py:7083(isnull)\n",
       "        1    0.000    0.000    0.000    0.000 api.py:262(<setcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 blocks.py:323(concat_same_type)\n",
       "        6    0.000    0.000    0.000    0.000 managers.py:730(<listcomp>)\n",
       "       20    0.000    0.000    0.000    0.000 managers.py:1814(<lambda>)\n",
       "        6    0.000    0.000    0.000    0.000 managers.py:1850(_shape_compat)\n",
       "       14    0.000    0.000    0.000    0.000 concat.py:450(_next_or_none)\n",
       "       12    0.000    0.000    0.000    0.000 format.py:1139(<lambda>)\n",
       "        3    0.000    0.000    0.000    0.000 format.py:1434(_has_names)\n",
       "        6    0.000    0.000    0.000    0.000 common.py:94(_expand_user)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:179(get_filepath_or_buffer)\n",
       "       10    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:318(__exit__)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:369(_get_cached)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:471(_validate_timestamp_pyc)\n",
       "        1    0.000    0.000    0.000    0.000 warnings.py:419(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 <ipython-input-3-56b4f6f86910>:9(__init__)\n",
       "        7    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.normalize_axis_index}\n",
       "        2    0.000    0.000    0.000    0.000 config.py:170(get_default_val)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method pandas._libs.lib.is_datetime_array}\n",
       "        1    0.000    0.000    0.000    0.000 console.py:149(in_ipython_frontend)\n",
       "       10    0.000    0.000    0.000    0.000 common.py:183(_any_not_none)\n",
       "        1    0.000    0.000    0.006    0.006 base.py:48(__str__)\n",
       "        6    0.000    0.000    0.000    0.000 base.py:100(_reset_cache)\n",
       "       19    0.000    0.000    0.000    0.000 base.py:704(ndim)\n",
       "        2    0.000    0.000    0.000    0.000 base.py:983(format)\n",
       "        6    0.000    0.000    0.000    0.000 base.py:1580(is_monotonic)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:2897(_convert_slice_indexer)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:5393(_trim_front)\n",
       "        5    0.000    0.000    0.000    0.000 concat.py:126(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 frame.py:491(shape)\n",
       "        1    0.000    0.000    0.000    0.000 frame.py:606(_info_repr)\n",
       "        2    0.000    0.000    0.000    0.000 generic.py:337(_from_axes)\n",
       "        4    0.000    0.000    0.000    0.000 generic.py:1765(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 generic.py:1778(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 api.py:44(_get_objs_combined_axis)\n",
       "        1    0.000    0.000    0.000    0.000 api.py:128(_union_indexes)\n",
       "        1    0.000    0.000    0.000    0.000 api.py:205(_sanitize_and_check)\n",
       "        2    0.000    0.000    0.000    0.000 range.py:180(_int64index)\n",
       "        2    0.000    0.000    0.000    0.000 range.py:330(equals)\n",
       "        1    0.000    0.000    0.000    0.000 indexing.py:2170(_get_slice_axis)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:226(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:225(get_dtypes)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:736(as_array)\n",
       "        2    0.000    0.000    0.000    0.000 managers.py:1796(_simple_blockify)\n",
       "        2    0.000    0.000    0.000    0.000 format.py:789(show_row_idx_names)\n",
       "        9    0.000    0.000    0.000    0.000 format.py:1004(_value_formatter)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:76(_is_url)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:544(UnicodeWriter)\n",
       "        1    0.000    0.000    0.000    0.000 construction.py:509(sanitize_index)\n",
       "        5    0.000    0.000    0.000    0.000 concat.py:510(<genexpr>)\n",
       "       12    0.000    0.000    0.000    0.000 merge.py:800(<lambda>)\n",
       "        5    0.000    0.000    0.000    0.000 {method 'pop' of 'list' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {method 'setdefault' of 'dict' objects}\n",
       "        7    0.000    0.000    0.000    0.000 {built-in method posix.fspath}\n",
       "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:51(_r_long)\n",
       "        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:403(cached)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:576(module_from_spec)\n",
       "        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:62(_path_split)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:951(path_stats)\n",
       "        3    0.000    0.000    0.000    0.000 _collections_abc.py:252(__subclasshook__)\n",
       "        3    0.000    0.000    0.000    0.000 _collections_abc.py:349(__subclasshook__)\n",
       "        1    0.000    0.000    0.000    0.000 warnings.py:20(_showwarnmsg_impl)\n",
       "        1    0.000    0.000    0.000    0.000 linecache.py:37(getlines)\n",
       "       22    0.000    0.000    0.000    0.000 numerictypes.py:655(<listcomp>)\n",
       "       32    0.000    0.000    0.000    0.000 numerictypes.py:587(<listcomp>)\n",
       "       17    0.000    0.000    0.000    0.000 _methods.py:26(_amax)\n",
       "        8    0.000    0.000    0.000    0.000 _methods.py:34(_sum)\n",
       "        1    0.000    0.000    0.000    0.000 _methods.py:48(_count_reduce_items)\n",
       "       12    0.000    0.000    0.000    0.000 {method 'get_count' of 'pandas._libs.hashtable.Int64Factorizer' objects}\n",
       "       15    0.000    0.000    0.000    0.000 printing.py:62(_join_unicode)\n",
       "        1    0.000    0.000    0.000    0.000 console.py:90(in_interactive_session)\n",
       "        6    0.000    0.000    0.000    0.000 common.py:164(<genexpr>)\n",
       "       12    0.000    0.000    0.000    0.000 common.py:201(<genexpr>)\n",
       "        3    0.000    0.000    0.001    0.000 algorithms.py:835(rank)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:3072(_can_reindex)\n",
       "        4    0.000    0.000    0.000    0.000 concat.py:92(<genexpr>)\n",
       "        4    0.000    0.000    0.000    0.000 concat.py:97(<genexpr>)\n",
       "       10    0.000    0.000    0.000    0.000 concat.py:137(<genexpr>)\n",
       "        3    0.000    0.000    0.000    0.000 generic.py:279(<dictcomp>)\n",
       "       24    0.000    0.000    0.000    0.000 generic.py:1627(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:5250(values)\n",
       "        2    0.000    0.000    0.000    0.000 api.py:67(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 indexing.py:148(_slice)\n",
       "        6    0.000    0.000    0.000    0.000 managers.py:376(<dictcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 format.py:943(<lambda>)\n",
       "        2    0.000    0.000    0.000    0.000 series.py:4209(isnull)\n",
       "        2    0.000    0.000    0.000    0.000 concat.py:507(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 concat.py:434(_get_result_dim)\n",
       "        1    0.000    0.000    0.000    0.000 <ipython-input-3-56b4f6f86910>:14(corpus_config)\n",
       "        2    0.000    0.000    0.000    0.000 <ipython-input-5-b9d6816890d4>:20(get_system_type)\n",
       "        1    0.000    0.000    0.293    0.293 <string>:1(<module>)\n",
       "        8    0.000    0.000    0.000    0.000 csvs.py:112(<genexpr>)\n",
       "        4    0.000    0.000    0.000    0.000 {method 'insert' of 'list' objects}\n",
       "        6    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _imp.is_builtin}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _imp._fix_co_filename}\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:35(_new_module)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:307(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:311(__enter__)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:424(has_location)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:84(_path_is_mode_type)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:93(_path_isfile)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:401(_check_name_wrapper)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:884(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 warnings.py:96(_showwarnmsg)\n",
       "        1    0.000    0.000    0.000    0.000 warnings.py:117(_formatwarnmsg)\n",
       "        1    0.000    0.000    0.000    0.000 linecache.py:15(getline)\n",
       "        3    0.000    0.000    0.000    0.000 <string>:1(__new__)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'timestamp' of 'datetime.datetime' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'mean' of 'numpy.ndarray' objects}\n",
       "        4    0.000    0.000    0.000    0.000 {method 'get_count' of 'pandas._libs.hashtable.Factorizer' objects}\n",
       "        2    0.000    0.000    0.000    0.000 common.py:162(_not_none)\n",
       "        2    0.000    0.000    0.000    0.000 common.py:175(_all_none)\n",
       "        4    0.000    0.000    0.000    0.000 base.py:4683(_maybe_cast_indexer)\n",
       "        3    0.000    0.000    0.000    0.000 base.py:4698(_validate_indexer)\n",
       "        4    0.000    0.000    0.000    0.000 concat.py:93(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:1818(__iter__)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:3110(_is_view)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:5337(get_values)\n",
       "        1    0.000    0.000    0.000    0.000 api.py:243(_get_consensus_names)\n",
       "        2    0.000    0.000    0.000    0.000 range.py:176(_data)\n",
       "        8    0.000    0.000    0.000    0.000 indexing.py:937(_convert_for_reindex)\n",
       "        1    0.000    0.000    0.000    0.000 blocks.py:136(is_view)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:627(is_view)\n",
       "        1    0.000    0.000    0.000    0.000 managers.py:2058(<listcomp>)\n",
       "        3    0.000    0.000    0.000    0.000 concat.py:383(<genexpr>)\n",
       "        2    0.000    0.000    0.000    0.000 format.py:781(has_index_names)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:785(has_column_names)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:795(show_col_idx_names)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:816(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:163(is_s3_url)\n",
       "        1    0.000    0.000    0.000    0.000 common.py:171(is_gcs_url)\n",
       "        8    0.000    0.000    0.000    0.000 merge.py:1742(validate_operand)\n",
       "        1    0.000    0.000    0.000    0.000 csvs.py:28(CSVFormatter)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method from_bytes}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'find' of 'str' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}\n",
       "        4    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:321(<genexpr>)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:719(create_module)\n",
       "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap_external>:909(get_filename)\n",
       "        6    0.000    0.000    0.000    0.000 parse.py:98(_noop)\n",
       "        1    0.000    0.000    0.000    0.000 parse.py:394(_checknetloc)\n",
       "        1    0.000    0.000    0.000    0.000 interactiveshell.py:698(get_ipython)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        8    0.000    0.000    0.000    0.000 fromnumeric.py:2847(ndim)\n",
       "        1    0.000    0.000    0.000    0.000 __init__.py:275(u)\n",
       "        2    0.000    0.000    0.000    0.000 config.py:578(_get_registered_option)\n",
       "        1    0.000    0.000    0.000    0.000 base.py:5399(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 generic.py:1775(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 generic.py:1904(__array__)\n",
       "        1    0.000    0.000    0.000    0.000 api.py:226(<setcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 range.py:165(_validate_dtype)\n",
       "        1    0.000    0.000    0.000    0.000 indexing.py:2700(need_slice)\n",
       "        1    0.000    0.000    0.000    0.000 blocks.py:327(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method pandas._libs.internals.slice_len}\n",
       "        1    0.000    0.000    0.000    0.000 format.py:351(should_show_dimensions)\n",
       "        1    0.000    0.000    0.000    0.000 format.py:648(<listcomp>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%time\n",
    "def partly_unordered_permutations(lst, k):\n",
    "    elems = set(lst)\n",
    "    for c in combinations(lst, k):\n",
    "        for d in permutations(elems - set(c)):\n",
    "            yield c + d\n",
    "            \n",
    "def main():\n",
    "    \n",
    "    #rtype = int(input(\"Run: 1->Single systems; 2->Ensemble; 3->Tests; 4-> MM Test\"))\n",
    "   \n",
    "    '''\n",
    "        corpora: i2b2, mipacq, fv017\n",
    "        analyses: entity only (exact span), cui by document, full (aka (entity and cui on exaact span/exact cui)\n",
    "        systems: ctakes, biomedicus, clamp, metamap, quick_umls\n",
    "        \n",
    "        TODO -> Vectorization (entity only and full):\n",
    "                add switch for use of TN on single system performance evaluations \n",
    "                add switch for overlap matching versus exact span\n",
    "             -> Other tasks besides concept extraction\n",
    "        \n",
    "    ''' \n",
    "    analysisConf =  AnalysisConfig()\n",
    "    print(analysisConf.systems, analysisConf.corpus_config())\n",
    "    \n",
    "    if (rtype == 1):\n",
    "        generate_metrics(analysis_type, corpus)\n",
    "    elif (rtype == 2):\n",
    "        \n",
    "        systems = ['ctakes','biomedicus','clamp','metamap','quick_umls']\n",
    "        \n",
    "        #for corpus in corpora:\n",
    "        for l in partly_unordered_permutations(systems, 2):\n",
    "            print('corpus:', corpus, 'systems', l)\n",
    "            run_ensemble(l, analysis_type, corpus) \n",
    "            \n",
    "    elif (rtype == 3):\n",
    "        systems = ['biomedicus']\n",
    "        t = ['concept_jaccard_score_false']\n",
    "        test_systems(analysis_type, systems, corpus)  \n",
    "        test_count(analysis_type, corpus)\n",
    "        test_ensemble(analysis_type, corpus)\n",
    "    elif (rtype == 4):\n",
    "        generate_metrics_test(analysis_type, corpus)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    %prun main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
