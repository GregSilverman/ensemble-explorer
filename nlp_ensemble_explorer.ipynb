{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n  Copyright (c) 2019 Regents of the University of Minnesota.\\n \\n  Licensed under the Apache License, Version 2.0 (the \"License\");\\n  you may not use this file except in compliance with the License.\\n  You may obtain a copy of the License at\\n \\n      http://www.apache.org/licenses/LICENSE-2.0\\n \\n  Unless required by applicable law or agreed to in writing, software\\n  distributed under the License is distributed on an \"AS IS\" BASIS,\\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n  See the License for the specific language governing permissions and\\n  limitations under the License.\\n '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "  Copyright (c) 2019 Regents of the University of Minnesota.\n",
    " \n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    " \n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License.\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pymysql\n",
    "import time \n",
    "import functools as ft\n",
    "import glob   \n",
    "import operator as op\n",
    "import shelve\n",
    "from itertools import combinations, product\n",
    "from sqlalchemy.engine import create_engine\n",
    "from datetime import datetime\n",
    "from ast import literal_eval\n",
    "from scipy import stats  \n",
    "from scipy.stats.mstats import gmean\n",
    "from pythonds.basic.stack import Stack\n",
    "from pythonds.trees.binaryTree import BinaryTree\n",
    "from collections import defaultdict\n",
    "from typing import List, Set, Tuple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls'] ('analytical_cui_mipacq_concepts.csv', 'test.mipacq_all')\n"
     ]
    }
   ],
   "source": [
    "# config class for analysis\n",
    "class AnalysisConfig(object):\n",
    "    \"\"\"\n",
    "    Configuration object:\n",
    "    notes by test, full per corpus\n",
    "    paths by output, gold and system location\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self = self    \n",
    "       \n",
    "        self.systems = ['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls']\n",
    "        #self.systems = ['biomedicus', 'clamp', 'ctakes', 'metamap']\n",
    "        #self.systems = ['metamap']\n",
    "        #self.systems = ['biomedicus']\n",
    "        #self.systems = ['quick_umls']\n",
    "        self.data_dir = '/Users/gms/development/nlp/nlpie/data/amicus-u01/output/'\n",
    "    \n",
    "    def corpus_config(self, corpus):\n",
    "        \n",
    "        if corpus == 'mipacq':\n",
    "            usys_data = 'analytical_cui_mipacq_concepts.csv'\n",
    "            ref_data = 'test.mipacq_all'\n",
    "        elif corpus == 'i2b2':\n",
    "            usys_data = 'analytical_cui_i2b2_concepts.csv'\n",
    "            ref_data = 'test.i2b2_all'\n",
    "            \n",
    "        return usys_data, ref_data\n",
    "        \n",
    "corpus = 'mipacq'\n",
    "analysisConf =  AnalysisConfig()\n",
    "print(analysisConf.systems, analysisConf.corpus_config(corpus))\n",
    "usys, ref = analysisConf.corpus_config(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation class for UIMA systems\n",
    "class AnnotationSystems(object):\n",
    "    \"\"\"   \n",
    "    CAS XMI Annotations of interest\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\" \n",
    "        annotation base types\n",
    "        \"\"\"   \n",
    "        \n",
    "        self.biomedicus_dir = \"biomedicus_out/\"\n",
    "        self.biomedicus_types = [\"biomedicus.v2.UmlsConcept\"]\n",
    "                                  #\"biomedicus.v2.Negated\"\n",
    "                                 #\"biomedicus.v2.Acronym\",\n",
    "                                 #\"biomedicus.v2.DictionaryTerm\",\n",
    "                                 #\"biomedicus.v2.Historical\"]\n",
    "        \n",
    "        self.clamp_dir = \"clamp_out/\"\n",
    "        self.clamp_types = [\"edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA\"]\n",
    "                             #\"org.apache.ctakes.typesystem.type.syntax.ConllDependencyNode\",\n",
    "                             #\"edu.uth.clamp.nlp.typesystem.ClampRelationUIMA\"]    \n",
    "        \n",
    "        self.ctakes_dir = \"ctakes_out/\"\n",
    "        self.ctakes_types = ['ctakes_mentions_all']#\"org.apache.ctakes.typesystem.type.textspan.Sentence\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.DiseaseDisorderMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.MedicationMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.ProcedureMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.refsem.UmlsConcept\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.SignSymptomMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.AnatomicalSiteMention\"]\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.MeasurementAnnotation\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.EventMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.EntityMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.Predicate\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.syntax.WordToken\"]\n",
    "        \n",
    "        self.metamap_dir = \"metamap_out/\"\n",
    "        self.metamap_types = [#\"org.metamap.uima.ts.Utterance\",\n",
    "                              #\"org.metamap.uima.ts.Span\",\n",
    "                              #\"org.metamap.uima.ts.Phrase\"]\n",
    "                              \"org.metamap.uima.ts.Candidate\"]\n",
    "                              #\"org.metamap.uima.ts.CuiConcept\",\n",
    "                              #\"org.metamap.uima.ts.Negation\"]\n",
    "                \n",
    "        self.quick_umls_types = [#'concept']#,\n",
    "                                #'concept_cosine_length_false',\n",
    "                                #'concept_cosine_length_true',\n",
    "                                #'concept_cosine_score_false',\n",
    "                                #'concept_cosine_score_true',\n",
    "                                #'concept_dice_length_false',\n",
    "                                #'concept_dice_length_true',\n",
    "                                #'concept_dice_score_false',\n",
    "                                #'concept_dice_score_true',\n",
    "                                #'concept_jaccard_length_false',\n",
    "                                #'concept_jaccard_length_true',\n",
    "                                'concept_jaccard_score_False']\n",
    "                                #'concept_jaccard_score_true']\n",
    "                \n",
    "        \n",
    "        '''\n",
    "\n",
    "        self.biomedicus_dir = \"biomedicus_out/\"\n",
    "        self.biomedicus_types = [#\"biomedicus.v2.UmlsConcept\"]\n",
    "                                  #\"biomedicus.v2.Negated\"\n",
    "                                 \"biomedicus.v2.Acronym\",\n",
    "                                 \"biomedicus.v2.DictionaryTerm\",\n",
    "                                 \"biomedicus.v2.Historical\"]\n",
    "        \n",
    "        \n",
    "        self.clamp_dir = \"clamp_out/\"\n",
    "        #self.clamp_types = [#\"edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA\"]\n",
    "                             #\"org.apache.ctakes.typesystem.type.syntax.ConllDependencyNode\",\n",
    "                             #\"edu.uth.clamp.nlp.typesystem.ClampRelationUIMA\"]\n",
    "        \n",
    "        \n",
    "        self.ctakes_dir = \"ctakes_out/\"\n",
    "        self.ctakes_types = [\"org.apache.ctakes.typesystem.type.textspan.Sentence\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.DiseaseDisorderMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.MedicationMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.ProcedureMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.refsem.UmlsConcept\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.SignSymptomMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.AnatomicalSiteMention\"]\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.MeasurementAnnotation\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.EventMention\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.EntityMention\",\n",
    "                             \"org.apache.ctakes.typesystem.type.textsem.Predicate\",\n",
    "                             \"org.apache.ctakes.typesystem.type.syntax.WordToken\"]\n",
    "        \n",
    "        self.metamap_dir = \"metamap_out/\"\n",
    "        self.metamap_types = [\"org.metamap.uima.ts.Utterance\",\n",
    "                              \"org.metamap.uima.ts.Span\",\n",
    "                              \"org.metamap.uima.ts.Phrase\"]\n",
    "                              #\"org.metamap.uima.ts.Candidate\"]\n",
    "                              #\"org.metamap.uima.ts.CuiConcept\",\n",
    "                              #\"org.metamap.uima.ts.Negation\"]\n",
    "                              \n",
    "        '''\n",
    "       \n",
    "    def get_system_type(self, system):\n",
    "        \n",
    "        \"\"\"\n",
    "        return system types\n",
    "        \"\"\"\n",
    "        \n",
    "        if system == \"biomedicus\":\n",
    "            view = \"Analysis\"\n",
    "        else:\n",
    "            view = \"_InitialView\"\n",
    "\n",
    "        if system == 'biomedicus':\n",
    "            types = self.biomedicus_types\n",
    "            output = self.biomedicus_dir\n",
    "\n",
    "        elif system == 'clamp':\n",
    "            types = self.clamp_types\n",
    "            output = self.clamp_dir\n",
    "\n",
    "        elif system == 'ctakes':\n",
    "            types = self.ctakes_types\n",
    "            output = self.ctakes_dir\n",
    "\n",
    "        elif system == 'metamap':\n",
    "            types = self.metamap_types\n",
    "            output = self.metamap_dir\n",
    "        \n",
    "        elif system == \"quick_umls\":\n",
    "            types = self.quick_umls_types\n",
    "            output = None\n",
    "            \n",
    "        return types, view, output\n",
    "    \n",
    "annSys = AnnotationSystems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qumls = pd.read_csv('/Users/gms/development/nlp/nlpie/data/amicus-u01/output/qumls_similarity.csv')\n",
    "\n",
    "# #print(len(qumls))\n",
    "# #print(len(qumls.drop_duplicates()))\n",
    "\n",
    "# cols_to_keep = ['begin', 'end', 'note_id', 'cui', 'similarity', 'overlap', 'best_match', 'system', 'type'] \n",
    "\n",
    "# qumls = qumls[cols_to_keep].drop_duplicates()\n",
    "\n",
    "# #print(len(qumls))\n",
    "\n",
    "# #print(len(qumls[['overlap']=='score']))\n",
    "# print(len(qumls[(qumls.similarity=='jaccard') & (qumls.overlap=='length') & (qumls.best_match=='true')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='jaccard') & (qumls.overlap=='length') & (qumls.best_match=='false')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='jaccard') & (qumls.overlap=='score') & (qumls.best_match=='true')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='jaccard') & (qumls.overlap=='score') & (qumls.best_match=='false')].drop_duplicates()))\n",
    "\n",
    "# print(len(qumls[(qumls.similarity=='cosine') & (qumls.overlap=='length') & (qumls.best_match=='true')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='cosine') & (qumls.overlap=='length') & (qumls.best_match=='false')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='cosine') & (qumls.overlap=='score') & (qumls.best_match=='true')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='cosine') & (qumls.overlap=='score') & (qumls.best_match=='false')].drop_duplicates()))\n",
    "\n",
    "# print(len(qumls[(qumls.similarity=='dice') & (qumls.overlap=='length') & (qumls.best_match=='true')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='dice') & (qumls.overlap=='length') & (qumls.best_match=='false')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='dice') & (qumls.overlap=='score') & (qumls.best_match=='true')].drop_duplicates()))\n",
    "# print(len(qumls[(qumls.similarity=='dice') & (qumls.overlap=='score') & (qumls.best_match=='false')].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0595040941-0', '0778429553-0', '1014681675', '2889522952-2', '3080383448-5', '3300000926-3', '3360037185-3', '3580973392', '3627629462-3', '4323116051-4', '477704053-4', '528317073', '531702602', '534061073', '54832076', '5643725437-6', '5944412090-5', '6613169476-6', '7261075903-7', '7504944368-7', '7999462393-7', '8131081430', '8171084310', '8193787896', '8295055184-8', '8823185307-8']\n"
     ]
    }
   ],
   "source": [
    "def get_notes(analysis_type: str, corpus: str) -> List[str]:\n",
    "    \n",
    "    if 'test' in analysis_type:\n",
    "        # test set of notes\n",
    "        if corpus == 'mipacq':\n",
    "            notes = ['522412787',\n",
    "             '617637585',\n",
    "             '3307880735-8',\n",
    "             '9080688558',\n",
    "             '618370565',\n",
    "             '573718188',\n",
    "             '534584',\n",
    "             '60891',\n",
    "             '62620',\n",
    "             '616172834']\n",
    "            \n",
    "        elif corpus == 'i2b2':\n",
    "            print('TODO')\n",
    "        \n",
    "        print('TEST NOTES!')\n",
    "        #,\n",
    "#          '4130154172-4',\n",
    "#          '3580478614',\n",
    "#          '5024581165-5',\n",
    "#          '4486835700-9',\n",
    "#          '534828617',\n",
    "#          '8154986253',\n",
    "#          '533855209',\n",
    "#          '60118',\n",
    "#          '3537704982-3',\n",
    "#          '617637585',\n",
    "#          '60118',\n",
    "#          '9045889026',\n",
    "#          '8819868493-8',\n",
    "#          '533698',\n",
    "#          '535978760']\n",
    "     \n",
    "    else:\n",
    "        \n",
    "        if corpus == 'mipacq':\n",
    "        # these did not meet the minimal criteria for parsing\n",
    "            notes = [\"0595040941-0\",\n",
    "                    \"0778429553-0\",\n",
    "                    \"1014681675\",\n",
    "                    \"2889522952-2\",\n",
    "                    \"3080383448-5\",\n",
    "                    \"3300000926-3\",\n",
    "                    \"3360037185-3\",\n",
    "                    \"3580973392\",\n",
    "                    \"3627629462-3\",\n",
    "                    \"4323116051-4\",\n",
    "                    \"477704053-4\",\n",
    "                    \"528317073\",\n",
    "                    \"531702602\",\n",
    "                    \"534061073\",\n",
    "                    \"54832076\",\n",
    "                    \"5643725437-6\",\n",
    "                    \"5944412090-5\",\n",
    "                    \"6613169476-6\",\n",
    "                    \"7261075903-7\",\n",
    "                    \"7504944368-7\",\n",
    "                    \"7999462393-7\",\n",
    "                    \"8131081430\",\n",
    "                    \"8171084310\",\n",
    "                    \"8193787896\",\n",
    "                    \"8295055184-8\",\n",
    "                    \"8823185307-8\"]\n",
    "            \n",
    "        elif corpus == 'i2b2':\n",
    "            # these notes were not processed \n",
    "            notes = ['0081', \n",
    "                     '0401']\n",
    "\n",
    "    return notes# training_notes\n",
    "print(get_notes('entity', corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np # access to Numpy from Python layer\n",
    "import math\n",
    "\n",
    "class Metrics(object):\n",
    "    \"\"\"\n",
    "    metrics class:\n",
    "    returns an instance with confusion matrix metrics\n",
    "    \"\"\"\n",
    "    def __init__(self, system_only, gold_only, gold_system_match, system_n, neither = 0):\n",
    "\n",
    "        self = self    \n",
    "        self.system_only = system_only\n",
    "        self.gold_only = gold_only\n",
    "        self.gold_system_match = gold_system_match\n",
    "        self.system_n = system_n\n",
    "        self.neither = neither\n",
    "        \n",
    "    def get_confusion_metrics(self, test = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        compute confusion matrix measures, as per  \n",
    "        https://stats.stackexchange.com/questions/51296/how-do-you-calculate-precision-and-recall-for-multiclass-classification-using-co\n",
    "        \"\"\"\n",
    "        cdef:\n",
    "            int TP, FP, FN\n",
    "            double TM\n",
    "\n",
    "        TP = self.gold_system_match\n",
    "        FP = self.system_only\n",
    "        FN = self.gold_only\n",
    "        TM = TP/math.sqrt(self.system_n) # TigMetric\n",
    "        \n",
    "        if not test:\n",
    "            if self.neither == 0:\n",
    "                confusion = [[0, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "            else:\n",
    "                confusion = [[self.neither, self.system_only],[self.gold_only,self.gold_system_match]]\n",
    "            c = np.asarray(confusion)\n",
    "            recall = np.diag(c) / np.sum(c, axis = 1)\n",
    "            precision = np.diag(c) / np.sum(c, axis = 0)\n",
    "            F = 2*(precision*recall)/(precision + recall)\n",
    "        else:\n",
    "            precision = TP/(TP+FP)\n",
    "            recall = TP/(TP+FN)\n",
    "            F = 2*(precision*recall)/(precision + recall)\n",
    "        \n",
    "        # Tignanelli Metric\n",
    "        if FN == 0:\n",
    "            TP_FN_R = TP\n",
    "        elif FN > 0:\n",
    "            TP_FN_R = TP/FN\n",
    " \n",
    "        return F, recall, precision, TP, FP, FN, TP_FN_R, TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out(name: str, analysis_type: str, c: object):\n",
    "   \n",
    "    \"\"\"\n",
    "    write matching and reference-only sets to file for ease in merging combinations\n",
    "    \"\"\"\n",
    "    \n",
    "    # write output to file\n",
    "    dir_out = analysisConf.data_dir + 'single_system_out/'\n",
    "    with open(dir_out + name + '_' + analysis_type + '_' + c.corpus + '_matches.txt', 'w') as f:\n",
    "        for item in list(c.matches):\n",
    "            f.write(\"%s\\n\" % str(item))\n",
    "\n",
    "    # write to file\n",
    "    with open(dir_out + name + '_' + analysis_type + '_' + c.corpus + '_ref_only.txt', 'w') as f:\n",
    "        for item in list(c.false_negatives):\n",
    "            f.write(\"%s\\n\" % str(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%cython \n",
    "\n",
    "#from __main__ import write_out\n",
    "\n",
    "#import numpy as np # access to Numpy from Python layer\n",
    "def label_vector(doc: str, ann: List[int], labels: List[str]) -> np.array:\n",
    "\n",
    "    #print(ann, doc, labels)\n",
    "\n",
    "    v = np.zeros(doc)\n",
    "    labels = list(labels)\n",
    "    \n",
    "    for (i, lab) in enumerate(labels):\n",
    "        i += 1  # 0 is reserved for no label\n",
    "        idxs = [np.arange(a.begin, a.end) for a in ann if a.label == lab]\n",
    "            \n",
    "        idxs = [j for mask in idxs for j in mask]\n",
    "        v[idxs] = i\n",
    "\n",
    "    return v\n",
    "\n",
    "# test confusion matrix elements for vectorized annotation set; includes TN\n",
    "def confused(sys1, ann1):\n",
    "    TP = np.sum(np.logical_and(sys1 >= 1, ann1 == sys1 ))\n",
    "\n",
    "    # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "    TN = np.sum(np.logical_and(sys1 == 0, ann1 == 0))\n",
    "\n",
    "    # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "    FP = np.sum(np.logical_and(sys1 >= 1, ann1 == 0))\n",
    "\n",
    "    # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "    FN = np.sum(np.logical_and(sys1 == 0, ann1 >= 1))\n",
    "    \n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "\n",
    "def get_cooccurences(ref, sys, analysis_type: str, corpus: str, single_sys = True, name = None):\n",
    "    \"\"\"\n",
    "    get coocurences between system and reference; exact match; TODO: add relaxed\n",
    "    \"\"\"\n",
    "    # test cooccurences\n",
    "    class Coocurences(object):\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.ref_system_match = 0\n",
    "            self.ref_only = 0\n",
    "            self.system_only = 0\n",
    "            self.system_n = 0\n",
    "            self.ref_n = 0\n",
    "            self.matches = set()\n",
    "            self.false_negatives = set()\n",
    "            self.corpus = corpus\n",
    "            self.cases = set(ref[\"file\"].tolist()) # cases to label \n",
    "\n",
    "    c = Coocurences()\n",
    "    \n",
    "    # test for converting to vectorization and i-o labeling\n",
    "    def test_io():\n",
    "        test = c.cases\n",
    "        if analysis_type == 'entity':\n",
    "            docs = [(x, len(open(\"/Users/gms/development/nlp/nlpie/data/amicus-u01/i2b2/source_data/test_data/\" + x + \".txt\", 'r').read())) for x in test]\n",
    "        elif analysis_type == 'full':\n",
    "            docs = [(x, len(open(\"/Users/gms/development/nlp/nlpie/data/amicus-u01/mipacq/source_data/source/\" + x + \".source\", 'r').read())) for x in test]\n",
    "\n",
    "        ann = ref.copy()\n",
    "        ann = ann.rename(index=str, columns={\"start\": \"begin\", \"file\": \"case\"}).copy()\n",
    "        cols_to_keep = ['begin', 'end', 'case', 'label']\n",
    "        if analysis_type == 'entity':\n",
    "            labels = [\"concept\"]\n",
    "            ann[\"label\"] = 'concept'\n",
    "            ann = ann[cols_to_keep].copy()\n",
    "        elif analysis_type == 'full':  \n",
    "            ann[\"label\"] = ann[\"value\"]\n",
    "            sys[\"label\"] = sys[\"cui\"]\n",
    "            labels = set(ref['value'].tolist())\n",
    "            print('labels', len(set(labels)))\n",
    "\n",
    "        sys_ = sys.rename(index=str, columns={\"note_id\": \"case\"}).copy()\n",
    "        \n",
    "        # need for enttity-only\n",
    "        if analysis_type == 'entity':\n",
    "            sys_[\"label\"] = 'concept'\n",
    "        \n",
    "        sys_ = sys_[cols_to_keep]\n",
    "       \n",
    "        tp = []\n",
    "        tn = []\n",
    "        fp = []\n",
    "        fn = []\n",
    "        cvals = []\n",
    "        out = []\n",
    "        t = []\n",
    "        d = defaultdict(list)\n",
    "        \n",
    "        for n in range(len(docs)):\n",
    "            a1 = [i for i in ann[ann[\"case\"] == docs[n][0]].copy().itertuples(index=False)]\n",
    "            s1 = [i for i in sys_[sys_[\"case\"] == docs[n][0]].copy().itertuples(index=False)]\n",
    "\n",
    "            ann1 = label_vector(docs[n][1], a1, labels)\n",
    "            sys1 = label_vector(docs[n][1], s1, labels)\n",
    "            \n",
    "            TP, TN, FP, FN = confused(sys1, ann1)\n",
    "            cvals.append([TP, TN, FP, FN])\n",
    "            \n",
    "                 \n",
    "            d['sys'].append(list([int(i) for i in sys1]))\n",
    "            d['oracle'].append(list([int(i) for i in ann1]))\n",
    "            d['case'].append(docs[n][0])\n",
    "            \n",
    "            '''\n",
    "            print(\"tn:\", np.intersect1d(np.where(ann1 == 0)[0], np.where(sys1 == 0)[0]),  \n",
    "                  \"tp:\", np.intersect1d(np.where(ann1 == 1)[0], np.where(sys1 == 1)[0]), \n",
    "                  \"fn:\", np.intersect1d(np.where(ann1 == 1)[0], np.where(sys1 == 0)[0]), \n",
    "                  \"fp:\", np.intersect1d(np.where(ann1 == 0)[0], np.where(sys1 == 1)[0]))\n",
    "            '''\n",
    "        d['labels'] = labels\n",
    "        \n",
    "        corp = shelve.open('/Users/gms/Desktop/' + sys.name + '.dat')\n",
    "        \n",
    "        for k in d:\n",
    "            corp[k] = d[k]\n",
    "        \n",
    "        corp.close()\n",
    "       \n",
    "        return cvals\n",
    "    \n",
    "    TP, TN, FP, FN = np.sum(test_io(), axis=0)\n",
    "    F, recall, precision, TP, FP, FN, TP_FN_R, TM = Metrics(FP, FN, TP, len(sys), TN).get_confusion_metrics() #no TN\n",
    "    print('test_io():', TP, TN, FP, FN, F, recall, precision)\n",
    "    \n",
    "    # non-vectorized:\n",
    "    if 'entity' in analysis_type and single_sys: # mipacq n -> 16793\n",
    "        cols_to_keep = ['begin', 'end', 'note_id']\n",
    "        sys = sys[cols_to_keep].drop_duplicates()\n",
    "        ref = ref[['start', 'end', 'file']].drop_duplicates()\n",
    "        sys.name = name\n",
    "    elif 'cui' in analysis_type and single_sys: # mipacq n -> 10799\n",
    "        cols_to_keep = ['cui', 'note_id']\n",
    "        sys = sys[cols_to_keep].drop_duplicates()\n",
    "        # do not overestimate FP\n",
    "        sys = sys[~sys['cui'].isnull()] \n",
    "        ref = ref[['value', 'file']].drop_duplicates()\n",
    "        ref = ref[~ref['value'].isnull()]\n",
    "        sys.name = name\n",
    "    elif 'full' in analysis_type and single_sys: # mipacq n -> 17393\n",
    "        cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "        sys = sys[cols_to_keep].drop_duplicates()\n",
    "        sys = sys[~sys['cui'].isnull()]\n",
    "        ref = ref[['start', 'end', 'value', 'file']].drop_duplicates()\n",
    "        ref = ref[~ref['value'].isnull()]\n",
    "        sys.name = name\n",
    "    \n",
    "    # matches via inner join\n",
    "    matches = pd.merge(sys, ref, how = 'inner', left_on=['begin','end','note_id'], right_on = ['start','end','file']) \n",
    "    # reference-only via left outer join\n",
    "    fn = pd.merge(ref, sys, how = 'left', left_on=['start','end','file'], right_on = ['begin','end','note_id']) \n",
    "    \n",
    "    fn = fn[fn['begin'].isnull()] # get as outer join with no match\n",
    "    \n",
    "    if 'entity' in analysis_type and single_sys:\n",
    "        cols_to_keep = ['start', 'end', 'file']\n",
    "    else:\n",
    "        cols_to_keep = ['start', 'end', 'value', 'file']\n",
    "        \n",
    "        \n",
    "    matches = matches[cols_to_keep]\n",
    "    fn = fn[cols_to_keep]\n",
    "    \n",
    "    # use for metrics \n",
    "    c.matches = c.matches.union(df_to_set(matches, analysis_type, 'ref'))\n",
    "    c.false_negatives = c.false_negatives.union(df_to_set(fn, analysis_type, 'ref'))\n",
    "    c.ref_system_match = len(c.matches)\n",
    "    c.system_only = len(sys) - len(c.matches)\n",
    "    c.system_n = len(sys)\n",
    "    c.ref_n = len(ref)\n",
    "    c.ref_only = len(c.false_negatives)\n",
    "\n",
    "    # sanity check\n",
    "    if len(ref) - c.ref_system_match < 0:\n",
    "        print('Error: ref_system_match > len(ref)!')\n",
    "    if len(ref) != c.ref_system_match + c.ref_only:\n",
    "        print('Error: ref count mismatch!')\n",
    "   \n",
    "    # save TP/FN\n",
    "    if single_sys:\n",
    "        print(analysis_type)\n",
    "        write_out(sys.name, analysis_type, c)\n",
    "\n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769 µs ± 6.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "768 µs ± 3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# merging test for i-o labeled data\n",
    "\n",
    "import numpy as np\n",
    "import shelve\n",
    "# load shelve\n",
    "def read_shelve():\n",
    "        corp = shelve.open('/Users/gms/Desktop/test.dat')\n",
    "        #print(corp['case'])\n",
    "        \n",
    "        return corp\n",
    "        \n",
    "test = read_shelve()\n",
    "\n",
    "\n",
    "t0 = np.array(test['oracle'][3][0:750])\n",
    "t1 = np.array(test['oracle'][5][0:750])\n",
    "\n",
    "#l0 = list(t0)\n",
    "#l1 = list(t1)\n",
    "\n",
    "def test_merge_vector(test):\n",
    "    # get sample for testing\n",
    "    for case in test['case'][3:5]:\n",
    "        for i in range(len(test['case'][3:5])):\n",
    "            if i == 3:\n",
    "                t0 = test['oracle'][3][0:750]\n",
    "            else:\n",
    "                t1 = test['oracle'][4][0:750]\n",
    "\n",
    "            #print('case:', case, test['sys'][i], test['oracle'][i], confused(np.array(test['sys'][i]), np.array(test['oracle'][i])))\n",
    "        #print(t0, t1)\n",
    "\n",
    "    t0 = np.array(test['oracle'][3][0:750])\n",
    "    t1 = np.array(test['oracle'][5][0:750])\n",
    "\n",
    "    l0 = list(t0)\n",
    "    l1 = list(t1)\n",
    "    \n",
    "    #l0 = [0, 4, 1, 4, 4, 0, 0, 0, 8, 0, 0] \n",
    "    #l1 = [0, 1, 4, 4, 0, 0, 0, 0, 8, 8, 8]\n",
    "\n",
    "    def intersection(lst1, lst2): \n",
    "        out = list()\n",
    "        if isinstance(lst1, set) and isinstance(lst2, set):\n",
    "            out = (set(lst1) & set(lst2))\n",
    "        elif isinstance(lst1, set) and isinstance(lst2, np.int64):\n",
    "            out = (set(lst1) & set([lst2]))\n",
    "        elif isinstance(lst1, np.int64) and isinstance(lst2, set):\n",
    "            out = (set([lst1]) & set(lst2))\n",
    "        elif isinstance(lst1, np.int64) and isinstance(lst2, np.int64):\n",
    "            out = (set([lst1]) & set([lst2]))\n",
    "        #if len(out) > 1:\n",
    "        return out\n",
    "        #elif len(out) == 1:\n",
    "        #    return out[0]\n",
    "        #else:\n",
    "        #    return 0\n",
    "\n",
    "    def union(lst1, lst2): \n",
    "        out = list()\n",
    "        if isinstance(lst1, set) and isinstance(lst2, set):\n",
    "            out = set(lst1) | set(lst2)\n",
    "        elif isinstance(lst1, set) and isinstance(lst2, np.int64):\n",
    "            out = set(lst1) | set([lst2])\n",
    "        elif isinstance(lst1, np.int64) and isinstance(lst2, set):\n",
    "            out = set([lst1]) | set(lst2)\n",
    "        elif isinstance(lst1, np.int64) and isinstance(lst2, np.int64):\n",
    "            out = set([lst1]) | set([lst2])\n",
    "        #if len(out) == 1:\n",
    "        #    #out = out[0]\n",
    "        return out\n",
    "\n",
    "    # union and intersect\n",
    "    def umerges(l0, l1):\n",
    "        #un = [0]*len(l0)\n",
    "        #for i in range(len(l0)):\n",
    "        #    un[i] = union(l0[i], l1[i])\n",
    "\n",
    "        return [union(l0[i], l1[i]) for i in range(len(l0))]\n",
    "\n",
    "    %timeit un = umerges(l0, l1)\n",
    "    \n",
    "    x = umerges(l0, l1)\n",
    "\n",
    "    #l2 = [1, {1, 4}, {3}, {2, 4}, {1}, 0, 2, 3, {0, 8}, {1, 8}]\n",
    "    \n",
    "    #print(umerges(x, l2))\n",
    "    \n",
    "    def imerges(l0, l1):\n",
    "        #inter = [0]*len(l0)\n",
    "        #for i in range(len(l0)):\n",
    "        \n",
    "\n",
    "        return [intersection(l0[i], l1[i]) for i in range(len(l0))]\n",
    "    \n",
    "    %timeit inter = imerges(l0, l1)\n",
    "    \n",
    "    #print(imerges(l0, l1))\n",
    "\n",
    "    #y = imerges(l2, x)\n",
    "    \n",
    "    #print(y)\n",
    "    '''\n",
    "    union = [\n",
    "        ( [set(x) | set(y)] if isinstance(x, list) and isinstance(y, list)\n",
    "          else [set(x) | set([y])] if isinstance(x, list) and isinstance(y, int)\n",
    "          else [set([x]) | set(y)] if isinstance(x, int) and isinstance(y, list)\n",
    "          else [set([x]) | set([y])])\n",
    "\n",
    "         for x, y in zip(l0, l1)\n",
    "    ]\n",
    "\n",
    "    # unpack map object\n",
    "    #*y, = list(map(list, zip(*union)))\n",
    "    #%timeit list(map(list, zip(*union)))\n",
    "\n",
    "    intersection = [\n",
    "        ( [set(x) & set(y)] if isinstance(x, list) and isinstance(y, list)\n",
    "          else [set(x) & set([y])] if isinstance(x, list) and isinstance(y, int)\n",
    "          else [set([x]) & set(y)] if isinstance(x, int) and isinstance(y, list)\n",
    "          else [set([x]) & set([y])])\n",
    "          for x, y in zip(l0, l1)\n",
    "\n",
    "    ]\n",
    "\n",
    "    #*x, = list(map(list, zip(*intersection)))\n",
    "    #%timeit list(map(list, zip(*intersection)))\n",
    "    '''\n",
    "test_merge_vector(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 1: 0.0011352590000015539\n",
      "905 µs ± 7.65 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "time 2: 0.0009650700000030099\n",
      "901 µs ± 6.82 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "655 µs ± 4.97 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "#%%cython\n",
    "import numpy as np # access to Numpy from Python layer\n",
    "import time\n",
    "from __main__ import read_shelve\n",
    "test = read_shelve()\n",
    "t0 = np.array(test['oracle'][3][0:750])\n",
    "t1 = np.array(test['oracle'][5][0:750])\n",
    "\n",
    "l0 = list(t0)\n",
    "l1 = list(t1)\n",
    "\n",
    "#union = [\n",
    "#    [list((set(x) | set(y)))]\n",
    "#      for x, y in zip(t0, t1)\n",
    "#]\n",
    "\n",
    "def imerge(l0, l1):\n",
    "\n",
    "    return [\n",
    "        ( [\n",
    "            set(x) & set(y)] if isinstance(x, list) and  isinstance(y, list)\n",
    "            else [set(x) & set([y])] if isinstance(x, list) and  isinstance(y, np.int64)\n",
    "            else [set(x) & y] if isinstance(x, list) and isinstance(y, set)\n",
    "            else [x & y] if isinstance(x, set) and isinstance(y, set)\n",
    "            else [x & set(y)] if isinstance(x, set) and isinstance(y, list)\n",
    "            else [x & set([y])] if isinstance(x, set) and isinstance(y, np.int64)\n",
    "            else [set([x]) & set(y)] if isinstance(x, np.int64) and  isinstance(y, list)\n",
    "            else [set([x]) & y] if isinstance(x, np.int64) and isinstance(y, set)\n",
    "            else [set([x]) & set([y])])\n",
    "        for x, y in zip(l0, l1)\n",
    "    ]\n",
    "\n",
    "def umerge(l0, l1):\n",
    "\n",
    "    return [\n",
    "        ( [\n",
    "            set(x) | set(y)] if isinstance(x, list) and  isinstance(y, list)\n",
    "            else [set(x) | set([y])] if isinstance(x, list) and  isinstance(y, np.int64)\n",
    "            else [set(x) | y] if isinstance(x, list) and isinstance(y, set)\n",
    "            else [x | y] if isinstance(x, set) and isinstance(y, set)\n",
    "            else [x | set(y)] if isinstance(x, set) and isinstance(y, list)\n",
    "            else [x | set([y])] if isinstance(x, set) and isinstance(y, np.int64)\n",
    "            else [set([x]) | y] if isinstance(x, np.int64) and isinstance(y, set)\n",
    "            else [set([x]) | set(y)] if isinstance(x, np.int64) and  isinstance(y, list)\n",
    "            else [set([x]) | set([y])])\n",
    "        for x, y in zip(l0, l1)\n",
    "    ]\n",
    "\n",
    "start = time.perf_counter()\n",
    "z = *map(list, zip(*umerge(l0, l1))),\n",
    "#print(*map(list, zip(*imerge(l0, l1))),)\n",
    "elapsed = (time.perf_counter() - start)\n",
    "print('time 1:', elapsed)\n",
    "%timeit  *map(list, zip(*umerge(l0, l1))),\n",
    "\n",
    "*map(list, zip(*umerge(z[0], l1))),\n",
    "\n",
    "\n",
    "start = time.perf_counter()\n",
    "z = *map(list, zip(*imerge(l0, l1))),\n",
    "elapsed = (time.perf_counter() - start)\n",
    "print('time 2:', elapsed)\n",
    "%timeit *map(list, zip(*imerge(l0, l1))),\n",
    "\n",
    "%timeit *map(list, zip(*imerge(z[0], l1))),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_data(training_notes: List[str], analysis_type: str, corpus: str):\n",
    "\n",
    "    engine = create_engine('mysql+pymysql://gms:nej123@localhost/test', pool_pre_ping=True, pool_size=20, max_overflow=30)\n",
    "    \n",
    "    usys_file, ref_table = AnalysisConfig().corpus_config(corpus)\n",
    "    systems = AnalysisConfig().systems\n",
    "    \n",
    "    sys_ann = pd.read_csv(analysisConf.data_dir + usys_file, dtype={'note_id': str})\n",
    "    \n",
    "    if 'test' not in analysis_type:\n",
    "        sql = \"SELECT * FROM \" + ref_table + \" where file not in %(training_notes)s\"  \n",
    "        sys_ann = sys_ann[~sys_ann['note_id'].isin(training_notes)]\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        sql = \"SELECT * FROM \" + ref_table + \" where file in %(training_notes)s\"  \n",
    "        sys_ann = sys_ann[sys_ann['note_id'].isin(training_notes)]\n",
    "    \n",
    "    ref_ann = pd.read_sql(sql, params={\"training_notes\":training_notes}, con=engine)\n",
    "    sys_ann = sys_ann.drop_duplicates()\n",
    "    \n",
    "    return ref_ann, sys_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "def geometric_mean(metrics):\n",
    "    \"\"\"\n",
    "    1. Get rank average of F1, TP/FN, TM\n",
    "        http://www.datasciencemadesimple.com/rank-dataframe-python-pandas-min-max-dense-rank-group/\n",
    "        https://stackoverflow.com/questions/46686315/in-pandas-how-to-create-a-new-column-with-a-rank-according-to-the-mean-values-o?rq=1\n",
    "    2. Take geomean of 2.\n",
    "        https://stackoverflow.com/questions/42436577/geometric-mean-applied-on-row\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.DataFrame() \n",
    "\n",
    "    metrics['F1 rank']=metrics['F'].rank(ascending=0,method='average')\n",
    "    metrics['TP/FN rank']=metrics['TP/FN'].rank(ascending=0,method='average')\n",
    "    metrics['TM rank']=metrics['TM'].rank(ascending=0,method='average')\n",
    "    metrics['Gmean'] = gmean(metrics.iloc[:,-3:],axis=1)\n",
    "\n",
    "    return metrics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metrics(analysis_type: str, corpus: str, single_sys = None):\n",
    "    start = time.time()\n",
    "\n",
    "    systems = AnalysisConfig().systems\n",
    "    metrics = pd.DataFrame()\n",
    "    print('corpus:', corpus)\n",
    "\n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    ref_ann, sys_ann = get_metric_data(training_notes, analysis_type, corpus)\n",
    "    \n",
    "    for sys in systems:\n",
    "            types, _, _ = AnnotationSystems().get_system_type(sys) # system types for iterable\n",
    "            for t in types:\n",
    "                print(t)\n",
    "                system = pd.DataFrame()\n",
    "                \n",
    "                system_annotations = sys_ann.copy()\n",
    "                \n",
    "                system = system_annotations[system_annotations['type'] == str(t)]\n",
    "            \n",
    "                if sys == 'quick_umls':\n",
    "                    system = system[system.similarity.astype(float) >= 0.75]\n",
    "            \n",
    "                system = system.drop_duplicates()\n",
    "                system.name = sys\n",
    "                \n",
    "                c = get_cooccurences(ref_ann, system, analysis_type, corpus, True, system.name) # get matches, FN, etc.\n",
    "                \n",
    "                print(c.ref_n, c.ref_only, c.system_n, c.system_only, c.ref_system_match)\n",
    "                \n",
    "            if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "                F, recall, precision, TP, FP, FN, TP_FN_R, TM = Metrics(c.system_only, c.ref_only, c.ref_system_match, c.system_n).get_confusion_metrics()\n",
    "                d = {'system': sys, \n",
    "                     'type': t, \n",
    "                     'F': F[1], \n",
    "                     'precision': precision[1], \n",
    "                     'recall': recall[1], \n",
    "                     'TP': TP, \n",
    "                     'FN': FN, \n",
    "                     'FP': FP, \n",
    "                     'TP/FN': TP_FN_R,\n",
    "                     'n_gold': c.ref_n, \n",
    "                     'n_sys': c.system_n, \n",
    "                     'TM': TM}\n",
    "\n",
    "                data = pd.DataFrame(d,  index=[0])\n",
    "                metrics = pd.concat([metrics, data], ignore_index=True)\n",
    "                metrics.drop_duplicates(keep='last', inplace=True)\n",
    "            else:\n",
    "                print(\"NO EXACT MATCHES FOR\", t)\n",
    "            elapsed = (time.time() - start)\n",
    "            print(\"elapsed:\", sys, elapsed)\n",
    "     \n",
    "    elapsed = (time.time() - start)\n",
    "    print(geometric_mean(metrics))\n",
    "    \n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    \n",
    "    if single_sys is None:\n",
    "        file_name = 'metrics_'\n",
    "    \n",
    "    metrics.to_csv(analysisConf.data_dir + corpus + '_' + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "    \n",
    "    print(\"total elapsed time:\", elapsed) \n",
    "\n",
    "# used to iterate through mm scores\n",
    "def generate_metrics_mm(analysis_type: str, corpus: str, single_sys = None):\n",
    "    start = time.time()\n",
    "    from pandas.api.types import is_numeric_dtype\n",
    "    #systems = [\"biomedicus\",\"ctakes\",\"metamap\",\"clamp\",\"quick_umls\"]\n",
    "    systems = AnalysisConfig().systems\n",
    "    #systems = [\"quick_umls\"]\n",
    "    metrics = pd.DataFrame()\n",
    "    print('corpus:', corpus)\n",
    "\n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    ref_ann, sys_ann = get_metric_data(training_notes, analysis_type, corpus)\n",
    "    \n",
    "    sys_ann = sys_ann[(sys_ann.score.notnull()) & (sys_ann['system'] == 'metamap')]\n",
    "    sys_ann = sys_ann[['begin', 'end', 'note_id', 'system', 'score']].drop_duplicates()\n",
    "    sys_ann.score = sys_ann.score.astype(int)\n",
    "    \n",
    "    for sys in systems:\n",
    "        types, _, _ = AnnotationSystems().get_system_type(sys) # system types for iterable\n",
    "        for t in types:\n",
    "            print(t)\n",
    "\n",
    "            for i in range(500, 1050, 50): \n",
    "\n",
    "                sys_ann = sys_ann[(sys_ann[\"score\"] >= i)].copy()\n",
    "\n",
    "                print('score:', i, len(sys_ann), sys_ann.columns) \n",
    "\n",
    "                sys_ann.name = sys + str(i)\n",
    "\n",
    "                c = get_cooccurences(ref_ann, sys_ann, analysis_type, corpus, True, sys_ann.name) # get matches, FN, etc.\n",
    "\n",
    "                print(c.ref_n, c.ref_only, c.system_n, c.system_only, c.ref_system_match)\n",
    "\n",
    "                #print(i, len(system))\n",
    "\n",
    "                if c.ref_system_match > 0: # compute confusion matrix metrics and write to dictionary -> df\n",
    "                    F, recall, precision, TP, FP, FN, TP_FN_R, TM = Metrics(c.system_only, c.ref_only, c.ref_system_match, c.system_n).get_confusion_metrics()\n",
    "                    d = {'system': sys + '_score_' + str(i), \n",
    "                         'type': t, \n",
    "                         'F': F[1], \n",
    "                         'precision': precision[1], \n",
    "                         'recall': recall[1], \n",
    "                         'TP': TP, \n",
    "                         'FN': FN, \n",
    "                         'FP': FP, \n",
    "                         'TP/FN': TP_FN_R,\n",
    "                         'n_gold': c.ref_n, \n",
    "                         'n_sys': c.system_n, \n",
    "                         'TM': TM}\n",
    "\n",
    "                    data = pd.DataFrame(d,  index=[0])\n",
    "                    metrics = pd.concat([metrics, data], ignore_index=True)\n",
    "                    metrics.drop_duplicates(keep='last', inplace=True)\n",
    "                else:\n",
    "                    print(\"NO EXACT MATCHES FOR\", t)\n",
    "                elapsed = (time.time() - start)\n",
    "                print(\"elapsed:\", sys, elapsed)\n",
    "     \n",
    "    elapsed = (time.time() - start)\n",
    "    print(geometric_mean(metrics))\n",
    "    \n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    # UIMA or QuickUMLS\n",
    "    if single_sys is None:\n",
    "        file_name = 'mm_metrics_'\n",
    "    metrics.to_csv(analysisConf.data_dir + corpus + '_' + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "    \n",
    "    print(\"total elapsed time:\", elapsed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in system matches from file\n",
    "\n",
    "def get_ref_n(analysis_type: str) -> int:\n",
    "    \n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    ref_ann, _ = get_metric_data(training_notes, analysis_type, corpus)\n",
    "    \n",
    "    # do not overestimate fn\n",
    "    if 'entity' in analysis_type:\n",
    "        ref_ann = ref_ann[['start', 'end', 'file']].drop_duplicates()\n",
    "    elif 'cui' in analysis_type:\n",
    "        ref_ann = ref_ann[['value', 'file']].drop_duplicates()\n",
    "    elif 'full' in analysis_type:\n",
    "        ref_ann = ref_ann[['start', 'end', 'value', 'file']].drop_duplicates()\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    ref_n = len(ref_ann.drop_duplicates())\n",
    "    \n",
    "    return ref_n\n",
    "\n",
    "def get_sys_data(system: str, analysis_type: str, corpus: str) -> int: \n",
    "    \n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    _, data = get_metric_data(training_notes, analysis_type, corpus)\n",
    "        \n",
    "    out = data[data['system']== system].copy()\n",
    "    \n",
    "    if system == 'quick_umls':\n",
    "        out = out[(out.similarity.astype(float) >= 0.75) & (out[\"type\"] == 'concept_jaccard_score_False')]\n",
    "\n",
    "    if 'entity' in analysis_type:\n",
    "        cols_to_keep = ['begin', 'end', 'note_id']\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['cui', 'note_id']\n",
    "    elif 'full' in analysis_type:\n",
    "        cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "\n",
    "    out = out[cols_to_keep]\n",
    "    \n",
    "    return out.drop_duplicates()\n",
    "\n",
    "def get_system_matches(system: str, analysis_type: str, corpus: str):\n",
    "    dir_test = analysisConf.data_dir + 'single_system_out/'\n",
    "\n",
    "    file = dir_test + system + '_' + analysis_type + '_' + corpus + '_matches.txt'\n",
    "    data_matches = set(literal_eval(line.strip()) for line in open(file))\n",
    "\n",
    "    file = dir_test + system + '_' + analysis_type + '_' + corpus + '_ref_only.txt'\n",
    "    data_fn = set(literal_eval(line.strip()) for line in open(file)) #{ f for f in file.readlines() }\n",
    "\n",
    "    return data_matches, data_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to generate QuickUMLS system annotations (must run from shell):\n",
    "\n",
    "import os, glob\n",
    "from client import get_quickumls_client\n",
    "from quickumls import QuickUMLS\n",
    "import pandas as pd\n",
    "\n",
    "directory_to_parse = '/Users/gms/development/nlp/nlpie/data/amicus-u01/mipacq/data_in/'\n",
    "quickumls_fp = '/Users/gms/development/nlp/engines_misc_tools/QuickUMLS/data/'\n",
    "os.chdir(directory_to_parse)\n",
    "\n",
    "#similarity = ['dice', 'cosine', 'jaccard', 'overlap']\n",
    "similarity = ['jaccard']\n",
    "overlapping_criteria = ['score', 'length']\n",
    "\n",
    "for s in similarity:\n",
    "    for o in overlapping_criteria:\n",
    "        #matcher = get_quickumls_client(similarity_name)\n",
    "        matcher = QuickUMLS(quickumls_fp=quickumls_fp, overlapping_criteria, threshold=0.7, window=5, similarity_name=s)\n",
    "        test = pd.DataFrame()\n",
    "        for fname in glob.glob(directory_to_parse + '*.txt'):\n",
    "            t = os.path.basename(fname)\n",
    "            u = t.split('.')[0]\n",
    "            with open(directory_to_parse + u + '.txt') as f:\n",
    "                f1 = f.read()\n",
    "                out = matcher.match(f1, best_match=True, ignore_syntax=False)\n",
    "                for i in out:\n",
    "                    i[0]['note_id'] = u\n",
    "                    frames = [ test, pd.DataFrame(i[0], index = [0]) ]\n",
    "                    test = pd.concat(frames, ignore_index=True)\n",
    "        test['system'] = 'quick_umls'\n",
    "        test['similarity'] = s\n",
    "        test['overlap'] = o\n",
    "        test['type'] = 'concept'\n",
    "        test['note_id'] = u\n",
    "        testt['best_match'] = 'true'\n",
    "        temp = test.rename(columns={'start': 'begin'}).copy()\n",
    "        print(temp.tail())\n",
    "\n",
    "        temp.to_csv('/Users/gms/development/nlp/nlpie/data/amicus-u01/output/qumls.csv', mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERATE merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetTotals(object):\n",
    "    \"\"\"\n",
    "    returns an instance with merged match set numbers using either union or intersection of elements in set \n",
    "    \"\"\"\n",
    "    def __init__(self, ref_n, sys_n, match_set): #_left, match_set_right):\n",
    "\n",
    "        self = self    \n",
    "        self.ref_ann = ref_n\n",
    "        self.sys_n = sys_n\n",
    "        self.match_set = match_set\n",
    "\n",
    "    def get_ref_sys(self):\n",
    "\n",
    "        ref_only = self.ref_ann - len(self.match_set)\n",
    "        sys_only = self.sys_n - len(self.match_set)\n",
    "\n",
    "        return ref_only, sys_only, len(self.match_set), self.match_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def merge_eval(ref_only: int, system_only: int, ref_system_match: int, matches, system_n: int, ref_n: int):\n",
    "def merge_eval(ref_only: int, system_only: int, ref_system_match: int, system_n: int, ref_n: int) -> dict:\n",
    "    \"\"\"\n",
    "    Generate confusion matrix params\n",
    "    :params: ref_only, system_only, reference_system_match -> sets\n",
    "    matches, system_n, reference_n -> counts\n",
    "    :return: dictionary object\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if ref_only + ref_system_match != ref_n:\n",
    "        print('ERROR!')\n",
    "\n",
    "    # get evaluation metrics\n",
    "    d = {}\n",
    "    \n",
    "    F, recall, precision, TP, FP, FN, TP_FN_R, TM  = Metrics(system_only, ref_only, ref_system_match, system_n).get_confusion_metrics()\n",
    "\n",
    "    d = {\n",
    "         'F': F[1], \n",
    "         'precision': precision[1], \n",
    "         'recall': recall[1], \n",
    "         'TP': TP, \n",
    "         'FN': FN, \n",
    "         'FP': FP, \n",
    "         'TP/FN': TP_FN_R,\n",
    "         'n_gold': ref_n, \n",
    "         'n_sys': system_n, \n",
    "         'TM': TM\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if system_n - FP != TP:\n",
    "        print('inconsistent system n!')\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUERY TO VALIDATE qumls system counts\n",
    "select count(*), type from (select cui, begin, end, note_id, type from\n",
    "(SELECT distinct *\n",
    " FROM test.qumls_cui\n",
    "where note_id not in (\"0595040941-0\",\n",
    "                            \"0778429553-0\",\n",
    "                            \"1014681675\",\n",
    "                            \"2889522952-2\",\n",
    "                            \"3080383448-5\",\n",
    "                            \"3300000926-3\",\n",
    "                            \"3360037185-3\",\n",
    "                            \"3580973392\",\n",
    "                            \"3627629462-3\",\n",
    "                            \"4323116051-4\",\n",
    "                            \"477704053-4\",\n",
    "                            \"528317073\",\n",
    "                            \"531702602\",\n",
    "                            \"534061073\",\n",
    "                            \"54832076\",\n",
    "                            \"5643725437-6\",\n",
    "                            \"5944412090-5\",\n",
    "                            \"6613169476-6\",\n",
    "                            \"7261075903-7\",\n",
    "                            \"7504944368-7\",\n",
    "                            \"7999462393-7\",\n",
    "                            \"8131081430\",\n",
    "                            \"8171084310\",\n",
    "                            \"8193787896\",\n",
    "                            \"8295055184-8\",\n",
    "                            \"8823185307-8\") \n",
    "                            and similarity >= 0.8 ) t\n",
    "group by cui, begin, end, note_id, type) t\n",
    "group by type;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import operator as op\n",
    "import pandas as pd\n",
    "from __main__ import get_system_matches, get_sys_data\n",
    "\n",
    "def process_sentence(pt, sentence, analysis_type, corpus):\n",
    "    \"\"\"\n",
    "    Recursively evaluate parse tree, \n",
    "    with check for existence before build\n",
    "       :param sentence: to process\n",
    "       :return class of merged annotations, boolean operated system df \n",
    "    \"\"\"\n",
    "    \n",
    "    class Results(object):\n",
    "        def __init__(self):\n",
    "            self.results = set()\n",
    "            #self.operations = []\n",
    "            self.sytem_merges = pd.DataFrame()\n",
    "            \n",
    "    r = Results()\n",
    "    \n",
    "    if 'entity' in analysis_type: \n",
    "        cols_to_keep = ['begin', 'end', 'note_id'] # entity only\n",
    "    elif 'full' in analysis_type: \n",
    "        cols_to_keep = ['cui', 'begin', 'end', 'note_id'] # entity only\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['cui', 'note_id'] # entity only\n",
    "        \n",
    "    \n",
    "    def evaluate(parseTree):\n",
    "        oper = {'&': op.and_, '|': op.or_}\n",
    "        \n",
    "        if parseTree:\n",
    "            leftC = evaluate(parseTree.getLeftChild())\n",
    "            rightC = evaluate(parseTree.getRightChild())\n",
    "            \n",
    "            if leftC and rightC:\n",
    "                query = set()\n",
    "                system_query = pd.DataFrame()\n",
    "                fn = oper[parseTree.getRootVal()]\n",
    "                \n",
    "                if isinstance(leftC, str):\n",
    "                    \n",
    "                    # get system as leaf node \n",
    "                    left, _ = get_system_matches(leftC, analysis_type, corpus)\n",
    "                    left_sys = get_sys_data(leftC, analysis_type, corpus)\n",
    "                \n",
    "                elif isinstance(leftC, tuple):\n",
    "                    left = leftC[0]\n",
    "                    l_sys = leftC[1]\n",
    "                \n",
    "                if isinstance(rightC, str):\n",
    "                    \n",
    "                    # get system as leaf node\n",
    "                    right, _ = get_system_matches(rightC, analysis_type, corpus)\n",
    "                    right_sys = get_sys_data(rightC, analysis_type, corpus)\n",
    "                    \n",
    "                elif isinstance(rightC, tuple):\n",
    "                    right = rightC[0]\n",
    "                    r_sys = rightC[1]\n",
    "                    \n",
    "                # create match set based on boolean operation\n",
    "                match_set = fn(left, right)\n",
    "                \n",
    "                if fn == op.or_:\n",
    "                    r.results = r.results.union(match_set)\n",
    "                   \n",
    "                    if isinstance(leftC, str) and isinstance(rightC, str):\n",
    "                        df = left_sys.append(right_sys)\n",
    "                        df = df[cols_to_keep].drop_duplicates()\n",
    "                        \n",
    "                    elif isinstance(leftC, str) and isinstance(rightC, tuple):\n",
    "                        df = left_sys.append(r_sys)\n",
    "                        df = df[cols_to_keep].drop_duplicates()\n",
    "                    \n",
    "                    elif isinstance(leftC, tuple) and isinstance(rightC, str):\n",
    "                        df = right_sys.append(l_sys)\n",
    "                        df = df[cols_to_keep].drop_duplicates()\n",
    "                    \n",
    "                    elif isinstance(leftC, tuple) and isinstance(rightC, tuple):\n",
    "                        df = l_sys.append(r_sys)\n",
    "                        df = df[cols_to_keep].drop_duplicates()\n",
    "        \n",
    "                if fn == op.and_:\n",
    "                    if len(r.results) == 0:\n",
    "                        r.results = match_set\n",
    "                    r.results = r.results.intersection(match_set)\n",
    "                    \n",
    "                    if isinstance(leftC, str) and isinstance(rightC, str):\n",
    "                        df = left_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                        df = df[cols_to_keep].drop_duplicates()\n",
    "                     \n",
    "                    elif isinstance(leftC, str) and isinstance(rightC, tuple):\n",
    "                        df = left_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                        df = df[cols_to_keep].drop_duplicates()\n",
    "                    \n",
    "                    elif isinstance(leftC, tuple) and isinstance(rightC, str):\n",
    "                        df = l_sys.merge(right_sys, on=cols_to_keep, how='inner')\n",
    "                        df = df[cols_to_keep].drop_duplicates()\n",
    "                    \n",
    "                    elif isinstance(leftC, tuple) and isinstance(rightC, tuple):\n",
    "                        df = l_sys.merge(r_sys, on=cols_to_keep, how='inner')\n",
    "                        df = df[cols_to_keep].drop_duplicates()\n",
    "                \n",
    "                # get matched results\n",
    "                query.update(r.results)\n",
    "                \n",
    "                # get combined system results\n",
    "                r.sytem_merges = df\n",
    "                \n",
    "                if len(df) > 0:\n",
    "                    system_query = system_query.append(df)\n",
    "                else:\n",
    "                    print('wtf!')\n",
    "                    \n",
    "                return query, system_query\n",
    "            else:\n",
    "                return parseTree.getRootVal()\n",
    "    \n",
    "    if sentence.n_or > 0 or sentence.n_and > 0:\n",
    "        evaluate(pt)  \n",
    "    \n",
    "    # trivial case\n",
    "    elif sentence.n_or == 0 and sentence.n_and == 0:\n",
    "        \n",
    "        r.results, _ = get_system_matches(sentence.sentence, analysis_type, corpus)\n",
    "        r.sytem_merges = get_sys_data(sentence.sentence, analysis_type, corpus)\n",
    "        print('trivial:', sentence.sentence, len(r.results), len(r.sytem_merges))\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Incoming Boolean sentences are parsed into a binary tree.\n",
    "\n",
    "Test expressions to parse:\n",
    "\n",
    "sentence = '((((A&B)|C)|D)&E)'\n",
    "\n",
    "sentence = '(E&(D|(C|(A&B))))'\n",
    "\n",
    "sentence = '(((A|(B&C))|(D&(E&F)))|(H&I))'\n",
    "\n",
    "\"\"\"\n",
    "# build parse tree from passed sentence\n",
    "# using grammatical rules of Boolean logic\n",
    "def buildParseTree(fpexp):\n",
    "    \"\"\"\n",
    "       Iteratively build parse tree from passed sentence using grammatical rules of Boolean logic\n",
    "       :param fpexp: sentence to parse\n",
    "       :return eTree: parse tree representation\n",
    "       Incoming Boolean sentences are parsed into a binary tree.\n",
    "       Test expressions to parse:\n",
    "       sentence = '(A&B)'\n",
    "       sentence = '(A|B)'\n",
    "       sentence = '((A|B)&C)'\n",
    "       \n",
    "    \"\"\"\n",
    "\n",
    "    fplist = fpexp.split()\n",
    "    pStack = Stack()\n",
    "    eTree = BinaryTree('')\n",
    "    pStack.push(eTree)\n",
    "    currentTree = eTree\n",
    "\n",
    "    for i in fplist:\n",
    "\n",
    "        if i == '(':\n",
    "            currentTree.insertLeft('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getLeftChild()\n",
    "        elif i not in ['&', '|', ')']:\n",
    "            currentTree.setRootVal(i)\n",
    "            parent = pStack.pop()\n",
    "            currentTree = parent\n",
    "        elif i in ['&', '|']:\n",
    "            currentTree.setRootVal(i)\n",
    "            currentTree.insertRight('')\n",
    "            pStack.push(currentTree)\n",
    "            currentTree = currentTree.getRightChild()\n",
    "        elif i == ')':\n",
    "            currentTree = pStack.pop()\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    return eTree\n",
    "\n",
    "def make_parse_tree(payload):\n",
    "    \"\"\"\n",
    "    Ensure data to create tree are in standard form\n",
    "    :param sentence: sentence to preprocess\n",
    "    :return pt, parse tree graph\n",
    "            sentence, processed sentence to build tree\n",
    "            a: order\n",
    "    \"\"\"\n",
    "    def preprocess_sentence(sentence):\n",
    "        # prepare statement for case when a boolean AND/OR is given\n",
    "        sentence = payload.replace('(', ' ( '). \\\n",
    "            replace(')', ' ) '). \\\n",
    "            replace('&', ' & '). \\\n",
    "            replace('|', ' | '). \\\n",
    "            replace('  ', ' ')\n",
    "        return sentence\n",
    "\n",
    "    sentence = preprocess_sentence(payload)\n",
    "    print(sentence)\n",
    "    \n",
    "    pt = buildParseTree(sentence)\n",
    "    #pt.postorder() \n",
    "    \n",
    "    return pt\n",
    "\n",
    "class Sentence(object):\n",
    "\n",
    "    def __init__(self, sentence):\n",
    "        self = self\n",
    "        self.n_and = sentence.count('&')\n",
    "        self.n_or = sentence.count('|')\n",
    "        self.sentence = sentence\n",
    "    \n",
    "def get_metrics(boolean_expression: str, analysis_type: str, corpus: str):\n",
    "    \"\"\"\n",
    "    Traverse binary parse tree representation of Boolean sentence\n",
    "        :params: boolean expression in form of '(<annotator_engine_name1><boolean operator><annotator_engine_name2>)'\n",
    "                 analysis_type (string value of: 'test', 'entity', 'cui', 'full') used to filter set of reference and system annotations \n",
    "        :return: dictionary with values needed for confusion matrix\n",
    "    \"\"\"\n",
    "    sentence = Sentence(boolean_expression)   \n",
    "\n",
    "    pt = make_parse_tree(sentence.sentence)\n",
    "\n",
    "    r = process_sentence(pt, sentence, analysis_type, corpus)\n",
    "\n",
    "    system_n = len(r.sytem_merges)\n",
    "    reference_n = get_ref_n(analysis_type)\n",
    "\n",
    "    reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, r.results).get_ref_sys()\n",
    "\n",
    "    # get overall TP/TF and various other counts for running confusion matrix metric analysis\n",
    "    #return merge_eval(reference_only, system_only, reference_system_match, r.results, system_n, reference_n)\n",
    "    return merge_eval(reference_only, system_only, reference_system_match, system_n, reference_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all combinations of given list of annotators:\n",
    "def expressions(l, n):\n",
    "    for (operations, *operands), operators in product(\n",
    "            combinations(l, n), product(('&', '|'), repeat=n - 1)):\n",
    "        for operation in zip(operators, operands):\n",
    "            operations = [operations, *operation]\n",
    "        yield operations\n",
    "\n",
    "def run_ensemble(l, analysis_type, corpus):\n",
    "\n",
    "    metrics = pd.DataFrame()\n",
    "\n",
    "    for i in range(1, len(l)+1):\n",
    "        test = list(expressions(l, i))\n",
    "        for t  in test:\n",
    "            if i > 1:\n",
    "                # format Boolean sentence for parse tree \n",
    "                t = '(' + \" \".join(str(x) for x in t).replace('[','(').replace(']',')').replace(\"'\",\"\").replace(\",\",\"\").replace(\" \",\"\") + ')'\n",
    "\n",
    "            d = get_metrics(t, analysis_type, corpus)\n",
    "            d['merge'] = t\n",
    "            frames = [metrics, pd.DataFrame(d, index=[0]) ]\n",
    "            metrics = pd.concat(frames, ignore_index=True, sort=False) \n",
    "    \n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    \n",
    "    file_name = corpus + '_all_merge_metrics_'\n",
    "        \n",
    "    geometric_mean(metrics).to_csv(analysisConf.data_dir + file_name + analysis_type + '_' + str(timestamp) + '.csv')\n",
    "    print(geometric_mean(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_set(df, analysis_type = 'entity', df_type = 'sys'):\n",
    "    \n",
    "    #print(df[0:10])\n",
    "    \n",
    "    # get values for creation of series of type tuple\n",
    "    if 'entity' in analysis_type: \n",
    "        if df_type == 'sys':\n",
    "            arg = df.begin, df.end, df.note_id\n",
    "        else:\n",
    "            arg = df.start, df.end, df.file\n",
    "    elif 'cui' in analysis_type:\n",
    "        if df_type == 'sys':\n",
    "            arg = df.cui, df.note_id\n",
    "        else:\n",
    "            arg = df.value, df.file\n",
    "    elif 'full' in analysis_type:\n",
    "        if df_type == 'sys':\n",
    "            arg = df.begin, df.end, df.cui, df.note_id\n",
    "        else:\n",
    "            arg = df.start, df.end, df.value, df.file\n",
    "    \n",
    "    return set(list(zip(*arg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTS -> ensemble:\n",
    "def test_match_consistency(matches, ref_only, ref_n, sys):\n",
    "    \"\"\"test for reference only/match set consistency:\n",
    "        params: match, system and reference only sets\"\"\"\n",
    "   \n",
    "    print('len', len(sys), len(matches), len(matches.union(sys)), len(matches.intersection(sys)))\n",
    "    assert len(matches.union(ref_only)) == ref_n, 'Reference annotation mismatch union'\n",
    "    assert len(matches.intersection(sys)) == len(matches), 'System annotation mismatch intersect'\n",
    "    assert len(matches.union(sys)) == len(sys), 'System annotation mismatch union'\n",
    "    assert len(matches.intersection(ref_only)) == 0, 'Reference annotation mismatch intersect'\n",
    "\n",
    "def test_systems(analysis_type, systems, corpus):\n",
    "    sys = df_to_set(get_sys_data(systems[0], analysis_type, corpus), analysis_type)\n",
    "    test_match_consistency(*get_system_matches(systems[0], analysis_type, corpus), get_ref_n(analysis_type), sys)\n",
    "    print('Match consistency:', len(sys),get_ref_n(analysis_type))\n",
    "\n",
    "def test_metrics(ref, sys_m, match_m):\n",
    "    test = True\n",
    "    reference_n = len(ref)\n",
    "    system_n = len(sys_m)\n",
    "\n",
    "    print('Test metrics:', type(reference_n), type(system_n), type(match_m))\n",
    "\n",
    "    reference_only, system_only, reference_system_match, match_set = SetTotals(reference_n, system_n, match_m).get_ref_sys()\n",
    "    F, recall, precision, _, _, _, _, _ = Metrics(system_only, reference_only, reference_system_match, system_n).get_confusion_metrics()\n",
    "    F_, recall_, precision_, _, _, _, _, _ = Metrics(system_only, reference_only, reference_system_match, system_n).get_confusion_metrics(test)\n",
    "\n",
    "    assert F[1] == F_, 'F1 issue'\n",
    "    assert recall[1] == recall_, 'recall issue'\n",
    "    assert precision[1] == precision_, 'precision issue'\n",
    "    print(F[1], F_)\n",
    "    print(recall[1], recall_)\n",
    "    print(precision[1], precision_)\n",
    "\n",
    "def test_count(analysis_type, corpus):\n",
    "    # test match counts:\n",
    "    ctakes, _ = get_system_matches('ctakes', analysis_type, corpus)\n",
    "    clamp, _ = get_system_matches('clamp', analysis_type, corpus)\n",
    "    b9, _ = get_system_matches('biomedicus', analysis_type, corpus)\n",
    "    mm, _ = get_system_matches('metamap', analysis_type, corpus)\n",
    "\n",
    "    print('count:', len(mm.intersection(b9.intersection(clamp.intersection(ctakes)))))\n",
    "    \n",
    "def test_ensemble(analysis_type, corpus):\n",
    "    \n",
    "    print('ensemble:')\n",
    "    # Get mixed system_n\n",
    "    training_notes = get_notes(analysis_type, corpus)\n",
    "    ref_ann, data = get_metric_data(training_notes, analysis_type, corpus)\n",
    "\n",
    "    names = ['ctakes', 'biomedicus', 'metamap', 'clamp']\n",
    "    if 'entity' in analysis_type: \n",
    "        cols_to_keep = ['begin', 'end', 'note_id']\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['cui', 'note_id']\n",
    "    elif 'full' in analysis_type:\n",
    "        cols_to_keep = ['begin', 'end', 'cui', 'note_id']\n",
    "\n",
    "    biomedicus = data[data[\"system\"]=='biomedicus'][cols_to_keep].copy()\n",
    "    ctakes = data[data[\"system\"]=='ctakes'][cols_to_keep].copy()\n",
    "    clamp = data[data[\"system\"]=='clamp'][cols_to_keep].copy()\n",
    "    metamap = data[data[\"system\"]=='metamap'][cols_to_keep].copy()\n",
    "    quickumls = data[data[\"system\"]=='quick_umls'][cols_to_keep].copy()\n",
    "\n",
    "    print('systems:', len(biomedicus), len(clamp), len(ctakes), len(metamap), len(quickumls))\n",
    "\n",
    "    b9 = set()\n",
    "    cl = set()\n",
    "    ct = set()\n",
    "    mm = set()\n",
    "    qu = set()\n",
    "\n",
    "    b9 = df_to_set(get_sys_data('biomedicus', analysis_type, corpus), analysis_type)\n",
    "    print(len(b9))\n",
    "\n",
    "    ct = df_to_set(get_sys_data('ctakes', analysis_type, corpus), analysis_type)\n",
    "    print(len(ct))\n",
    "\n",
    "    cl = df_to_set(get_sys_data('clamp', analysis_type, corpus), analysis_type)\n",
    "    print(len(cl))\n",
    "\n",
    "    mm = df_to_set(get_sys_data('metamap', analysis_type, corpus), analysis_type)\n",
    "    print(len(mm))\n",
    "\n",
    "    qu = df_to_set(get_sys_data('quick_umls', analysis_type, corpus), analysis_type)\n",
    "    print(len(qu))\n",
    "    \n",
    "    print('various merges:')\n",
    "    print(len(b9), len(cl), len(ct), len(mm), len(qu))\n",
    "    print(len(mm.intersection(b9.intersection(cl.intersection(ct)))))\n",
    "    print(len(mm.union(b9.intersection(cl.intersection(ct)))))\n",
    "    print(len(mm.union(b9.union(cl.intersection(ct)))))\n",
    "    print(len(mm.union(b9.union(cl.union(ct)))))\n",
    "    print(len(b9.intersection(ct)))\n",
    "\n",
    "    sys_m = b9.intersection(ct.intersection(qu))\n",
    "    print('sys_m:', len(sys_m))\n",
    "\n",
    "    # Get match merges:\n",
    "    ct, _ = get_system_matches('ctakes', analysis_type, corpus)\n",
    "    cl, _ = get_system_matches('clamp', analysis_type, corpus)\n",
    "    b9, _ = get_system_matches('biomedicus', analysis_type, corpus)\n",
    "    mm, _ = get_system_matches('metamap', analysis_type, corpus)\n",
    "    qu, _ = get_system_matches('quick_umls', analysis_type, corpus)\n",
    "\n",
    "    match_m = b9.intersection(ct.intersection(qu))\n",
    "    print('match_m:', len(match_m))\n",
    "    # reference df to set\n",
    "    if 'entity' in analysis_type: \n",
    "        cols_to_keep = ['end', 'start','file']\n",
    "    elif 'cui' in analysis_type:\n",
    "        cols_to_keep = ['value','file']\n",
    "    elif 'full' in analysis_type:\n",
    "        cols_to_keep = ['end', 'start', 'value','file']\n",
    "\n",
    "    ref = df_to_set(ref_ann[cols_to_keep], analysis_type, 'ref')\n",
    "\n",
    "    print('ref:', len(ref))\n",
    "\n",
    "    # test difference:\n",
    "    print('FP:', len(sys_m - match_m), len(sys_m - ref))\n",
    "    assert len(sys_m - match_m) == len(sys_m - ref), 'FP mismatch'\n",
    "    print('FN:', len(ref - match_m), len(ref - sys_m))\n",
    "    assert len(ref - match_m) == len(ref - sys_m), 'FN mismatch'\n",
    "    \n",
    "    test_metrics(ref, sys_m, match_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Run: 1->Single systems; 2->Ensemble; 3->Tests; 4-> MM Test 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biomedicus', 'clamp', 'ctakes', 'metamap', 'quick_umls'] ('analytical_cui_i2b2_concepts.csv', 'test.i2b2_all')\n",
      "corpus: i2b2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:40: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biomedicus.v2.UmlsConcept\n",
      "test_io(): 277532 850253 160845 159775 [0.84136565 0.63386336] [0.84092046 0.63463882] [0.84181132 0.63308978]\n",
      "entity\n",
      "30380 16754 64917 51291 13626\n",
      "elapsed: biomedicus 6.265856981277466\n",
      "edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_io(): 393808 898077 113021 43499 [0.91984325 0.8342188 ] [0.88821954 0.90052983] [0.95380192 0.77700368]\n",
      "entity\n",
      "30380 7199 52884 29703 23181\n",
      "elapsed: clamp 8.938841104507446\n",
      "ctakes_mentions_all\n",
      "test_io(): 268242 909557 101541 169065 [0.8705061  0.66471397] [0.89957353 0.61339517] [0.84325834 0.72540382]\n",
      "entity\n",
      "30380 17600 52630 39850 12780\n",
      "elapsed: ctakes 11.546478986740112\n",
      "org.metamap.uima.ts.Candidate\n",
      "test_io(): 378054 535192 475906 59253 [0.66668037 0.58555512] [0.52931763 0.8645048 ] [0.90032215 0.44270692]\n",
      "entity\n",
      "30380 15719 107927 93266 14661\n",
      "elapsed: metamap 21.120309829711914\n",
      "concept_jaccard_score_False\n",
      "test_io(): 322782 765725 245373 114525 [0.8097135  0.64205708] [0.75732026 0.73811304] [0.86989492 0.56812314]\n",
      "entity\n",
      "30380 15801 85676 71097 14579\n",
      "elapsed: quick_umls 24.218407154083252\n",
      "       system                                              type         F  \\\n",
      "0  biomedicus                         biomedicus.v2.UmlsConcept  0.285969   \n",
      "1       clamp  edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA  0.556807   \n",
      "2      ctakes                               ctakes_mentions_all  0.307915   \n",
      "3     metamap                     org.metamap.uima.ts.Candidate  0.212007   \n",
      "4  quick_umls                       concept_jaccard_score_False  0.251241   \n",
      "\n",
      "   precision    recall     TP     FN     FP     TP/FN  n_gold   n_sys  \\\n",
      "0   0.209899  0.448519  13626  16754  51291  0.813298   30380   64917   \n",
      "1   0.438337  0.763035  23181   7199  29703  3.220031   30380   52884   \n",
      "2   0.242827  0.420671  12780  17600  39850  0.726136   30380   52630   \n",
      "3   0.135842  0.482587  14661  15719  93266  0.932693   30380  107927   \n",
      "4   0.170164  0.479888  14579  15801  71097  0.922663   30380   85676   \n",
      "\n",
      "           TM  F1 rank  TP/FN rank  TM rank     Gmean  \n",
      "0   53.479725      3.0         4.0      3.0  3.301927  \n",
      "1  100.802202      1.0         1.0      1.0  1.000000  \n",
      "2   55.707564      2.0         5.0      2.0  2.714418  \n",
      "3   44.627087      5.0         2.0      5.0  3.684031  \n",
      "4   49.807890      4.0         3.0      4.0  3.634241  \n",
      "total elapsed time: 24.218963861465454\n",
      "CPU times: user 29.2 s, sys: 2.18 s, total: 31.4 s\n",
      "Wall time: 43.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def main():\n",
    "    \n",
    "    rtype = int(input(\"Run: 1->Single systems; 2->Ensemble; 3->Tests; 4-> MM Test\"))\n",
    "   \n",
    "    '''\n",
    "        corpora: i2b2, mipacq, fv017\n",
    "        analyses: entity only (exact span), cui by document, full (aka (entity and cui on exaact span/exact cui)\n",
    "                  NB: add \"_test\" using mipacq to egnerate small test sample \n",
    "        systems: ctakes, biomedicus, clamp, metamap, quick_umls\n",
    "        \n",
    "        TODO -> Vectorization (entity only and full):\n",
    "                add switch for use of TN on single system performance evaluations \n",
    "                add switch for overlap matching versus exact span\n",
    "             -> Other tasks besides concept extraction\n",
    "             -> Use of https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html\n",
    "        \n",
    "    ''' \n",
    "    corpus = 'i2b2'\n",
    "    #corpus = 'mipacq'\n",
    "    analysis_type = 'entity'\n",
    "    #analysis_type = 'full'\n",
    "    analysisConf =  AnalysisConfig()\n",
    "    print(analysisConf.systems, analysisConf.corpus_config(corpus))\n",
    "    \n",
    "    if (rtype == 1):\n",
    "        generate_metrics(analysis_type, corpus)\n",
    "    elif (rtype == 2):\n",
    "        l = ['ctakes','biomedicus','clamp','metamap','quick_umls']\n",
    "        run_ensemble(l, analysis_type, corpus) \n",
    "    elif (rtype == 3):\n",
    "        systems = ['biomedicus']\n",
    "        t = ['concept_jaccard_score_false']\n",
    "        test_systems(analysis_type, systems, corpus)  \n",
    "        test_count(analysis_type, corpus)\n",
    "        test_ensemble(analysis_type, corpus)\n",
    "    elif (rtype == 4):\n",
    "        generate_metrics_test(analysis_type, corpus)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dkpro-cassis\n",
    "from cassis import *\n",
    "import numpy as np\n",
    "import collections\n",
    "from typing import List, Set\n",
    "\n",
    "ts_test = \"/Users/gms/development/nlp/nlpie/data/irr_mts/\"\n",
    "dir_test = \"/Users/gms/development/nlp/nlpie/data/irr_mts/\"\n",
    "#fname = \"527982345-v1.txt.xmi\"\n",
    "fname = \"92_1666/alber475.xmi\"\n",
    "case = fname.split('/')[0]\n",
    "view_name = \"_InitialView\"\n",
    "Span = collections.namedtuple('Span',['begin','end','label']) # define named tuple for span object \n",
    "  \n",
    "def get_ann(fname, dir_test, ts_test, view_name):\n",
    "    #t = \"org.apache.ctakes.typesystem.type.textsem.DiseaseDisorderMention\"\n",
    "    t = \"webanno.custom.Term\"\n",
    "    with open(ts_test + 'TypeSystem.xml', 'rb') as f:\n",
    "        typesystem = load_typesystem(f)\n",
    "    with open(dir_test + fname, 'rb') as f:\n",
    "        cas = load_cas_from_xmi(f, typesystem=typesystem)\n",
    "    view = cas.get_view(view_name)\n",
    "    #print([x for x in view.select(t)])\n",
    "    #print(view.sofa_string, len(view.sofa_string))\n",
    "    d = {}\n",
    "    ann = []\n",
    "    labels = set()\n",
    "    attribs = get_attribs(view.select(t))\n",
    "    # only parse if type exists in file\n",
    "    if view.select(t):\n",
    "        for sentence in view.select(t): \n",
    "            for i in range(len(attribs)):\n",
    "                key = attribs[i]\n",
    "                # helper method to get val for given key\n",
    "                val = sentence.__getattribute__(attribs[i])\n",
    "                d[key] = val \n",
    "               \n",
    "                if key == 'termType':\n",
    "                    #print(key, val)\n",
    "                    labels.add(val)\n",
    "                \n",
    "                if i == len(attribs) - 1:\n",
    "                    ann.append( Span(d[\"begin\"], d[\"end\"], d[\"termType\"]))\n",
    "    \n",
    "    #print(ann, labels)\n",
    "    return ann, view.sofa_string, labels\n",
    "# extract attributes from cas Annotation object\n",
    "\n",
    "def get_attribs(v):\n",
    "    attribs = []\n",
    "    for sentence in v:\n",
    "        #print(sentence)\n",
    "        for s in sentence.__dir__():\n",
    "            if '__' not in s:\n",
    "                if s not in attribs:\n",
    "                    #print(s)\n",
    "                    attribs.append(s)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    return attribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
