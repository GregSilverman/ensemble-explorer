{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "from xml.etree.ElementTree import fromstring\n",
    "from xmljson import badgerfish as bf\n",
    "import json\n",
    "from json import dumps\n",
    "from pandas.io.json import json_normalize\n",
    "from pathlib import Path\n",
    "\n",
    "from sqlalchemy.engine import create_engine\n",
    "from sqlalchemy.sql import text\n",
    "\n",
    "engine = create_engine('mysql+pymysql://gms:nej123@localhost/test')\n",
    "dir_test = \"/Users/gms/Desktop/4Greg/\" #medmentions_abstracts.json\"\n",
    "\n",
    "def parse_stuff():\n",
    "    df = pd.DataFrame()\n",
    "    #for fname in glob.glob(dir_test + '*.json'):\n",
    "\n",
    "    #if fname is not \"2889522952-2.xml\":\n",
    "        # get filename and use for processed output filename\n",
    "    #t = os.path.basename(fname)\n",
    "    #print(file_test)\n",
    "\n",
    "    engine = create_engine('mysql+pymysql://gms:nej123@localhost/test')\n",
    "    #case = t.split('.')[0]\n",
    "    \n",
    "    text_dir = Path(dir_test + \"source\")\n",
    "    text_dir.mkdir(parents=True, exist_ok=True)\n",
    "    #print(t)\n",
    "    with open(dir_test + 'medmentions_abstracts.json') as f:\n",
    "        f1 = f.read()\n",
    "\n",
    "        #print(type(json.loads(f1)), f1)\n",
    "        d = json.loads(f1)\n",
    "        stuff = dict()\n",
    "\n",
    "        for k, v in d.items():\n",
    "            print(k)\n",
    "            k = k + '.txt'\n",
    "            f = open(text_dir / str(k), 'w')\n",
    "            f.write(v)\n",
    "            f.close()\n",
    "            \n",
    "            '''\n",
    "            if key == 'text':\n",
    "                stuff['text'] = item\n",
    "            if key == 'spans':\n",
    "                #print(item[0], item[0]['start'], item[0]['end'])\n",
    "                stuff['start'] = item[0]['start']\n",
    "                stuff['end'] = item[0]['end']\n",
    "                stuff['expansion'] = item[0]['expansion']\n",
    "            '''\n",
    "            \n",
    "\n",
    "        stuff['case'] = case\n",
    "        #print(stuff)\n",
    "        frames = [ df, pd.DataFrame(stuff, index=[0]) ]\n",
    "        df = pd.concat(frames, ignore_index=True)\n",
    "        #print(df)\n",
    "\n",
    "#df.to_sql('medmentions_abstracts', engine, if_exists=\"replace\") \n",
    "\n",
    "%time parse_stuff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "sql = \"select * from amia_2019_accuracy_set where `system` = 'biomedicus'\"  \n",
    "        \n",
    "df = pd.read_sql(sql, con=engine)\n",
    "x = df[['maxScore']].values.astype(float)\n",
    "#print(x)\n",
    "\n",
    "# Create a minimum and maximum processor object\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "robust_scaler = preprocessing.RobustScaler()\n",
    "\n",
    "# Create an object to transform the data to fit minmax processor\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "\n",
    "x_scaled = robust_scaler.fit_transform(x)\n",
    "df_normalized = pd.DataFrame(x_scaled)\n",
    "\n",
    "#print(df_normalized[0].value_counts())\n",
    "#print(df_normalized[0].value_counts())\n",
    "#print(df['maxScore'].max())\n",
    "\n",
    "#df = df[df['maxScore'] <= 1]\n",
    "print(df['maxScore'].value_counts())\n",
    "df.hist(column='maxScore', bins=50)\n",
    "\n",
    "df.loc[df['maxScore'] > 1] = 1\n",
    "\n",
    "print(df['maxScore'].value_counts())\n",
    "df.hist(column='maxScore', bins=50)\n",
    "\n",
    "sql = \"select * from amia_2019_accuracy_set where `system` = 'metamap'\"  \n",
    "        \n",
    "df = pd.read_sql(sql, con=engine)\n",
    "#df.hist(column='maxScore', bins=50)\n",
    "\n",
    "x = df[['maxScore']].values.astype(float)\n",
    "# Create an object to transform the data to fit minmax processor\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "# Run the normalizer on the dataframe\n",
    "df_normalized = pd.DataFrame(x_scaled)\n",
    "\n",
    "sql = \"select * from amia_2019_accuracy_set where `system` = 'clamp'\"  \n",
    "        \n",
    "df = pd.read_sql(sql, con=engine)\n",
    "#df.hist(column='maxScore', bins=50)\n",
    "x = df[['maxScore']].values.astype(float)\n",
    "\n",
    "# Create an object to transform the data to fit minmax processor\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "\n",
    "#x_scaled = robust_scaler.fit_transform(x)\n",
    "\n",
    "#print(df['maxScore'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlaps():\n",
    "\n",
    "    sql = \"select `system`, score, maxScore, `case` from amia_2019_accuracy_set\"  \n",
    "\n",
    "    df = pd.read_sql(sql, con=engine)\n",
    "    \n",
    "\n",
    "    # normalize data\n",
    "    b9 = df[df['system'] == 'biomedicus'].copy()\n",
    "    # take care of outliers\n",
    "    b9.loc[b9['score'] > 1, 'score'] = 0.9\n",
    "    b9['sscore'] = (b9['score'] - b9['score'].min())/(b9['score'].max() - b9['score'].min())\n",
    "\n",
    "    clamp = df[df['system'] == 'clamp'].copy()\n",
    "    clamp['sscore'] = (clamp['score'] - clamp['score'].min())/(clamp['score'].max() - clamp['score'].min())\n",
    "\n",
    "    mm = df[df['system'] == 'metamap'].copy()\n",
    "    mm['sscore'] = (abs(mm['score']) - abs(mm['score']).min())/(abs(mm['score']).max() - abs(mm['score']).min())\n",
    "    \n",
    "    #print(mm.head(200))\n",
    "\n",
    "    frames = [b9, clamp, mm]\n",
    "\n",
    "    return pd.concat(frames), set(pd.concat(frames)['case'].tolist())\n",
    "\n",
    "def get_ensemble():\n",
    "    \n",
    "    df, cases = get_overlaps()\n",
    "\n",
    "    #cases.remove(1)\n",
    "    \n",
    "    matches = []\n",
    "    n = 0\n",
    "    for c in cases:\n",
    "\n",
    "        if n%1000 == 0:\n",
    "            print(c)\n",
    "        \n",
    "        t = [i for i in df[df[\"case\"] == c].copy().itertuples(index=False)] \n",
    "\n",
    "        d = {}\n",
    "        for i in range(len(t)):\n",
    "\n",
    "            d[t[i].system] = [t[i].case, t[i].sscore]\n",
    "            if i < len(t):\n",
    "\n",
    "                if i == len(t) - 1:\n",
    "                    \n",
    "                    itemMaxValue = max(d.items(), key=lambda x: x[1])\n",
    "\n",
    "                    listOfKeys = list()\n",
    "                    # Iterate over all the items in dictionary to find keys with max value\n",
    "                    for key, value in d.items():\n",
    "                        if value == itemMaxValue[1]:\n",
    "                            listOfKeys.append(key)\n",
    "\n",
    "                    matches.append({t[i].case:listOfKeys})\n",
    "\n",
    "        n += 1\n",
    "    return matches\n",
    "\n",
    "%time ensemble = get_ensemble()\n",
    "\n",
    "#print(ensemble)\n",
    "print('fini!')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ensemble) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ensemble[0:2])\n",
    "\n",
    "m = 0\n",
    "n = 0\n",
    "o = 0\n",
    "for i in ensemble:\n",
    "    for k, v in i.items():\n",
    "        if len(v) == 1:\n",
    "            m += 1\n",
    "            if v[0] != 'biomedicus':\n",
    "                o += 1\n",
    "        \n",
    "        if len(v) == 2:\n",
    "            n +=1\n",
    "print(m, n, o)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "d = {}\n",
    "for i in ensemble:\n",
    "    for k, v in i.items():\n",
    "        if len(v) == 1:\n",
    "            #print(v[0], i)\n",
    "            d[k] = v[0]\n",
    "        if len(v) == 2:\n",
    "            \n",
    "            if 'biomedicus' in v:\n",
    "                d[k] = 'biomedicus'\n",
    "                #print('biomedicus')\n",
    "                #d[k] = 'biomedicus'\n",
    "            else:\n",
    "                #print('metamap', i)\n",
    "                #d[k] = 'metamap'\n",
    "            \n",
    "                d[k] = random.choice(v)\n",
    "            \n",
    "#print(d)\n",
    "df = pd.DataFrame(list(d.items()), columns=['case', 'system'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.head(200))\n",
    "\n",
    "df.to_sql('amia_2019_disambiguation_ensemble', engine, if_exists=\"replace\")\n",
    "\n",
    "print('fini!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(seq1, seq2):  \n",
    "    \"\"\"\n",
    "    from https://stackabuse.com/levenshtein-distance-and-text-similarity-in-python/\n",
    "    \"\"\"\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros ((size_x, size_y))\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1, y] + 1,\n",
    "                    matrix[x-1, y-1],\n",
    "                    matrix[x, y-1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1,y] + 1,\n",
    "                    matrix[x-1,y-1] + 1,\n",
    "                    matrix[x,y-1] + 1\n",
    "                )\n",
    "    #print (matrix)\n",
    "    return (matrix[size_x - 1, size_y - 1])\n",
    "\n",
    "sql = \"select * from amia_2019_disambiguation_v\"\n",
    "df = pd.read_sql(sql, con=engine)\n",
    "\n",
    "n = 0\n",
    "m = 0\n",
    "o = 0\n",
    "for row in df.itertuples():\n",
    "    if row.sys_expansion:\n",
    "        if levenshtein(row.sys_expansion, row.expansion) < 10:\n",
    "            n += 1\n",
    "        else:\n",
    "            o += 1\n",
    "    else:\n",
    "            m += 1\n",
    "            #print(row.case, row.sys_expansion, row.expansion, levenshtein(row.sys_expansion, row.expansion))\n",
    "print(n, m, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
