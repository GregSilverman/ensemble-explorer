{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import pandas as pd\n",
    "from xml.etree.ElementTree import fromstring\n",
    "from xmljson import badgerfish as bf\n",
    "import json\n",
    "from json import dumps\n",
    "from pandas.io.json import json_normalize\n",
    "from sqlalchemy.engine import create_engine\n",
    "from sqlalchemy.sql import text\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "engine = create_engine('mysql+pymysql://gms:nej123@localhost/concepts', pool_pre_ping=True)\n",
    "def parse_fv_reference():\n",
    "    # parse mipacq reference standard\n",
    "    \n",
    "    # for systems comparison  \n",
    "    #dir_test = \"/Users/gms/development/nlp/nlpie/data/ensembling-u01/fairview/annotation/Jen/\"\n",
    "    # for IAA\n",
    "    dir_test = \"/Users/gms/development/nlp/nlpie/data/ensembling-u01/fairview/annotation/**/\"\n",
    "    \n",
    "    for fname in glob.glob(dir_test + '*.xml'):\n",
    "    #for fname in glob.glob(dir_test + '0000089745.xml'):\n",
    "\n",
    "        #if fname is not \"2889522952-2.xml\":\n",
    "            # get filename and use for processed output filename\n",
    "        t = os.path.basename(fname)\n",
    "        \n",
    "        case = t.split('.')\n",
    "        #print(t)\n",
    "        u = t.split('.')[0]\n",
    "        print(t, u, fname.split('/')[10:11][0])\n",
    "        \n",
    "        fv = pd.DataFrame()\n",
    "        #with open(fname, 'rb') as f:\n",
    "        with open(fname, 'rb') as f:\n",
    "            f1 = f.read()\n",
    "            \n",
    "            data = dumps(bf.data(fromstring(f1)))\n",
    "            d = json.loads(data)\n",
    "\n",
    "            #print(data)\n",
    "\n",
    "            ann = d.get(\"GateDocument\").get(\"AnnotationSet\")\n",
    "            \n",
    "            # \"Original markups\"\n",
    "            #print(ann[1][\"Annotation\"])\n",
    "            \n",
    "            #print(ann[0][\"Annotation\"])\n",
    "\n",
    "            test = ann[0][\"Annotation\"]\n",
    "            \n",
    "            test1 = ann[1][\"Annotation\"]\n",
    "            \n",
    "            df = pd.DataFrame(test)\n",
    "            \n",
    "            df1 = pd.DataFrame(test1)\n",
    "            \n",
    "            if \"Feature\" in df1:\n",
    "                df1 = df1[~df1[\"Feature\"].isna()]\n",
    "                \n",
    "                df1 = df1.Feature.apply(pd.Series) \\\n",
    "                    .merge(df1, left_index = True, right_index = True) \\\n",
    "                    .melt(id_vars = ['@EndNode', '@Id', '@StartNode', '@Type','Feature'], value_name = \"feature\")\n",
    "            \n",
    "            # https://mikulskibartosz.name/how-to-split-a-list-inside-a-dataframe-cell-into-rows-in-pandas-9849d8ff2401\n",
    "            df = df.Feature.apply(pd.Series) \\\n",
    "                .merge(df, left_index = True, right_index = True) \\\n",
    "                .melt(id_vars = ['@EndNode', '@Id', '@StartNode', '@Type','Feature'], value_name = \"feature\")\n",
    "            \n",
    "                #.drop([\"Feature\"], axis = 1) \\\n",
    "            \n",
    "            \n",
    "            #print(df)\n",
    "            df['idx']=df.index\n",
    "            for row in df.itertuples():\n",
    "                if isinstance(row.feature, dict):\n",
    "                \n",
    "                    if 'Name' in row.feature:\n",
    "                        if '$' in row.feature['Name']:\n",
    "                            df.loc[df['idx'] == row.Index, 'name'] = row.feature['Name']['$']\n",
    "                        if '$' in row.feature['Value']:\n",
    "                            df.loc[df['idx'] == row.Index, 'value'] = row.feature['Value']['$']\n",
    "                        else: \n",
    "                            df.loc[df['idx'] == row.Index, 'value'] = None\n",
    "                    else: \n",
    "                        if '$' in row.feature:\n",
    "                            if \"Name\" in row.feature:\n",
    "                                if '$' in row.feature[\"Name\"]:\n",
    "                                    #print('no Name!', row.Feature[\"Name\"][\"$\"])\n",
    "                                    df.loc[df['idx'] == row.Index, 'name'] = row.Feature['Name']['$']\n",
    "                        \n",
    "                            if \"Value\" in row.feature:\n",
    "                                if '$' in row.feature[\"Value\"]:\n",
    "                                    #print('no Name!', row.Feature[\"Value\"][\"$\"])\n",
    "                                    df.loc[df['idx'] == row.Index, 'value'] = row.Feature['Value']['$']\n",
    "                                \n",
    "            if \"Feature\" in df1:                \n",
    "                df1['idx']=df1.index\n",
    "                for row in df1.itertuples():\n",
    "\n",
    "                    if isinstance(row.feature, dict):\n",
    "\n",
    "                        if 'Name' in row.feature:\n",
    "                            if '$' in row.feature['Name']:\n",
    "                                df1.loc[df1['idx'] == row.Index, 'name'] = row.feature['Name']['$']\n",
    "                            if '$' in row.feature['Value']:\n",
    "                                df1.loc[df1['idx'] == row.Index, 'value'] = row.feature['Value']['$']\n",
    "                            else: \n",
    "                                df1.loc[df1['idx'] == row.Index, 'value'] = None\n",
    "                        else: \n",
    "                            if '$' in row.feature:\n",
    "                                if \"Name\" in row.feature:\n",
    "                                    if '$' in row.feature[\"Name\"]:\n",
    "                                        #print('no Name!', row.Feature[\"Name\"][\"$\"])\n",
    "                                        df1.loc[df1['idx'] == row.Index, 'name'] = row.Feature['Name']['$']\n",
    "                                if \"Value\" in row.feature:\n",
    "                                    if '$' in row.feature[\"Value\"]:\n",
    "                                        #print('no Name!', row.Feature[\"Value\"][\"$\"])\n",
    "                                        df1.loc[df1['idx'] == row.Index, 'value'] = row.Feature['Value']['$']\n",
    "\n",
    "            # non-labeled markups\n",
    "            df['annotator'] = fname.split('/')[10:11][0]\n",
    "            df['case'] = u\n",
    "            df = df.rename(columns={'@EndNode': 'end', '@StartNode': 'start', '@Type': 'Type', '@Id': 'id'})\n",
    "            df.feature = df.feature.astype(str)\n",
    "\n",
    "            # Original markups\n",
    "            if \"Feature\" in df1:\n",
    "                df1['annotator'] = fname.split('/')[10:11][0]\n",
    "                df1['case'] = u\n",
    "                df1 = df1.rename(columns={'@EndNode': 'end', '@StartNode': 'start', '@Type': 'Type', '@Id': 'id'})\n",
    "                df1.feature = df.feature.astype(str)\n",
    "            \n",
    "            cols_to_keep = [\"end\", \"id\", \"start\", \"Type\", \"name\", \"value\", \"annotator\", \"case\"]\n",
    "           \n",
    "            if \"Feature\" in df1:\n",
    "                frames = [df1, df]\n",
    "                df2 = pd.concat(frames, ignore_index=True)\n",
    "        \n",
    "                frames = [fv, df2]\n",
    "                fv = pd.concat(frames, ignore_index=True)\n",
    "            else:\n",
    "                frames = [fv, df]\n",
    "                fv = pd.concat(frames, ignore_index=True)\n",
    "            \n",
    "            fv = fv.drop([\"Feature\"], axis = 1)\n",
    "            fv = fv[cols_to_keep]\n",
    "            fv = fv[fv[\"Type\"] == 'clinical-entities']\n",
    "            \n",
    "            fv.drop_duplicates().dropna(subset = ['name', 'value']).to_sql('fv_reference_all', engine, if_exists=\"append\") \n",
    "            #print(\"df\", fv[fv[\"start\"]==38064])\n",
    "            #return fv\n",
    "\n",
    "#parse_fv_reference()\n",
    "\n",
    "#print(df[df[\"start\"] == 38064])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punctuation(my_str):    \n",
    "    # define punctuation\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    # To take input from the user\n",
    "    # my_str = input(\"Enter a string: \")\n",
    "    # remove punctuation from the string\n",
    "    no_punct = \"\"\n",
    "    for char in my_str:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char\n",
    "    # display the unpunctuated string\n",
    "    #print(no_punct)\n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_dict(d, names, values, span, case, txt):\n",
    "   \n",
    "    semtypes = []\n",
    "    test = dict()\n",
    "    d['name'] = names\n",
    "    d['value'] = values\n",
    "\n",
    "    if 'comment' in d['name']: # concept is annotated, text may be same or abbrev\n",
    "        #print('concept w/ txt:', d['name'].index('comment'), d['value'][d['name'].index('comment')], txt[0])\n",
    "        concept = d['value'][d['name'].index('comment')]\n",
    "    else: # concept is not annotated, but instead extracted from texzt\n",
    "        #print('concept:', d['name'], d['value'], txt[0])\n",
    "        concept = txt[0]\n",
    "\n",
    "    if 'Acronym' in d['name']:\n",
    "        st = 'Acronym'\n",
    "        semtypes.append(st)\n",
    "    if 'Anatomy' in d['name']:\n",
    "        st = 'Anatomy'\n",
    "        semtypes.append(st)\n",
    "    if 'Finding' in d['name']:\n",
    "        st = 'Finding'\n",
    "        semtypes.append(st)\n",
    "    if 'Procedure' in d['name']:\n",
    "        st = 'Procedure'\n",
    "        semtypes.append(st)\n",
    "    if 'Drug' in d['name']:\n",
    "        st = 'Drug'\n",
    "        semtypes.append(st)\n",
    "\n",
    "    test['span'] = span\n",
    "    test['case'] = case\n",
    "    test['concept'] = concept.lower()\n",
    "    test['txt'] = txt[0].lower()\n",
    "\n",
    "    if len(semtypes) == 1: \n",
    "        test['semtypes'] = semtypes[0]\n",
    "    else:\n",
    "        test['semtypes'] = semtypes\n",
    "\n",
    "    #print(test)\n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case: 0000521643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:68: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case: 0001117827\n",
      "case: 0000469349\n",
      "case: 0000198143\n",
      "case: 0000323786\n",
      "case: 0000220714\n"
     ]
    }
   ],
   "source": [
    "#case = '0000089745'\n",
    "#sql = \"select * from concepts.fv_reference_all where start = 15485 and annotator = 'Jen' and value not in ('safe_regex') and name not in ('isEmptyAndSpan','line','Note Type') and `case` = %(case)s\"  \n",
    "#sql = \"select * from concepts.fv_reference_all where start in (38064) and end in (38067) and annotator = 'Jen' and value not in ('safe_regex') and name not in ('isEmptyAndSpan','line','Note Type') and `case` = %(case)s\"  \n",
    "#sql = \"select * from concepts.fv_reference_all where start = 159611 and annotator = 'Jen' and value not in ('safe_regex') and name not in ('isEmptyAndSpan','line','Note Type') and `case` = %(case)s\"  \n",
    "data_directory = '/Users/gms/development/nlp/nlpie/data/ensembling-u01/output/'\n",
    "\n",
    "writer = pd.ExcelWriter(data_directory + 'fv_concepts.xlsx')\n",
    "annotators = ['Jen', 'George', 'Angel']\n",
    "annotator = 'Jen'\n",
    "\n",
    "directory_to_parse = \"/Users/gms/development/nlp/nlpie/data/ensembling-u01/fairview/annotation/\" + annotator + \"/txt/\"\n",
    "os.chdir(directory_to_parse)\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "#for fname in glob.glob('0000089745.txt'):\n",
    "\n",
    "concepts = pd.DataFrame()\n",
    "\n",
    "for fname in glob.glob('*.txt'):\n",
    "    with open(fname) as f:\n",
    "            f1 = f.read()\n",
    "            doc = nlp(f1)\n",
    "            \n",
    "            t = os.path.basename(fname)\n",
    "        \n",
    "            case = t.split('.')[0]\n",
    "            print('case:', case)\n",
    "            \n",
    "            sql = \"select * from concepts.fv_reference_all where annotator = %(annotator)s and `case` = %(case)s order by annotator, `case`, start, end \"\n",
    "            df = pd.read_sql(sql, params={\"annotator\":annotator, \"case\":case}, con=engine)\n",
    "            #df = df.head(50)\n",
    "            \n",
    "            df['span'] = list(zip(df.start, df.end))\n",
    "            spans = set(df['span'].tolist())\n",
    "            \n",
    "            for s in spans:\n",
    "                span = []\n",
    "                test = df[(df['start'] == s[0]) & (df['end'] == s[1])].copy()\n",
    "                \n",
    "                for row in test.itertuples():\n",
    "\n",
    "                    my_str = [token.text for token in doc if token.idx >= row.start and token.idx <= row.end]\n",
    "                    my_str = (' ').join(my_str)\n",
    "\n",
    "                    if (row.start, row.end) not in span: \n",
    "                        d = dict()\n",
    "                        txt = []\n",
    "                        txt.append(strip_punctuation(my_str))\n",
    "                        span.append((row.start, row.end))\n",
    "                        names = []\n",
    "                        values = []\n",
    "                        d['case'] = case \n",
    "\n",
    "                    names.append(row.name)\n",
    "                    values.append(row.value)\n",
    "\n",
    "                d['name'] = names\n",
    "                d['value'] = values\n",
    "                d['annotator'] = annotator\n",
    "                \n",
    "                test = mk_dict(d, names, values, s, case, txt)\n",
    "                \n",
    "                #print(test)\n",
    "                temp = pd.DataFrame.from_dict(test, orient='index').reset_index()\n",
    "                frames = [ concepts, temp ]\n",
    "                \n",
    "                concepts = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "print(concepts.head(25))\n",
    "print(concepts.tail(25))\n",
    "\n",
    "concepts.to_excel(writer,annotator)\n",
    "    \n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = 'this is a test'\n",
    "str.find(str, 0)\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "parsed_sentence = nlp(u'This is my sentence')\n",
    "print([(token.text,token.i) for token in parsed_sentence])\n",
    "\n",
    "print([(token.text,token.idx) for token in parsed_sentence])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update byte diff flag\n",
    "engine = create_engine('mysql+pymysql://gms:nej123@localhost/concepts', pool_pre_ping=True)\n",
    "dir_file = '/Users/gms/development/nlp/nlpie/data/ensembling-u01/fairview/system_annotations/data_in_preprocessed/'\n",
    "\n",
    "df=pd.read_table(dir_file + 'jbdc',header=None)\n",
    "print(df)\n",
    "sql = text(\"SELECT * FROM concepts.fv_reference WHERE `case` = :e1\")\n",
    "\n",
    "for row in df.itertuples():\n",
    "    #print(row._1)\n",
    "    resp = engine.execute(sql, e1=row._1).fetchall()\n",
    "    if len(resp) > 0:\n",
    "        #print(resp)\n",
    "        sql_u = text(\"UPDATE concepts.fv_reference SET byte_diff = 1 WHERE `case` = :e1\")\n",
    "        engine.execute(sql_u, e1=row._1)\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_file = '/Users/gms/development/nlp/nlpie/data/ensembling-u01/output/' \n",
    "engine = create_engine('mysql+pymysql://gms:nej123@localhost/concepts', pool_pre_ping=True)\n",
    "\n",
    "df=pd.read_csv(dir_file + 'qumls_system_fv_bm.csv', dtype={'note_id': str})\n",
    "print(df.columns)\n",
    "cols_to_keep = ['cui', 'end', 'begin', 'corpus', 'similarity', 'system', 'type', 'note_id', 'semtypes']\n",
    "df = df[cols_to_keep]\n",
    "print(df.head())\n",
    "df.to_sql('qumls_system', engine, if_exists=\"append\", index=False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
