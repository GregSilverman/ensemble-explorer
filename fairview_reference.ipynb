{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import pandas as pd\n",
    "from xml.etree.ElementTree import fromstring\n",
    "from xmljson import badgerfish as bf\n",
    "import json\n",
    "from json import dumps\n",
    "from pandas.io.json import json_normalize\n",
    "from sqlalchemy.engine import create_engine\n",
    "from sqlalchemy.sql import text\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "engine = create_engine('mysql+pymysql://gms:nej123@localhost/concepts', pool_pre_ping=True)\n",
    "def parse_fv_reference():\n",
    "    # parse mipacq reference standard\n",
    "    \n",
    "    # for systems comparison  \n",
    "    dir_test = \"/Users/gms/development/nlp/nlpie/data/ensembling-u01/fairview/annotation/Jen/test/\"\n",
    "    # for IAA\n",
    "    #dir_test = \"/Users/gms/development/nlp/nlpie/data/ensembling-u01/fairview/annotation/**/\"\n",
    "    \n",
    "    for fname in glob.glob(dir_test + '*.xml'):\n",
    "    #for fname in glob.glob(dir_test + '0000089745.xml'):\n",
    "\n",
    "        #if fname is not \"2889522952-2.xml\":\n",
    "            # get filename and use for processed output filename\n",
    "        t = os.path.basename(fname)\n",
    "        \n",
    "        case = t.split('.')\n",
    "        #print(t)\n",
    "        u = t.split('.')[0]\n",
    "        print(t, u, fname.split('/')[10:11][0])\n",
    "        \n",
    "        fv = pd.DataFrame()\n",
    "        #with open(fname, 'rb') as f:\n",
    "        with open(fname, 'rb') as f:\n",
    "            f1 = f.read()\n",
    "            \n",
    "            data = dumps(bf.data(fromstring(f1)))\n",
    "            d = json.loads(data)\n",
    "\n",
    "            #print(data)\n",
    "\n",
    "            ann = d.get(\"GateDocument\").get(\"AnnotationSet\")\n",
    "            \n",
    "            # \"Original markups\"\n",
    "            #print(ann[1][\"Annotation\"])\n",
    "            \n",
    "            #print(ann[0][\"Annotation\"])\n",
    "\n",
    "            test = ann[0][\"Annotation\"]\n",
    "            \n",
    "            test1 = ann[1][\"Annotation\"]\n",
    "            \n",
    "            df = pd.DataFrame(test)\n",
    "            \n",
    "            df1 = pd.DataFrame(test1)\n",
    "            \n",
    "            if \"Feature\" in df1:\n",
    "                df1 = df1[~df1[\"Feature\"].isna()]\n",
    "                \n",
    "                df1 = df1.Feature.apply(pd.Series) \\\n",
    "                    .merge(df1, left_index = True, right_index = True) \\\n",
    "                    .melt(id_vars = ['@EndNode', '@Id', '@StartNode', '@Type','Feature'], value_name = \"feature\")\n",
    "            \n",
    "            # https://mikulskibartosz.name/how-to-split-a-list-inside-a-dataframe-cell-into-rows-in-pandas-9849d8ff2401\n",
    "            df = df.Feature.apply(pd.Series) \\\n",
    "                .merge(df, left_index = True, right_index = True) \\\n",
    "                .melt(id_vars = ['@EndNode', '@Id', '@StartNode', '@Type','Feature'], value_name = \"feature\")\n",
    "            \n",
    "                #.drop([\"Feature\"], axis = 1) \\\n",
    "            \n",
    "            #print(df)\n",
    "            df['idx']=df.index\n",
    "            for row in df.itertuples():\n",
    "                if isinstance(row.feature, dict):\n",
    "                \n",
    "                    if 'Name' in row.feature:\n",
    "                        if '$' in row.feature['Name']:\n",
    "                            df.loc[df['idx'] == row.Index, 'name'] = row.feature['Name']['$']\n",
    "                        if '$' in row.feature['Value']:\n",
    "                            df.loc[df['idx'] == row.Index, 'value'] = row.feature['Value']['$']\n",
    "                        else: \n",
    "                            df.loc[df['idx'] == row.Index, 'value'] = None\n",
    "                    else: \n",
    "                        if '$' in row.feature:\n",
    "                            if \"Name\" in row.feature:\n",
    "                                if '$' in row.feature[\"Name\"]:\n",
    "                                    #print('no Name!', row.Feature[\"Name\"][\"$\"])\n",
    "                                    df.loc[df['idx'] == row.Index, 'name'] = row.Feature['Name']['$']\n",
    "                        \n",
    "                            if \"Value\" in row.feature:\n",
    "                                if '$' in row.feature[\"Value\"]:\n",
    "                                    #print('no Name!', row.Feature[\"Value\"][\"$\"])\n",
    "                                    df.loc[df['idx'] == row.Index, 'value'] = row.Feature['Value']['$']\n",
    "                                \n",
    "            if \"Feature\" in df1:                \n",
    "                df1['idx']=df1.index\n",
    "                for row in df1.itertuples():\n",
    "\n",
    "                    if isinstance(row.feature, dict):\n",
    "\n",
    "                        if 'Name' in row.feature:\n",
    "                            if '$' in row.feature['Name']:\n",
    "                                df1.loc[df1['idx'] == row.Index, 'name'] = row.feature['Name']['$']\n",
    "                            if '$' in row.feature['Value']:\n",
    "                                df1.loc[df1['idx'] == row.Index, 'value'] = row.feature['Value']['$']\n",
    "                            else: \n",
    "                                df1.loc[df1['idx'] == row.Index, 'value'] = None\n",
    "                        else: \n",
    "                            if '$' in row.feature:\n",
    "                                if \"Name\" in row.feature:\n",
    "                                    if '$' in row.feature[\"Name\"]:\n",
    "                                        #print('no Name!', row.Feature[\"Name\"][\"$\"])\n",
    "                                        df1.loc[df1['idx'] == row.Index, 'name'] = row.Feature['Name']['$']\n",
    "                                if \"Value\" in row.feature:\n",
    "                                    if '$' in row.feature[\"Value\"]:\n",
    "                                        #print('no Name!', row.Feature[\"Value\"][\"$\"])\n",
    "                                        df1.loc[df1['idx'] == row.Index, 'value'] = row.Feature['Value']['$']\n",
    "\n",
    "            # non-labeled markups\n",
    "            df['annotator'] = fname.split('/')[10:11][0]\n",
    "            df['case'] = u\n",
    "            df = df.rename(columns={'@EndNode': 'end', '@StartNode': 'start', '@Type': 'Type', '@Id': 'id'})\n",
    "            df.feature = df.feature.astype(str)\n",
    "\n",
    "            # Original markups\n",
    "            if \"Feature\" in df1:\n",
    "                df1['annotator'] = fname.split('/')[10:11][0]\n",
    "                df1['case'] = u\n",
    "                df1 = df1.rename(columns={'@EndNode': 'end', '@StartNode': 'start', '@Type': 'Type', '@Id': 'id'})\n",
    "                df1.feature = df.feature.astype(str)\n",
    "            \n",
    "            cols_to_keep = [\"end\", \"id\", \"start\", \"Type\", \"name\", \"value\", \"annotator\", \"case\"]\n",
    "           \n",
    "            if \"Feature\" in df1:\n",
    "                frames = [df1, df]\n",
    "                df2 = pd.concat(frames, ignore_index=True)\n",
    "        \n",
    "                frames = [fv, df2]\n",
    "                fv = pd.concat(frames, ignore_index=True)\n",
    "            else:\n",
    "                frames = [fv, df]\n",
    "                fv = pd.concat(frames, ignore_index=True)\n",
    "            \n",
    "            fv = fv.drop([\"Feature\"], axis = 1)\n",
    "            fv = fv[cols_to_keep]\n",
    "            fv = fv[fv[\"Type\"] == 'clinical-entities']\n",
    "            \n",
    "            fv.drop_duplicates().dropna(subset = ['name', 'value']).to_sql('fv_reference_all_new', engine, if_exists=\"append\") \n",
    "            #print(\"df\", fv[fv[\"start\"]==38064])\n",
    "            #return fv\n",
    "\n",
    "#parse_fv_reference()\n",
    "\n",
    "#print(df[df[\"start\"] == 38064])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fv = pd.read_csv('/Users/gms/development/nlp/nlpie/data/ensembling-u01/output/Jen_fv_concepts_test.csv')\n",
    "#fv.to_sql('fv_reference_all_jen_new', engine, if_exists=\"append\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punctuation(my_str):    \n",
    "    # define punctuation\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?#$%^&*_~'''\n",
    "    # To take input from the user\n",
    "    # my_str = input(\"Enter a string: \")\n",
    "    # remove punctuation from the string\n",
    "    no_punct = \"\"\n",
    "    for char in my_str:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char\n",
    "    # display the unpunctuated string\n",
    "    #print(no_punct)\n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_dict(d, names, values, span, case, txt):\n",
    "  \n",
    "    semtypes = []\n",
    "    test = {}\n",
    "    d['name'] = names\n",
    "    d['value'] = values\n",
    "   \n",
    "    if 'acronym' in names:\n",
    "        concept = \" \".join(values[0].split()).strip()\n",
    "    else: # concept is not annotated, but extracted from texzt\n",
    "        concept = \" \".join(txt[0].split()).strip()\n",
    "\n",
    "    if 'acronym' in d['name']:\n",
    "        st = 'Acronym'\n",
    "        semtypes.append(st)\n",
    "    if 'Anatomy' in d['name']:\n",
    "        st = 'Anatomy'\n",
    "        semtypes.append(st)\n",
    "    if 'Finding' in d['name']:\n",
    "        st = 'Finding'\n",
    "        semtypes.append(st)\n",
    "    if 'Procedure' in d['name']:\n",
    "        st = 'Procedure'\n",
    "        semtypes.append(st)\n",
    "    if 'Drug' in d['name']:\n",
    "        st = 'Drug'\n",
    "        semtypes.append(st)\n",
    "    \n",
    "    test['span'] = span\n",
    "    test['start'] = span[0][0]\n",
    "    test['end'] = span[0][1]\n",
    "    test['case'] = case\n",
    "    test['concept'] = concept.lower()\n",
    "    test['txt'] = \" \".join(txt[0].split()).lower()\n",
    "\n",
    "    if len(semtypes) == 1: \n",
    "        test['semtypes'] = semtypes[0]\n",
    "    elif len(semtypes) > 1:\n",
    "        test['semtypes'] = semtypes\n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case: 0000006497 Jen\n",
      "              span  start   end        case                                    concept                                        txt   semtypes annotator\n",
      "739     (320, 348)    320   348  0000006497                     incontinence frequency                     incontinence frequency    Finding       Jen\n",
      "126     (379, 407)    379   407  0000006497           incontinence frequency procedure           incontinence frequency procedure    Finding       Jen\n",
      "196     (424, 464)    424   464  0000006497  stage i and stage ii placement anesthesia  stage i and stage ii placement anesthesia  Procedure       Jen\n",
      "1005    (658, 676)    658   676  0000006497                        symptoms of urgency                        symptoms of urgency    Finding       Jen\n",
      "933     (669, 676)    669   676  0000006497                                 of urgency                                 of urgency    Anatomy       Jen\n",
      "1007    (689, 696)    689   696  0000006497                                   nocturia                                   nocturia    Finding       Jen\n",
      "653     (698, 707)    698   707  0000006497                                        and                                        and    Finding       Jen\n",
      "9       (709, 717)    709   717  0000006497                          urge incontinence                          urge incontinence    Finding       Jen\n",
      "595     (723, 740)    723   740  0000006497                              who initially                              who initially    Finding       Jen\n",
      "1201    (773, 800)    773   800  0000006497             testing to which she responded             testing to which she responded  Procedure       Jen\n",
      "296     (881, 900)    881   900  0000006497                        as a more permanent                        as a more permanent  Procedure       Jen\n",
      "925     (937, 955)    937   955  0000006497                         symptoms initially                         symptoms initially    Finding       Jen\n",
      "752     (948, 955)    948   955  0000006497                                  initially                                  initially    Anatomy       Jen\n",
      "724    (999, 1019)    999  1019  0000006497                              in the office                              in the office  Procedure       Jen\n",
      "1135  (1184, 1193)   1184  1193  0000006497                                        the                                        the  Procedure       Jen\n",
      "162   (1264, 1272)   1264  1272  0000006497                                       need                                       need    Finding       Jen\n",
      "189   (1274, 1283)   1274  1283  0000006497                                to exchange                                to exchange    Finding       Jen\n",
      "1069  (1315, 1336)   1315  1336  0000006497                           pain at the site                           pain at the site    Finding       Jen\n",
      "816   (1338, 1370)   1338  1370  0000006497       of the incision as well as inability       of the incision as well as inability    Finding       Jen\n",
      "113   (1399, 1402)   1399  1402  0000006497                                        she                                        she  Procedure       Jen\n",
      "114   (1399, 1402)   1399  1402  0000006497                 magnetic resonance imaging                                        she    Acronym       Jen\n",
      "1254  (1581, 1591)   1581  1591  0000006497                                bilaterally                                bilaterally    Anatomy       Jen\n",
      "948   (1765, 1775)   1765  1775  0000006497                              she responded                              she responded    Anatomy       Jen\n",
      "410   (1854, 1857)   1854  1857  0000006497                                        the                                        the    Anatomy       Jen\n",
      "172   (2256, 2260)   2256  2260  0000006497                                                                                          Anatomy       Jen\n",
      "                  span   start     end        case                        concept                            txt   semtypes annotator\n",
      "1143  (133117, 133126)  133117  133126  0000006497                      southdale                      southdale       Drug       Jen\n",
      "775   (133124, 133126)  133124  133126  0000006497                    long acting                                   Acronym       Jen\n",
      "421   (134321, 134336)  134321  134336  0000006497                     25000 type                     25000 type    Finding       Jen\n",
      "562   (134364, 134376)  134364  134376  0000006497                bottem jennifer                bottem jennifer    Finding       Jen\n",
      "409   (134403, 134417)  134403  134417  0000006497                   1020 pm nace                   1020 pm nace    Finding       Jen\n",
      "240   (134444, 134447)  134444  134447  0000006497                            715                            715    Finding       Jen\n",
      "241   (134444, 134447)  134444  134447  0000006497        urinary tract infection                            715    Acronym       Jen\n",
      "465   (134449, 134472)  134449  134472  0000006497           am wiklund rachel on           am wiklund rachel on    Finding       Jen\n",
      "968   (134558, 134570)  134558  134570  0000006497                    notes notes                    notes notes    Finding       Jen\n",
      "141   (134633, 134648)  134633  134648  0000006497                             pt                             pt    Finding       Jen\n",
      "489   (134708, 134736)  134708  134736  0000006497  please call her urine culture  please call her urine culture    Finding       Jen\n",
      "642   (134829, 134834)  134829  134834  0000006497                                                                  Anatomy       Jen\n",
      "594   (134829, 134841)  134829  134841  0000006497                    by jennifer                    by jennifer    Finding       Jen\n",
      "1177  (134960, 134973)  134960  134973  0000006497                  436 pm please                  436 pm please    Finding       Jen\n",
      "1179  (134970, 134973)  134970  134973  0000006497        urinary tract infection                         please    Acronym       Jen\n",
      "969   (135035, 135044)  135035  135044  0000006497                     antibiotic                     antibiotic    Finding       Jen\n",
      "565   (135110, 135124)  135110  135124  0000006497                    lipid panel                    lipid panel    Finding       Jen\n",
      "452   (135185, 135195)  135185  135195  0000006497                   final result                   final result    Finding       Jen\n",
      "830   (135260, 135278)  135260  135278  0000006497         comprehensive metpanel         comprehensive metpanel    Finding       Jen\n",
      "1191  (135271, 135278)  135271  135278  0000006497                       metpanel                       metpanel    Anatomy       Jen\n",
      "1221  (135347, 135366)  135347  135366  0000006497           final result 1252008           final result 1252008  Procedure       Jen\n",
      "84    (135442, 135449)  135442  135449  0000006497                       81003003                       81003003       Drug       Jen\n",
      "85    (135442, 135449)  135442  135449  0000006497                       81003003                       81003003    Finding       Jen\n",
      "540   (135505, 135507)  135505  135507  0000006497                gastroinestinal                                   Acronym       Jen\n",
      "484   (135505, 135514)  135505  135514  0000006497                   final result                   final result    Finding       Jen\n"
     ]
    }
   ],
   "source": [
    "#case = '0000181343' -> problems! hangs mm, for examaple, spans don't align\n",
    "#case = '0040006867'\n",
    "#sql = \"select * from concepts.fv_reference_all where start = 15485 and annotator = 'Jen' and value not in ('safe_regex') and name not in ('isEmptyAndSpan','line','Note Type') and `case` = %(case)s\"  \n",
    "#sql = \"select * from concepts.fv_reference_all where start in (38064) and end in (38067) and annotator = 'Jen' and value not in ('safe_regex') and name not in ('isEmptyAndSpan','line','Note Type') and `case` = %(case)s\"  \n",
    "#sql = \"select * from concepts.fv_reference_all where start = 159611 and annotator = 'Jen' and value not in ('safe_regex') and name not in ('isEmptyAndSpan','line','Note Type') and `case` = %(case)s\"  \n",
    "data_directory = '/Users/gms/development/nlp/nlpie/data/ensembling-u01/output/'\n",
    "\n",
    "#writer = pd.ExcelWriter(data_directory + 'fv_concepts_test.xlsx')\n",
    "#annotators = ['Jen', 'George', 'Angel']\n",
    "annotator = ['Jen']\n",
    "\n",
    "for annotator in annotator:\n",
    "    #directory_to_parse = \"/Users/gms/development/nlp/nlpie/data/ensembling-u01/fairview/annotation/\" + annotator + \"/txt/\"\n",
    "    directory_to_parse = \"/Users/gms/development/nlp/nlpie/data/ensembling-u01/fairview/annotation/\" + annotator + \"/txt/\"\n",
    "    directory_to_parse = \"/Users/gms/development/nlp/nlpie/data/ensembling-u01/fairview/system_annotations/data_in_preprocessed/jen_test\"\n",
    "    os.chdir(directory_to_parse)\n",
    "    import spacy\n",
    "    nlp = spacy.load('en')\n",
    "\n",
    "    concepts = pd.DataFrame()\n",
    "    \n",
    "    #cases = ['0000089745', '0000220714', '0001908516', '0000234695', '0001053477', '0002340241', '0007340048', \n",
    "    #         '0001157129', '0001112285', '0001053477', '0000473442', '0000469349', '0000202738', '0000513005', \n",
    "    #         '0000258427']\n",
    "    \n",
    "    #cases = ['0000234695']\n",
    "    \n",
    "    \n",
    "    #cases = ['0000200926' , '0000513005', '0001112285', '0002340241']\n",
    "    #cases = ['0000513005', '0001112285', '0002340241']\n",
    "    \n",
    "    cases = ['0000006497']\n",
    "    \n",
    "    for case in cases:\n",
    "    \n",
    "        for fname in glob.glob(case + '*.txt'):\n",
    "        #for fname in glob.glob('0000258427.txt'):\n",
    "\n",
    "            # '0000202738.txt' -> 5 off offset for George; 2 -> Angel\n",
    "            # '0000513005.txt' -> 3 off offset \"    \" and Angel\n",
    "            # 0001053477, 0002340241, 0007340048, 0001157129, 0001112285, 0000473442, 0000469349 -> 2 off for G and Angel\n",
    "            # 0000258427 no offset\n",
    "\n",
    "            # JUNK: 0001908516, 0000234695\n",
    "\n",
    "\n",
    "            # George: 0000089745 upto (1483, 1485) definitely junk! erratic pattern to track!\n",
    "            # George: 0000220714 junk\n",
    "\n",
    "\n",
    "            with open(fname) as f:\n",
    "\n",
    "                    f1 = f.read()\n",
    "                    # remove all dashes, since it affects tokenization\n",
    "                    f1= f1.replace('-', \" \")\n",
    "                    doc = nlp(f1)\n",
    "\n",
    "                    t = os.path.basename(fname)\n",
    "\n",
    "                    #case = t.split('.')[0]\n",
    "                    print('case:', case, annotator)\n",
    "\n",
    "                    sql = \"select * from concepts.fv_reference_all where annotator = %(annotator)s and `case` = %(case)s order by annotator, `case`, start, end \"\n",
    "                    df = pd.read_sql(sql, params={\"annotator\":annotator, \"case\":case}, con=engine)\n",
    "                    #df = df.head(50)\n",
    "\n",
    "                    df['span'] = list(zip(df.start, df.end))\n",
    "                    spans = set(df['span'].tolist())\n",
    "\n",
    "                    #spans = [(478, 483)]\n",
    "                    for s in spans:\n",
    "                        span = []\n",
    "                        #print(s)\n",
    "                        test = df[(df['start'] == s[0]) & (df['end'] == s[1])].copy()\n",
    "                        #test = df[(df['start'] >= s[0] - 20)].copy()\n",
    "\n",
    "                        #print(test)\n",
    "\n",
    "                        for row in test.itertuples():\n",
    "                            \n",
    "                            #print(test)\n",
    "\n",
    "    #                             if annotator in ['Angel', 'George']:\n",
    "    #                                 c1 = ['0001053477', '0002340241', '0007340048', '0001157129', '0001112285', '0001053477', '0000473442', '0000469349']\n",
    "    #                                 c2 = '0000202738'\n",
    "    #                                 c3 = '0000513005'\n",
    "    #                                 c4 = '0000258427'\n",
    "    #                                 c5 = '0000089745' # junk for George\n",
    "    #                                 if case in c1:\n",
    "    #                                     n = 2\n",
    "    #                                 elif case == c2 and annotator == 'Angel':\n",
    "    #                                     n = 2\n",
    "    #                                 elif case == c3:\n",
    "    #                                     n = 3\n",
    "    #                                 elif case == c4:\n",
    "    #                                     if annotator == 'George' or (annotator == 'Angel' and row.start <= 28343):\n",
    "    #                                         n = 0\n",
    "    #                                     elif (annotator == 'Angel' and row.start > 28343 and row.start < 28845): \n",
    "    #                                         n = 1\n",
    "    #                                     elif (annotator == 'Angel' and row.start > 28845 and row.start < 28975): \n",
    "    #                                         n = 2\n",
    "    #                                     elif (annotator == 'Angel' and row.start >= 28975 and row.start < 29000): \n",
    "    #                                         n = 3\n",
    "    #                                     elif (annotator == 'Angel' and row.start >= 29000 and row.start < 29012): \n",
    "    #                                         n = 2\n",
    "    #                                     elif (annotator == 'Angel' and row.start >= 28012 and row.start < 61882) :\n",
    "    #                                         n = 5\n",
    "    #                                     else:\n",
    "    #                                         n = 21\n",
    "    # #                                 elif case == c5:\n",
    "    # #                                     if annotator == 'George' and row.start >= 1483 and row.start < 1526:\n",
    "    # #                                         n = 1\n",
    "    # #                                     elif annotator == 'George' and row.start >= 1526 and row.start < 1556:\n",
    "    # #                                         n = 2\n",
    "    # #                                     elif annotator == 'George' and row.start >= 1556 and row.start < 1582:\n",
    "    # #                                         n = 3\n",
    "    # #                                     elif annotator == 'George' and row.start >= 1582 and row.start < 1630:\n",
    "    # #                                         n = 4\n",
    "    # #                                     elif annotator == 'George' and row.start >= 1630 and row.start < 1667:\n",
    "    # #                                         n = 5\n",
    "    # #                                     elif annotator == 'George' and row.start >= 1667:\n",
    "    # #                                         n = 6\n",
    "    # #                                     else:\n",
    "    # #                                         n = 0\n",
    "    #                                 else:\n",
    "    #                                     n = 0\n",
    "\n",
    "\n",
    "    #                             else:\n",
    "    #                                 n = 0\n",
    "\n",
    "                            my_str = [strip_punctuation(token.text.strip('\\n')) for token in doc if token.idx >= (row.start) and token.idx <= (row.end)]\n",
    "                            #my_str = [token.text for token in doc if token.idx >= (row.start - 0) and token.idx <= (row.end - 0  )]\n",
    "                            #my_str = [token.text for token in doc if token.idx == row.start]\n",
    "                            my_str = (' ').join(my_str)\n",
    "\n",
    "                            add = False\n",
    "\n",
    "                            if (row.start, row.end) not in span: \n",
    "                                d = {}\n",
    "                                txt = []\n",
    "                                names = []\n",
    "                                values = []\n",
    "                                txt.append(my_str)\n",
    "                                #span.append((row.start, row.end))\n",
    "                                d['case'] = case \n",
    "\n",
    "                            if row.name in ['Anatomy', 'Drug', 'Finding', 'Procedure', 'comment']:\n",
    "                                #print('row', row.name)\n",
    "                                if row.name == 'comment' and len(row.value) > 0:\n",
    "                                    #print('here')\n",
    "                                    names.append('acronym')\n",
    "                                else:\n",
    "                                    names.append(row.name)    \n",
    "                                values.append(row.value)\n",
    "                                add = True\n",
    "\n",
    "                            d['name'] = names\n",
    "                            d['value'] = values\n",
    "                            d['start'] = row.start\n",
    "                            d['end'] = row.end\n",
    "\n",
    "                            if add:\n",
    "                                out = mk_dict(d, names, values, [s], case, txt)\n",
    "                                out['annotator'] = annotator\n",
    "\n",
    "                                temp = pd.DataFrame(out, index=[0])\n",
    "                                frames = [ concepts, temp ]\n",
    "\n",
    "                                concepts = pd.concat(frames, ignore_index=True)\n",
    "    \n",
    "    concepts = concepts.sort_values(by=['case', 'start', 'end'])\n",
    "    #concepts.to_excel(writer, annotator)\n",
    "    print(concepts.head(25))\n",
    "    print(concepts.tail(25))\n",
    "    \n",
    "    concepts.to_csv(data_directory + annotator + '_fv_concepts_test_problems_4.csv')\n",
    "\n",
    "#writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "directory_to_parse = \"/Users/gms/development/nlp/nlpie/data/ensembling-u01/fairview/annotation/George/txt/\"\n",
    "os.chdir(directory_to_parse)\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for fname in glob.glob('0000513005.txt'):\n",
    "    with open(fname) as f:\n",
    "            f1 = f.read()\n",
    "            doc = nlp(f1)\n",
    "\n",
    "\n",
    "            print([(token.text,token.idx) for token in doc if token.idx >= 17961 and token.idx <= 1796])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-6f2c14ea74e1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-6f2c14ea74e1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    d =\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "d = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = 'this is a test'\n",
    "str.find(str, 0)\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "parsed_sentence = nlp(u'This is my sentence')\n",
    "print([(token.text,token.i) for token in parsed_sentence])\n",
    "\n",
    "print([(token.text,token.idx) for token in parsed_sentence])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update byte diff flag\n",
    "engine = create_engine('mysql+pymysql://gms:nej123@localhost/concepts', pool_pre_ping=True)\n",
    "dir_file = '/Users/gms/development/nlp/nlpie/data/ensembling-u01/fairview/system_annotations/data_in_preprocessed/'\n",
    "\n",
    "df=pd.read_table(dir_file + 'jbdc',header=None)\n",
    "print(df)\n",
    "sql = text(\"SELECT * FROM concepts.fv_reference WHERE `case` = :e1\")\n",
    "\n",
    "for row in df.itertuples():\n",
    "    #print(row._1)\n",
    "    resp = engine.execute(sql, e1=row._1).fetchall()\n",
    "    if len(resp) > 0:\n",
    "        #print(resp)\n",
    "        sql_u = text(\"UPDATE concepts.fv_reference SET byte_diff = 1 WHERE `case` = :e1\")\n",
    "        engine.execute(sql_u, e1=row._1)\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'span', 'start', 'end', 'case', 'concept', 'txt',\n",
      "       'semtypes', 'annotator'],\n",
      "      dtype='object')\n",
      "   Unnamed: 0        span  start  end        case                concept                    txt semtypes annotator\n",
      "0       10295  (422, 433)    422  433  0000089745            elbow right            elbow right  Anatomy    George\n",
      "1       10896  (667, 671)    667  671  0000089745                   rash                   rash  Finding    George\n",
      "2       11335  (682, 686)    682  686  0000089745                   rash                   rash  Finding    George\n",
      "3       11507  (707, 728)    707  728  0000089745  rash is gradual onset  rash is gradual onset  Finding    George\n",
      "4       11301  (765, 782)    765  782  0000089745       rash right elbow       rash right elbow  Finding    George\n"
     ]
    }
   ],
   "source": [
    "dir_file = '/Users/gms/development/nlp/nlpie/data/ensembling-u01/output/' \n",
    "engine = create_engine('mysql+pymysql://gms:nej123@localhost/concepts', pool_pre_ping=True)\n",
    "\n",
    "df=pd.read_csv(dir_file + 'George_fv_concepts_test.csv', dtype={'case': str})\n",
    "print(df.columns)\n",
    "#cols_to_keep = ['cui', 'end', 'begin', 'corpus', 'similarity', 'system', 'type', 'note_id', 'semtypes']\n",
    "#df = df[cols_to_keep]\n",
    "print(df.head())\n",
    "df.to_sql('George_fv_concepts_test', engine, if_exists=\"append\", index=False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
