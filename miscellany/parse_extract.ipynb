{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation class for UIMA systems\n",
    "class AnnotationSystems(object):\n",
    "    \"\"\"\n",
    "    CAS XMI Annotations of interest\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\" \n",
    "        annotation base types\n",
    "        \"\"\"\n",
    "        \n",
    "        self.biomedicus_dir = \"biomedicus_out/\"\n",
    "        self.biomedicus_types = [\"biomedicus.v2.UmlsConcept\",\n",
    "                                 \"biomedicus.v2.Negated\"]#,\n",
    "                                 #\"biomedicus.v2.Acronym\",\n",
    "                                 #\"biomedicus.v2.TemporalPhrase\"]\n",
    "        \n",
    "        \n",
    "        self.clamp_dir = \"clamp_out/\"\n",
    "        #self.clamp_dir = \"Annotated_XMI/\"\n",
    "        self.clamp_types = [\"edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA\",\n",
    "                            \"edu.uth.clamp.nlp.typesystem.ClampRelationUIMA\"]\n",
    "        \n",
    "        \n",
    "        self.ctakes_dir = \"ctakes_out/\"\n",
    "        self.ctakes_types = [\"org.apache.ctakes.typesystem.type.textsem.DiseaseDisorderMention\",\n",
    "                             \"org.apache.ctakes.typesystem.type.textsem.MedicationMention\",\n",
    "                             \"org.apache.ctakes.typesystem.type.textsem.ProcedureMention\",\n",
    "                             \"org.apache.ctakes.typesystem.type.textsem.SignSymptomMention\",\n",
    "                             \"org.apache.ctakes.typesystem.type.textsem.AnatomicalSiteMention\"]#,\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.DateAnnotation\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.MeasurementAnnotation\"]\n",
    "       \n",
    "        self.metamap_dir = \"metamap_out/\"\n",
    "        self.metamap_types = [\"org.metamap.uima.ts.Candidate\",\n",
    "                              #\"org.metamap.uima.ts.CuiConcept\",\n",
    "                              \"org.metamap.uima.ts.Negation\"]\n",
    "                \n",
    "       \n",
    "    def get_system_type(self, system):\n",
    "        \n",
    "        \"\"\"\n",
    "        return system types\n",
    "        \"\"\"\n",
    "        \n",
    "        if system == \"biomedicus\":\n",
    "            view = \"Analysis\"\n",
    "            \n",
    "        else:\n",
    "            view = \"_InitialView\"\n",
    "\n",
    "        if system == 'biomedicus':\n",
    "            types = self.biomedicus_types\n",
    "            output = self.biomedicus_dir\n",
    "\n",
    "        elif system == 'clamp':\n",
    "            types = self.clamp_types\n",
    "            output = self.clamp_dir\n",
    "\n",
    "        elif system == 'ctakes':\n",
    "            types = self.ctakes_types\n",
    "            output = self.ctakes_dir\n",
    "\n",
    "        elif system == 'metamap':\n",
    "            types = self.metamap_types\n",
    "            output = self.metamap_dir\n",
    "            \n",
    "        return types, view, output\n",
    "    \n",
    "annSys = AnnotationSystems()\n",
    "\n",
    "# extract attributes from cas Annotation object\n",
    "def get_attribs(v):\n",
    "    attribs = []\n",
    "    for sentence in v:\n",
    "        #print(sentence)\n",
    "        for s in sentence.__dir__():\n",
    "            if '__' not in s:\n",
    "                if s not in attribs:\n",
    "                    #print(s)\n",
    "                    attribs.append(s)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    return attribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cassis(system, typesystem):\n",
    "   \n",
    "    #tic=timeit.default_timer() \n",
    "\n",
    "    print(system)\n",
    "\n",
    "    # types for metamap\n",
    "\n",
    "    if system == 'metamap':\n",
    "        t = typesystem.create_type(name='org.apache.uima.examples.SourceDocumentInformation', supertypeName='uima.tcas.Annotation')\n",
    "        typesystem.add_feature(t, name='uri', rangeTypeName='uima.cas.String')\n",
    "        typesystem.add_feature(t, name=\"offsetInSource\", rangeTypeName=\"uima.cas.Integer\")\n",
    "        typesystem.add_feature(t, name=\"documentSize\", rangeTypeName=\"uima.cas.Integer\")\n",
    "        typesystem.add_feature(t, name=\"lastSegment\", rangeTypeName=\"uima.cas.Integer\")\n",
    "\n",
    "    # features for ctakes\n",
    "\n",
    "    if system == 'ctakes':\n",
    "        t = typesystem.get_type('org.apache.ctakes.typesystem.type.structured.Metadata')\n",
    "        typesystem.add_feature(t, name='patientIdentifier', rangeTypeName='uima.cas.String')\n",
    "        \n",
    "        t = typesystem.get_type('org.apache.ctakes.typesystem.type.textsem.DiseaseDisorderMention')\n",
    "        typesystem.add_feature(t, name='id', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='ontologyConceptArr', rangeTypeName='uima.cas.FSArray', elementType='org.apache.ctakes.typesystem.type.refsem.UmlsConcept')\n",
    "        typesystem.add_feature(t, name='subject', rangeTypeName='uima.cas.String')\n",
    "        #typesystem.add_feature(t, name='typeID', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='discoveryTechnique', rangeTypeName='uima.cas.Integer')\n",
    "        #typesystem.add_feature(t, name='confidence', rangeTypeName='uima.cas.Double')\n",
    "        typesystem.add_feature(t, name='polarity', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='uncertainty', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='conditional', rangeTypeName='uima.cas.Boolean')\n",
    "        typesystem.add_feature(t, name='generic', rangeTypeName='uima.cas.Boolean')\n",
    "        typesystem.add_feature(t, name='historyOf', rangeTypeName='uima.cas.Integer')\n",
    "\n",
    "        t = typesystem.get_type('org.apache.ctakes.typesystem.type.textsem.SignSymptomMention')\n",
    "        typesystem.add_feature(t, name='id', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='ontologyConceptArr', rangeTypeName='uima.cas.FSArray', elementType='org.apache.ctakes.typesystem.type.refsem.UmlsConcept')\n",
    "        typesystem.add_feature(t, name='subject', rangeTypeName='uima.cas.String')\n",
    "        #typesystem.add_feature(t, name='typeID', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='discoveryTechnique', rangeTypeName='uima.cas.Integer')\n",
    "        #typesystem.add_feature(t, name='confidence', rangeTypeName='uima.cas.Double')\n",
    "        typesystem.add_feature(t, name='polarity', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='uncertainty', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='conditional', rangeTypeName='uima.cas.Boolean')\n",
    "        typesystem.add_feature(t, name='generic', rangeTypeName='uima.cas.Boolean')\n",
    "        typesystem.add_feature(t, name='historyOf', rangeTypeName='uima.cas.Integer')\n",
    "\n",
    "        t = typesystem.get_type('org.apache.ctakes.typesystem.type.textsem.MedicationMention')\n",
    "        typesystem.add_feature(t, name='id', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='ontologyConceptArr', rangeTypeName='uima.cas.FSArray', elementType='org.apache.ctakes.typesystem.type.refsem.UmlsConcept')\n",
    "        typesystem.add_feature(t, name='subject', rangeTypeName='uima.cas.String')\n",
    "        #typesystem.add_feature(t, name='typeID', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='discoveryTechnique', rangeTypeName='uima.cas.Integer')\n",
    "        #typesystem.add_feature(t, name='confidence', rangeTypeName='uima.cas.Double')\n",
    "        typesystem.add_feature(t, name='polarity', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='uncertainty', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='conditional', rangeTypeName='uima.cas.Boolean')\n",
    "        typesystem.add_feature(t, name='generic', rangeTypeName='uima.cas.Boolean')\n",
    "        typesystem.add_feature(t, name='historyOf', rangeTypeName='uima.cas.Integer')\n",
    "\n",
    "        t = typesystem.get_type('org.apache.ctakes.typesystem.type.textsem.ProcedureMention')\n",
    "        typesystem.add_feature(t, name='id', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='subject', rangeTypeName='uima.cas.String')\n",
    "        #typesystem.add_feature(t, name='typeID', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='discoveryTechnique', rangeTypeName='uima.cas.Integer')\n",
    "        #typesystem.add_feature(t, name='confidence', rangeTypeName='uima.cas.Double')\n",
    "        typesystem.add_feature(t, name='polarity', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='uncertainty', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='conditional', rangeTypeName='uima.cas.Boolean')\n",
    "        typesystem.add_feature(t, name='generic', rangeTypeName='uima.cas.Boolean')\n",
    "        typesystem.add_feature(t, name='historyOf', rangeTypeName='uima.cas.Integer')\n",
    "\n",
    "        t = typesystem.get_type('org.apache.ctakes.typesystem.type.textsem.AnatomicalSiteMention')\n",
    "        typesystem.add_feature(t, name='id', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='ontologyConceptArr', rangeTypeName='uima.cas.FSArray', elementType='org.apache.ctakes.typesystem.type.refsem.UmlsConcept')\n",
    "        typesystem.add_feature(t, name='subject', rangeTypeName='uima.cas.String')\n",
    "        #typesystem.add_feature(t, name='typeID', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='discoveryTechnique', rangeTypeName='uima.cas.Integer')\n",
    "        #typesystem.add_feature(t, name='confidence', rangeTypeName='uima.cas.Double')\n",
    "        typesystem.add_feature(t, name='polarity', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='uncertainty', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='conditional', rangeTypeName='uima.cas.Boolean')\n",
    "        typesystem.add_feature(t, name='generic', rangeTypeName='uima.cas.Boolean')\n",
    "        typesystem.add_feature(t, name='historyOf', rangeTypeName='uima.cas.Integer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sofa -> db\n",
    "def write_sofa(u):\n",
    "    d = {}\n",
    "    d[\"note_id\"] = str(u)\n",
    "    d[\"sofa\"] = view.sofa_string\n",
    "    d[\"corpus\"] = corpus\n",
    "\n",
    "    # does it exist?\n",
    "    if engine.dialect.has_table(engine, \"sofas\"):\n",
    "        sql = text(\"SELECT * FROM test.sofas WHERE note_id = :e1\")\n",
    "        resp = engine.execute(sql, e1=u).fetchall()\n",
    "    else:\n",
    "        resp = []\n",
    "\n",
    "    if len(resp) == 0:            \n",
    "        #pd.DataFrame(d, index=[0]).to_sql(\"sofas\", engine, if_exists=\"append\")  \n",
    "        pd.DataFrame(d, index=[0]).to_sql(\"sofas\", engine, if_exists=\"append\")  \n",
    "        \n",
    "\n",
    "def get_dict(keys, sentence, d):\n",
    "    for i in range(len(keys)):\n",
    "        key = keys[i]\n",
    "        val = sentence.__getattribute__(keys[i])\n",
    "        d[key] = val\n",
    "    return d\n",
    "\n",
    "def add_keys(df, system, t, u, corpus, fname):\n",
    "    df[\"system\"] = system\n",
    "    df[\"type\"] = t\n",
    "    df[\"note_id\"] = u\n",
    "    df[\"corpus\"] = corpus\n",
    "    df[\"filename\"] = fname\n",
    "    \n",
    "    return df\n",
    "\n",
    "def append_to_df(df, d):\n",
    "    import pandas as pd\n",
    "    frames = [ df, pd.DataFrame(d, index=[0]) ]\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse system annotations\n",
    "def main():\n",
    "    import os, glob\n",
    "    import pymysql\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    from sqlalchemy.engine import create_engine\n",
    "    from sqlalchemy.sql import text\n",
    "    from cassis import load_typesystem, load_cas_from_xmi\n",
    "\n",
    "    # connection string\n",
    "    engine = create_engine('mysql+pymysql://gms:nej123@localhost/mhealth', pool_pre_ping=True)\n",
    "    systems = [\"clamp\", \"metamap\", \"ctakes\", \"biomedicus\"]\n",
    "    #systems = ['ctakes']\n",
    "    \n",
    "    corpora = [\"fairview\"]\n",
    "    parse_to_sql = True\n",
    "    \n",
    "    if parse_to_sql:\n",
    "        \n",
    "        for corpus in corpora:\n",
    "            print(\"CORPUS:\", corpus)\n",
    "\n",
    "            for system in systems:\n",
    "\n",
    "                print(\"SYSTEM:\", system)\n",
    "\n",
    "                types, view_, output = annSys.get_system_type(system)\n",
    "                \n",
    "                dir_test = '/Users/gms/development/nlp/nlpie/scripts/jupyter/ensembling/system_annotations/typesystems/' + system + '/'\n",
    "\n",
    "                with open(dir_test + 'TypeSystem.xml', 'rb') as f:\n",
    "                    typesystem = load_typesystem(f)\n",
    "                \n",
    "                init_cassis(system, typesystem)\n",
    "                \n",
    "                # parse directory\n",
    "                if corpus in [\"i2b2\", \"mipacq\"]:\n",
    "                    directory_to_parse = '/Users/gms/development/nlp/nlpie/data/ensembling-u01/' + corpus + '/rerun-november-2019/' +  system + '_out/' #data_to_analyze/all/'\n",
    "                \n",
    "                elif corpus == 'fairview':\n",
    "                    directory_to_parse = '/Users/gms/development/nlp/nlpie/data/ensembling-u01/' + corpus + '/system_annotations/data_in_preprocessed/' +  system + '_out/'#data_to_analyze/all/'\n",
    "               \n",
    "                elif 'clinical_trial' in corpus:\n",
    "                    # if parsing gold standard, change to_sql below to ensure proper target directory\n",
    "                    #directory_to_parse = '/Users/gms/development/nlp/nlpie/data/clinical_trial2/' +  system + '_out/'\n",
    "                    #directory_to_parse = '/Users/gms/development/nlp/nlpie/data/clinical_trial2/gold_standard/'\n",
    "                    directory_to_parse = '/Users/gms/development/nlp/nlpie/data/clinical_trial2/bert-output/'\n",
    "                else:\n",
    "                    directory_to_parse = '/Users/gms/development/nlp/nlpie/data/mimic/'\n",
    "                    \n",
    "                print(directory_to_parse)\n",
    "\n",
    "                for fname in glob.glob(directory_to_parse + '/*.xmi'):\n",
    "                #for fname in glob.glob(directory_to_parse + '/527982345.txt.xmi'):\n",
    "\n",
    "                    file = os.path.basename(fname)\n",
    "                    u = file.split('.')[0]\n",
    "\n",
    "                    print(u)\n",
    "\n",
    "                    # load cas\n",
    "                    with open(directory_to_parse + file, 'rb') as f:\n",
    "                        cas = load_cas_from_xmi(f, typesystem=typesystem)\n",
    "\n",
    "                    # load view\n",
    "                    view = cas.get_view(view_)\n",
    "                    \n",
    "                    # write sofa object here:\n",
    "                    #write_sofa(u)\n",
    "\n",
    "                    for t in types:\n",
    "                        print(\"TYPE:\", t)\n",
    "                       \n",
    "                        # get list for filtering df\n",
    "                        \n",
    "                        attribs = get_attribs(view.select(t))\n",
    "                        x = t.split('.')\n",
    "                        table_name = system[0:3] + '_' + x[0] + '_' + x[len(x)-1]\n",
    "\n",
    "                        # Annotation object -> dataframe\n",
    "                        def get_df(v, attribs):\n",
    "                            df = pd.DataFrame()\n",
    "                            d = dict() \n",
    "\n",
    "                            # only parse if type exists in file\n",
    "                            if view.select(t):\n",
    "                                for sentence in view.select(t):\n",
    "                                    \n",
    "                                    dd = dict()\n",
    "                                    if t == 'org.metamap.uima.ts.Negation':\n",
    "                                        keys = ['cuiConcepts', 'ncSpans', 'negTrigger', 'negType']\n",
    "                                        d = get_dict(keys, sentence, d)\n",
    "\n",
    "                                        if len(d['cuiConcepts']) == len(d['ncSpans']):\n",
    "                                             for x in range(len(d['cuiConcepts'])):\n",
    "                                                dd['cui'] = d['cuiConcepts'][x].negExCui \n",
    "                                                dd['concept'] = d['cuiConcepts'][x].negExConcept \n",
    "                                                dd['begin'] = d['ncSpans'][x].begin \n",
    "                                                dd['end'] = d['ncSpans'][x].end\n",
    "                                                dd['negTrigger'] = d['negTrigger']\n",
    "                                                dd['negType'] = d['negType']\n",
    "                                                dd['cuiConcepts'] = ' '.join(map(str,d['cuiConcepts']))\n",
    "                                                dd['ncSpans'] = ' '.join(map(str,d['ncSpans']))\n",
    "                                            \n",
    "                                                df = append_to_df(df, dd)\n",
    "                                                df = add_keys(df, system, t, u, corpus, fname)\n",
    "                                            \n",
    "                                        elif len(d['cuiConcepts']) > len(d['ncSpans']) and len(d['ncSpans']) == 1:\n",
    "                                            for x in range(len(d['cuiConcepts'])):\n",
    "                                                dd['cui'] = d['cuiConcepts'][x].negExCui \n",
    "                                                dd['concept'] = d['cuiConcepts'][x].negExConcept\n",
    "                                                dd['begin'] = d['ncSpans'][0].begin\n",
    "                                                dd['end'] = d['ncSpans'][0].end\n",
    "                                                dd['negTrigger'] = d['negTrigger']\n",
    "                                                dd['negType'] = d['negType']\n",
    "                                                dd['cuiConcepts'] = ' '.join(map(str,d['cuiConcepts']))\n",
    "                                                dd['ncSpans'] = ' '.join(map(str,d['ncSpans']))\n",
    "                                                \n",
    "                                                df = append_to_df(df, dd)\n",
    "                                                df = add_keys(df, system, t, u, corpus, fname)\n",
    "                                            \n",
    "                                                \n",
    "                                        elif len(d['cuiConcepts']) < len(d['ncSpans']) and len(d['cuiConcepts']) == 1:\n",
    "                                            for x in range(len(d['ncSpans'])):\n",
    "                                                dd['cui'] = d['cuiConcepts'][0].negExCui \n",
    "                                                dd['concept'] = d['cuiConcepts'][0].negExConcept\n",
    "                                                dd['begin'] = d['ncSpans'][x].begin\n",
    "                                                dd['end'] = d['ncSpans'][x].end\n",
    "                                                dd['negTrigger'] = d['negTrigger']\n",
    "                                                dd['negType'] = d['negType']\n",
    "                                                dd['cuiConcepts'] = ' '.join(map(str,d['cuiConcepts']))\n",
    "                                                dd['ncSpans'] = ' '.join(map(str,d['ncSpans']))\n",
    "                                                \n",
    "                                                df = append_to_df(df, dd)\n",
    "                                                df = add_keys(df, system, t, u, corpus, fname)\n",
    "                                                \n",
    "                                        else:\n",
    "                                            print('Error: MetaMap Negation', d)\n",
    "                                    \n",
    "                                    elif t == 'edu.uth.clamp.nlp.typesystem.ClampRelationUIMA':\n",
    "                                        keys = ['semanticTag', 'entTo', 'entFrom']\n",
    "                                        d = get_dict(keys, sentence, d)\n",
    "                                        \n",
    "                                        if d['semanticTag'] == 'NEG_Of':\n",
    "                                            #print(d['entTo'], '\\n', d['entFrom'])\n",
    "                                            attribute =  json.loads(d['entFrom'].attribute)\n",
    "                                            if 'umlsCuiDesc' in attribute:\n",
    "                                                dd['cui'] = d['entFrom'].cui.split()[0].replace(',','')  \n",
    "                                                dd['concept'] = attribute['umlsCuiDesc']\n",
    "                                                dd['begin'] = d['entFrom'].begin\n",
    "                                                dd['end'] = d['entFrom'].end\n",
    "                                                dd['semanticTag'] = d['semanticTag']\n",
    "                                                dd['entTo'] = str(d['entTo'])\n",
    "                                                dd['entFrom'] = str(d['entFrom'])\n",
    "                                                \n",
    "                                                df = append_to_df(df, dd)\n",
    "                                                df = add_keys(df, system, t, u, corpus, fname)\n",
    "                                        \n",
    "                                    elif t == 'biomedicus.v2.Negated':\n",
    "                                        keys = ['begin', 'end']\n",
    "                                        d = get_dict(keys, sentence, d)\n",
    "                                        df = append_to_df(df, d)\n",
    "                                        df = add_keys(df, system, t, u, corpus, fname)\n",
    "                                        \n",
    "                                    elif t == 'biomedicus.v2.UmlsConcept':\n",
    "                                        keys = ['begin', 'end', 'cui', 'tui', 'confidence']\n",
    "                                        d = get_dict(keys, sentence, d)\n",
    "                                        \n",
    "                                        df = append_to_df(df, d)\n",
    "                                        df = add_keys(df, system, t, u, corpus, fname)\n",
    "                                    \n",
    "                                    # no labvalue -> no UMLS info\n",
    "                                    elif t == 'edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA':\n",
    "                                        keys = ['assertion', 'cui', 'begin', 'end', 'attribute', 'semanticTag']\n",
    "                                        d = get_dict(keys, sentence, d)\n",
    "                                        \n",
    "                                        if 'attribute' in d and d['attribute']:\n",
    "                                            attribute =  json.loads(d['attribute'])\n",
    "                                            #if d['assertion'] == 'present':\n",
    "                                            if 'umlsCuiDesc' in attribute:\n",
    "                                                dd['concept'] = attribute['umlsCuiDesc']\n",
    "                                                dd['sentence_prob'] = attribute['sentence_prob'] \n",
    "                                                dd['concept_prob'] = attribute['concept_prob']\n",
    "                                                dd['assertion'] = d['assertion']\n",
    "                                                dd['cui'] = d['cui'].split()[0].replace(',','') \n",
    "                                                dd['begin'] = d['begin']\n",
    "                                                dd['end'] = d['end']\n",
    "                                                dd['semanticTag'] = d['semanticTag']\n",
    "                                                dd['attribute'] = str(d['attribute'])\n",
    "\n",
    "                                                df = append_to_df(df, dd)\n",
    "                                                df = add_keys(df, system, t, u, corpus, fname)\n",
    "                                        else:\n",
    "                                            if d['semanticTag'] == 'drug':\n",
    "                                                if d['cui'] and 'C' in d['cui']:\n",
    "                                                    dd['cui'] = str(d['cui'].split()[0]).replace(',','') \n",
    "                                                    dd['begin'] = d['begin']\n",
    "                                                    dd['end'] = d['end']\n",
    "                                                    dd['semanticTag'] = d['semanticTag']\n",
    "\n",
    "                                                    df = append_to_df(df, dd)\n",
    "                                                    df = add_keys(df, system, t, u, corpus, fname)\n",
    "                                            \n",
    "                                                    \n",
    "                                    elif 'Mention' in t:\n",
    "                                        keys = ['begin', 'conditional', 'confidence', 'end', 'historyOf', 'ontologyConceptArr', 'polarity', \n",
    "                                                'uncertainty']\n",
    "                                        d = get_dict(keys, sentence, d)\n",
    "                                        \n",
    "                                        for x in range(len(d['ontologyConceptArr'])):\n",
    "                                                dd['cui'] = d['ontologyConceptArr'][x].cui\n",
    "                                                dd['tui'] = d['ontologyConceptArr'][x].tui\n",
    "                                                dd['concept'] = d['ontologyConceptArr'][x].preferredText\n",
    "                                                dd['ontologyConceptArr'] = ' '.join(map(str,d['ontologyConceptArr']))\n",
    "                                                dd['begin'] = d['begin']\n",
    "                                                dd['end'] = d['end']\n",
    "                                                dd['conditional'] = d['conditional']\n",
    "                                                dd['confidence'] = d['confidence']\n",
    "                                                dd['historyOf'] = d['historyOf']\n",
    "                                                dd['polarity'] = d['polarity']\n",
    "                                                dd['uncertainty'] = d['uncertainty']                    \n",
    "                                                                    \n",
    "                                                df = append_to_df(df, dd)\n",
    "                                                df = add_keys(df, system, t, u, corpus, fname)\n",
    "                                                \n",
    "                                    elif t == 'org.metamap.uima.ts.Candidate':\n",
    "                                        keys =  ['begin', 'concept', 'cui', 'end',\n",
    "                                                 'preferred', 'score', 'semanticTypes', 'matchedwords']\n",
    "                                        d = get_dict(keys, sentence, d)\n",
    "                                        d['semanticTypes'] = \" \".join(d['semanticTypes'])\n",
    "                                        d['matchedwords'] = \" \".join(d['matchedwords'])\n",
    "                                        \n",
    "                                        df = append_to_df(df, d)\n",
    "                                        df = add_keys(df, system, t, u, corpus, fname)\n",
    "                            \n",
    "                            return df\n",
    "                            \n",
    "                        annotations = get_df(view.select(t), attribs)\n",
    "                        \n",
    "                        # write to database\n",
    "                        #annotations.to_csv('~/Desktop/test.csv')\n",
    "                        annotations.to_sql(table_name, engine, if_exists=\"append\") \n",
    "                        \n",
    "    # write out annotations for non-cui tables\n",
    "    else:\n",
    "        \n",
    "        sys_ann_other = pd.DataFrame()\n",
    "        for system in systems:\n",
    "                \n",
    "            types, view_, output = annSys.get_system_type(system)\n",
    "            print(\"SYSTEM:\", system)\n",
    "           \n",
    "            for t in types:\n",
    "\n",
    "                x = t.split('.')\n",
    "                table_name = system[0:3] + '_' + x[0] + '_' + x[len(x)-1]\n",
    "\n",
    "                sql = \"SELECT * FROM test.\" + table_nam \n",
    "                df = pd.read_sql(sql, engine)\n",
    "\n",
    "                cols_to_keep = ['begin', 'end', 'type', 'system', 'note_id', 'corpus', 'filename']\n",
    "                #print(system, t, table_name, list(df[cols_to_keep].columns.values))\n",
    "                \n",
    "                frames = [ sys_ann_other, df[cols_to_keep] ]\n",
    "                sys_ann_other = pd.concat(frames, ignore_index=True)\n",
    "        \n",
    "        print(sys_ann_other.drop_duplicates())\n",
    "        sys_ann_other.drop_duplicates().to_csv('/Users/gms/development/nlp/nlpie/data/amia-2019/output/analytical_' + corpus + '.csv')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    %prun main()\n",
    "    print('done!')\n",
    "    pass\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use mhealth;\n",
    "\n",
    "SELECT u.begin, u.end, null as concept, u.cui, u.confidence as score, u.tui as semType, u.note_id, u.type, u.system, -1 as polarity FROM bio_biomedicus_UmlsConcept u left join bio_biomedicus_Negated n\n",
    "on u.begin = n.begin and u.end = n.end and u.note_id = n.note_id\n",
    "where n.begin is not null and n.end is not null and n.note_id is not null\n",
    "union distinct\n",
    "SELECT u.begin, u.end, null as concept, u.cui, u.confidence as score, u.tui as semType, u.note_id, u.type, u.system, 1 as polarity FROM bio_biomedicus_UmlsConcept u left join bio_biomedicus_Negated n\n",
    "on u.begin = n.begin and u.end = n.end and u.note_id = n.note_id\n",
    "where n.begin is null and n.end is null and n.note_id is null\n",
    "\n",
    "union\n",
    "\n",
    "select u.begin, u.end, u.concept, u.cui, u.concept_prob as score, u.semanticTag as semType, u.note_id, u.type, u.system, -1 as polarity from cla_edu_ClampNameEntityUIMA u left join cla_edu_ClampRelationUIMA r\n",
    "on u.begin = r.begin and u.end = r.end and u.note_id = r.note_id\n",
    "where u.assertion = 'absent' and r.begin is not null and r.end is not null and r.note_id is not null\n",
    "union distinct\n",
    "select u.begin, u.end, u.concept, u.cui, u.concept_prob as score, u.semanticTag as semType, u.note_id, u.type, u.system, 1 as polarity from cla_edu_ClampNameEntityUIMA u left join cla_edu_ClampRelationUIMA r\n",
    "on u.begin = r.begin and u.end = r.end and u.note_id = r.note_id\n",
    "where (u.assertion = 'present' or u.assertion is null) and r.begin is null and r.end is null and r.note_id is null\n",
    "\n",
    "union\n",
    "\n",
    "select c.begin, c.end, c.preferred as concept, c.cui, abs(c.score) as score, c.semanticTypes as semType, c.note_id, c.type, c.system, -1 as polarity from met_org_candidate c left join met_org_Negation n\n",
    "on c.begin = n.begin and c.end = n.end and c.note_id = n.note_id\n",
    "where n.begin is not null and n.end is not null and n.note_id is not null\n",
    "union distinct\n",
    "select c.begin, c.end, c.preferred as concept, c.cui, abs(c.score) as score, c.semanticTypes as semType, c.note_id, c.type, c.system, 1 as polarity from met_org_candidate c left join met_org_Negation n\n",
    "on c.begin = n.begin and c.end = n.end and c.note_id = n.note_id\n",
    "where n.begin is null and n.end is null and n.note_id is null\n",
    "\n",
    "union\n",
    "\n",
    "select  begin, end, concept, cui, null as score, substring_index(substring_index(type,',', 7),'.', -(1)) as semtype,\n",
    "\t\tnote_id, 'ctakes_mentions' as type,  `system`, polarity  \n",
    "    from cta_org_anatomicalsitemention \n",
    "union distinct \n",
    "select begin, end, concept, cui, null as score, substring_index(substring_index(type,',', 7),'.', -(1)) as semtype,\n",
    "\t   note_id, 'ctakes_mentions' as type,  `system`, polarity \n",
    "    from cta_org_diseasedisordermention \n",
    "union distinct \n",
    "select begin, end, concept, cui, null as score, substring_index(substring_index(type,',', 7),'.', -(1)) as semtype,\n",
    "\t   note_id, 'ctakes_mentions' as type,  `system`, polarity \n",
    "    from cta_org_medicationmention \n",
    "union distinct \n",
    "select begin, end, concept, cui, null as score, substring_index(substring_index(type,',', 7),'.', -(1)) as semtype,\n",
    "\t   note_id, 'ctakes_mentions' as type,  `system`, polarity \n",
    "    from cta_org_proceduremention \n",
    "union distinct \n",
    "select begin, end, concept, cui, null as score, substring_index(substring_index(type,',', 7),'.', -(1)) as semtype,\n",
    "\t   note_id, 'ctakes_mentions' as type,  `system`, polarity \n",
    "    from cta_org_signsymptommention;\n",
    "    \n",
    "\n",
    "SELECT begin, end, term as concept, cui, similarity as score, semtypes as semType, note_id, type, `system`, 0 as polarity FROM mhealth.qumls_all;\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tables for\n",
    "# https://stackoverflow.com/questions/12680754/split-explode-pandas-dataframe-string-entry-to-separate-rows\n",
    "\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "#import modin.pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy.engine import create_engine\n",
    "\n",
    "def get_analytical_set(sql):\n",
    "    \n",
    "    engine = create_engine('mysql+pymysql://gms:nej123@localhost/mhealth', pool_pre_ping=True)\n",
    "\n",
    "    df = pd.read_sql(sql, engine)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    print('BEGIN')\n",
    "    \n",
    "    #GENERATE analytical table\n",
    "\n",
    "    analytical_cui = pd.DataFrame()\n",
    "\n",
    "    ### ------> qumls\n",
    "\n",
    "    \n",
    "    # TODO get cases method\n",
    "    \n",
    "    \n",
    "    for case in cases:\n",
    "        print(case)\n",
    "    \n",
    "        sql = \"SELECT begin, end, term as concept, cui, similarity as score, semtypes as semtype, note_id, type, `system`, 0 as polarity FROM mhealth.qumls_all;\"\n",
    "\n",
    "        df = get_analytical_set(sql)\n",
    "\n",
    "        test = df[df['note_id'] == case].copy()\n",
    "        print('qumls', len(test))\n",
    "\n",
    "        frames = [ analytical_cui, disambiguate(test) ]\n",
    "        analytical_cui = pd.concat(frames, ignore_index=True, sort=False) \n",
    "\n",
    "        print(analytical_cui[0:1], analytical_cui.columns)\n",
    "\n",
    "        ### ------> b9\n",
    "\n",
    "        sql = \"\"\"\n",
    "        select u.begin, u.end, null as concept, u.cui, u.confidence as score, u.tui as semtype, u.note_id, u.type, u.system, -1 as polarity \n",
    "        from bio_biomedicus_umlsconcept u left join bio_biomedicus_negated n\n",
    "            on u.begin = n.begin and u.end = n.end and u.note_id = n.note_id\n",
    "            where n.begin is not null and n.end is not null and n.note_id is not null\n",
    "\n",
    "        union distinct\n",
    "\n",
    "        select u.begin, u.end, null as concept, u.cui, u.confidence as score, u.tui as semtype, u.note_id, u.type, u.system, 1 as polarity \n",
    "        from bio_biomedicus_umlsconcept u left join bio_biomedicus_negated n\n",
    "            on u.begin = n.begin and u.end = n.end and u.note_id = n.note_id\n",
    "            where n.begin is null and n.end is null and n.note_id is null;\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        df = get_analytical_set(sql)\n",
    "\n",
    "        test = df[df['note_id'] == case].copy()\n",
    "        print('b9 umls', len(test))\n",
    "\n",
    "        frames = [ analytical_cui, disambiguate(test) ]\n",
    "        analytical_cui = pd.concat(frames, ignore_index=True, sort=False) \n",
    "\n",
    "        print(analytical_cui[0:1], analytical_cui.columns)\n",
    "\n",
    "        ### ------> clamp\n",
    "\n",
    "        sql = \"\"\"\n",
    "        select u.begin, u.end, u.concept, u.cui, u.concept_prob as score, u.semanticTag as semtype, u.note_id, u.type, u.system, -1 as polarity \n",
    "        from cla_edu_ClampNameEntityUIMA u left join cla_edu_ClampRelationUIMA r\n",
    "            on u.begin = r.begin and u.end = r.end and u.note_id = r.note_id\n",
    "            where u.assertion = 'absent' and r.begin is not null and r.end is not null and r.note_id is not null\n",
    "\n",
    "        union distinct\n",
    "\n",
    "        select u.begin, u.end, u.concept, u.cui, u.concept_prob as score, u.semanticTag as semtype, u.note_id, u.type, u.system, 1 as polarity \n",
    "        from cla_edu_ClampNameEntityUIMA u left join cla_edu_ClampRelationUIMA r\n",
    "            on u.begin = r.begin and u.end = r.end and u.note_id = r.note_id\n",
    "            where (u.assertion = 'present' or u.assertion is null) and r.begin is null and r.end is null and r.note_id is null;\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        df = get_analytical_set(sql)\n",
    "\n",
    "        test = df[df['note_id'] == case].copy()\n",
    "        print('clamp', len(test))\n",
    "\n",
    "        frames = [ analytical_cui, disambiguate(test) ]\n",
    "        analytical_cui = pd.concat(frames, ignore_index=True, sort=False) \n",
    "\n",
    "        print(analytical_cui[0:1], analytical_cui.columns)\n",
    "\n",
    "        ### ------> ctakes\n",
    "\n",
    "        sql = \"\"\"\n",
    "        select  begin, end, concept, cui, null as score, substring_index(substring_index(type,',', 7),'.', -(1)) as semtype, note_id, 'ctakes_mentions' as type,  `system`, polarity  \n",
    "        from cta_org_anatomicalsitemention \n",
    "\n",
    "        union distinct \n",
    "\n",
    "        select begin, end, concept, cui, null as score, substring_index(substring_index(type,',', 7),'.', -(1)) as semtype, note_id, 'ctakes_mentions' as type,  `system`, polarity \n",
    "        from cta_org_diseasedisordermention \n",
    "\n",
    "        union distinct \n",
    "\n",
    "        select begin, end, concept, cui, null as score, substring_index(substring_index(type,',', 7),'.', -(1)) as semtype, note_id, 'ctakes_mentions' as type,  `system`, polarity \n",
    "        from cta_org_medicationmention \n",
    "\n",
    "        union distinct \n",
    "\n",
    "        select begin, end, concept, cui, null as score, substring_index(substring_index(type,',', 7),'.', -(1)) as semtype, note_id, 'ctakes_mentions' as type,  `system`, polarity \n",
    "        from cta_org_proceduremention \n",
    "\n",
    "        union distinct \n",
    "\n",
    "        select begin, end, concept, cui, null as score, substring_index(substring_index(type,',', 7),'.', -(1)) as semtype, note_id, 'ctakes_mentions' as type,  `system`, polarity \n",
    "        from cta_org_signsymptommention;\n",
    "\n",
    "        \"\"\"\n",
    "        df = get_analytical_set(sql)\n",
    "\n",
    "        test = df[df['note_id'] == case].copy()\n",
    "        print('ctakes mentitons', len(test))\n",
    "\n",
    "        frames = [ analytical_cui, disambiguate(test) ]\n",
    "        analytical_cui = pd.concat(frames, ignore_index=True, sort=False) \n",
    "\n",
    "        print(analytical_cui[0:1], analytical_cui.columns)\n",
    "\n",
    "        ### ------> mm\n",
    "\n",
    "        sql = \"\"\"\n",
    "        select c.begin, c.end, c.preferred as concept, c.cui, abs(c.score) as score, c.semanticTypes as semtype, c.note_id, c.type, c.system, -1 as polarity \n",
    "        from met_org_candidate c left join met_org_Negation n\n",
    "            on c.begin = n.begin and c.end = n.end and c.note_id = n.note_id\n",
    "            where n.begin is not null and n.end is not null and n.note_id is not null\n",
    "\n",
    "        union distinct\n",
    "\n",
    "        select c.begin, c.end, c.preferred as concept, c.cui, abs(c.score) as score, c.semanticTypes as semtype, c.note_id, c.type, c.system, 1 as polarity from met_org_candidate c left join met_org_Negation n\n",
    "            on c.begin = n.begin and c.end = n.end and c.note_id = n.note_id\n",
    "            where n.begin is null and n.end is null and n.note_id is null;\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        df = get_analytical_set(sql)\n",
    "\n",
    "        test = df[df['note_id'] == case].copy()\n",
    "        print('mm candidate', len(test))\n",
    "\n",
    "        frames = [ analytical_cui, disambiguate(test) ]\n",
    "        analytical_cui = pd.concat(frames, ignore_index=True, sort=False) \n",
    "\n",
    "        print(analytical_cui[0:1], analytical_cui.columns)\n",
    "\n",
    "    analytical_cui.to_csv('/Users/gms/development/nlp/nlpie/data/ensembling-u01/output/disambiguate.csv'')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    %prun main()\n",
    "    #main()\n",
    "    print('Fini!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguate(arg):\n",
    "    \n",
    "    arg['length'] = (arg.end - arg.begin).abs()\n",
    "    \n",
    "    arg.sort_values(by=['note_id','begin'],inplace=True)\n",
    "    \n",
    "    df = arg[['begin', 'end', 'note_id', 'cui', 'concept', 'semtype', 'score', 'length', 'type', 'system', 'polarity']].copy()\n",
    "    df.sort_values(by=['note_id','begin'],inplace=True)\n",
    "\n",
    "    i = 0\n",
    "    data = []\n",
    "    out = pd.DataFrame()\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        # get overlapping intervals: \n",
    "        # https://stackoverflow.com/questions/58192068/is-it-possible-to-use-pandas-overlap-in-a-dataframe\n",
    "        iix = pd.IntervalIndex.from_arrays(df.begin, df.end, closed='neither')\n",
    "        span_range = pd.Interval(row.begin, row.end)\n",
    "        fx = df[iix.overlaps(span_range)].copy()\n",
    "\n",
    "        maxLength = fx['length'].max()\n",
    "        minLength = fx['length'].min()\n",
    "        maxScore = abs(float(fx['score'].max()))\n",
    "        minScore = abs(float(fx['score'].min()))\n",
    "        \n",
    "        if maxScore > minScore:\n",
    "            fx = fx[fx['score'] == maxScore]\n",
    "\n",
    "        elif maxLength > minLength:\n",
    "            fx = fx[fx['length'] == maxLength]\n",
    "            \n",
    "        data.append(fx)\n",
    "\n",
    "    out = pd.concat(data, axis=0)\n",
    "   \n",
    "    # randomly reindex to keep random row when dropping duplicates: https://gist.github.com/cadrev/6b91985a1660f26c2742\n",
    "    out.reset_index(inplace=True)\n",
    "    out = out.reindex(np.random.permutation(out.index))\n",
    "    \n",
    "    return out.drop_duplicates(subset=['begin', 'end', 'note_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
