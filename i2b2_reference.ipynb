{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/gms/development/nlp/nlpie/data/amicus-u01/fairview_annotation/adapt_2019/data_in/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0b56032890c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mget_token_offset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# extract phrase from offset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-0b56032890c3>\u001b[0m in \u001b[0;36mget_token_offset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#directory_to_parse = '/Users/gms/development/nlp/nlpie/data/amia-2019/analysis/mipacq/data_in/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdirectory_to_parse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/gms/development/nlp/nlpie/data/amicus-u01/fairview_annotation/adapt_2019/data_in/\"\u001b[0m \u001b[0;31m#2009-12-28/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_to_parse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/gms/development/nlp/nlpie/data/amicus-u01/fairview_annotation/adapt_2019/data_in/'"
     ]
    }
   ],
   "source": [
    "# get token offset in document\n",
    "\n",
    "def search_tuple(tups, elem):\n",
    "    return filter(lambda tup: elem in tup, tups)\n",
    "\n",
    "def get_token_offset():\n",
    "    import re, os, glob, path\n",
    "    import spacy\n",
    "\n",
    "    #directory_to_parse = '/Users/gms/development/nlp/nlpie/data/amia-2019/analysis/mipacq/data_in/'\n",
    "    #directory_to_parse = '/Users/gms/development/nlp/nlpie/data/amia-2019/analysis/mipacq/data_in/'\n",
    "    directory_to_parse = \"/Users/gms/development/nlp/nlpie/data/amicus-u01/fairview_annotation/adapt_2019/data_in/\" #2009-12-28/\"\n",
    "    os.chdir(directory_to_parse)\n",
    "\n",
    "    nlp = spacy.load(\"en\")\n",
    "\n",
    "    for fname in glob.glob(directory_to_parse + '0050143945.txt'):\n",
    "\n",
    "        # get filename and use for processed output filename\n",
    "        t = os.path.basename(fname)\n",
    "        u = t.split('.')[0] + '.txt'\n",
    "        \n",
    "        # get span of text \n",
    "        #with open(directory_to_parse + '0445.txt') as f:\n",
    "        with open(directory_to_parse + '0050143945.txt') as f:\n",
    "            f1 = f.read()\n",
    "            #print(tokens)\n",
    "            doc = nlp(f1)\n",
    "            #print(doc.text)\n",
    "            print([(token.text,token.idx) for token in doc])\n",
    "            \n",
    "get_token_offset()\n",
    "\n",
    "# extract phrase from offset\n",
    "def text_offset_lookup(begin, ntokens, fname):\n",
    "    import re, os, glob, path\n",
    "    import spacy\n",
    "\n",
    "    #directory_to_parse = \"/Users/gms/development/nlp/nlpie/data/amia-2019/mipacq/source_data/source/\"\n",
    "    directory_to_parse = \"/Users/gms/development/nlp/nlpie/data/amicus-u01/fairview_annotation/data/txt/2009-12-28/\"\n",
    "    os.chdir(directory_to_parse)\n",
    "\n",
    "    nlp = spacy.load(\"en\")\n",
    "\n",
    "    for fname in glob.glob(fname):\n",
    "\n",
    "        # get filename and use for processed output filename\n",
    "        t = os.path.basename(fname)\n",
    "        u = t.split('.')[0] + '.source'\n",
    "        text = \"\"\n",
    "        \n",
    "        # get span of text \n",
    "        with open(fname) as f:\n",
    "            f1 = f.read()\n",
    "            doc = nlp(f1)\n",
    "            tokens = [(token.text,token.idx) for token in doc]\n",
    "            hit = list(search_tuple(tokens, begin))\n",
    "            idx = tokens.index(hit[0])\n",
    "            text = tokens[idx: idx + ntokens]\n",
    "            #print(text)\n",
    "    \n",
    "            return text\n",
    "            \n",
    "#text = \"stated that she\"\n",
    "#n = len(text.split())\n",
    "#begin = 505\n",
    "\n",
    "#get_sentence_from_offset(begin, n, '5024581165-5.source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# parse i2b2 reference standard\n",
    "def parse_i2b2_reference():\n",
    "    import glob, os, re\n",
    "    import pandas as pd\n",
    "\n",
    "    from sqlalchemy.engine import create_engine\n",
    "    from sqlalchemy.sql import text\n",
    "    from cassis import load_typesystem, load_cas_from_xmi\n",
    "\n",
    "    # connection string\n",
    "    engine = create_engine('mysql+pymysql://gms:nej123@localhost/test')\n",
    "\n",
    "    dir_test = \"/Users/gms/development/nlp/nlpie/data/amia-2019/i2b2/source_data/reference_standard_for_test_data/concepts/\"\n",
    "\n",
    "    def doit(text):      \n",
    "        matches=re.findall(r'\\\"(.+?)\\\"',text)\n",
    "        # matches is now ['String 1', 'String 2', 'String3']\n",
    "        return \"|\".join(matches)\n",
    "\n",
    "    # NB: issue when n:m NOT used to delineate line:position!!!!\n",
    "    def doit2(text):      \n",
    "        matches=re.findall(r'\\d+:\\d+',text)\n",
    "        # matches is now ['String 1', 'String 2', 'String3']\n",
    "        return \"|\".join(matches)                            \n",
    "\n",
    "    for fname in glob.glob(dir_test + '*.con'):\n",
    "        # get filename and use for processed output filename\n",
    "        t = os.path.basename(fname)\n",
    "\n",
    "        case = t.replace('-v1','').split('.')\n",
    "        #print(case[0])\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        with open(dir_test + t) as f:\n",
    "            test = {}\n",
    "            for line in f:\n",
    "                a = re.sub('\\|\\|', ' ', line)\n",
    "                #print(a.replace(\"c=\",\"\").replace(\"t=\",\"\"))\n",
    "                #print(doit(line).split(','))\n",
    "                #print(doit2(line))\n",
    "                s = doit(line).split('|')\n",
    "                d = doit2(line).split('|')\n",
    "\n",
    "                t = s + d\n",
    "                #print(t)\n",
    "                for i in range(len(t)):\n",
    "                    if i == 0:\n",
    "                        test[\"text\"] = t[i]\n",
    "                    elif i == 1:\n",
    "                        test[\"type\"] = t[i]\n",
    "                    elif i == 2:\n",
    "                        test[\"start_token\"] = t[i]\n",
    "                    else:\n",
    "                        test[\"end_token\"] = t[i]\n",
    "                        frames = [ df, pd.DataFrame(test, index=[0]) ]\n",
    "                        df = pd.concat(frames, ignore_index=True)\n",
    "                        df['line'], df['start'] = df['start_token'].str.split(':', 1).str\n",
    "                        _, df['end'] = df['end_token'].str.split(':', 1).str\n",
    "\n",
    "                        df[\"file\"] = case[0]\n",
    "                        df[\"corpus\"] = \"i2b2\"\n",
    "\n",
    "            # write to database\n",
    "            #print(case)\n",
    "            df.to_sql('i2b2_reference_new', engine, if_exists=\"append\") \n",
    "\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update i2b2 reference span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "def get_position(str line, int length, int start_char, int end_char):\n",
    "    '''\n",
    "    get postion of hit in document\n",
    "    '''\n",
    "    l = len(line)\n",
    "    \n",
    "    if (int(l) > 0 and isinstance(start_char, int) and isinstance(end_char, int)):\n",
    "        start = length - l + start_char\n",
    "        end = length - l + end_char\n",
    "        \n",
    "        return {'start':int(start), 'end':int(end)}\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_i2b2_text_from_span(str fname, int idx_start, int idx_end, int line_n, str text):\n",
    "    import glob, os, re\n",
    "\n",
    "    #dir_test = \"/Users/gms/development/nlp/nlpie/data/amia-2019/i2b2/source_data/test_data/\"\n",
    "    dir_test = \"/Users/gms/development/nlp/nlpie/data/amia-2019/i2b2/source_data/test_data/\"\n",
    "    i = 1\n",
    "    idx_start = int(idx_start) \n",
    "    idx_end = int(idx_end) \n",
    "    line_n = int(line_n)\n",
    "    length = 0\n",
    "    \n",
    "    with open(dir_test + fname) as f:\n",
    "        stop = False\n",
    "        for line in f:\n",
    "            length += len(line) # cummulative length of lines processed\n",
    "            if i == line_n:\n",
    "               \n",
    "                # issue when double quotes were nested\n",
    "                if '\"' in line and \"'\" in text:\n",
    "                    text = text.replace(\"'\",\"\\\"\")\n",
    "                \n",
    "                # get full sentence up to occurance of substring\n",
    "                t = \" \".join(line.split()[0:idx_start+len(text.split())])\n",
    "                start_char = t.lower().rindex(text.lower()) # get last index of substring in string\n",
    "                end_char = start_char + len(text)\n",
    "                \n",
    "                # we've processed line of interest, so exit\n",
    "                stop = True\n",
    "                return (get_position(line, length, start_char, end_char))\n",
    "            i += 1\n",
    "            if stop:\n",
    "                break    \n",
    "                \n",
    "\n",
    "def add_span_i2b2_ref():\n",
    "    #get_i2b2_text_from_span('0333.txt')\n",
    "    import pymysql\n",
    "    import pandas as pd\n",
    "    from sqlalchemy.engine import create_engine\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    start = time.time()\n",
    "\n",
    "    # connection string\n",
    "    engine = create_engine('mysql+pymysql://gms:nej123@localhost/test', pool_pre_ping=True)\n",
    "\n",
    "    sql = 'SELECT * FROM test.i2b2_reference' # where file = \"0445\"'\n",
    "    df = pd.read_sql(sql, engine)\n",
    "    #df = df.sort_values(by=['line'])\n",
    "    df['offset_start'] = None\n",
    "    df['offset_end'] = None\n",
    "\n",
    "    files = []\n",
    "    i = 0\n",
    "    for index, row in df.iterrows():\n",
    "        fname = row['file'] + '.txt'\n",
    "        \n",
    "        if fname not in files:\n",
    "            files.append(fname)\n",
    "            print(i, fname)\n",
    "            i += 1\n",
    "            \n",
    "        span = get_i2b2_text_from_span(fname, int(row['start']), int(row['end']), int(row['line']), row['text'])\n",
    "        df.loc[df['id'] == row[\"id\"], ['offset_start']] = span['start']\n",
    "        df.loc[df['id'] == row[\"id\"], ['offset_end']] = span['end']\n",
    "        \n",
    "    df.to_csv('/Users/gms/development/nlp/nlpie/data/amia-2019/output/i2b2_reference_offset.csv')\n",
    "    df.to_sql('i2b2_reference_offset', engine, if_exists=\"append\")\n",
    "\n",
    "    #print(df)\n",
    "\n",
    "    elapsed = (time.time() - start)\n",
    "    print(\"elapse:\", elapsed)\n",
    "\n",
    "#add_span_i2b2_ref()\n",
    "\"\"\"removing double space annotation for case 0427, due to it blowing up:\n",
    "c=\"mild symmetric left ventricular  hypertrophy\" 70:2 70:6||t=\"problem\"\n",
    "\n",
    "and 0445\n",
    "c=\"a distended 1.6 cm  non mobile stone in the neck\" 78:4 78:13||t=\"problem\"\n",
    "may fix later\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up files in all corpora\n",
    "def clean_up_notes():\n",
    "    import re, os, glob, path\n",
    "    import regex\n",
    "\n",
    "    #directory_to_parse = '/Users/gms/development/nlp/nlpie/data/amia-2019/i2b2/quarantine/'\n",
    "\n",
    "    #for fname in glob.iglob(\"/Users/gms/development/nlp/nlpie/data/amia-2019/i2b2/quarantine/period2/*.txt\"):\n",
    "\n",
    "    directory_to_parse = '/Users/gms/development/nlp/nlpie/data/amia-2019/i2b2/rerun_post_validation/quarantine/'\n",
    "\n",
    "    for fname in glob.iglob(\"/Users/gms/development/nlp/nlpie/data/amia-2019/i2b2/rerun_post_validation/quarantine/*.txt\"):\n",
    "\n",
    "        # get filename and use for processed output filename\n",
    "        t = os.path.basename(fname)\n",
    "        u = t.split('.')[0] + '-v1.txt'\n",
    "\n",
    "        with open(fname) as f:\n",
    "            #with open('/Users/gms/development/nlp/nlpie/data/amia-2019/i2b2/rerun_post_validation/nonprintables/' + u, 'w') as f2:\n",
    "\n",
    "                # read file\n",
    "                #f1 = f.read()\n",
    "                for line in f:\n",
    "                    # for mipacq:\n",
    "                    \"\"\"\n",
    "                    ggrep --color='auto' -Pn -- '\\d+/\\d+-'\n",
    "                    ggrep --color='auto' -Pn -- '\\s+\\.+\\s+' \n",
    "                    \"\"\"\n",
    "                    regex_a = r\"\\d+/\\d+-\" # e.g. \"45/53-\" -> replace \"-\" with \" \"\n",
    "                    regex_b = r\"[^\\x00-\\x7F]\" # # i.e. unicode -> replace characters with closest ascii\n",
    "                    #regex_c = r\"\\s+\\.+\\s+\"  # e.g. \" . \" -> replace \".\" with \" \"\n",
    "                    #regex_d = r\"^\\.+\" # i.e. period alone at the beginning of a line -> replace \".\" with \" \"a\n",
    "                    #regex_e = r\"[^[:alpha:]]\" # i.e. control codes -> delete matched characters\n",
    "\n",
    "                    # find string and replace '-' if exists\n",
    "\n",
    "                    a = re.compile(regex_a)\n",
    "                    if a.search(line):\n",
    "                        match = a.findall(line)\n",
    "                        l = len(match)\n",
    "                        for i in range(len(match)):\n",
    "                            l2 = len(match[i])\n",
    "                            t = ''\n",
    "                            for j in range(l2):\n",
    "                                t += '9'\n",
    "                            #line = line.replace(match[i], match[i].replace(match[i], t))\n",
    "                            #if i == len(match) - 1 and len(match) == 2:\n",
    "                            #    print(\"match:\", i, match, f1)\n",
    "\n",
    "                    # replace if exists in file object             \n",
    "                    b = re.search(regex_b, line)\n",
    "\n",
    "                    #c = re.search(regex_c, f1)\n",
    "                    #d = re.search(regex_d, line)\n",
    "                    #e = regex.search(regex_e, line)\n",
    "\n",
    "                    if b:\n",
    "                        print(b)\n",
    "                        #print('b:', f1, re.sub('[^\\x00-\\x7F]','', f1))\n",
    "                    #line = re.sub('[^\\x00-\\x7F]','', line)\n",
    "                    #if c:\n",
    "                        #print('c:', f1, re.sub('\\s+\\.+\\s+',' ', f1))\n",
    "                    #    f1 = re.sub('\\s+\\.+\\s+',' ', f1)\n",
    "                    #if d:\n",
    "                        #print('d:', f1, re.sub('^\\.+', ' ', f1))\n",
    "                    #    f1 = re.sub('^\\.+', ' ', f1)\n",
    "                    if e:\n",
    "                        print(e)\n",
    "                        #print('e:', f1, re.sub('[^[:alpha:]]', '', f1))\n",
    "                    #line = re.sub('[^[:alpha:]]', '', line)\n",
    "\n",
    "\n",
    "                    # write to new file \n",
    "                    #if a or b or e:\n",
    "                    #f2.write(line) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
