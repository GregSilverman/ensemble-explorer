{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation class for UIMA systems\n",
    "class AnnotationSystems(object):\n",
    "    \"\"\"\n",
    "    CAS XMI Annotations of interest\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\" \n",
    "        annotation base types\n",
    "        \"\"\"\n",
    "        \n",
    "        self.biomedicus_dir = \"biomedicus_out/\"\n",
    "        self.biomedicus_types = [\"biomedicus.v2.UmlsConcept\",\n",
    "                                 \"biomedicus.v2.Negated\"]#,\n",
    "                                 #\"biomedicus.v2.Acronym\",\n",
    "                                 #\"biomedicus.v2.TemporalPhrase\"]\n",
    "        \n",
    "        \n",
    "        self.clamp_dir = \"clamp_out/\"\n",
    "        #self.clamp_dir = \"Annotated_XMI/\"\n",
    "        self.clamp_types = [\"edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA\",\n",
    "                            \"edu.uth.clamp.nlp.typesystem.ClampRelationUIMA\"]\n",
    "        \n",
    "        \n",
    "        self.ctakes_dir = \"ctakes_out/\"\n",
    "        self.ctakes_types = [\"org.apache.ctakes.typesystem.type.textsem.DiseaseDisorderMention\",\n",
    "                             \"org.apache.ctakes.typesystem.type.textsem.MedicationMention\",\n",
    "                             \"org.apache.ctakes.typesystem.type.textsem.ProcedureMention\",\n",
    "                             \"org.apache.ctakes.typesystem.type.textsem.SignSymptomMention\",\n",
    "                             \"org.apache.ctakes.typesystem.type.textsem.AnatomicalSiteMention\"]#,\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.DateAnnotation\",\n",
    "                             #\"org.apache.ctakes.typesystem.type.textsem.MeasurementAnnotation\"]\n",
    "       \n",
    "        self.metamap_dir = \"metamap_out/\"\n",
    "        self.metamap_types = [\"org.metamap.uima.ts.Candidate\",\n",
    "                              #\"org.metamap.uima.ts.CuiConcept\",\n",
    "                              \"org.metamap.uima.ts.Negation\"]\n",
    "                \n",
    "       \n",
    "    def get_system_type(self, system):\n",
    "        \n",
    "        \"\"\"\n",
    "        return system types\n",
    "        \"\"\"\n",
    "        \n",
    "        if system == \"biomedicus\":\n",
    "            view = \"Analysis\"\n",
    "            \n",
    "        else:\n",
    "            view = \"_InitialView\"\n",
    "\n",
    "        if system == 'biomedicus':\n",
    "            types = self.biomedicus_types\n",
    "            output = self.biomedicus_dir\n",
    "\n",
    "        elif system == 'clamp':\n",
    "            types = self.clamp_types\n",
    "            output = self.clamp_dir\n",
    "\n",
    "        elif system == 'ctakes':\n",
    "            types = self.ctakes_types\n",
    "            output = self.ctakes_dir\n",
    "\n",
    "        elif system == 'metamap':\n",
    "            types = self.metamap_types\n",
    "            output = self.metamap_dir\n",
    "            \n",
    "        return types, view, output\n",
    "    \n",
    "annSys = AnnotationSystems()\n",
    "\n",
    "# extract attributes from cas Annotation object\n",
    "def get_attribs(v):\n",
    "    attribs = []\n",
    "    for sentence in v:\n",
    "        #print(sentence)\n",
    "        for s in sentence.__dir__():\n",
    "            if '__' not in s:\n",
    "                if s not in attribs:\n",
    "                    #print(s)\n",
    "                    attribs.append(s)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    return attribs\n",
    "\n",
    "def get_cols_to_keep(system, t):\n",
    "    \n",
    "    if system == 'biomedicus':\n",
    "        \n",
    "        if 'Temporal' in t:\n",
    "            cols_to_keep = ['begin', 'end', 'system', 'note_id', 'corpus',\n",
    "                            'filename', 'type']\n",
    "            \n",
    "        # umlsconcept\n",
    "        if 'Umls' in t:\n",
    "            cols_to_keep = ['begin', 'confidence', 'cui', 'end', 'tui',\n",
    "                            'type', 'system', 'note_id', 'corpus',\n",
    "                            'filename']\n",
    "\n",
    "        # acronym\n",
    "        if 'Acronym' in t:\n",
    "            cols_to_keep = ['begin', 'end', 'score', 'text', 'type',\n",
    "                            'system', 'note_id', 'corpus', 'filename']\n",
    "\n",
    "        # negated\n",
    "        if 'Negated' in t:\n",
    "            cols_to_keep = ['begin', 'cueTerms', 'end', 'type', \n",
    "                           'system', 'note_id', 'corpus', 'filename']\n",
    "        \n",
    "    elif system == 'clamp':\n",
    "        #NE uima\n",
    "        if 'NameEntityUIMA' in t:\n",
    "            cols_to_keep = ['assertion', 'attribute', 'begin',\n",
    "                           'cui', 'end', 'semanticTag', 'type', \n",
    "                           'system', 'note_id', 'corpus', 'filename', 'umlsCuiDesc', \n",
    "                            'concept_prob', 'sentence_prob']\n",
    "\n",
    "        # relation uima: TODO -> deal with named tuples!\n",
    "        if 'Relation' in t:\n",
    "#             cols_to_keep = ['attr1', 'attr2', 'attr3', 'attr4', 'attribute', 'cui', 'entFrom', 'entTo',\n",
    "#                            'semanticTag', 'type', 'xmiID', 'system', 'note_id',\n",
    "#                            'corpus', 'filename']\n",
    "            cols_to_keep = ['begin', 'end', 'cui', 'umlsCuiDesc', 'semanticTag', 'type', 'system', 'note_id',\n",
    "                           'corpus', 'filename', 'src']\n",
    "        \n",
    "    elif system == 'ctakes':\n",
    "        # disease\n",
    "        if 'Disease' in t:\n",
    "            cols_to_keep = ['begin', 'conditional', 'confidence', 'end', 'historyOf', \n",
    "                           'ontologyConceptArr', 'polarity', 'type', 'typeID', 'uncertainty',\n",
    "                           'system', 'note_id', 'corpus', 'filename', 'cui', 'tui', 'preferredText']\n",
    "\n",
    "        # medication\n",
    "        if 'Medication' in t:\n",
    "            cols_to_keep = ['begin', 'conditional', 'confidence', 'end',\n",
    "                           'historyOf', 'ontologyConceptArr', 'polarity',\n",
    "                           'type', 'uncertainty', 'system',\n",
    "                           'note_id', 'corpus', 'filename', 'cui', 'tui', 'preferredText']\n",
    "\n",
    "        # proc\n",
    "        if 'Procedure' in t:\n",
    "            cols_to_keep = ['begin', 'conditional', 'confidence', 'end',\n",
    "                           'historyOf', 'ontologyConceptArr', 'polarity',\n",
    "                           'type', 'uncertainty', 'system',\n",
    "                           'note_id', 'corpus', 'filename', 'cui', 'tui', 'preferredText']\n",
    "\n",
    "        # sign\n",
    "        if 'SignSymptom' in t:\n",
    "            cols_to_keep = ['begin', 'conditional', 'confidence', 'end',\n",
    "                           'historyOf', 'ontologyConceptArr', 'polarity',\n",
    "                           'type', 'uncertainty', 'system',\n",
    "                           'note_id', 'corpus', 'filename', 'cui', 'tui', 'preferredText']\n",
    "\n",
    "        # anatomy\n",
    "        if 'Anatomical' in t:\n",
    "            cols_to_keep = ['begin', 'conditional', 'confidence', 'end',\n",
    "                           'historyOf', 'ontologyConceptArr', 'polarity',\n",
    "                           'type', 'uncertainty', 'system',\n",
    "                           'note_id', 'corpus', 'filename', 'cui', 'tui', 'preferredText']\n",
    "            \n",
    "        if 'Date' in t:\n",
    "            cols_to_keep = ['begin', 'end', 'type', 'confidence', 'historyOf',  \n",
    "                           'polarity', 'uncertainty', 'corpus', 'note_id', 'filename', 'system']\n",
    "        \n",
    "        if 'Measurement' in t:\n",
    "            cols_to_keep = ['begin', 'end', 'type', 'confidence', 'historyOf',  \n",
    "                           'polarity', 'uncertainty', 'corpus', 'note_id', 'filename', 'system']\n",
    "        \n",
    "    elif system == 'metamap':\n",
    "        #candidate\n",
    "        if 'Candidate' in t:\n",
    "            cols_to_keep = ['begin', 'concept', 'cui', 'end', 'matchedwords',\n",
    "                           'preferred', 'score', 'semanticTypes', 'type', 'system', 'note_id',\n",
    "                           'corpus', 'filename']\n",
    "\n",
    "        #cuiconcept\n",
    "        if 'CuiConcept' in t:\n",
    "            cols_to_keep = ['id', 'negExConcept', 'negExCui', 'type', 'xmiID',\n",
    "                           'system', 'note_id', 'corpus', 'filename']\n",
    "\n",
    "        #negation\n",
    "        if 'Negation' in t:\n",
    "            cols_to_keep = ['begin', 'cuiConcepts', 'end', 'cui', 'cuiConcept',\n",
    "                           'ncSpans', 'negTrigger', 'negType',\n",
    "                           'type', 'system', 'note_id', 'corpus', 'filename']\n",
    "    \n",
    "    else:\n",
    "        cols_to_keep = []\n",
    "\n",
    "    return cols_to_keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cassis(system, typesystem):\n",
    "   \n",
    "    #tic=timeit.default_timer() \n",
    "\n",
    "    print(system)\n",
    "\n",
    "    # types for metamap\n",
    "\n",
    "    if system == 'metamap':\n",
    "        t = typesystem.create_type(name='org.apache.uima.examples.SourceDocumentInformation', supertypeName='uima.tcas.Annotation')\n",
    "        typesystem.add_feature(t, name='uri', rangeTypeName='uima.cas.String')\n",
    "        typesystem.add_feature(t, name=\"offsetInSource\", rangeTypeName=\"uima.cas.Integer\")\n",
    "        typesystem.add_feature(t, name=\"documentSize\", rangeTypeName=\"uima.cas.Integer\")\n",
    "        typesystem.add_feature(t, name=\"lastSegment\", rangeTypeName=\"uima.cas.Integer\")\n",
    "\n",
    "    # features for ctakes\n",
    "\n",
    "    if system == 'ctakes':\n",
    "        t = typesystem.get_type('org.apache.ctakes.typesystem.type.structured.Metadata')\n",
    "        typesystem.add_feature(t, name='patientIdentifier', rangeTypeName='uima.cas.String')\n",
    "        \n",
    "        t = typesystem.get_type('org.apache.ctakes.typesystem.type.textsem.DiseaseDisorderMention')\n",
    "        typesystem.add_feature(t, name='id', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='ontologyConceptArr', rangeTypeName='uima.cas.FSArray', elementType='org.apache.ctakes.typesystem.type.refsem.UmlsConcept')\n",
    "        typesystem.add_feature(t, name='subject', rangeTypeName='uima.cas.String')\n",
    "        #typesystem.add_feature(t, name='typeID', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='discoveryTechnique', rangeTypeName='uima.cas.Integer')\n",
    "        #typesystem.add_feature(t, name='confidence', rangeTypeName='uima.cas.Double')\n",
    "        typesystem.add_feature(t, name='polarity', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='uncertainty', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='conditional', rangeTypeName='uima.cas.Boolean')\n",
    "        typesystem.add_feature(t, name='generic', rangeTypeName='uima.cas.Boolean')\n",
    "        typesystem.add_feature(t, name='historyOf', rangeTypeName='uima.cas.Integer')\n",
    "\n",
    "        t = typesystem.get_type('org.apache.ctakes.typesystem.type.textsem.SignSymptomMention')\n",
    "        typesystem.add_feature(t, name='id', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='ontologyConceptArr', rangeTypeName='uima.cas.FSArray', elementType='org.apache.ctakes.typesystem.type.refsem.UmlsConcept')\n",
    "        typesystem.add_feature(t, name='subject', rangeTypeName='uima.cas.String')\n",
    "        #typesystem.add_feature(t, name='typeID', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='discoveryTechnique', rangeTypeName='uima.cas.Integer')\n",
    "        #typesystem.add_feature(t, name='confidence', rangeTypeName='uima.cas.Double')\n",
    "        typesystem.add_feature(t, name='polarity', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='uncertainty', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='conditional', rangeTypeName='uima.cas.Boolean')\n",
    "        typesystem.add_feature(t, name='generic', rangeTypeName='uima.cas.Boolean')\n",
    "        typesystem.add_feature(t, name='historyOf', rangeTypeName='uima.cas.Integer')\n",
    "\n",
    "        t = typesystem.get_type('org.apache.ctakes.typesystem.type.textsem.MedicationMention')\n",
    "        typesystem.add_feature(t, name='id', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='ontologyConceptArr', rangeTypeName='uima.cas.FSArray', elementType='org.apache.ctakes.typesystem.type.refsem.UmlsConcept')\n",
    "        typesystem.add_feature(t, name='subject', rangeTypeName='uima.cas.String')\n",
    "        #typesystem.add_feature(t, name='typeID', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='discoveryTechnique', rangeTypeName='uima.cas.Integer')\n",
    "        #typesystem.add_feature(t, name='confidence', rangeTypeName='uima.cas.Double')\n",
    "        typesystem.add_feature(t, name='polarity', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='uncertainty', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='conditional', rangeTypeName='uima.cas.Boolean')\n",
    "        typesystem.add_feature(t, name='generic', rangeTypeName='uima.cas.Boolean')\n",
    "        typesystem.add_feature(t, name='historyOf', rangeTypeName='uima.cas.Integer')\n",
    "\n",
    "        t = typesystem.get_type('org.apache.ctakes.typesystem.type.textsem.ProcedureMention')\n",
    "        typesystem.add_feature(t, name='id', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='subject', rangeTypeName='uima.cas.String')\n",
    "        #typesystem.add_feature(t, name='typeID', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='discoveryTechnique', rangeTypeName='uima.cas.Integer')\n",
    "        #typesystem.add_feature(t, name='confidence', rangeTypeName='uima.cas.Double')\n",
    "        typesystem.add_feature(t, name='polarity', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='uncertainty', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='conditional', rangeTypeName='uima.cas.Boolean')\n",
    "        typesystem.add_feature(t, name='generic', rangeTypeName='uima.cas.Boolean')\n",
    "        typesystem.add_feature(t, name='historyOf', rangeTypeName='uima.cas.Integer')\n",
    "\n",
    "        t = typesystem.get_type('org.apache.ctakes.typesystem.type.textsem.AnatomicalSiteMention')\n",
    "        typesystem.add_feature(t, name='id', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='ontologyConceptArr', rangeTypeName='uima.cas.FSArray', elementType='org.apache.ctakes.typesystem.type.refsem.UmlsConcept')\n",
    "        typesystem.add_feature(t, name='subject', rangeTypeName='uima.cas.String')\n",
    "        #typesystem.add_feature(t, name='typeID', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='discoveryTechnique', rangeTypeName='uima.cas.Integer')\n",
    "        #typesystem.add_feature(t, name='confidence', rangeTypeName='uima.cas.Double')\n",
    "        typesystem.add_feature(t, name='polarity', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='uncertainty', rangeTypeName='uima.cas.Integer')\n",
    "        typesystem.add_feature(t, name='conditional', rangeTypeName='uima.cas.Boolean')\n",
    "        typesystem.add_feature(t, name='generic', rangeTypeName='uima.cas.Boolean')\n",
    "        typesystem.add_feature(t, name='historyOf', rangeTypeName='uima.cas.Integer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sofa -> db\n",
    "def write_sofa(u):\n",
    "    d = {}\n",
    "    d[\"note_id\"] = str(u)\n",
    "    d[\"sofa\"] = view.sofa_string\n",
    "    d[\"corpus\"] = corpus\n",
    "\n",
    "    # does it exist?\n",
    "    if engine.dialect.has_table(engine, \"sofas\"):\n",
    "        sql = text(\"SELECT * FROM test.sofas WHERE note_id = :e1\")\n",
    "        resp = engine.execute(sql, e1=u).fetchall()\n",
    "    else:\n",
    "        resp = []\n",
    "\n",
    "    if len(resp) == 0:            \n",
    "        #pd.DataFrame(d, index=[0]).to_sql(\"sofas\", engine, if_exists=\"append\")  \n",
    "        pd.DataFrame(d, index=[0]).to_sql(\"sofas\", engine, if_exists=\"append\")  \n",
    "        \n",
    "\n",
    "def get_dict(keys, sentence, d):\n",
    "    for i in range(len(keys)):\n",
    "        key = keys[i]\n",
    "        val = sentence.__getattribute__(keys[i])\n",
    "        d[key] = val\n",
    "    return d\n",
    "\n",
    "def add_keys(df, system, t, u, corpus, fname):\n",
    "    df[\"system\"] = system\n",
    "    df[\"type\"] = t\n",
    "    df[\"note_id\"] = u\n",
    "    df[\"corpus\"] = corpus\n",
    "    df[\"filename\"] = fname\n",
    "    \n",
    "    return df\n",
    "\n",
    "def append_to_df(df, d):\n",
    "    import pandas as pd\n",
    "    frames = [ df, pd.DataFrame(d, index=[0]) ]\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS: fairview\n",
      "SYSTEM: metamap\n",
      "metamap\n",
      "/Users/gms/development/nlp/nlpie/data/ensembling-u01/fairview/system_annotations/data_in_preprocessed/metamap_out/\n",
      "0002204202\n",
      "TYPE: org.metamap.uima.ts.Candidate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:39: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE: org.metamap.uima.ts.Negation\n",
      "0000513005\n",
      "TYPE: org.metamap.uima.ts.Candidate\n",
      "TYPE: org.metamap.uima.ts.Negation\n",
      "0000200926\n",
      "TYPE: org.metamap.uima.ts.Candidate\n",
      "TYPE: org.metamap.uima.ts.Negation\n",
      "0000202738\n",
      "TYPE: org.metamap.uima.ts.Candidate\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "# parse system annotations\n",
    "def main():\n",
    "    import os, glob\n",
    "    import pymysql\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    from sqlalchemy.engine import create_engine\n",
    "    from sqlalchemy.sql import text\n",
    "    from cassis import load_typesystem, load_cas_from_xmi\n",
    "\n",
    "    # connection string\n",
    "    engine = create_engine('mysql+pymysql://gms:nej123@localhost/mhealth', pool_pre_ping=True)\n",
    "    systems = [\"metamap\", \"ctakes\", \"biomedicus\", \"clamp\"]\n",
    "    #systems = ['ctakes']\n",
    "    \n",
    "    corpora = [\"fairview\"]\n",
    "    parse_to_sql = True\n",
    "    \n",
    "    if parse_to_sql:\n",
    "        \n",
    "        for corpus in corpora:\n",
    "            print(\"CORPUS:\", corpus)\n",
    "\n",
    "            for system in systems:\n",
    "\n",
    "                print(\"SYSTEM:\", system)\n",
    "\n",
    "                types, view_, output = annSys.get_system_type(system)\n",
    "                \n",
    "                dir_test = '/Users/gms/development/nlp/nlpie/scripts/jupyter/ensembling/system_annotations/typesystems/' + system + '/'\n",
    "\n",
    "                with open(dir_test + 'TypeSystem.xml', 'rb') as f:\n",
    "                    typesystem = load_typesystem(f)\n",
    "                \n",
    "                init_cassis(system, typesystem)\n",
    "                \n",
    "                # parse directory\n",
    "                if corpus in [\"i2b2\", \"mipacq\"]:\n",
    "                    directory_to_parse = '/Users/gms/development/nlp/nlpie/data/ensembling-u01/' + corpus + '/rerun-november-2019/' +  system + '_out/' #data_to_analyze/all/'\n",
    "                \n",
    "                elif corpus == 'fairview':\n",
    "                    directory_to_parse = '/Users/gms/development/nlp/nlpie/data/ensembling-u01/' + corpus + '/system_annotations/data_in_preprocessed/' +  system + '_out/'#data_to_analyze/all/'\n",
    "               \n",
    "                elif 'clinical_trial' in corpus:\n",
    "                    # if parsing gold standard, change to_sql below to ensure proper target directory\n",
    "                    #directory_to_parse = '/Users/gms/development/nlp/nlpie/data/clinical_trial2/' +  system + '_out/'\n",
    "                    #directory_to_parse = '/Users/gms/development/nlp/nlpie/data/clinical_trial2/gold_standard/'\n",
    "                    directory_to_parse = '/Users/gms/development/nlp/nlpie/data/clinical_trial2/bert-output/'\n",
    "                else:\n",
    "                    directory_to_parse = '/Users/gms/development/nlp/nlpie/data/mimic/'\n",
    "                    \n",
    "                print(directory_to_parse)\n",
    "\n",
    "                for fname in glob.glob(directory_to_parse + '/*.xmi'):\n",
    "                #for fname in glob.glob(directory_to_parse + '/527982345.txt.xmi'):\n",
    "\n",
    "                    file = os.path.basename(fname)\n",
    "                    u = file.split('.')[0]\n",
    "\n",
    "                    print(u)\n",
    "\n",
    "                    # load cas\n",
    "                    with open(directory_to_parse + file, 'rb') as f:\n",
    "                        cas = load_cas_from_xmi(f, typesystem=typesystem)\n",
    "\n",
    "                    # load view\n",
    "                    view = cas.get_view(view_)\n",
    "                    \n",
    "                    # write sofa object here:\n",
    "                    #write_sofa(u)\n",
    "\n",
    "                    for t in types:\n",
    "                        print(\"TYPE:\", t)\n",
    "                       \n",
    "                        # get list for filtering df\n",
    "                        cols_to_keep = get_cols_to_keep(system, t)\n",
    "                        \n",
    "                        attribs = get_attribs(view.select(t))\n",
    "                        x = t.split('.')\n",
    "                        table_name = system[0:3] + '_' + x[0] + '_' + x[len(x)-1]\n",
    "\n",
    "                        # Annotation object -> dataframe\n",
    "                        def get_df(v, attribs):\n",
    "                            df = pd.DataFrame()\n",
    "                            d = dict() \n",
    "\n",
    "                            # only parse if type exists in file\n",
    "                            if view.select(t):\n",
    "                                for sentence in view.select(t):\n",
    "                                    \n",
    "                                    dd = dict()\n",
    "                                    if t == 'org.metamap.uima.ts.Negation':\n",
    "                                        keys = ['cuiConcepts', 'ncSpans', 'negTrigger', 'negType']\n",
    "                                        d = get_dict(keys, sentence, d)\n",
    "\n",
    "                                        if len(d['cuiConcepts']) == len(d['ncSpans']):\n",
    "                                             for x in range(len(d['cuiConcepts'])):\n",
    "                                                dd['cui'] = d['cuiConcepts'][x].negExCui \n",
    "                                                dd['concept'] = d['cuiConcepts'][x].negExConcept \n",
    "                                                dd['begin'] = d['ncSpans'][x].begin \n",
    "                                                dd['end'] = d['ncSpans'][x].end\n",
    "                                                dd['negTrigger'] = d['negTrigger']\n",
    "                                                dd['negType'] = d['negType']\n",
    "                                                dd['cuiConcepts'] = ' '.join(map(str,d['cuiConcepts']))\n",
    "                                                dd['ncSpans'] = ' '.join(map(str,d['ncSpans']))\n",
    "                                            \n",
    "                                                df = append_to_df(df, dd)\n",
    "                                                df = add_keys(df, system, t, u, corpus, fname)\n",
    "                                            \n",
    "                                        elif len(d['cuiConcepts']) > len(d['ncSpans']) and len(d['ncSpans']) == 1:\n",
    "                                            for x in range(len(d['cuiConcepts'])):\n",
    "                                                dd['cui'] = d['cuiConcepts'][x].negExCui \n",
    "                                                dd['concept'] = d['cuiConcepts'][x].negExConcept\n",
    "                                                dd['begin'] = d['ncSpans'][0].begin\n",
    "                                                dd['end'] = d['ncSpans'][0].end\n",
    "                                                dd['negTrigger'] = d['negTrigger']\n",
    "                                                dd['negType'] = d['negType']\n",
    "                                                dd['cuiConcepts'] = ' '.join(map(str,d['cuiConcepts']))\n",
    "                                                dd['ncSpans'] = ' '.join(map(str,d['ncSpans']))\n",
    "                                                \n",
    "                                                df = append_to_df(df, dd)\n",
    "                                                df = add_keys(df, system, t, u, corpus, fname)\n",
    "                                            \n",
    "                                                \n",
    "                                        elif len(d['cuiConcepts']) < len(d['ncSpans']) and len(d['cuiConcepts']) == 1:\n",
    "                                            for x in range(len(d['ncSpans'])):\n",
    "                                                dd['cui'] = d['cuiConcepts'][0].negExCui \n",
    "                                                dd['concept'] = d['cuiConcepts'][0].negExConcept\n",
    "                                                dd['begin'] = d['ncSpans'][x].begin\n",
    "                                                dd['end'] = d['ncSpans'][x].end\n",
    "                                                dd['negTrigger'] = d['negTrigger']\n",
    "                                                dd['negType'] = d['negType']\n",
    "                                                dd['cuiConcepts'] = ' '.join(map(str,d['cuiConcepts']))\n",
    "                                                dd['ncSpans'] = ' '.join(map(str,d['ncSpans']))\n",
    "                                                \n",
    "                                                df = append_to_df(df, dd)\n",
    "                                                df = add_keys(df, system, t, u, corpus, fname)\n",
    "                                                \n",
    "                                        else:\n",
    "                                            print('Error: MetaMap Negation', d)\n",
    "                                    \n",
    "                                    elif t == 'edu.uth.clamp.nlp.typesystem.ClampRelationUIMA':\n",
    "                                        keys = ['semanticTag', 'entTo', 'entFrom']\n",
    "                                        d = get_dict(keys, sentence, d)\n",
    "                                        \n",
    "                                        if d['semanticTag'] == 'NEG_Of':\n",
    "                                            #print(d['entTo'], '\\n', d['entFrom'])\n",
    "                                            attribute =  json.loads(d['entFrom'].attribute)\n",
    "                                            if 'umlsCuiDesc' in attribute:\n",
    "                                                dd['cui'] = d['entFrom'].cui.split()[0] \n",
    "                                                dd['concept'] = attribute['umlsCuiDesc']\n",
    "                                                dd['begin'] = d['entFrom'].begin\n",
    "                                                dd['end'] = d['entFrom'].end\n",
    "                                                dd['semanticTag'] = d['semanticTag']\n",
    "                                                dd['entTo'] = str(d['entTo'])\n",
    "                                                dd['entFrom'] = str(d['entFrom'])\n",
    "                                                \n",
    "                                                df = append_to_df(df, dd)\n",
    "                                                df = add_keys(df, system, t, u, corpus, fname)\n",
    "                                        \n",
    "                                    elif t == 'biomedicus.v2.Negated':\n",
    "                                        keys = ['begin', 'end']\n",
    "                                        d = get_dict(keys, sentence, d)\n",
    "                                        df = append_to_df(df, d)\n",
    "                                        df = add_keys(df, system, t, u, corpus, fname)\n",
    "                                        \n",
    "                                    elif t == 'biomedicus.v2.UmlsConcept':\n",
    "                                        keys = ['begin', 'end', 'cui', 'tui', 'confidence']\n",
    "                                        d = get_dict(keys, sentence, d)\n",
    "                                        \n",
    "                                        df = append_to_df(df, d)\n",
    "                                        df = add_keys(df, system, t, u, corpus, fname)\n",
    "                                    \n",
    "                                    # no drugs -> no concept description; no labvalues -> no UMLS info\n",
    "                                    elif t == 'edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA':\n",
    "                                        keys = ['assertion', 'cui', 'begin', 'end', 'attribute', 'semanticTag']\n",
    "                                        d = get_dict(keys, sentence, d)\n",
    "                                        \n",
    "                                        if 'attribute' in d and d['attribute']:\n",
    "                                            attribute =  json.loads(d['attribute'])\n",
    "                                            #if d['assertion'] == 'present':\n",
    "                                            if 'umlsCuiDesc' in attribute:\n",
    "                                                dd['concept'] = attribute['umlsCuiDesc']\n",
    "                                                dd['sentence_prob'] = attribute['sentence_prob'] \n",
    "                                                dd['concept_prob'] = attribute['concept_prob']\n",
    "                                                dd['assertion'] = d['assertion']\n",
    "                                                dd['cui'] = d['cui'].split()[0] \n",
    "                                                dd['begin'] = d['begin']\n",
    "                                                dd['end'] = d['end']\n",
    "                                                dd['semanticTag'] = d['semanticTag']\n",
    "                                                dd['attribute'] = str(d['attribute'])\n",
    "\n",
    "                                                df = append_to_df(df, dd)\n",
    "                                                df = add_keys(df, system, t, u, corpus, fname)\n",
    "                                                    \n",
    "                                    elif 'Mention' in t:\n",
    "                                        keys = ['begin', 'conditional', 'confidence', 'end', 'historyOf', 'ontologyConceptArr', 'polarity', \n",
    "                                                'uncertainty']\n",
    "                                        d = get_dict(keys, sentence, d)\n",
    "                                        \n",
    "                                        for x in range(len(d['ontologyConceptArr'])):\n",
    "                                                dd['cui'] = d['ontologyConceptArr'][x].cui\n",
    "                                                dd['tui'] = d['ontologyConceptArr'][x].tui\n",
    "                                                dd['concept'] = d['ontologyConceptArr'][x].preferredText\n",
    "                                                dd['ontologyConceptArr'] = ' '.join(map(str,d['ontologyConceptArr']))\n",
    "                                                dd['begin'] = d['begin']\n",
    "                                                dd['end'] = d['end']\n",
    "                                                dd['conditional'] = d['conditional']\n",
    "                                                dd['confidence'] = d['confidence']\n",
    "                                                dd['historyOf'] = d['historyOf']\n",
    "                                                dd['polarity'] = d['polarity']\n",
    "                                                dd['uncertainty'] = d['uncertainty']                    \n",
    "                                                                    \n",
    "                                                df = append_to_df(df, dd)\n",
    "                                                df = add_keys(df, system, t, u, corpus, fname)\n",
    "                                                \n",
    "                                    elif t == 'org.metamap.uima.ts.Candidate':\n",
    "                                        keys =  ['begin', 'concept', 'cui', 'end',\n",
    "                                                 'preferred', 'score', 'semanticTypes', 'matchedwords']\n",
    "                                        d = get_dict(keys, sentence, d)\n",
    "                                        d['semanticTypes'] = \" \".join(d['semanticTypes'])\n",
    "                                        d['matchedwords'] = \" \".join(d['matchedwords'])\n",
    "                                        \n",
    "                                        df = append_to_df(df, d)\n",
    "                                        df = add_keys(df, system, t, u, corpus, fname)\n",
    "                            \n",
    "                            return df\n",
    "                            \n",
    "                        annotations = get_df(view.select(t), attribs)\n",
    "                        \n",
    "                        # write to database\n",
    "                        #annotations = annotations[cols_to_keep]\n",
    "                        #annotations.to_csv('~/Desktop/test.csv')\n",
    "                        annotations.to_sql(table_name, engine, if_exists=\"append\") \n",
    "                        \n",
    "    # write out annotations for non-cui tables\n",
    "    else:\n",
    "        \n",
    "        sys_ann_other = pd.DataFrame()\n",
    "        for system in systems:\n",
    "                \n",
    "            types, view_, output = annSys.get_system_type(system)\n",
    "            print(\"SYSTEM:\", system)\n",
    "           \n",
    "            for t in types:\n",
    "\n",
    "                x = t.split('.')\n",
    "                table_name = system[0:3] + '_' + x[0] + '_' + x[len(x)-1]\n",
    "\n",
    "                sql = \"SELECT * FROM test.\" + table_nam \n",
    "                df = pd.read_sql(sql, engine)\n",
    "\n",
    "                cols_to_keep = ['begin', 'end', 'type', 'system', 'note_id', 'corpus', 'filename']\n",
    "                #print(system, t, table_name, list(df[cols_to_keep].columns.values))\n",
    "                \n",
    "                frames = [ sys_ann_other, df[cols_to_keep] ]\n",
    "                sys_ann_other = pd.concat(frames, ignore_index=True)\n",
    "        \n",
    "        print(sys_ann_other.drop_duplicates())\n",
    "        sys_ann_other.drop_duplicates().to_csv('/Users/gms/development/nlp/nlpie/data/amia-2019/output/analytical_' + corpus + '.csv')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    %prun main()\n",
    "    print('done!')\n",
    "    pass\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-45af1014282c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m '''\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# pull 100 cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#rnd = df.apply(lambda x: x.sample(n=100)).uuid.copy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# write out new data text\n",
    "sql = '''\n",
    "select sofa, note_id from clinical_trial.sofas\n",
    "where corpus='clinical_trial2'\n",
    "\n",
    "'''\n",
    "df = pd.read_sql(sql, engine)\n",
    "# pull 100 cases \n",
    "#rnd = df.apply(lambda x: x.sample(n=100)).uuid.copy()\n",
    "#print(rnd.dropna())\n",
    "\n",
    "#sample = rnd.dropna().to_frame().merge(df, how='inner')\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "dir_to_write = '/Users/gms/development/nlp/nlpie/data/clinical_trial2/data_in/'\n",
    "for row in df.itertuples():\n",
    "    print(row.note_id)\n",
    "    f = open(dir_to_write + str(row.note_id) +'.txt', 'w')\n",
    "    f.write(row.sofa)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate cTAKES TUIs\n",
    "import os, glob\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "import json\n",
    "#from superjson import json\n",
    "\n",
    "from sqlalchemy.engine import create_engine\n",
    "from sqlalchemy.sql import text\n",
    "from cassis import load_typesystem, load_cas_from_xmi\n",
    "\n",
    "\n",
    "engine = create_engine('mysql+pymysql://gms:nej123@localhost/concepts', pool_pre_ping=True)\n",
    "\n",
    "df = pd.read_sql('SELECT ontologyConceptArr, semtypes from concepts.ctakes_concept',con=engine)\n",
    "df['tui'] = df['ontologyConceptArr'].str.extract(pat = 'tui=(.[^,]+)').apply(lambda s:s.str.replace(\"'\", \"\")) \n",
    "\n",
    "df[['tui', 'semtypes']].drop_duplicates().to_sql(\"ctakes_tui_semtype\", engine, if_exists=\"replace\")\n",
    "print('Fini!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tui consistency check wrt semantic groups\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy.engine import create_engine\n",
    "sql = '''SELECT tui,\n",
    "CASE\n",
    "    WHEN semtypes in ('DiseaseDisorderMention','SignSymptomMention') \n",
    "    THEN concat ('DiseaseDisorderMention,SignSymptomMention')\n",
    "    ELSE semtypes\n",
    "END as semtypes\n",
    "FROM concepts.ctakes_tui_semtype;'''\n",
    "\n",
    "engine = create_engine('mysql+pymysql://gms:nej123@localhost/concepts', pool_pre_ping=True)\n",
    "\n",
    "new = pd.read_sql(sql, con=engine)\n",
    "\n",
    "sql = 'select distinct tui, ctakes_name from semantic_groups where ctakes_name is not null'\n",
    "old = pd.read_sql(sql, con=engine)\n",
    "\n",
    "test = pd.merge(old, new, how = 'right', left_on=['tui','ctakes_name'], right_on = ['tui','semtypes'], indicator=True) \n",
    "test = test[test[\"_merge\"] != \"both\"]\n",
    "print((test))\n",
    "\n",
    "test = pd.merge(old, new, how = 'left', left_on=['tui','ctakes_name'], right_on = ['tui','semtypes'], indicator=True) \n",
    "test = test[test[\"_merge\"] != \"both\"]\n",
    "print((test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tables for cTAKES UMLS concept -> mention table\n",
    "# https://stackoverflow.com/questions/12680754/split-explode-pandas-dataframe-string-entry-to-separate-rows\n",
    "\n",
    "def mk_ctakes_concepts(sql, table_name):\n",
    "    import pymysql\n",
    "    import pandas as pd\n",
    "    from sqlalchemy.engine import create_engine\n",
    "\n",
    "    engine = create_engine('mysql+pymysql://gms:nej123@localhost/test', pool_pre_ping=True)\n",
    "\n",
    "    df = pd.read_sql(sql, engine)\n",
    "\n",
    "    # MetaMap\n",
    "    b = pd.DataFrame(df.cuiConcepts.str.split(' ').tolist(), index=df.cuiConcepts).stack()\n",
    "    b = b.reset_index()[[0, 'cuiConcepts']] # var1 variable is currently labeled 0\n",
    "    b.columns = ['linked_id', 'cuiConcepts'] # renaming var1\n",
    "    \n",
    "    # CTAKES:\n",
    "     \n",
    "    #b = pd.DataFrame(df.ontologyConceptArr.str.split(' ').tolist(), index=df.ontologyConceptArr).stack()\n",
    "    #b = b.reset_index()[[0, 'ontologyConceptArr']] # var1 variable is currently labeled 0\n",
    "    #b.columns = ['linked_id', 'ontologyConceptArr'] # renaming var1\n",
    "    \n",
    "\n",
    "    c = pd.merge(df, b)\n",
    "   \n",
    "    c.to_sql(table_name, engine, if_exists=\"replace\") \n",
    "\n",
    "    print(c[0:2])\n",
    "\n",
    "''' TABLES to make\n",
    "sql = \"SELECT * FROM test.met_org_negation\"\n",
    "table_name = 'mm_negation'\n",
    "\n",
    "mk_ctakes_concepts(sql, table_name)\n",
    "\n",
    "'''\n",
    "'''\n",
    "sql = \"SELECT * FROM test.cta_disease\"\n",
    "table_name = 'cTAKES_disease'\n",
    "\n",
    "mk_ctakes_concepts(sql, table_name)\n",
    "\n",
    "sql = \"SELECT * FROM test.cta_med\"\n",
    "table_name = 'cTAKES_medication'\n",
    "\n",
    "mk_ctakes_concepts(sql, table_name)\n",
    "\n",
    "\n",
    "sql = \"SELECT * FROM test.cta_proc\"\n",
    "table_name = 'cTAKES_procedure'\n",
    "mk_ctakes_concepts(sql, table_name)\n",
    "\n",
    "\n",
    "sql = \"SELECT * FROM test.cta_sign_symptom\"\n",
    "table_name = 'cTAKES_sign_symptom'\n",
    "\n",
    "mk_ctakes_concepts(sql, table_name)\n",
    "\n",
    "sql = \"SELECT * FROM test.cta_anatomical_site\"\n",
    "table_name = 'cTAKES_anatomical_site'\n",
    "\n",
    "mk_ctakes_concepts(sql, table_name)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix XMI, adding \"id\" for those with linked features\n",
    "\n",
    "def add_attribute_sys_type(regexpatterns):\n",
    "    import re, os, glob, path\n",
    "    import regex\n",
    "\n",
    "    #directory_to_parse = '/Users/gms/development/nlp/nlpie/data/amia-2019/i2b2/quarantine/'\n",
    "\n",
    "    #for fname in glob.iglob(\"/Users/gms/development/nlp/nlpie/data/amia-2019/i2b2/quarantine/period2/*.txt\"):\n",
    "\n",
    "    for fname in glob.iglob(\"/Users/gms/development/nlp/nlpie/data/amia-2019/analysis/mipacq/rerun_post_validation/metamap_out/*.txt.xmi\"):\n",
    "\n",
    "        # get filename and use for processed output filename\n",
    "        t = os.path.basename(fname)\n",
    "        cd = os.path.dirname(fname)\n",
    "        u = t.split('.')[0] + '-v2.txt.xmi'\n",
    "        #print(t, cd)\n",
    "\n",
    "        #print(t)\n",
    "        with open(fname) as f:\n",
    "            with open(cd + '/' + u, 'w') as f2:\n",
    "                for line in f:\n",
    "                    for r in regexpatterns:\n",
    "                        line = re.sub(r, r + ' id=\"0\"', line)\n",
    "                    f2.write(line)\n",
    "    \n",
    "\n",
    "#regexpatterns = [r\"refsem:UmlsConcept\"]\n",
    "#regexpatterns = [r\"ts2:CuiConcept\"]\n",
    "#regexpatterns = [r\"ts2:Negation\"]\n",
    "\n",
    "#add_attribute_sys_type(regexpatterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dkpro-cassis\n",
    "from cassis import *\n",
    "def test_dkpro(fname, dir_test, ts_test, view_name):\n",
    "    with open(ts_test + 'TypeSystem.xml', 'rb') as f:\n",
    "        typesystem = load_typesystem(f)\n",
    "    init_cassis('biomedicus', typesystem)\n",
    "    \n",
    "    with open(dir_test + fname, 'rb') as f:\n",
    "        cas = load_cas_from_xmi(f, typesystem=typesystem)\n",
    "    view = cas.get_view(view_name)\n",
    "    #print([x for x in view.select_all()])\n",
    "    print([x for x in view.select(\"biomedicus.v2.TemporalPhrase\")]) \n",
    "    #print([x for x in view.select(\"biomedicus.v2.UmlsConcept\")]) \n",
    "    #print([x for x in view.select(\"org.apache.ctakes.typesystem.type.refsem.UmlsConcept\")]) \n",
    "    \n",
    "    #print([x for x in view.select(\"org.apache.ctakes.typesystem.type.textsem.DateAnnotation\")])\n",
    "    #print([x for x in view.select(\"org.metamap.uima.ts.Candidate\")])\n",
    "    \n",
    "    #print([x for x in view.select(\"edu.uth.clamp.nlp.typesystem.ClampNameEntityUIMA\")])\n",
    "\n",
    "dir_test = '/Users/gms/development/nlp/nlpie/data/clinical_trial/biomedicus_out/'\n",
    "#view_name = \"_InitialView\"\n",
    "view_name = \"Analysis\"\n",
    "ts_test = \"/Users/gms/development/nlp/nlpie/scripts/jupyter/ensembling/system_annotations/typesystems/biomedicus/\"\n",
    "fname = 'M2.txt.xmi'\n",
    "                \n",
    "                \n",
    "test_dkpro(fname, dir_test, ts_test, view_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tables for\n",
    "# https://stackoverflow.com/questions/12680754/split-explode-pandas-dataframe-string-entry-to-separate-rows\n",
    "\n",
    "def get_analytical_set(sql):\n",
    "    import pymysql\n",
    "    import pandas as pd\n",
    "    from sqlalchemy.engine import create_engine\n",
    "\n",
    "    engine = create_engine('mysql+pymysql://gms:nej123@localhost/test', pool_pre_ping=True)\n",
    "\n",
    "    df = pd.read_sql(sql, engine)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#GENERATE analytical tables\n",
    "'''\n",
    "analytical_cui = pd.DataFrame()\n",
    "\n",
    "sql = \"SELECT * FROM test.biomedicus_cui where corpus = 'mipacq'\"\n",
    "\n",
    "print(get_analytical_set(sql)[0:1])\n",
    "\n",
    "frames = [ analytical_cui, get_analytical_set(sql) ]\n",
    "analytical_cui = pd.concat(frames, ignore_index=True, sort=False) \n",
    "\n",
    "sql = \"SELECT * FROM test.clamp_all_cui where corpus = 'mipacq'\"\n",
    "\n",
    "print(get_analytical_set(sql)[0:1])\n",
    "\n",
    "frames = [ analytical_cui, get_analytical_set(sql) ]\n",
    "analytical_cui = pd.concat(frames, ignore_index=True, sort=False) \n",
    "\n",
    "sql = \"SELECT * FROM test.ctakes_all_types_cui where corpus = 'mipacq'\"\n",
    "\n",
    "print(get_analytical_set(sql)[0:1])\n",
    "\n",
    "frames = [ analytical_cui, get_analytical_set(sql) ]\n",
    "analytical_cui = pd.concat(frames, ignore_index=True, sort=False) \n",
    "\n",
    "sql = \"SELECT * FROM test.metamap_all_cui where corpus = 'mipacq'\"\n",
    "\n",
    "print(get_analytical_set(sql)[0:1])\n",
    "frames = [ analytical_cui, get_analytical_set(sql) ]\n",
    "analytical_cui = pd.concat(frames, ignore_index=True, sort=False) \n",
    "\n",
    "print(analytical_cui[0:1])\n",
    "analytical_cui.to_csv('/Users/gms/development/nlp/nlpie/data/amia-2019/output/analytical_cui_mipacq_concepts.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up cui list in clamp\n",
    "#df['stridx']=df.index\n",
    "\n",
    "\n",
    "#print(df[df['cui'].str.contains(',')==True])\n",
    "\n",
    "#df['new_cui' ] = np.where(df.cui.str.contains(','), df['cui'].str.split(r'\\s*,\\s*|\\s*\\.\\s*').str[0], df['cui'])\n",
    "\n",
    "#print(df['cui'])\n",
    "#print(df[df['cui'].str.contains(',')==False])\n",
    "\n",
    "#new = df.rename(columns={'cui': 'old_cui', 'new_cui': 'cui'}).copy() \n",
    "#new.to_csv('/Users/gms/development/nlp/nlpie/data/amia-2019/output/analytical_cui_mipacq_concepts_new.csv')\n",
    "#writer = pd.ExcelWriter('/Users/gms/development/nlp/nlpie/data/amia-2019/output/ensemble/merged_metrics.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
