{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up section headers in mipacq text\n",
    "\n",
    "import re, os, glob, path\n",
    "\n",
    "def clean_files():\n",
    "    directory_to_parse = '/Users/gms/development/nlp/nlpie/data/amia-2019/mipacq/quarantine/'\n",
    "    \n",
    "\n",
    "    os.chdir(directory_to_parse)\n",
    "\n",
    "    regex = r\"\\[(.*?)\\]\"\n",
    "\n",
    "    for fname in glob.glob(directory_to_parse + '*.source'):\n",
    "\n",
    "        # get filename and use for processed output filename\n",
    "        t = os.path.basename(fname)\n",
    "        u = t.split('.')[0] + '.txt'\n",
    "\n",
    "        with open(fname) as f:\n",
    "                with open(directory_to_parse + u, 'w') as f2:\n",
    "                    for line in f:\n",
    "                        a = re.compile(regex)\n",
    "                        if a.search(line):\n",
    "                            match = a.findall(line)\n",
    "                            l = len(match)\n",
    "                            for i in range(len(match)):\n",
    "                                l2 = len(match[i])\n",
    "                                t = ''\n",
    "                                for j in range(l2):\n",
    "                                    t += '^'\n",
    "                                line = line.replace(line, t) + '\\n'\n",
    "                        f2.write(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Admission Date :\n",
      "2015-04-16\n",
      "Discharge Date :\n",
      "2015-04-24\n",
      "Date of Birth :\n",
      "1937-04-06\n",
      "Sex :\n",
      "F\n",
      "Service :\n",
      "SURGERY\n",
      "Allergies :\n",
      "Patient recorded as having No Known Allergies to Drugs\n",
      "Attending : Mattie R. A. Hamilton , M.D.\n",
      "Chief Complaint :\n",
      "Altered mental status\n",
      "Diaphoresis\n",
      "Abdominal pain\n",
      "Major Surgical or Invasive Procedure :\n",
      "Lap converted to open cholecystectomy\n",
      "History of Present Illness :\n",
      "This 78 year old Mexican speaking female presented to the emergency department on 2015-04-16 .\n",
      "Her family memebers stated that she was found at home with altered mental status and diaphoresis .\n",
      "She is diabetic .\n",
      "Mental status improved after binasal cannula oxygen applied , at which time she localized right upper quadrant pain .\n",
      "She developed leukocytosis and transaminasemia .\n",
      "Past Medical History :\n",
      "DM on insulin\n",
      "HTN\n",
      "gerd\n",
      "pvd\n",
      "osteo-arthritis\n",
      "osteoporosis\n",
      "anemia\n",
      "cholelithiasis\n",
      "left humeral fracture\n",
      "b/l cataracts s/p surgery\n",
      "s/p uterine myomectomy\n",
      "Social History :\n",
      "distant tobacco\n",
      "drinks often - including vodka\n",
      "walks without cane\n",
      "trained engineer in Papua New Guinea\n",
      "Has brother , nephew and daughter in law near by .\n",
      "Family History :\n",
      "denies cad\n",
      "Physical Exam :\n",
      "T : 99.2 HR 97 BP 131/68 RR : 20 Spo2 100% on RA\n",
      "Constitutional : alert & oriented x 3 .\n",
      "Head / eyes : EOMI , PERRL\n",
      "Chest / respiratory : clear to auscultation bilaterally\n",
      "Cardiovascular : Regular rate & rhythm + S1/S2 .\n",
      "No Mumur / regurgiation / gallop\n",
      "GI / Abdomen : soft .\n",
      "+ Haskell sign , right upper quadrant tenderness .\n",
      "Nondistended .\n",
      "No rebounding\n",
      "Musculoskeletal : 5/5 strength all extremities\n",
      "Skin : no C/C/E\n",
      "Neuro : CN II-XII intact , no dysdiadokinesis .\n",
      "decreased Babinski 's\n",
      "Pertinent Results :\n",
      "2015-04-16 07:10 PM BLOOD Albumin - 3.8 Calcium - 9.0 Phos - 2.6 *# Mg - 1.9\n",
      "2015-04-16 07:10 PM BLOOD ALT - 295 * AST - 56 * CK(CPK) - 60 AlkPhos - 208 * Amylase - 23 TotBili - 1.6 *\n",
      "2015-04-16 07:10 PM BLOOD Glucose - 280 * UreaN - 18 Creat - 1.0 Na - 135 K - 4.2 Cl - 98 HCO3 - 22 AnGap - 19\n",
      "2015-04-16 07:10 PM BLOOD WBC - 18.4 *# RBC - 4.27 Hgb - 11.9 * Hct - 34.8 * MCV - 82 MCH - 27.9 MCHC - 34.1 RDW - 15.0 Plt Ct - 317\n",
      "2015-04-24 06:35 AM BLOOD ALT - 82 * AST - 62 * AlkPhos - 145 * Amylase - 51 TotBili - 0.4\n",
      "2015-04-24 06:35 AM BLOOD Glucose - 134 * UreaN - 8 Creat - 0.6 Na - 144 K - 3.7 Cl - 106 HCO3 - 31 AnGap - 11\n",
      "2015-04-24 06:35 AM BLOOD WBC - 10.5 RBC - 3.49 * Hgb - 9.6 * Hct - 28.0 * MCV - 80 * MCH - 27.5 MCHC - 34.3 RDW - 15.8 * Plt Ct - 504 *\n",
      "GB US 2015-04-16\n",
      "IMPRESSION :\n",
      "1. Distended gallbladder with impacted 1.6 cm gallstone , focal wall thickening , and positive sonAllan Stephens sign - findings consistent with acute cholecystitis .\n",
      "2. Right renal cysts - lower pole cyst is unchanged from prior exam of 2010-10-25 and upper pole exophytic cyst was not previously seen sonStacy .\n",
      "ERCP report 2015-04-21\n",
      "IMPRESSION :\n",
      "1. Successful removal of CBD stone and placement of plastic biliary stent for confirmed cystic duct leak .\n",
      "Brief Hospital Course :\n",
      "Ms Malek was admitted on 2015-04-16 due to altered mental status and acute right upper quadrant abdominal pain .\n",
      "Gallbladder ultra sound revealed a distended 1.6 cm  non mobile stone in the neck .\n",
      "HD # 01-20 She was monitored closely in the CMED .\n",
      "On 2015-04-17 she was taken to the OR for lap converted to open cholecystectomy .\n",
      "She tolerated the procedure well , see op report for details .\n",
      "She was extubated and recovered well in PACU .\n",
      "She remained NPO with IV fluids , foley catheter and Dilaudid IV for pain control , Unasyn for antibiotic coverage .\n",
      "She returned to CMED for further monitoring .\n",
      "She was noted to have low urine output at times .\n",
      "She responded well to fluid bolusing .\n",
      "POD#1 she was transferred to CC6 for further recovery .\n",
      "She remained afebrile , she was given ice chips .\n",
      "Physical therapy was consulted for strength and mobility .\n",
      "POD#2 her pain was somewhat uncontrolled , she was placed on Dilaudid PCA with fair effect .\n",
      "Urine output remained adequate .\n",
      "She was monitored on telemetry for mild tachycardia and recieved IV beta blockers .\n",
      "She ambulated with assistance .\n",
      "POD#3 she was advanced to sips and clears , foley catheter was discontinued .\n",
      "POD#4 , bilious drainage was noted in her JP .\n",
      "She had worsening abdominal pain on exam .\n",
      "She was held NPO .\n",
      "She was taken to ERCP where sphincterotomy was performed and biliary stent was placed .\n",
      "She tolerated the procedure well and returned to CC6 post-procedure .\n",
      "JP remained intact with serosainguinous drainage .\n",
      "POD#5 she c/o difficulty voiding , pt was straight cathed after bladder scan was obtained and revealed > 600 ccs urine .\n",
      "POD# 05-25 she continued with intermittent complaints of urinary retention .\n",
      "However she was able to void .\n",
      "Renal function remained normal .\n",
      "She did not require further straight catheterization .\n",
      "She was advanced to clear liquids again without nausea or vomiting .\n",
      "Her pain was well controlled by Tylenol .\n",
      "Her home regimen of lantus was resumed for elevated blood glucose .\n",
      "POD#6 , her diet was advanced to regular .\n",
      "She required disimpaction and had hard stool in the rectum .\n",
      "She was initiated on a bowel regimen and had no further incidents of diarrhea or constipation .\n",
      "POD#7 she was discharged to rehab in stable condition .\n",
      "Appropriate follow up appointments are recommended as well as prescriptions .\n",
      "She should return in 6 weeks for removal of biliary stent .\n",
      "Medications on Admission :\n",
      "Tramadol 50 mg Tablet Sig : Two ( 2 ) Tablet PO TID ( 3 times a day) .\n",
      "2. Senna 8.6 mg Tablet Sig : One ( 1 ) Tablet PO BID ( 2 times a day ).\n",
      "3. Nortriptyline 25 mg Capsule Sig : One ( 1 ) Capsule PO BID ( 2 times a day ).\n",
      "4. Atenolol 25 mg Tablet Sig : One ( 1 ) Tablet PO DAILY ( Daily ).\n",
      "5. Buspirone 10 mg Tablet Sig : Two ( 2 ) Tablet PO BID ( 2 times a day ).\n",
      "6. Clonazepam 0.5 mg Tablet Sig : One ( 1 ) Tablet PO BID ( 2 times a day ).\n",
      "7. Cholecalciferol ( Vitamin D3 ) 400 unit Tablet Sig : One ( 1 ) Tablet PO DAILY ( Daily ).\n",
      "8. Pantoprazole 40 mg Tablet , Delayed Release ( E.C. ) Sig : One ( 1 ) Tablet , Delayed Release ( E.C. ) PO Q24H ( every 24 hours ).\n",
      "9. Docusate Sodium 50 mg / 5 mL Liquid Sig : Ten ( 10 ) mL PO BID ( 2 times a day ).\n",
      "10. Hexavitamin Tablet Sig : One ( 1 ) Cap PO DAILY ( Daily ).\n",
      "11. Dicyclomine 10 mg Capsule Sig : Two ( 2 ) Capsule PO BID ( 2 times a day ).\n",
      "12. Atorvastatin 10 mg Tablet Sig : One ( 1 ) Tablet PO DAILY ( Daily ).\n",
      "15. Bisacodyl 5 mg Tablet , Delayed Release ( E.C. ) Sig : Two ( 2 ) Tablet , Delayed Release ( E.C. ) PO BID ( 2 times a day ) as needed .\n",
      "16. Acetaminophen 325 mg Tablet Sig : 1-2 Tablets PO Q4-6H ( every 4 to 6 hours ) as needed for pain .\n",
      "17. Ibuprofen 600 mg Tablet Sig : One ( 1 ) Tablet PO Q8H ( every 8 hours ) as needed for pain .\n",
      "18. Insulin sliding scale Insulin SC Sliding Scale Q6H Regular Glucose Insulin Dose 0-60 mg / dL 01-20 amp D50 61-120 mg / dL 0 Units 121-140 mg / dL 3 Units 141-160 mg/dL 5 Units 161-180 mg/dL 7 \n",
      "Units 181-200 mg / dL 9 Units 201-220 mg / dL 11 Units 221-240 mg / dL 13 Units 241-260 mg / dL 15 Units 261-280 mg / dL 17 Units 281-300 mg / dL 19 Units 301-320 mg / dL 21 Units > 320 mg / dL Notify M.D.\n",
      "19. Lantus 30 units Lantus insulin with breakfast\n",
      "Discharge Medications :\n",
      "1. Tramadol 50 mg Tablet Sig : Two ( 2 ) Tablet PO TID ( 3 times a day ).\n",
      "2. Senna 8.6 mg Tablet Sig : One ( 1 ) Tablet PO BID ( 2 times a day ).\n",
      "3. Nortriptyline 25 mg Capsule Sig : One ( 1 ) Capsule PO BID ( 2 times a day ).\n",
      "4. Atenolol 25 mg Tablet Sig : One ( 1 ) Tablet PO DAILY ( Daily ).\n",
      "5. Buspirone 10 mg Tablet Sig : Two ( 2 ) Tablet PO BID ( 2 times a day ).\n",
      "6. Clonazepam 0.5 mg Tablet Sig : One ( 1 ) Tablet PO BID ( 2 times a day ).\n",
      "7. Cholecalciferol ( Vitamin D3 ) 400 unit Tablet Sig : One ( 1 ) Tablet PO DAILY ( Daily ).\n",
      "8. Pantoprazole 40 mg Tablet , Delayed Release ( E.C. ) Sig : One ( 1 ) Tablet , Delayed Release ( E.C. ) PO Q24H ( every 24 hours ).\n",
      "9. Docusate Sodium 50 mg / 5 mL Liquid Sig : Ten ( 10 ) mL PO BID ( 2 times a day ).\n",
      "10. Hexavitamin Tablet Sig : One ( 1 ) Cap PO DAILY ( Daily ).\n",
      "11. Dicyclomine 10 mg Capsule Sig : Two ( 2 ) Capsule PO BID ( 2 times a day ).\n",
      "12. Atorvastatin 10 mg Tablet Sig : One ( 1 ) Tablet PO DAILY ( Daily ).\n",
      "13. Amoxicillin-Pot Clavulanate 875-125 mg Tablet Sig : One ( 1 ) Tablet PO BID ( 2 times a day ) for 4 days .\n",
      "14. Heparin ( Porcine ) 5,000 unit / mL Solution Sig : One ( 1 ) injection Injection BID ( 2 times a day ).\n",
      "15. Bisacodyl 5 mg Tablet , Delayed Release ( E.C. ) Sig : Two ( 2 ) Tablet , Delayed Release ( E.C. ) PO BID ( 2 times a day ) as needed .\n",
      "16. Acetaminophen 325 mg Tablet Sig : 1-2 Tablets PO Q4-6H ( every 4 to 6 hours ) as needed for pain .\n",
      "17. Ibuprofen 600 mg Tablet Sig : One ( 1 ) Tablet PO Q8H ( every 8 hours ) as needed for pain .\n",
      "18. Insulin sliding scale\n",
      "Insulin SC Sliding Scale\n",
      "Q6H\n",
      "Regular\n",
      "Glucose Insulin Dose\n",
      "0-60 mg / dL 01-20 amp D50\n",
      "61-120 mg / dL 0 Units\n",
      "121-140 mg / dL 3 Units\n",
      "141-160 mg / dL 5 Units\n",
      "161-180 mg / dL 7 Units\n",
      "181-200 mg / dL 9 Units\n",
      "201-220 mg / dL 11 Units\n",
      "221-240 mg / dL 13 Units\n",
      "241-260 mg / dL 15 Units\n",
      "261-280 mg / dL 17 Units\n",
      "281-300 mg / dL 19 Units\n",
      "301-320 mg / dL 21 Units\n",
      "> 320 mg / dL Notify M.D.\n",
      "30 units Lantus insulin with breakfast\n",
      "Discharge Disposition :\n",
      "Extended Care\n",
      "Facility :\n",
      "New England Sinai Hospital & Rehab Center - Everett\n",
      "Discharge Diagnosis :\n",
      "Acute Cholecystitis\n",
      "Gangrenous cholecystitis with perforation\n",
      "Discharge Condition :\n",
      "good\n",
      "Discharge Instructions :\n",
      "Please call your surgeon if you develop chest pain , shortness of breath , fever greater than 101.5 , foul smelling or colorful drainage from your incisions , redness or swelling , severe abdominal pain or distention , persistent nausea or vomiting , inability to eat or drink , or any other symptoms which are concerning to you .\n",
      "No tub baths or swimming .\n",
      "You may shower .\n",
      "If there is clear drainage from your incisions , cover with a dry dressing .\n",
      "Leave white strips above your incisions in place , allow them to fall off on their own .\n",
      "Activity : No heavy lifting of items 11-02 pounds until the follow up appointment with your doctor .\n",
      "Medications : Resume your home medications .\n",
      "You should take a stool softener , Colace 100 mg twice daily as needed for constipation .\n",
      "You will be given pain medication which may make you drowsy .\n",
      "No driving while taking pain medicine .\n",
      "Followup Instructions :\n",
      "Provider : Gregory X Wingard , M.D. Phone :( 855 ) 651-4418\n",
      "Date / Time : 2015-06-03 9:30\n",
      "Please call ( 808 ) 246-1373 and schedule an appointment to see Dr. Lewis in 2 weeks .\n",
      "You will be contacted by the gastrointestinal doctors Pat Barrow the removal of your biliary stent in 6 weeks .\n",
      "Joe Kari MD 09-697\n",
      "Completed by : Carol Robert NP 80-CIY 2015-04-24 @ 1506\n",
      "Signed electronically by : DR. Marion SCHNEIDER on : FRI 2015-04-24 5:11 PM\n",
      "( End of Report )\n"
     ]
    }
   ],
   "source": [
    "# get token offset in document\n",
    "\n",
    "def search_tuple(tups, elem):\n",
    "    return filter(lambda tup: elem in tup, tups)\n",
    "\n",
    "def get_token_offset():\n",
    "    import re, os, glob, path\n",
    "    import spacy\n",
    "\n",
    "    #directory_to_parse = '/Users/gms/development/nlp/nlpie/data/amia-2019/analysis/mipacq/data_in/'\n",
    "    #directory_to_parse = '/Users/gms/development/nlp/nlpie/data/amia-2019/analysis/mipacq/data_in/'\n",
    "    directory_to_parse = \"/Users/gms/development/nlp/nlpie/data/amicus-u01/i2b2/source_data/test_data/\"\n",
    "    os.chdir(directory_to_parse)\n",
    "\n",
    "    nlp = spacy.load(\"en\")\n",
    "\n",
    "    for fname in glob.glob(directory_to_parse + '0445.txt'):\n",
    "\n",
    "        # get filename and use for processed output filename\n",
    "        t = os.path.basename(fname)\n",
    "        u = t.split('.')[0] + '.txt'\n",
    "        \n",
    "        # get span of text \n",
    "        with open(directory_to_parse + '0445.txt') as f:\n",
    "            f1 = f.read()\n",
    "            print(f1)\n",
    "            #print(tokens)\n",
    "            doc = nlp(f1)\n",
    "            #print(len(doc))\n",
    "            span = doc[0:len(doc)]\n",
    "            #print(doc.text)\n",
    "            #print([(token.text,token.idx) for token in doc[0:20]])\n",
    "            \n",
    "get_token_offset()\n",
    "\n",
    "# extract phrase from offset\n",
    "def text_offset_lookup(begin, ntokens, fname):\n",
    "    import re, os, glob, path\n",
    "    import spacy\n",
    "\n",
    "    directory_to_parse = \"/Users/gms/development/nlp/nlpie/data/amia-2019/mipacq/source_data/source/\"\n",
    "    os.chdir(directory_to_parse)\n",
    "\n",
    "    nlp = spacy.load(\"en\")\n",
    "\n",
    "    for fname in glob.glob(fname):\n",
    "\n",
    "        # get filename and use for processed output filename\n",
    "        t = os.path.basename(fname)\n",
    "        u = t.split('.')[0] + '.source'\n",
    "        text = \"\"\n",
    "        \n",
    "        # get span of text \n",
    "        with open(fname) as f:\n",
    "            f1 = f.read()\n",
    "            doc = nlp(f1)\n",
    "            tokens = [(token.text,token.idx) for token in doc]\n",
    "            hit = list(search_tuple(tokens, begin))\n",
    "            idx = tokens.index(hit[0])\n",
    "            text = tokens[idx: idx + ntokens]\n",
    "            #print(text)\n",
    "    \n",
    "            return text\n",
    "            \n",
    "#text = \"stated that she\"\n",
    "#n = len(text.split())\n",
    "#begin = 505\n",
    "\n",
    "#get_sentence_from_offset(begin, n, '5024581165-5.source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods for mipacq corpora\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def unnesting(df, explode):\n",
    "    idx=df.index.repeat(df[explode[0]].str.len())\n",
    "    df1=pd.concat([pd.DataFrame({x:np.concatenate(df[x].values)} )for x in explode],axis=1)\n",
    "    df1.index=idx\n",
    "    return df1.join(df.drop(explode,1),how='left')\n",
    "\n",
    "def class_to_dataframe(className, t):\n",
    "    # get dictionary element and flatten into df\n",
    "    if len(className) > 0:\n",
    "        df = pd.io.json.json_normalize(className)\n",
    "        df[\"file\"] = t\n",
    "    else:\n",
    "        df = pd.DataFrame()\n",
    "    return df\n",
    "\n",
    "def stack_span(annotations):\n",
    "    # case where multiple annotations embedded in signle xml item:\n",
    "    d = {}\n",
    "    unstacked = pd.DataFrame()\n",
    "    \n",
    "    # rename to sane names:\n",
    "    \"\"\"Index(['annotator.$', 'annotator.@id', 'creationDate.$', 'mention.@id', 'span',\n",
    "       'span.@end', 'span.@start', 'spannedText.$', 'file'],\n",
    "      dtype='object')\"\"\"\n",
    "    \n",
    "    \n",
    "    if 'span' in annotations.columns:\n",
    "        temp = annotations.rename(columns={'mention.@id': 'mentionId',\n",
    "                                           'annotator.@id': 'annotatorId',\n",
    "                                           'spannedText.$': 'text',\n",
    "                                           'span.@end': 'end',\n",
    "                                           'span.@start': 'start',\n",
    "                                           'annotator.$': 'annotator'}).copy() \n",
    "        \n",
    "        for row in temp.itertuples():\n",
    "            if 'span' in row._fields:\n",
    "                if isinstance(row.span, list):\n",
    "\n",
    "                    for item in row.span:\n",
    "                        d[\"mention.@id\"] = row.mentionId\n",
    "                        d[\"spannedText.$\"] = row.text\n",
    "                        d[\"span.@start\"] = item[\"@start\"]\n",
    "                        d[\"span.@end\"] = item[\"@end\"]\n",
    "                        d[\"annotator.@id\"] = row.annotatorId\n",
    "                        d[\"annotator.$\"] = row.annotator\n",
    "                        d[\"file\"] = row.file\n",
    "                        frames = [ annotations, pd.DataFrame(d, index=[0]) ]\n",
    "                        annotations = pd.concat(frames, ignore_index=True, sort=False)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    return annotations\n",
    "        \n",
    "\n",
    "class ReferenceAnnotations(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 annotations, \n",
    "                 classes, \n",
    "                 complexes, \n",
    "                 strings,\n",
    "                 booleans = None):\n",
    "        \n",
    "        self = self\n",
    "        self.annotations = annotations\n",
    "        self.classes = classes\n",
    "        self.complexes = complexes\n",
    "        self.strings = strings\n",
    "        self.booleans = booleans\n",
    "        \n",
    "    def is_instance_complex_negation(self):\n",
    "\n",
    "        odds_n_sods = pd.DataFrame()\n",
    "\n",
    "        # iterate over ann and class df to determine whether complex: if not complex then is it boolean?\n",
    "        for index, row in self.annotations.iterrows():\n",
    "            for idx, r in self.classes.iterrows():\n",
    "                # match annotation to class\n",
    "                if row[\"mention.@id\"] == r['@id']:\n",
    "\n",
    "                    # should be a list associated with key, but not necessarily\n",
    "                    if 'hasSlotMention' in self.classes:\n",
    "\n",
    "                        # get list of ids:\n",
    "                        ids = self.classes.loc[self.classes[\"@id\"] == (row[\"mention.@id\"]), \n",
    "                                               'hasSlotMention'].values[0]\n",
    "\n",
    "                        # for negation there will only be 2-elements in the list: the first maps to a stringSlotMention, the second to a booleanSlotMention\n",
    "                        # need to test not in complexSlotMention\n",
    "                        # also case for two stringSlotMentions where one is text other is degree of certainty\n",
    "\n",
    "                        # if not list: do nothing!\n",
    "                        if isinstance(ids, list) :\n",
    "                            notComplex = True\n",
    "                            notBoolean = True\n",
    "                            notDegree = True\n",
    "\n",
    "                            val = '' \n",
    "                            for i in ids:\n",
    "\n",
    "                                val += ' ' + i[\"@id\"]\n",
    "                                # these are complex relationships\n",
    "                                if not self.complexes[self.complexes[\"@id\"] == i[\"@id\"]].empty:\n",
    "                                    notComplex = False\n",
    "                                else:\n",
    "                                    if not self.booleans is None:\n",
    "                                        # these are boolean/negations\n",
    "                                        if notComplex and not self.booleans.empty:\n",
    "                                            if not self.booleans[self.booleans[\"@id\"] == i[\"@id\"]].empty:\n",
    "                                                notBoolean = False\n",
    "\n",
    "                                    # these are degree of certaintly\n",
    "                                    if notBoolean and notComplex:\n",
    "                                        if not self.strings[self.strings[\"@id\"] == i[\"@id\"]].empty:\n",
    "                                            notDegree = False\n",
    "\n",
    "                            # write to summary df\n",
    "                            if not notComplex:\n",
    "                                out = self.classes[self.classes[\"@id\"] == (row[\"mention.@id\"])] # complex_df[complex_df[\"@id\"] == t[\"@id\"]].copy()\n",
    "                                if not out.empty:\n",
    "                                    d  = out.to_dict('record')\n",
    "                                    d[0][\"classType\"] = 'complex'\n",
    "                                    d[0][\"linked.ids\"] = val\n",
    "                                    d[0][\"class.id\"] = row[\"mention.@id\"]\n",
    "                                    #frames = [ odds_n_sods, pd.DataFrame(d, index=[0]) ]\n",
    "                                    #odds_n_sods = pd.concat(frames, ignore_index=True, sort=False) \n",
    "\n",
    "                            elif not notBoolean:\n",
    "                                out = self.classes[self.classes[\"@id\"] == (row[\"mention.@id\"])] #test #boolean_df[boolean_df[\"@id\"] == t[\"@id\"]].copy()\n",
    "                                if not out.empty:\n",
    "                                    d  = out.to_dict('record')\n",
    "                                    d[0][\"classType\"] = \"boolean\"\n",
    "                                    d[0][\"linked.ids\"] = val\n",
    "                                    d[0][\"class.id\"] = row[\"mention.@id\"]\n",
    "                                    #frames = [ odds_n_sods, pd.DataFrame(d, index=[0]) ]\n",
    "                                    #odds_n_sods = pd.concat(frames, ignore_index=True, sort=False) \n",
    "\n",
    "                            elif not notDegree:\n",
    "                                out = self.classes[self.classes[\"@id\"] == (row[\"mention.@id\"])] #test # string_df[string_df[\"@id\"] == t[\"@id\"]].copy()\n",
    "                                if not out.empty:\n",
    "                                    d  = out.to_dict('record')\n",
    "                                    d[0][\"classType\"] = \"degree\"\n",
    "                                    d[0][\"linked.ids\"] = val\n",
    "                                    d[0][\"class.id\"] = row[\"mention.@id\"]\n",
    "\n",
    "                            if not out.empty:\n",
    "                                frames = [ odds_n_sods, pd.DataFrame(d, index=[0]) ]\n",
    "                                odds_n_sods = pd.concat(frames, ignore_index=True, sort=False) \n",
    "\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "        #with pd.option_context('display.max_rows', None, 'display.max_columns', None):  \n",
    "        #    print(odds_n_sods)           \n",
    "\n",
    "        return odds_n_sods     \n",
    "\n",
    "    # update string df with linked ids and class type for bool, complex, or degree of certainty\n",
    "    def update_str(self, oas):\n",
    "\n",
    "        for index, row in oas.iterrows():\n",
    "            for i, r in self.iterrows():\n",
    "\n",
    "                test = row[\"linked.ids\"]\n",
    "\n",
    "                if r[\"@id\"] in test:\n",
    "                    \n",
    "                    # df.loc[df.filename == 'test2.dat', 'n'] = df2[df2.filename == 'test2.dat'].loc[0]['n']\n",
    "                    #self['classType'] = self.loc[self['@id'] == r[\"@id\"], ['classType']] = row[\"classType\"]\n",
    "                    #self['linked.ids'] = self.loc[self['@id'] == r[\"@id\"], ['linked.ids']] = row[\"linked.ids\"]\n",
    "                    \n",
    "                    #df.loc[df['item'] == 'apple', 'freshness'] = apple\n",
    "                    self.loc[self['@id'] == r[\"@id\"], ['classType']] = row[\"classType\"]\n",
    "                    #self.loc[self['@id'] == r[\"@id\"], ['linked.ids']] = row[\"linked.ids\"]\n",
    "                    #self.loc[self['@id'] == r[\"@id\"], ['class.id']] = row[\"@id\"]\n",
    "                    #self['corpus'] = 'MiPACQ'\n",
    "\n",
    "        #print('s:', self[[\"classType\", \"linked.ids\"]][(self[\"classType\"]  == 'complex') | (self[\"classType\"]  == 'boolean') | (self[\"classType\"]  == 'degree')] )\n",
    "                    #print(\"out test:\", r[\"@id\"], row[\"linked.ids\"], row[\"classType\"])\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "def parse_mipacq_reference():\n",
    "    # parse mipacq reference standard\n",
    "    import glob, os\n",
    "    import pandas as pd\n",
    "    from xml.etree.ElementTree import fromstring\n",
    "    from xmljson import badgerfish as bf\n",
    "    import json\n",
    "    from json import dumps\n",
    "    from pandas.io.json import json_normalize\n",
    "\n",
    "    from sqlalchemy.engine import create_engine\n",
    "    from sqlalchemy.sql import text\n",
    "\n",
    "    engine = create_engine('mysql+pymysql://gms:nej123@localhost/test')\n",
    "    dir_test = \"/Users/gms/development/nlp/nlpie/data/amicus-u01/mipacq/source_data/reference/\"\n",
    "\n",
    "    good_files = []\n",
    "    no_data = []\n",
    "    processed_files = []\n",
    "    empty_annotations = []\n",
    "    missing_key = []\n",
    "\n",
    "    # TEST CASES -> bad: 60891 good: \n",
    "    #for fname in glob.glob(dir_test + '*.xml'):\n",
    "    # test for degreeness: 0580150767-0\n",
    "    for fname in glob.glob(dir_test + '20132.xml'):\n",
    "\n",
    "        #if fname is not \"2889522952-2.xml\":\n",
    "            # get filename and use for processed output filename\n",
    "        t = os.path.basename(fname)\n",
    "\n",
    "        case = t.replace('-v1','').split('.')\n",
    "        print(t)\n",
    "        with open(dir_test + t) as f:\n",
    "            f1 = f.read()\n",
    "\n",
    "        data = dumps(bf.data(fromstring(f1)))\n",
    "        d = json.loads(data)\n",
    "\n",
    "        ann = d.get(\"annotations\").get(\"annotation\")\n",
    "       \n",
    "        \n",
    "        annSuper = d.get(\"annotations\")\n",
    "        classMention = annSuper.get(\"classMention\")\n",
    "        stringMention = annSuper.get(\"stringSlotMention\")\n",
    "        booleanMention = annSuper.get(\"booleanSlotMention\")\n",
    "        complexMention = annSuper.get(\"complexSlotMention\")\n",
    "        \n",
    "        print(fname, complexMention)\n",
    "        \n",
    "        a\n",
    "        \n",
    "\n",
    "        if ann is None:\n",
    "            empty_annotations.append(t)\n",
    "        else:  \n",
    "            annotations = class_to_dataframe(ann, case[0])\n",
    "            \n",
    "            # unpack list of spans to stack in multiple rows\n",
    "            annotations = stack_span(annotations)\n",
    "\n",
    "            if classMention:\n",
    "                classes = class_to_dataframe(classMention, case[0])\n",
    "            if stringMention:\n",
    "                strings = class_to_dataframe(stringMention, case[0])\n",
    "            if booleanMention:\n",
    "                booleans = class_to_dataframe(booleanMention, case[0])\n",
    "            if complexMention:\n",
    "                complexes = class_to_dataframe(complexMention, case[0])\n",
    "\n",
    "            ann_to_class = pd.merge(annotations, \n",
    "                                    classes, \n",
    "                                    left_on='mention.@id', \n",
    "                                    right_on='@id')\n",
    "\n",
    "            if 'hasSlotMention.@id' in ann_to_class.columns:\n",
    "\n",
    "                # instantiate \n",
    "                if not booleanMention:\n",
    "                    booleans = None\n",
    "                refAnn = ReferenceAnnotations(annotations, \n",
    "                                              classes, \n",
    "                                              complexes, \n",
    "                                              strings, \n",
    "                                              booleans)\n",
    "\n",
    "                oas = ReferenceAnnotations.is_instance_complex_negation(refAnn)\n",
    "\n",
    "\n",
    "                # update string_df with attributes and keys:\n",
    "                strings[\"classType\"] = np.nan\n",
    "\n",
    "                analyticAnn = (strings)\n",
    "\n",
    "                out = ReferenceAnnotations.update_str(analyticAnn, oas)\n",
    "\n",
    "                # this only gives non intersting stuff:\n",
    "                ann_to_string = pd.merge(ann_to_class, \n",
    "                                         #strings,\n",
    "                                         out,\n",
    "                                         left_on='hasSlotMention.@id', \n",
    "                                         right_on='@id')\n",
    "\n",
    "            else:\n",
    "                missing_key.append(t)\n",
    "\n",
    "            cols_to_keep = [\"mention.@id\", \n",
    "                            \"span.@end\", \n",
    "                            \"span.@start\", \n",
    "                            \"spannedText.$\", \n",
    "                            \"hasSlotMention.@id\", \n",
    "                            \"mentionClass.$\", \n",
    "                            \"mentionClass.@id\", \n",
    "                            \"mentionSlot.@id\", \n",
    "                            \"stringSlotMentionValue.@value\", \n",
    "                            \"file\"]\n",
    "\n",
    "            ann_to_string = ann_to_string[cols_to_keep]\n",
    "\n",
    "            processed_files.append(t)\n",
    "\n",
    "            if oas.empty or strings.empty:\n",
    "                empty_annotations.append(t)\n",
    "            else:\n",
    "                if \"hasSlotMention\" in ann_to_class.columns:\n",
    "                    cls = classes[classes['hasSlotMention'].notnull()]\n",
    "                    s=unnesting(cls,['hasSlotMention']).reset_index()      \n",
    "\n",
    "                    # https://stackoverflow.com/questions/35711059/extract-dictionary-value-from-column-in-data-frame\n",
    "                    s['linked.id'] = s.hasSlotMention.apply(pd.Series)\n",
    "\n",
    "                    ann_to_cls = pd.merge(ann_to_class[~ann_to_class[\"span.@start\"].isnull()], \n",
    "                                          s,\n",
    "                                          left_on='@id', \n",
    "                                          right_on='@id')\n",
    "\n",
    "                    mess = pd.merge(ann_to_cls, \n",
    "                                          oas,\n",
    "                                          left_on='@id', \n",
    "                                          right_on='@id')\n",
    "                    \n",
    "                    a_real_mess = pd.merge(mess,\n",
    "                                           strings,\n",
    "                                           left_on=\"linked.id\",\n",
    "                                           right_on=\"@id\")\n",
    "\n",
    "                    if not a_real_mess.empty:\n",
    "\n",
    "                        cols_to_keep = ['mention.@id',  \n",
    "                                        'span.@end', \n",
    "                                        'span.@start', \n",
    "                                        'spannedText.$',  \n",
    "                                        'mentionClass.$_x',\n",
    "                                        'mentionClass.@id_x', \n",
    "                                        'index', \n",
    "                                        'linked.id', \n",
    "                                        'class.id', \n",
    "                                        'mentionClass.$', \n",
    "                                        'mentionClass.@id', \n",
    "                                        'mentionSlot.@id',\n",
    "                                        'stringSlotMentionValue.@value', \n",
    "                                        'file', \n",
    "                                        'classType_y', \n",
    "                                        'linked.ids']\n",
    "\n",
    "                        a_real_mess = a_real_mess[cols_to_keep]\n",
    "\n",
    "                        a_real_mess = a_real_mess.rename(columns={'mentionClass.$_x': 'mentionClass.$',\n",
    "                                                                 #'hasSlotMention_x': 'hasSlotMention',\n",
    "                                                                  'mentionClass.@id_x': 'mentionClass.@id',\n",
    "                                                                  'classType_y': 'classType'})    \n",
    "                        \n",
    "                        #print(a_real_mess.columns, a_real_mess[a_real_mess['span.@start'].isnull()])\n",
    "                        \n",
    "                        a_real_mess = a_real_mess.drop_duplicates()\n",
    "                        a_real_mess.to_sql('mipacq_reference_mixed_only', engine, if_exists=\"append\") \n",
    "                \n",
    "                ann_to_string = ann_to_string.drop_duplicates()\n",
    "                ann_to_string.to_sql('mipacq_reference_strings_only', engine, if_exists=\"append\") \n",
    "                \n",
    "                strings.drop_duplicates()\n",
    "                strings.to_sql('mipacq_strings', engine, if_exists=\"append\")\n",
    "\n",
    "\n",
    "    print(\"n problem files:\", len(missing_key), \"empty ann:\", len(empty_annotations), \"n processed files:\", len(processed_files))\n",
    "    print(\"problem files:\", missing_key, \"empty ann:\", empty_annotations)\n",
    "    \n",
    "parse_mipacq_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up files in all corpora\n",
    "def clean_up_notes():\n",
    "    import re, os, glob, path\n",
    "    import regex\n",
    "\n",
    "    #directory_to_parse = '/Users/gms/development/nlp/nlpie/data/amia-2019/i2b2/quarantine/'\n",
    "\n",
    "    #for fname in glob.iglob(\"/Users/gms/development/nlp/nlpie/data/amia-2019/i2b2/quarantine/period2/*.txt\"):\n",
    "\n",
    "    directory_to_parse = '/Users/gms/development/nlp/nlpie/data/amia-2019/i2b2/rerun_post_validation/quarantine/'\n",
    "\n",
    "    for fname in glob.iglob(\"/Users/gms/development/nlp/nlpie/data/amia-2019/i2b2/rerun_post_validation/quarantine/*.txt\"):\n",
    "\n",
    "        # get filename and use for processed output filename\n",
    "        t = os.path.basename(fname)\n",
    "        u = t.split('.')[0] + '-v1.txt'\n",
    "\n",
    "        with open(fname) as f:\n",
    "            #with open('/Users/gms/development/nlp/nlpie/data/amia-2019/i2b2/rerun_post_validation/nonprintables/' + u, 'w') as f2:\n",
    "\n",
    "                # read file\n",
    "                #f1 = f.read()\n",
    "                for line in f:\n",
    "                    # for mipacq:\n",
    "                    \"\"\"\n",
    "                    ggrep --color='auto' -Pn -- '\\d+/\\d+-'\n",
    "                    ggrep --color='auto' -Pn -- '\\s+\\.+\\s+' \n",
    "                    \"\"\"\n",
    "                    regex_a = r\"\\d+/\\d+-\" # e.g. \"45/53-\" -> replace \"-\" with \" \"\n",
    "                    regex_b = r\"[^\\x00-\\x7F]\" # # i.e. unicode -> replace characters with closest ascii\n",
    "                    #regex_c = r\"\\s+\\.+\\s+\"  # e.g. \" . \" -> replace \".\" with \" \"\n",
    "                    #regex_d = r\"^\\.+\" # i.e. period alone at the beginning of a line -> replace \".\" with \" \"a\n",
    "                    #regex_e = r\"[^[:alpha:]]\" # i.e. control codes -> delete matched characters\n",
    "\n",
    "                    # find string and replace '-' if exists\n",
    "\n",
    "                    a = re.compile(regex_a)\n",
    "                    if a.search(line):\n",
    "                        match = a.findall(line)\n",
    "                        l = len(match)\n",
    "                        for i in range(len(match)):\n",
    "                            l2 = len(match[i])\n",
    "                            t = ''\n",
    "                            for j in range(l2):\n",
    "                                t += '9'\n",
    "                            #line = line.replace(match[i], match[i].replace(match[i], t))\n",
    "                            #if i == len(match) - 1 and len(match) == 2:\n",
    "                            #    print(\"match:\", i, match, f1)\n",
    "\n",
    "                    # replace if exists in file object             \n",
    "                    b = re.search(regex_b, line)\n",
    "\n",
    "                    #c = re.search(regex_c, f1)\n",
    "                    #d = re.search(regex_d, line)\n",
    "                    #e = regex.search(regex_e, line)\n",
    "\n",
    "                    if b:\n",
    "                        print(b)\n",
    "                        #print('b:', f1, re.sub('[^\\x00-\\x7F]','', f1))\n",
    "                    #line = re.sub('[^\\x00-\\x7F]','', line)\n",
    "                    #if c:\n",
    "                        #print('c:', f1, re.sub('\\s+\\.+\\s+',' ', f1))\n",
    "                    #    f1 = re.sub('\\s+\\.+\\s+',' ', f1)\n",
    "                    #if d:\n",
    "                        #print('d:', f1, re.sub('^\\.+', ' ', f1))\n",
    "                    #    f1 = re.sub('^\\.+', ' ', f1)\n",
    "                    if e:\n",
    "                        print(e)\n",
    "                        #print('e:', f1, re.sub('[^[:alpha:]]', '', f1))\n",
    "                    #line = re.sub('[^[:alpha:]]', '', line)\n",
    "\n",
    "\n",
    "                    # write to new file \n",
    "                    #if a or b or e:\n",
    "                    #f2.write(line) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
